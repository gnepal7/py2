"Id","DatasetId","OwnerUserId","CreationDate","Description","ForumId","Title","Subtitle","Deadline","TotalVotes"
"5764","7","7660288","08/16/2021 13:37:33","## Task Details
This task is for scraping the data from a subreddit

## Expected Submission
Method to scrape data from a subreddit

### Further help
If you need additional inspiration, check out the documentation of praw:
https://praw.readthedocs.io/en/stable/","","Scraping data from Reddit","Scraping data from a subreddit","","0"
"125","9","2566546","12/08/2019 20:33:53","## Task Details
Predict who will be the first 4x Grandmaster.
4 Kaggle categories of data science expertise:
-  Competitions
- Datasets
- Kernels
- Discussion

## Expected Submission
Using Kaggle Notebook.
Analysis using Meta Kaggle datasets.
Expected **User Name**.","","Predict 1st. 4x Grandmaster","Who will be the first 4x Grandmaster?","","3"
"797","18","4357186","04/22/2020 05:10:11","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

I am aiming to build a machine learning model around the data for the analysis of the comments in order to say whether a review is positive or not. 
My main aim is to implement whatever i've learned till now and get the best possible result.","","To determine whether a review is positive or negative and build a machine learning model around it .","","","35"
"1732","19","4857935","08/18/2020 05:29:27","## Task Details
Predict the species of the iris flower.

## Expected Submission
 Solve the task primarily using Notebooks .

## Evaluation
A model with higher accuracy score.","","Species Classification","","","95"
"356","21","1727702","01/08/2020 06:07:39","Some of the EDA to be performed on the dataset available

How do plan rates vary across state?
How do plan benefits vary across state?
How did plan rates change from 2014 to 2016?
How do plan benefits relate to plan rates?
How do plan rates vary by age?
How do plan rates vary with additional family?
How do plan rates vary between smokes and non-smokers?
How do plans vary across insurance network providers?
How can you analyze one source of variation in this data while controlling for the others?

Source: Ben Hamner
https://www.kaggle.com/hhs/health-insurance-marketplace/discussion/19023","","Dental Plans EDA","","","1"
"3085","29","1908968","12/31/2020 22:26:28","## Task Details
It is the pre-process data

## Expected Submission
You will have a high quality of data

## Evaluation
clean and readable code","","Process data","","","9"
"5008","43","6332659","07/05/2021 12:34:02","![King's Landing](https://i.pinimg.com/originals/ea/a5/f5/eaa5f59352027bd68eca18e64052da82.jpg)

Show your passion and love for the show or the books.
Make variety of visualization and present your findings.

- Find the House who have lost the most amount of men","","Song of Ice and Fire","","","1"
"711","43","4747851","04/08/2020 10:36:29","j## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","GOT project","","04/10/2020 00:00:00","21"
"1013","55","3757755","05/30/2020 22:48:37","Project Introduction
In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement, or plea deal with the government, or testified in exchange for prosecution immunity.

Resources Needed

You should have python and sklearn running on your computer, as well as the starter code (both python scripts and the Enron dataset) that you downloaded as part of the first mini-project in the Intro to Machine Learning course. The starter code can be found in the final_project directory of the codebase that you downloaded for use with the mini-projects. Some relevant files:

poi_id.py : starter code for the POI identifier, you will write your analysis here

final_project_dataset.pkl : the dataset for the project, more details below

tester.py : when you turn in your analysis for evaluation by a Udacity evaluator, you will submit the algorithm, dataset and list of features that you use (these are created automatically in poi_id.py). The evaluator will then use this code to test your result, to make sure we see performance that‚Äôs similar to what you report. You don‚Äôt need to do anything with this code, but we provide it for transparency and for your reference.

emails_by_address : this directory contains many text files, each of which contains all the messages to or from a particular email address. It is for your reference, if you want to create more advanced features based on the details of the emails dataset.
 
Can be found here:-https://www.kaggle.com/sagarnildass/enron-person-of-interest-dataset

Steps to Success

We will provide you with starter code, that reads in the data, takes your features of choice, then puts them into a numpy array, which is the input form that most sklearn functions assume. Your job is to engineer the features, pick and tune an algorithm, test, and evaluate your identifier. Several of the mini-projects were designed with this final project in mind, so be on the lookout for ways to use the work you‚Äôve already done.

The features in the data fall into three major types, namely financial features, email features and POI labels.

    financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)
    email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‚Äòemail_address‚Äô, which is a text string)
    POI label: [‚Äòpoi‚Äô] (boolean, represented as integer)

You are encouraged to make, transform or rescale new features from the starter features. If you do this, you should store the new feature to my_dataset, and if you use the new feature in the final algorithm, you should also add the feature name to my_feature_list, so your coach can access it during testing. For a concrete example of a new feature that you could add to the dataset, refer to the lesson on Feature Selection.

Project Evaulation Instructions

When you're finished, your project will have 2 parts: the code/classifier you create and some written documentation of your work. Share your project with others and self-evaluate your project according to the rubric here.(https://review.udacity.com/#!/rubrics/27/view)

Before you start working on the project: Review the final project rubric carefully. Think about the following questions - How will you incorporate each of the rubric criterion into your project? Why are these aspects important? What is your strategy to ensure that your project ‚Äúmeets specifications‚Äù in the given criteria? Once you are convinced that you understand each part of the rubric, please start working on your project. Remember to refer to the rubric often to ensure that you are on the right track. 

When making your classifier, you will create three pickle files (my_dataset.pkl, my_classifier.pkl, my_feature_list.pkl). The project evaluator will test these using the tester.py script. You are encouraged to use this script before checking to gauge if your performance is good enough.

Evaluation:- Submission of notebook file in the name format:- username_accuracy.ipynb.
Note:- In the notebook file you will be calling functions or your own code and evaluating and displaying accuracy from the .pkl files.","","Building a person of interest identifier based on email data.","POI Indentifier.","","20"
"176","58","647691","12/10/2019 03:37:01","## Task Details
the visualization of specific Country data 
reference:
https://www.kaggle.com/zanjibar/year-1997-top-20-export-import-graph","","Country Data visualization","","","0"
"98","63","910830","12/08/2019 06:18:46","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","21"
"99","63","910830","12/08/2019 06:19:04","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","28"
"100","63","910830","12/08/2019 06:20:36","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","9"
"101","63","910830","12/08/2019 06:20:54","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","2"
"102","63","910830","12/08/2019 06:21:11","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","5"
"103","63","910830","12/08/2019 06:21:28","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","7"
"104","63","910830","12/08/2019 06:21:44","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","1"
"105","63","910830","12/08/2019 06:22:03","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","3"
"106","63","910830","12/08/2019 06:22:18","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","1"
"2803","63","6139878","11/27/2020 23:13:56","Univariate analysis","","Univariate analysis","","11/30/2020 23:59:00","22"
"166","80","1952004","12/09/2019 19:18:14","## Task Details
This is a very nice dataset with really important health information. Is it possible to include data from years after 2015 so that we can later to a temporal analysis?

## Expected Submission
Dataset with the same format 2015_Air_quality_in_northern_Taiwan.csv for years after 2015 up to the most recent year.","","Add data from the most recent years","","","0"
"506","95","20717","03/04/2020 17:05:26","## Task Details
There might be SKU features like weight/volume/length/hight/width hidden in the text . Also the package suzes for logistics (item, carton, pallet size etc.)

## Expected Submission
Dataset is enlarged by seperate columns for features 
""weight"" / ""volume"" / width/height/length and actual unit of measures","","Feature Extraction from Column ""Description""","","","0"
"2370","100","1914939","10/07/2020 23:28:37","## Task Details
Show step-by-step how to analyze and visualize the dataset to better understand 911 calls and what originates them.

## Expected Submission
A notebook that accomplishes the task.

## Evaluation
During the notebook try to answer the following questions:

* **Which features are available in the dataset?**
* **How many rows and columns does the dataset have?**
* **Which features are categorical?**
* **Which features are numerical?**
* **Which features contain blank, null or empty values?**
* **What are the data types for various features?**
* **How many zip codes does the dataset have?**
* **What are the top 5 zip codes for 911 calls?**
* **What are the top 5 townships (twp) for 911 calls?**
* **How many unique title of emergency codes are there?**
* **What is the most common Reason for a 911 call based off of this new column?**","","Emergency 911 Calls - Exploratory Data Analysis","","","24"
"2635","121","6061208","11/05/2020 23:10:41","## Task Details
With the given data find out:

1. Which pokemon type is the strongest.
2. Which pokemon type should we stay away from?
3. Which pokemon type has the most legendaries
4. which pokemon type don't have legendaries
5. The  strongest legendary and non-legendary pokemon","","Best Pok√©mon type","","11/12/2020 23:59:00","17"
"3198","121","6459396","01/15/2021 06:07:30","Most Strongest Legendaries and non Legendaries
Total Types of Pokemon
Type 1 and Type 2 based arranging
Statistics
Strongest Types of Pokemon
Best Fighting Styles
Plotting","","Basic Data gathering and Plotting","","01/18/2021 23:59:00","6"
"3533","121","5717744","02/18/2021 01:28:36","## Task Details
Find out what is the number of combinations between Type 1 and Type 2, and also find out what are the top 10 highest combinations of Pokemon's types.

## Expected Submission
The solution should be a notebook with a count_values or a plot highlighting the number of combinations and the top 10 most common combinations.

## Evaluation
The exact result should be the solution

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
https://www.kaggle.com/residentmario/summary-functions-and-maps","","How many combinations of Type 1 and Type 2 are there? And what's the most common combination?","","","9"
"4680","128","7618590","06/08/2021 10:54:25","Can someone tell me how to do feature ranking?","","Feature Ranking","","06/15/2021 00:00:00","2"
"955","128","3551877","05/23/2020 18:25:43","This is a pretty intuitive task. We can use several different machine learning models to predict prices using the given features.","","Predicting prices based on other features","Simple/beginner ML task","","51"
"368","129","3859738","01/10/2020 00:29:36","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Daily News for Stock Market Prediction","Daily News for Stock Market Prediction","01/25/2020 00:00:00","10"
"417","138","4242054","01/30/2020 09:47:44","I want try to predict the commercial film success.","","draft_1","","03/01/2020 00:00:00","9"
"2274","138","4396359","09/28/2020 15:44:06","## Task Details
Predict Genres based on the different features in data

## Expected Submission
Predict Genres

## Evaluation
Evaluation metrics is micro f1-score","","Predict Genres based on overview of movie and other features","","","18"
"6262","142","3219554","10/06/2021 04:45:58","## Task Details
The classic saying ""Six degrees of separation"" has been overused, and there may or may not be vbe valid. First, the average Twitter follower count is often higher than Dunbar's number (50 or 150). Secondly, it may be influenced by community clustering.

## Expected Submission
1. Calculate the Average and Standard Deviation of the minimum distance for each user pair, and also include as many centrality measures as possible for each node
  - (preferably) Use Netrankr and iGraph for R
  - (preferably) Use NetworkX and iGraph for Python
2. Drop friend connections randomly until it reaches either 50, 150, or 500 maximum friend connections, and replicate task 1 at least 50 times with different randomization parameter
3. Apply preferential attachment, based on task 1's PageRank (and preferably other centrality measures) to drop friend connections until it reaches either 50, 150, or 500 maximum friend connections, and replicate task 1

## Expected Submission
1. Display the average effect size of task 2 compared to task 1 using Cohen's d
2. Display the effect size of task 1 to each method in task 3 using Cohen's d

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
- https://github.com/happygirlzt/six-degrees-of-separation-in-twitter","","Evaluating Six Degrees of Separation","Because Twitter Connections are Unrealistic (?)","","0"
"420","180","2364471","01/31/2020 03:47:39","When I use normalisation method my accuracy is great for this breasts cancer dataset. Still I wanna increase my accuracy so I used GrideSearchCV and tried but because of it accuracy get decreased.why this is happed","","Increasing in accuracy","","","37"
"299","180","1462452","12/18/2019 07:10:46","## Task Details
This dataset is a good material for ML study. 

## Expected Submission
- Notebook is preferred. 
- The solution should contain the model, the prediction result and the description of the method

## Evaluation
The description is the most important part!","","How to predict the breast caner","","","121"
"4776","180","7683013","06/14/2021 17:11:37","EDA and Prediction","","EDA and Prediction","","06/15/2021 23:59:00","3"
"5053","204","7683681","07/08/2021 21:01:59","Use a ML model to predict if a person will die or not in a shark attack.","","Machine Learning","","","0"
"3571","225","4377547","02/22/2021 07:15:18","## Task Details
In this task, you will need to solve the classification problem (whether a person has a return above 50k or not - 0 or 1). It is necessary to select the most significant features

## Expected Submission
At the output, the algorithm should output 0 - the yield is lower than or equal to 50k, or 1 - the yield is higher than 50k.","","Build a model that predicts whether a person has an income above 50k.","","","8"
"4020","228","7038023","04/09/2021 16:25:38","Diabetes Prediction","","Diabetes Prediction","This Notebook is a submission for a Task on Pima Indians Diabetes Database","04/16/2021 23:59:00","5"
"4192","228","4559219","04/26/2021 13:05:41","Perform exploratory data analysis on the dataset using various statistical methods. Visualize the dataset. Derive the important variables.","","EDA of Pima Indians Diabetes Database","","","2"
"4206","228","7094350","04/27/2021 19:06:56","Write a summary of your understanding of the purpose and contents of this dataset and your assessment of the quality of the data. 

Write a summary (~ 300 words) in word processing document which includes the following:
‚Ä¢	The contents of the above dataset with detailed description.
‚Ä¢	The quality of the data with respect to validity, accuracy, completeness, consistency, and uniformity.
‚Ä¢	Estimate the amount of dirtiness of the data of each type and discuss its potential impact of the goal of the analysis.","","summary of the Database","","","7"
"4689","228","7618590","06/08/2021 18:29:00","EDA for Diabetes","","Diabetes EDA","","","1"
"4690","228","7622857","06/08/2021 20:15:54","Prediction for Diabetes","","Prediction for Diabetes","","","3"
"4882","228","7743712","06/22/2021 09:13:34","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","","Diabetes prediction","","06/26/2021 23:59:00","1"
"763","228","2093650","04/16/2020 17:05:18","i need this data science project","","diabetes prediction","","04/17/2020 00:00:00","171"
"1163","228","3633236","06/17/2020 17:43:24","If yes let me know the model with hyperparameter tuning","","Any better model to get high accuracy","","","40"
"2049","228","5370515","09/11/2020 08:47:16","What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?","","Diabetes predict","","09/25/2020 23:59:00","18"
"461","273","4323346","02/19/2020 10:04:37","## Task Details
I want this information for my University Database management project , so I have to work in postgreSql,and I want real unencoded data to match various attributes to implement sql queries.

## Expected Submission
Encoded table to decode id or original id data.

Thanks in advance.
Deadline is ASAP.","","Information about account_id and player_id encoder","I want to know about original id and encoder table.","02/22/2020 00:00:00","1"
"625","284","4732370","03/24/2020 10:28:56","# ## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","????????","?????","04/24/2020 00:00:00","8"
"1441","284","5406870","07/21/2020 10:35:36","## Expected Submission
A notebook with your insights

## Evaluation
Graphics, EDA and explainations","","Exploratory Data Analysis","","08/21/2020 00:00:00","76"
"1902","284","5699725","08/31/2020 07:51:21","You need to Piss and shit","","Piss and shit","","","6"
"5002","284","7323602","07/05/2021 01:24:53","Let's do some EDA about the video game!","","EDA of video game","","","1"
"3659","284","6848751","03/04/2021 17:42:24","## Task Details
This a small task mainly for the beginners to data science and Kagglers.
Data Science starts with small steps like calculating mean, median, mode, percentile, and many other basic terms. These small tasks provide us the reason as to what and why we need to perform a particular analysis. Your main task is to find the **five-number summary** of the dataset. This includes :
1. Minimum number
2. First Quartile
3. Median
4. Third Quartile
5. Maximum number


## Expected Submission
Find the five-number summary of each sales column separately.
## Evaluation
The correctness of the five-number summary for the data given.


### Further help
This is quite an easy task. The Internet alone can help you solve this. It is just about basic mathematical and statistical definitions. Most of the tasks had covered everything. This task is to emphasize more on ""why to do?"" than on ""how to do?"". A small addition to this is(not a part of the submission):
1. Try observing what are the outliers.
2. Try to clean the data so that you are focussing on the main data and not the outliers.
PS: This is my initial days of data science and my first ever time on Kaggle. I am open to any suggestions of any magnitude.","","Five number summary","For a few beginners to help them start with data science and kaggle.","12/31/2021 23:59:00","1"
"3921","284","2894631","03/31/2021 11:29:23","which genre","","WHICH GENRE SOLD THE MOST NA, JP AND EU SALES","","","9"
"3165","284","6495574","01/10/2021 09:28:50","visualize the data of games","","Games sales  visualization","visualization of sales","01/15/2021 23:59:00","22"
"3027","284","3505796","12/24/2020 00:45:17","## Task Details
The idea of the task is to show all the data and some usefull information for business decissions.

## Expected Submission
A notebook with some data cleaning and visualization. Furthermore, some conclusiones and decissions over the data to show the trends.
Compare genres, platforms, some posible ""partners"", etc","","Exploratory Data Analysis","Graphics, EDA and explainations","","9"
"6816","284","8848926","11/10/2021 23:52:15","Using the Apriori algorithm, there must be at least three significant operations executed on the datasets with justification, e.g. altering levels of rule confidence and rule support, selecting different combinations of attributes, dataset samples etc.","","Apriori algorithm","","11/11/2021 23:59:00","1"
"6728","306","1758918","10/31/2021 09:23:59","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Start...","","Credit Card Clients Predict","","","0"
"2682","306","5769479","11/12/2020 10:52:55","This task requires you to create a ML model with the best score possible, from various models available.","","Create ML model to predict default","","","12"
"71","310","910830","12/08/2019 06:02:02","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","250"
"72","310","910830","12/08/2019 06:06:20","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","33"
"73","310","910830","12/08/2019 06:06:45","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","41"
"74","310","910830","12/08/2019 06:07:27","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","17"
"75","310","910830","12/08/2019 06:07:46","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","13"
"76","310","910830","12/08/2019 06:08:19","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","17"
"77","310","910830","12/08/2019 06:08:37","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","61"
"78","310","910830","12/08/2019 06:08:56","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","25"
"79","310","910830","12/08/2019 06:09:12","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","21"
"3444","310","5002563","02/09/2021 03:56:27","Detecci√≥n de fraude con regresi√≥n log√≠stica y con una √≥ptica de negocio.","","Regresi√≥n Log√≠stica AUC = 97%","","02/28/2021 23:59:00","15"
"1882","323","5195560","08/29/2020 16:38:59","I am going to perform Exploratory Data Analysis and try to understand the various correlation. I would be cleaning the Dataset and performing feature engineering.","","EDA and Data cleaning","Exploratory Data Analysis and Feature Engineering of the dataset","09/05/2020 23:59:00","11"
"597","339","1270869","03/18/2020 03:27:04","Build a good recommender system","","Recommender System","","05/04/2020 00:00:00","9"
"621","355","3023720","03/23/2020 07:27:33","**Introduction:**
This dataset contains data for over 400,000 flights from 19 carriers and 284 airports from October to December 2008. We'll start by reading in and lightly cleaning the data. Then we'll do some data exploration by making plenty of figures and tables that give us a clearer idea of what is in this dataset. Once we feel more comfortable with the dataset, we will try some machine learning to predict flight cancellations and delay times. We find that cancellation predictions are not feasible with the given data. Combining this with current weather and aircraft/airport maintenance data could make cancellation predictions somewhat useful. Predicting flight delay times is similarly difficult and could also be improved with relevant real-time data. Even though this dataset is not particularly useful for predictions, there are nonetheless interesting relationships and distributions that can be shown.","","Airplane Delay & Cancellation","","","8"
"33","385","304806","12/05/2019 01:44:07","### Task Details

Create a calculator that allows the simulation of Pokemon battles that matches the standard calculations for battles as they happen in the game.

### Expected Submission
Provide a notebook that allows users to select which pokemon and moves they would like to battle and then allows the battle to come to completion.

### Evaluation

An accurate and easy to use simulator is the ideal solution. A couple of stretch goals:

- How are moves selected to be used each turn? Random or AI selection? Allowing user input?
- Allow for two teams of pokemon to be defined and go up against each other
- Automatically generate pokemon teams to battle
- Simulate multiple battles with different moves chosen each time, after hundreds of runs what is the percentage of times each pokemon beats each other?
- Run a tournament with all of the pokemon in the game, which are the strongest relative to each other?","","Create a Pokemon Battle Simulator","","","1"
"180","393","84413","12/10/2019 05:23:31","## Task Details
As of June 2018, this data set contains ~4,000 airbnb listings in Seattle.  The purpose of this task is to predict the price of Seattle Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
241032, 85
3308979, 575
7421966, 260
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Seattle Airbnb Rental Prices","","","3"
"182","395","84413","12/10/2019 05:35:10","## Task Details
As of November 2019, this data set contains over 3 thousand airbnb listings in Boston.  The purpose of this task is to predict the price of Boston Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
12147973, 175
3075044, 190
6976, 260
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Boston Airbnb Rental Prices","","","2"
"2782","399","5954838","11/26/2020 10:20:58","## Task Details
Can you teach me for this dataset
What and how  i can predict the answer ?
It is my dataset https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics.","","Population rate","","","0"
"2698","423","4943353","11/14/2020 09:51:59","Finish analysis on the real estate data set","","Real estate data analysis","","","5"
"4320","423","6944808","05/10/2021 05:58:27","We have to perform this task which includes below Data Science stages 

‚Ä¢ Business Understanding
‚Ä¢ Data Understanding
‚Ä¢ Data Preparation
‚Ä¢ Data Modelling
‚Ä¢ Model Evaluation
‚Ä¢ Deployment

Business Understanding -  Melbourne house price data

Data Understanding - 

1. Import the data set in Python.
2. View the dataset
3. See the structure and the summary of the dataset to understand the data.
4. Find out the number of:
 a.) Numeric attributes:
 b.) Categorical attributes:

Data Preparation : Data Cleaning - 

‚Ä¢ Duplicate values: Identify if the datasets have duplicate values or not and remove the duplicate values.

Find out the number of rows present in the dataset

‚Ä¢ Before removing duplicate values
‚Ä¢ After removing duplicate values

‚Ä¢ Variable type: Check if all the variables have the correct variable type, based on the data dictionary. If not, then change them.

For how many attributes did you need to change the data type?

‚Ä¢ Missing value treatment: Check which variables have missing values and use appropriate treatments.

For each of the variables, find the number of missing values and provide the value that they have been imputed with.
‚Ä¢ Outlier Treatment:

Identify the varibales : Make a subset of the dataset with all the numeric variables.

Outliers : For each variable of this subset, carry out the outlier detection. Find out the percentile distribution of each variable and carry out capping and flooring for outlier values. 

Data Preparation Feature Engineering: -
A - Feature Transformation:
1 - Identify variables that have non-linear trends.
2 - How many variables have non-linear trends?
3 - Transform them (as required)

B - Standardization:
Name the variables to be standardised before using a distance-based algorithm

C - Dummy encoding :
1- Identify the number of dummy variables to be created for the variable steel.","","Melbourne Housing Market Data Science Project","","","3"
"6004","423","4895014","09/04/2021 15:40:34","The task is to practice linear regression model for beginner","","Linear regression to predict house price","","","1"
"4757","435","7653749","06/13/2021 07:25:54","In today's era, data has an important role in all aspects, one of which is business.
Data can be used to gain insight, even to make better decisions in building a business
Use the dataset to perform exploratory data analysis to gain valuable insights and  apply time series analysis to get a forecast of sales after time period of 7 days.","","Perform EDA, Time Series Analysis, and Sales Forecasting","","","5"
"6891","454","6421142","11/21/2021 08:09:21","City Controller Office has released payroll information for all Los Angeles City employees on a quarterly basis since 2013. Data includes department titles, job titles, projected annual salaries (with breakdowns of quarterly pay), bonuses, and benefits information of employees. Government is interested in knowing answers to the questions mentioned below for future betterment of different departments. You are required to answer the following questions by using hypothesis testing. None of the questions can be answered as Yes or No. You are required to setup and run the experiments to reach a conclusion. Ensure that your conclusions are sound by keeping in mind the Dos and Don'ts for each concept, e.g. sample size, randomization and so on. All the intermediate steps, graphs and experimental results must be included while answering.

**You are required to use the attached City Payroll Dataset. There are many empty fields, youshould devise a mechanism to handle them.**
Questions:
Question 1. Do the employees working as Police Officer-II have a better chance of getting
Temporary Bonus Pay?
Question 2. Employees who get Permanent Bonus Pay are most likely to be from Public Works-Sanitation Department?
Question 3. Do the employees working in Water and Power (DWP) Department have a better chance of being employed overtime?
Question 4. In 2014, employees of Recreation and Parks Department were complaining that they have been denied the Longevity Bonus Pay. Confirm their complaint from the given dataset.
Question 5. Senior Clerk Typist from Harbor (Port of LA) Department has been telling Senior Clerk Typist of Water and Power (DWP) Department that they have more Average Health Cost than them. Accept or Reject this claim according to the dataset given. 

**Guidelines**:
Many concepts in the asked questions are subjective and are left for you to define, i.e. ‚Äúhighest pay raise‚Äù and others. Note that in some cases, you may need to look at multiple columns to get the required information.","","hypothesis testing","Analysis perform in R code","","1"
"868","474","4083615","05/09/2020 10:29:48","Haberman's survival dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer.

Attribute Information:
Age of patient at the  time of operation (numerical)
Patient's year of operation (year - 1900, numerical)
Number of positive axillary nodes detected (numerical)
Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 years","","Haberman's Survival : Exploratory Data Analysis","Haberman's Cancer survival :EDA","05/10/2020 00:00:00","38"
"364","478","2135786","01/09/2020 02:51:40","Checking neural network performance on the dataset","","Mushroom Classification using keras","","","31"
"1719","478","5556552","08/17/2020 05:33:24","## Task Details
Perform Mushroom Classification using Random Forest Classification.

## Evaluation
The notebook which gives highest accuracy.","","Mushroom Classification using Random Forest Classifier","","","60"
"1634","478","5331600","08/08/2020 12:55:06","Mushroom Classification By Xgboost","","Mushroom Classification By Xgboost","","","6"
"5821","478","6224917","08/21/2021 07:09:57","At first you must ignore first column that contains classes of data ; Then train a cluster algorithms on this dataset, that should cluster dataset in to two cluster that represent 'e' and 'p' in first column , After that you must evaluate your model with RandIndex.","","Do classification task with clustering algorithms!","","","5"
"1188","483","5281883","06/21/2020 09:04:35","Classify text messages on spam or ham.","","Spam or Ham Classifier","","","25"
"4953","483","2620227","06/30/2021 13:01:25","## Task Details
Perform Feature generation on spam and ham dataset and perform features evaluation which features really affect the classification of message into category.


## Expected Submission
Submission should contain full Exploratory Data Analysis of features and if any observations please list them and please provide reason in markdown why you are performing any transformation on the feature and how it will help.","","EDA and Feature Generation","","","1"
"2113","531","1696964","09/15/2020 19:33:00","## Task Details
This is a task.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","test4me","","","8"
"2838","571","6128842","12/02/2020 13:24:45","1. Of all anime having atleast 1000 ratings, which anime has the maximum average
rating? anime_id = 28977
2. How many anime with atleast 1000 ratings have an average rating greater than 9?
3. Which is the most watched anime i.e. the anime rated by most number of users?
4. What are the top three recommendations for the user with user_id 8086?
5. List top three users whom you would recommend the anime with anime_id 4935?","","Need a Small help with below questions","","12/03/2020 23:59:00","9"
"406","586","4387238","01/26/2020 19:34:38","Some description","","some random classifica","Some subtitle","05/22/2020 00:00:00","5"
"5402","586","4656934","07/28/2021 21:03:56","### If I ask you to name the animals that are similar to Tiger you might say Lion,Leopard,Cheetah etc..¬∂
### But,can a machine tell similar animals by giving some random animal names?
### Use this dataset to predict similar animals by giving random animal name.","","Predict Similar Animals.","","","0"
"2929","586","5753671","12/13/2020 14:54:11","Make a model with the best accuracy score! Good luck!","","Zoo Animal  Class Type Classification with Scikit-Learn","","","9"
"6056","618","1700889","09/10/2021 20:29:19","Find out the percentage of males and females, the one who survived and the ones who perished","","What is the percentage of male and females?","....","","0"
"5510","626","5575950","08/02/2021 20:24:29","## Task Details
I'm trying to practice with movielens dataset, using pyspark I need to convert csv to pysark dataframe.

I need need explanation, possible with a sample code to show how this can be achieved, I keep getting error when I try to use the toDF() method.","","Convert movielens csv to pysark dataframe","","","0"
"5153","634","4791194","07/16/2021 20:12:24","## Task Details
The goal is to predict apparent temperature for the given humidity.

## Expected Submission
Solve the task primarily using Notebooks.

## Evaluation
A model with higher accuracy score.

### Further help
You can refer to this notebook and try to improve the accuracy with good ethics:
- [Weather in Szeged R2_Score: 0.9796](https://www.kaggle.com/lasindudemel/weather-in-szeged-r2-score-0-9796)

All the bestü§ì","","Predict Apparent Temperature (C)","","","7"
"3178","686","5305041","01/13/2021 08:30:24","rainfall data set should be analysis to predict landslide

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","rainfall analysis to predict landslide vulnerability","","01/30/2021 23:59:00","0"
"6761","707","8337891","11/05/2021 11:19:48","EDA on Countries participated, disciplines, events & coaches.","","EDA on Countries participated, disciplines, events.","","","0"
"4528","719","590653","05/28/2021 13:47:08","How many people got killed and injured per year?","","How many people got killed and injured per year?","","","1"
"4529","719","590653","05/28/2021 13:47:44","Visualize suicide attacks on timeline","","Visualize suicide attacks on timeline","","","1"
"4530","719","590653","05/28/2021 13:48:05","Find out any correlation with number of suicide bombing attacks with drone attacks","","Find out any correlation with number of suicide bombing attacks with drone attacks","","","1"
"4531","719","590653","05/28/2021 13:48:31","Find out any correlation with suicide bombing attacks with influencing events given in the dataset","","Find out any correlation with suicide bombing attacks with influencing events given in the dataset","","","1"
"4532","719","590653","05/28/2021 13:48:51","Can we predict the next suicide bombing attack?","","Can we predict the next suicide bombing attack?","","","1"
"4533","719","590653","05/28/2021 13:49:08","Find the correlation between blast/explosive weight and number of people killed and injured","","Find the correlation between blast/explosive weight and number of people killed and injured","","","1"
"4534","719","590653","05/28/2021 13:49:26","Find the impact of holiday type on number of blast victims","","Find the impact of holiday type on number of blast victims","","","1"
"4535","719","590653","05/28/2021 13:49:44","Find the correlation between Islamic date and blast day/time/size/number of victims","","Find the correlation between Islamic date and blast day/time/size/number of victims","","","2"
"4536","719","590653","05/28/2021 13:50:09","Find the Top 10 locations of blasts","","Find the Top 10 locations of blasts","","","3"
"4537","719","590653","05/28/2021 13:50:19","Find the names of hospitals sorted by number of victim","","Find the names of hospitals sorted by number of victim","","","1"
"4526","721","590653","05/28/2021 13:35:58","Is the individual in question more susceptible to cancer?
Does he tend to gain weight?
Where is his place of origin?
Which gene determines certain biological feature (cancer susceptibility, fat generation rate, hair color etc.","","What you can find about the father?","","","0"
"4527","721","590653","05/28/2021 13:36:20","What else you can extract from this dataset when it comes to personal trait, intelligence level, ancestry and body makeup?","","Family Traits","","","1"
"4165","739","7169206","04/23/2021 23:56:11","## Task Details
I would like to be able to find out if countries that tend to be more religious are more prone to instigating wars? 

## Expected Submission
-Datasets on Religious groups within different countries
- Talks of wars mentioned within the groups?

## Evaluation
The more details on countries the better

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","What do the Correlations Say?","Are More Religious Countries Prone to Starting Wars?","","0"
"2046","745","3058691","09/11/2020 05:00:14","## Task Details
Analysis of the population characteristics of the ACS dataset","","ACS Data Analysis","","","0"
"852","746","5033128","05/06/2020 10:37:24","## Task Details 

Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

## Not getting the task need to think on it","","Solve this Aerial Bombing Operation","","05/10/2020 00:00:00","3"
"1603","792","5411639","08/05/2020 14:29:13","## Task Details
THE GOAL FOR THIS PROJECT IS TO DETERMINE WHAT FACTORS ARE IMPORTANT FOR US TO KNOW IN ORDER TO PREDICT IF A PATIENT WILL SHOW UP FOR THEIR SCHEDULED APPOINTMENT.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?


## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NO SHOW TASK","","","13"
"840","815","1015418","05/03/2020 13:10:16","## Task Details
EDA folllowed with devloping solutions on what leads a case to remain unsolved.

## Expected Submission
Primarily Notebooks will be preferred.

## Evaluation
Better Accuracy on target variable, Case Solved.","","To determine if the case was solved","","05/07/2020 00:00:00","7"
"5","894","998023","11/27/2019 03:40:28","### Task Details
This dataset shows the happiest countries on earth, which is great info when you're looking for your next move üòè But what if you wanted to create a new country with the goal of having the happiest citizens? What if you're a president looking to improve their country? How would you do that?

The goal of this task is to find out what factors contribute to happiness. You can join any other data and use any insights you might have that show a strong correlation between the factors you come up with.

### Expected Submission
Kaggle Notebooks are expected as the primary means, although I won't discard any other submitted information (e.g. a presentation submitted as a dataset). All solutions should contain the correlation between the factors you discovered and the happiness score of this dataset (I'm looking for the actual number). Any charts explaining this correlation will also help.

### Evaluation
This is highly subjective. The best solutions will both have a good correlation score, but are also creative and explain the work well.","","What makes people in a country happy?","Join any data you want to explain why countries are ranked as they are here","01/31/2020 00:00:00","78"
"1607","894","3650837","08/05/2020 20:00:38","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Visualiation","","Visualization World Happiness Report","","","47"
"2302","902","1914939","10/01/2020 13:14:54","## Task Details
Make a model to assess whether or not a new customer is likely to pay back the loan. You can use several different machine learning models. 

## Expected Submission
A notebook that accomplishes the task.

## Evaluation
After making the model try to answer the following questions: 
- **Would you offer this person a loan?**
- **Did this person actually end up paying back their loan?**","","Predict if a future customer will pay back the loan","","","23"
"2311","902","3063341","10/02/2020 17:33:17","## Task Details
In the accepted loans data, we have a column called ""grade."" It takes values from A-G (A is the best, G is the worst). The task is to build a classifier that, given some other features, can accurately categorize a loan by grade.

## Expected Submission
A Notebook that demonstrates high performance in the classification task.

## Evaluation
Multi-class accuracy is a decent metric to use with this data, but other metrics are welcome as well.","","Classify Loans by Grade","Categorize accepted loans into one of the seven loan grades","","5"
"6825","915","8848926","11/11/2021 23:40:08","Implementing Apriori Algorithm and use the confidence and min and support please!","","Implementing Apriori Algorithm","","","0"
"1556","1067","2155029","08/02/2020 14:39:16","## Task Details
Every task has a story. yTell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
yes","","HR analytics","analytics data visuliation","","57"
"3580","1069","286460","02/23/2021 13:35:05","## Task Details
Running this for practice to improve skills. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Fraud Prediction","","03/31/2021 23:59:00","0"
"5516","1107","2671541","08/03/2021 07:56:58","## Task Details

it is a test to follow up   30 days of ML!","","TEST a new task to participate to 30 days of ML !","what fun!","09/03/2021 23:59:00","1"
"5922","1107","3599213","08/30/2021 10:49:24","## Task Details
this task is about classifying pictures of flowers, reusing a pretrained Xceptionn model

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Train a Model to classify Pictures of Flowers reusing a pretrained Xception model(Transfer Learning)","","","0"
"3728","1150","4613921","03/10/2021 22:00:07","## Task Details
An important part of this task is building a classifier for categorizing different kind of jobs.","","Job classifier and similar jobs","","03/16/2021 23:59:00","1"
"405","1180","3639980","01/25/2020 08:14:06","## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Recommendation System for locations","","01/31/2020 00:00:00","2"
"513","1256","4618611","03/06/2020 08:08:32","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
wanna learn it
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
datasets
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
learn it quickly","","new task","","03/11/2020 00:00:00","24"
"1417","1275","5250557","07/19/2020 08:49:49","latihan dataset pertama
19/07/2020","","Titanic","Pracitce","","24"
"1723","1291","4857935","08/17/2020 09:30:25","## Task Details
Predict the price of the car based on the features in the dataset.

## Expected Submission
 Solve the task primarily using Notebooks .

## Evaluation
A model with good r2 score .","","Price Prediction","","","17"
"6031","1291","6121687","09/07/2021 13:31:47","## Task Details
Based on the other attributes, classify how safe the car is for driving. Note that the attribute isn't binary. Therefore, methods fore multi-class classification are required.

## Expected Submission
Solve the task primarily using Notebooks. Try out different algorithms.

## Evaluation
Check the performance of your model with different evaluation metrics, e.g. accuracy score, precision, recall, f1-score etc.","","Car safety classification","Safety corresponds to the attribute 'symboling'","","1"
"6953","1309","8796993","11/29/2021 02:23:19","## Task Details
Every task has a tory. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","helping chantwl","","","1"
"2032","1312","3332800","09/09/2020 17:13:52","Perform EDA and predict price of diamonds","","Perform EDA and predict price of diamonds","","","11"
"1062","1346","4331397","06/05/2020 08:38:11","g","","Bitcoin Historical","","07/02/2020 00:00:00","40"
"386","1358","3594270","01/19/2020 05:06:43","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?123","","11112312","","","7"
"27","1366","1314380","12/02/2019 17:58:14","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","November: Religious History Month","Create and describe a plot that relates to ""November: Religious History Month""","12/01/2020 00:00:00","1"
"107","1442","910830","12/08/2019 06:23:56","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","48"
"108","1442","910830","12/08/2019 06:24:26","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","17"
"109","1442","910830","12/08/2019 06:24:44","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","15"
"110","1442","910830","12/08/2019 06:25:20","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","1"
"111","1442","910830","12/08/2019 06:26:02","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","5"
"112","1442","910830","12/08/2019 06:26:25","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","1"
"113","1442","910830","12/08/2019 06:26:44","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","1"
"114","1442","910830","12/08/2019 06:27:08","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","2"
"115","1442","910830","12/08/2019 06:27:35","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","3"
"3180","1442","6449529","01/13/2021 10:51:07","my first project","","Ridge regression","","","2"
"20","1461","1314380","12/02/2019 17:51:18","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","April: Arab History Month","Create and describe a plot that relates to ""April: Arab History Month""","05/01/2020 00:00:00","0"
"4840","1474","4960963","06/19/2021 07:35:26","Explore IMDB data.","","IMDB Data Exploration EDA","","02/20/2024 23:59:00","0"
"4182","1489","7266751","04/25/2021 04:11:26","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
Project HPC","","Project","","07/07/2021 23:59:00","0"
"6929","1489","6935765","11/25/2021 15:18:10","### This is my prediction inspired by the TensorFlow regression method.","","Predict fuel efficiency","Using linear regression!","","0"
"471","1498","109123","02/23/2020 08:42:54","Replicate the analysis done by ProPublica and see if they missed checking for confounders (whether in the data or out of it). 


Indepth analysis by ProPublica can be found in their data methodology article: 
https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm

Original Article:
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing

Original data from ProPublica:
https://github.com/propublica/compas-analysis


Quoting from ProPublica:
""
Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).
White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent).
The analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.

Black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. And white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism, compared with black violent recidivists.
The violent recidivism analysis also showed that even when controlling for prior crimes, future recidivism, age, and gender, black defendants were 77 percent more likely to be assigned higher risk scores than white defendants.
""","","Replicate ProPublica's Analysis","","","1"
"472","1498","109123","02/23/2020 08:43:54","Check if the COMPAS's model's score has a racial bias.
Hint: Try to control/correct for variables that correlate with race, e.g. age or # of priors","","Check the COMPAS model for racial bias","","","1"
"504","1530","4606582","03/04/2020 14:21:09","## Task Details
Figure out the taxi company that is usually used in Chicago.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your analysis. A comparison can also be made with other companies.","","Taxi company usually used in Chicago","","","0"
"505","1530","4606582","03/04/2020 14:22:06","## Task Details
What is the the mode of payment that is usually used in Chicago

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your analysis.","","Mode of payment usually used","","","1"
"1033","1632","5045211","06/02/2020 11:52:19","1. upload the .csv file
2. filter the data 
    - impute empty cells
    - remove duplicated rows or columns 
3. select the best features to use in inputs and output (X, Y)","","upload and filter the data","","","31"
"350","1636","2848332","01/06/2020 07:06:47","python recommendation","","Movie recommendation","","","20"
"712","1674","3588424","04/08/2020 11:11:26","## Task Details
create a machine learning model for traffic prediction in particular place on particular day and time

## Expected Submission
submit the trained machine learning model and test code in the form of python notebook

## Evaluation
Accuracy matters","","Traffic density Prediction","","04/30/2020 00:00:00","0"
"521","1686","4613030","03/09/2020 11:15:33","## Task Details
creating this task to track temperature changes in towns of india during 2019

## Expected Submission
need statewise towns with temperature monthly for the year 2019 in india. 

## Evaluation
data has states, towns, monthly temperature in average high and average low","","weather town wise 2019","temperature of towns in india during the year 2019","","8"
"6170","1686","7173234","09/23/2021 09:40:01","## Task Details
## Rice vs Dice 80/20

Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

üôè","","ISoumya_000009","","09/30/2021 23:59:00","0"
"202","1688","769452","12/11/2019 10:04:25","## Task Details
Explore, document, review all columns, check for missing values, replace missing values

## Expected Submission

* Use Notebooks
* Use R or Python; for R, you can use RMarkdown rather than Notebook.

## Evaluation
Address all missing values; suggest replacements for the missing values. Interpret the results. Show a geographical distribution (at county level) of several values.","","Perform a detailed exploratory data analysis","Explore, document, review all columns, check for missing values, replace missing values","01/31/2020 00:00:00","1"
"2747","1692","1150494","11/22/2020 17:09:27","## Task Details

It would be interesting to see any correlations for ABC news, which is the most popular news source from Australia; impacting the prices of publicly traded companies operating within the country.

Telecoms behemoth Telstra (ticker=TLS) and the mining giant BHP Billiton (ticker=BHP) are highly referenced (with 1000 plus events) in the news and would make interesting case studies. Additional context of related news headlines (like references to the mining sector or raw minerals) can be included to widen the scope.

## Evaluation

A good solution would be able to use news and any other additional reference data to predict historical fluctuations in market prices of a company or index with high accuracy.

### Reference Data

Past data with all tickers on the ASX (Australian Securities Exchange) is available here: https://www.asxhistoricaldata.com

More recent data with complete list of asx companies, tickers and market_cap is available [here](https://www.marketindex.com.au/asx-listed-companies).

Another file with list of ticks only is available [here](https://pastebin.com/bYLH80QY) for reference.

OEC Economic Profile and Visualisations seen here: http://konivatsara.com/aus-oecs

Mineral resource wealth of the island continent as a tabulated chart available [here](https://en.wikipedia.org/wiki/Mining_in_Australia#Statistical_chart_of_Australia's_major_mineral_resources).","","Australia News and Economic Correlations","","","4"
"1521","1740","5544956","07/29/2020 08:05:40","#","","histogram","","","2"
"1012","1819","1150494","05/30/2020 19:31:37","## Task Details

Zeta index files occur when one of the fields in the dataset is grouped.

It consists solely of the grouped field value , the byte number of the start of value , the byte number of the end of the value. Each row generated within the index is expected to be of equal length.

If stored the grouped field can be dropped entirely from the dataset and recalculated by referring to the Œ∂ file. It is named as such because of the z pattern formed by the ordering of start and end values

If the field is ordered as well as grouped the end of value field can be excluded from the Œ∂ index. However it is best to retain it incase only a part of the Œ∂ index survives the ravages of time.

## Expected Submission

A notebook that accomplishes the steps below and makes the Œ∂ index available for download. 

1. Generate the Œ∂ index for the file using publish_date and shuffle the index.
2. Remove the publish_date field from the source file.
3. Calculate the reduction in size of the original file.
4. Derive the file by referring to the Œ∂ index.

## Evaluation

Compare with the derived file with the orignal to verify integrity.

What is the smallest Œ∂ index file that can be generated if characteristics of the source field are taken into account?","","What is a Zeta index?","Size compression and minimum space challenge","","0"
"2869","1908","5098820","12/05/2020 21:49:03","## Task Details
Establish a weekly volatility Index that ranks stocks based upon their volatility index.","","Volatility Index","","12/08/2020 23:59:00","7"
"4401","1908","7098558","05/15/2021 21:54:59","## Task Details
Submit your prediction for a single stock and compare it to the real price. Plot both to evaluate the accuracy of your model.","","Single stock prediction","","","4"
"502","1909","4410822","03/04/2020 04:03:00","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Not sure yet

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Thing of somwthing","","","0"
"178","1920","84413","12/10/2019 04:44:32","## Task Details
Immunization rates in children were decreasing over time in California from 2000-2014.  Changes to state law requiring incoming kindergarten students to have proof of vaccination or medical exemption (as opposed to more easily obtained personal belief exemption) slowed this trend, but vaccination rates in 2015 were still lower than rates in 2000.  See [this kernel](https://www.kaggle.com/broach/evaluating-immunization-rates-2015-update#Revisiting-Kindergarten-Immunization-Rates-in-CA) for more details.  The purpose of this task is to predict how many years it will take for immunization rates in the majority of counties in the state of CA to return to or exceed levels from the year 2000.

## Expected Submission
Users should submit a kernel/notebook demonstrating their approach, rationale, and they should include tables with county level predictions of predicted immunization rates beyond the time points included in the current dataset (ie, after 2015).

## Evaluation
A logical, easy to read and understand write-up makes for the most valuable solution.  Bonus points if external datasets or links to current information are provided to externally validate your results.","","Predict CA Kindergarten Immunization Rates","","","0"
"1203","1938","5199973","06/25/2020 11:19:37","## Task Details
I like books but don't always have enough time to browse in search of a new appealing book out there. So, i wonder if it is possible to construct a book recommendation system out of the data provided.

The aim is to suggest 10 random books in the first place and ask the user to select one of them. Based on the user's selections recommend a new collection of books each consists of the same amount of books in an iterative manner.

In each iteration, improve the performance of the new suggestion set in the eyes of the user. After a certain iteration times (lets say 4) ask the user if he/she is interested in buying or reading any of the final (5th) collection of books? (a yes/no evaluation feedback)

## Expected Submission
You should submit a model as well as the algorithm in a notebook or a script folder

## Evaluation
Based on the evaluation of users after 4 iterations mantined above, the recommendation model should perform at least a 80% satisfaction in (again at least) a sample group of 250 individual testers.","","Book Recommendation System","Help people find their specialized on-demand books and boost the sales of the online book shopping platforms","","1"
"18","1947","1314380","12/02/2019 17:49:02","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Feb: Black History Month","Create and describe a plot that relates to ""Feb: Black History Month""","03/01/2020 00:00:00","3"
"6964","1982","9029902","11/29/2021 16:24:25","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 80 Spotify Songs 2019)

O","","I loves mangoes","For collecting data","11/30/2021 23:59:00","0"
"4252","2115","7329298","05/03/2021 06:04:56","Plastic news","","Plastic","","05/31/2021 23:59:00","0"
"3956","2243","6763537","04/04/2021 17:01:14","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

True","","ROBT407 naizagai","","","4"
"218","2243","4192477","12/13/2019 10:35:47","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution `contain?`

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Kim_Huanyu_Handsome","","12/30/2019 00:00:00","18"
"1233","2296","5323381","07/01/2020 08:31:33","## Task Details
This Task Is Just for practising the Data Analysis
## Expected Submission?
EveryOne Just Try to Analyse this   data and try to create  and solve real time
problems with this  

## Evaluation
Evaluation is completely based on the depth of your  understanding with this data

### Further help
If you need any help regarding this task . Mail @gughanbabu4@gmailcom","","DATA_ANALYSIS","","","22"
"4631","2296","7583352","06/04/2021 04:47:19","test

| - 1. &gt; # ***`##`*** |  |
| --- | --- |
|  |  | Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Vivek demo","","06/06/2021 23:59:00","0"
"5575","2321","5194950","08/06/2021 12:34:22","## Task Details
For centuries chess players have debated which openings provide the most advantage against stronger, equal, and weaker opposition; for slow and fast time controls; for White or Black; etc.

## Expected Submission
Solutions should contain chess openings in the dataset along with either qualitative or quantitative information.

## Evaluation
Good solutions could apply well to predicting results of games either via cross-validation or for games not included in the dataset.  Use your imagination.","","Measure opening strength","","","1"
"4012","2321","1685485","04/08/2021 23:50:35","## Expected Submission
User should be able to understand various trends in Dataset","","Background Analysis","Analyse the dataset to find treds","","6"
"1690","2321","2190288","08/13/2020 13:50:51","This is my first task in kaggle.","","First_Task_chess","","10/13/2020 00:00:00","32"
"6767","2321","5417321","11/06/2021 11:55:09","## Task Details
Data analysis and visualization for beginners using python modules like NumPy, Pandas, Matplotlib and Seaborn

##Sub-Tasks
Q1) What are the most popular Chess Openings.
Q2) How Most of the Chess matches concluded.
Q3) What is the distribution of number of turns played in a match
Q4) Which Color Side is more likely to win.
Q5) What is the distribution of rated players.
$$ Add inferences and conclusions also.

## Expected Submission
The solution should also contain plots derived from the Dataframe","","Data visualization for beginners","","","1"
"4448","2358","7462189","05/20/2021 03:38:41","Understanding how to use a data set","","My Task","","","0"
"409","2374","4345442","01/27/2020 12:30:53","I search for mobile crowd sensing data set  for server side that consists of any number of tasks (sensing requests),sensing interval for every task ,sensors required and location of tasks","","Mobile crowd sensing datasets","","01/29/2020 00:00:00","3"
"58","2414","1213499","12/07/2019 19:54:38","## Task Details
When I see such an interesting dataset, I would like to learn about surprising results at the first instance. Aim of this task is finding which league is most competitive or in which leagues most surprising results comes out. These are the leagues with highest competition and potentially higher pay-off's to the bookers.

## Expected Submission
User should submit a kernel with relevant documentation.

## Evaluation
Creativity of the approach is key since users will define what makes a league competitive than others (e.g. victory of the underdog, goal difference between teams etc.)","","In which league underdogs prevail?","Which leagues are harder to predict?","","2"
"3906","2477","2192353","03/30/2021 11:53:24","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Teresia -Sentiment Analysis","","04/10/2021 23:59:00","0"
"2808","2592","109123","11/29/2020 08:22:00","Given the complaint, what is the most likely outcome: Predict the ""Target"" column and explain it

What affects it? The complaint itself? The frequency of different outcomes? The location? Date of the complaint?","","What determines misconduct outcomes?","Predict the target outcomes","","1"
"5286","2607","1888240","07/22/2021 16:14:31","## Task Details
This it is a small dataset, so the model score is sensitive to the test-data selection.
Nevertheless, I think it is will be nice to find the best prediction, and compare few models, such as simple neural networks, XGboost, random forest and so on. 

## Expected Submission
Please submit your algorithm and your test precision/recall.

## Evaluation
A good solution:
1. Shuffle the data
2. The test set should express the original distribution.
3. The size of the test set should be at least 20% of the data.","","Get the best prediction for Liver Patients","","07/22/2022 23:59:00","3"
"1919","2619","5558714","09/02/2020 07:05:42","Pokemon Tasks

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Pokemon Tasks","","09/02/2030 23:59:00","1"
"903","2635","844176","05/14/2020 15:32:37","## Details for Task Sets
1) Experiments with image recognition have to detect borders in sizes and numbers of pictures for exact labeling (how many pictures of this type and this size are enough for classification without confusion, in which cases only one image of each class makes the labeling process possible, etc.).
2) Discovering the capabilities of algorithms in the recognition of biological objects (is it possible to scientifically classify samples as a biological species, how complete this classification could be, how to detect concrete diseases visually, etc.).

## Expected Submission
1) 95-98% accuracy in classification for this dataset.
2) Creating other variants of datasets to find abilities' borders for algorithms.

## Evaluation
1) A consistently high indicator in the classification, demonstrated by the algorithm for a given set of parameters
2) New databases that improve the understanding that algorithms are able to recognize visually in biological objects.","","Goals in Image Recognition","Two Main Subtasks","","0"
"6232","2678","8490260","10/01/2021 18:36:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 201","","Ntebook","","","1"
"721","2709","3477749","04/09/2020 15:33:11","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","work on this data sets","","","26"
"410","2756","2654675","01/27/2020 14:59:19","Przewidujemy czy dany pokemon jest legendarny!","","Legendary pokemons","","","12"
"3168","2756","6479964","01/10/2021 22:24:42","## Task Details
The eighth generation of pok√©mon is out for more than a year, it will be nice to see an update to this dataset.","","Add eighth generation.","","","7"
"6437","2756","8581447","10/22/2021 20:23:40","Where is Pikachu in this given data sets?","","Pokemon","Pikachu","","3"
"292","2757","3882035","12/16/2019 21:44:28","## Task Details
Which flavor is favored(sold) the most by all the restaurants that sell pizza.


## Expected Submission
A notebook showing the results of your solution.

## Evaluation
A clear representation of your results together with some comparisons with other flavors.","","Flavour favored the most","","","1"
"293","2757","3882035","12/16/2019 21:45:54","## Task Details
Which city sells pizza the most?

## Expected Submission
A notebook with the results of your analysis.

## Evaluation
Good representation of your results.","","City that sells pizza the most","","","0"
"294","2757","3882035","12/16/2019 21:47:49","## Task Details
Which province sells pizza the most?

## Expected Submission
A notebook with the results of your analysis.


## Evaluation
Clear results showing the province that sells pizza the most.","","Province that sells pizza the most","","","0"
"295","2757","3882035","12/16/2019 21:49:41","## Task Details
What is the maximum amount of the flavour that is sold the most in the restaurants.

## Expected Submission
A notebook showing the results of your analysis.


## Evaluation
A good representation of your results.","","Maximum Amount","","","1"
"6774","2869","8812775","11/07/2021 09:40:32","just do it
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Ships discrimination","","","0"
"171","2916","1315079","12/09/2019 20:52:33","## Task Details
The column header `sensor readings (16 channels)`  is actually 16 separate columns.  Would need some help to figure out what the 16 columns actually represents.

## Expected Submission
Name and description of the 16 columns.

## Evaluation
Column name with units and description of what each column represent.","","Add missing column header (ethylene_CO.txt)","","","1"
"172","2916","1315079","12/09/2019 20:55:14","## Task Details
The data file is space delimited which is not well supported by Kaggle datasets.  It would be great if someone can upload a new version of the file replacing spaces with comma so it can be previewed in the data explorer.

## Expected Submission
A new dataset (version) with spaces replace by comma.

## Evaluation
The file can be rendered in the preview.","","Reformat the file (ethylene_CO.txt)","","","2"
"2840","3064","1468205","12/02/2020 16:28:42","## Task Details
Every task has a story. Tell users what this task is all about and why you created it : Analysing Titanic Data Set

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain? : For Study and research

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? Visualization

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Titanic EDA","Titanic EDA","","1"
"2692","3064","4533747","11/13/2020 14:49:02","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Created This task to understand Titanic dataset & learn different visualization techniques to plot data with different combinations.
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
solution contains how to understand Titanic data.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
The more visualization for datasets, it helps to understand data.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Titanic Dataset: EDA to Explore/Visualize","EDA to Explore/Visualize Titanic Dataset","","4"
"3515","3147","1399145","02/16/2021 08:31:54","## Task Details
about minist

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","w33ss3","","","0"
"1385","3173","2176094","07/16/2020 14:58:48","## Task Details
I am just testing the dataset now. 

Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","SQL Student Assignment - DMSB V1.0","","","1"
"3734","3190","4036784","03/11/2021 17:26:34",":ooooooo","","T E M I I","Tema 1","","0"
"5270","3258","7097960","07/21/2021 15:13:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
„ÖÅ„Ñ¥„Öá„Ñπ","","??????","","07/22/2021 23:59:00","1"
"5366","3405","7172099","07/27/2021 06:13:11","Task is that a Sindhi Language sentence written is an image is to be recognized.","","Singhi Sentence Recognition using Deep Learning","","","0"
"6835","3483","8278211","11/13/2021 11:59:17","Create a model to predict the price of the car

Task Details
Find the predictors and check how your model can accurately predict the price of the car

Expected Submission
1. Check the accuracy of your model
2. Highlight the bias, weights and the errors","","Predict the price of the car","","01/31/2022 23:59:00","0"
"6177","3483","8310400","09/23/2021 22:50:05","## Task Details
Select and analyse the dates about Toyota cars
## Expected Submission
Select columns and more","","Car_Sales_Toyota","","09/27/2021 23:59:00","1"
"349","3598","4276220","01/02/2020 12:40:28","If for any column(s), the variance is equal to zero, then you need to remove those variable(s).","","Mercedes-Benz Greener Manufacturing","","01/04/2020 00:00:00","6"
"422","3860","4351651","02/02/2020 12:10:34","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
1
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","MovirLens","MovirLens","02/29/2020 00:00:00","0"
"2837","3884","3438871","12/02/2020 13:08:56","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

my introduction for data processing","","Introduction","","","1"
"3811","4021","6978460","03/19/2021 11:30:20","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
nice to see you here","","ouliers","out","03/31/2021 23:59:00","1"
"438","4133","2878603","02/07/2020 14:27:58","## Task Details
The aim of this task is to identify customer intent when sending a tweet to customer support. One of the biggest challenges in NLP is the lack of labels in unstructured text data

## Expected Submission
The expected submission would outline the process to achieve an automatic classifier of tweet message intent.

## Evaluation
TBC","","Auto classification of message topics","Unsupervised NLP categorisation","","0"
"60","4369","84413","12/07/2019 22:24:28","## Task Details
Please see the ""Data"" tab for an overview of this dataset. One of the measures of interest in our research is the auditory N100 Event-Related Potential (ERP) component, which is calculated by picking a peak value (in the case of N100, a minimum value as it is a negative voltage deflection) or taking an average in a fixed time-window. For example, in [this kernel](https://www.kaggle.com/broach/replicating-did-i-do-that-paper-analyses-with-r), we took the average value 80-100ms after tone onset to quantify the N100. You can see this in the group average waveforms here:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F84413%2Fcc2205a83b6042cc09aa73cfbdaf6efe%2FerpResult.png?generation=1575757455542206&alt=media)

In this study, we observe that the N100 amplitude is reduced or suppressed when subjects are pressing a button to generate the sound (button-tone, condition=1)
compared to when they are passively listening to the playback of tones (playback, condition = 2).  The task is to build a single trial classification algorithm that can correctly differentiate between self-generated (condition 1) and playback (condition 2) trials

## Expected Submission
Users should submit a csv file with 4 columns, 3 from the original data files, and one column containing the prediction or probability of being a playback trial: subject, trialNumber, Condition, probability

## Evaluation
Area under the curve (AUC) is the best metric to evaluate your predictions.","","Classify EEG trials","binary classification task","","2"
"52","4369","84413","12/06/2019 20:28:08","## Task Details
Please see the ""Data"" tab for an overview of this dataset.  One of the measures of interest in our research is the auditory N100 Event-Related Potential (ERP) component, which is calculated by picking a peak value (in the case of N100, a minimum value as it is a negative voltage deflection) or taking an average in a fixed time-window.  For example, in [this kernel](https://www.kaggle.com/broach/replicating-did-i-do-that-paper-analyses-with-r), we took the average value 80-100ms after tone onset to quantify the N100.  You can see this in the group average waveforms here:

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F84413%2F50e909b2db93bde3b4561621c9a2672d%2FerpResult.png?generation=1575757433239185&alt=media)


Measurement of the N100 is most often done on the averaged ERP, and that average is calculated across many trials.  However, understanding single trial dynamics of ERP components is of interest in various fields.  The EEG signal is noisy on any given trial, and only a limited number of trials (max 100) are available per subject per condition in this study.  Thus, developing a machine learning algorithm to accurately predict single trial N100 is not an easy task.

## Expected Submission
The ideal submission file would be a csv with subject, trial, condition, and predicted N100 at electrode FCz as its four columns for all trials in the data set ""../input/ERPdata.csv"" from conditions 1 (button-tone) and 2 (playback) only as the third condition (control button press) had no N100.  See [this example kernel](https://www.kaggle.com/broach/predicting-single-trial-n100-starter) to get started

## Evaluation
The evaluation metric for this task is Root Mean Squared Error.  To make the problem even more difficult, one could consider training models exclusively on early trials (e.g., trials 1 through 70) and evaluating prediction exclusively on later trials (e.g., trials 71-100) so that your model is forced to predict N100 from future events.","","Predicting Single Trial N100","Event-related potential (ERP) amplitude regression problem","","1"
"1418","4458","4760409","07/19/2020 13:31:09","## Task Details
Every task has a story. You should tell us best equation for wine quality using Regression 


## Expected Submission
We expect to share the equation which can help to predict the quality of the wine using available data.","","Red Wine Quality Assurance","","","31"
"1738","4458","4414530","08/18/2020 13:37:48","## Task Details
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use the K Nearest Neighbours algorithm to make a prediction on the 'quality' column of the dataset. You are encouraged to explore the different parameters you can work with in your model and understand the importance of data understanding and feature selection.
(Note: Beginners can use the ""KNeighborsClassifier"" class available under ""sklearn.neighbors"" )

## Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of the model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'accuracy' metric.

## Evaluation
The aim is to understand the KNN algorithm and its parameters, and evaluation would be based on the accuracy of the model.","","Red Wine Quality Prediction Using KNN","","","60"
"4295","4458","6537397","05/07/2021 11:27:34","## Task Details
In this task, you are required to classify wine quality into three categories.
Quality Below 6 is Bad.
Quality 6 is Normal.
And quality above 6 is Good.

Based on this classification, create the best ML model that provides best accuracy.
Accuracy score of more than 70% is preferred. Anything less than that is automatically considered not feasible.","","Classify into three categories","","","3"
"3358","4458","4589594","01/31/2021 08:34:00","## Task Details

Predict the Quality Of the Wine

## Expected Submission

Predict the Quality Of the Wine

Tableau Representation Of the Wine Quality Predictions
Considered Parameters For Prediction using Random Forest Classification :
1)fixed acidity, 
2)volatile acidity,
3)citric acid,
4)residual sugar,
5)chlorides,
6)free sulfur dioxide,
7)total sulfur dioxide,
8)density,
9)pH,
10)sulphates,
11)alcohol,
12)quality","","Wine_Quality_ Predictions","Wine_Quality_ Predictions","02/28/2021 23:59:00","7"
"4569","4458","7486601","05/30/2021 18:09:10","Get Information from the Data.","","Get Insights from the Data.","","","1"
"4826","4458","7454638","06/18/2021 06:38:54","##Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Red Wine Quality Accuracy","","","1"
"4684","4458","7618590","06/08/2021 17:11:43","Predict the quality of wine","","Wine Quality Predictions","","","1"
"4685","4458","7618590","06/08/2021 17:12:02","EDA for wine","","EDA for Wine","","","1"
"4687","4458","7622857","06/08/2021 17:34:42","EDA for wines","","EDA for wines","","","0"
"4733","4458","7643228","06/11/2021 14:25:11","Prediction using Radom Forest","","Prediction using Radom Forest","","06/12/2021 00:00:00","1"
"2572","4538","6060150","10/29/2020 15:32:06","# hey every budy ## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Mitezh","Nothing to do but let's play","10/31/2020 23:59:00","1"
"258","4549","3882035","12/15/2019 19:10:38","## Task Details
Figure out if there is a correlation between the number of views and the number of likes.

## Expected Submission
A notebook showing the correlation between the number of views and the number of likes by means of a visual representation.


## Evaluation
A clear visual representation showing the correlation between the number of likes and the number of views.","","Correlation between views and likes","","","131"
"259","4549","3882035","12/15/2019 19:19:15","## Task Details
Figure out the distribution of attributes like ""views"", ""comments"" and ""likes"".

## Expected Submission
A notebook with visual representations of the distribution of the attributes you have selected. This will be in the form of histograms.

## Evaluation
Clear visual representations of your results.","","Distribution of attributes","","","24"
"270","4549","4211598","12/16/2019 10:20:59","Top game
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Top game","2019","","10"
"478","4549","1751987","02/24/2020 18:26:27","This is my first test task on kaggle. 
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","YouTube","","","4"
"611","4549","4128700","03/20/2020 21:04:28","## Task Details
No. likes depends on several factor like comment disabled, number of views etc.","","Predict number of likes","","","20"
"5013","4549","5377216","07/06/2021 01:32:38","## Task Details
I'd imagine it'd be interesting (culturally) to see what videos people from around the world prefer, but also which videos are loved equally on a global level.

## Expected Submission
- A website allowing users to sort by country but also by a ""globally loved"" category
- Allow users to watch the videos (add the link to it)
- (bonus) generate/fetch video thumbnails

## Evaluation
- Allowing users to intuitively explore the dataset and get a general idea of what is the most loved content based on countries/regions worldwide

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create a website based on the datasets","","","1"
"5015","4549","5377216","07/06/2021 03:30:20","## Task Details
I thought it'd be a good exercise to create a search-bot using NLP-based query analysis.

## Evaluation
Gracefully answer following questions and variations thereof:
- Which channel has the most views globally?
- What channel has the most views in the US?
- Which channel has the most disliked and liked videos?
- What video is the most watched video on a global scale?

#### This might help
https://spacy.io/","","Create a NLP-based interactive Knowledge Base","answer questions such as ""What video is the most watched video on a global scale?""""","","1"
"1451","4555","5422391","07/22/2020 10:37:29","Can you translate all courses content into urdu?","","Translation","Course Content Translation","07/31/2020 00:00:00","0"
"216","4685","2370491","12/12/2019 21:08:16","It would be useful to have a small high-level summary of the data.

It should be either a Kaggle notebook or a .pdf file containing images which rapidly allow a reader to understand the dataset. 

## Evaluation
A good solution will summarise the key points of the data set in as few lines as possible.","","High level summary","","","2"
"3282","4685","5994879","01/26/2021 22:04:56","## Task Details
Primary evaluation of energy consumption trajectory

## Expected Submission
Notebooks&Scripts

## Evaluation
Strict coding practice plz. No breaking the code structure.

### Further help
On its way...","","cons_predict","","","0"
"727","5044","844176","04/10/2020 12:45:15","## Task Details
The story of this task is absolutely natural.
The first thought when we explore MNIST and other similar datasets is to expand classification and recognition by algorithms to all human usable symbols.

## Expected Submission
Classification for this dataset has four main subtasks:
1) recognition of image backgrounds (striped=&gt;0, gridded=&gt;1, no background=&gt;2, graph paper=&gt;3);
2) labeling letter images (33 classes of symbols);
3) multilabel classification (backgrounds and letters at the same time);
4) recognition inside words.

## Evaluation for each subtask
1) very easy to have high results in the prediction accuracy (even 100%);
2) it's possible to reach the 95-98%-level and higher with applying different techniques;
3) the level of 99% and 95% is a good result in this case;
4) an ability to ""read"" a whole handwritten word (""symbol by symbol"") using algorithms.","","Classification","Symbol Recognition","","9"
"729","5044","844176","04/10/2020 14:51:36","## Task Details
The main idea is ""teaching"" algorithms on how to generate images using data examples. Here they will learn to write in Russian :).

## Expected Submission
We can highlight several subtasks:
1) generating random images based on the data with letters without graphical backgrounds;
2) generation a certain symbol using on its images;
3) mixing style of letter images with other symbol types (traditional decors, for example) and creating new handwritten fonts automatically.

## Evaluation for each subtask
1) in many cases based on ""loss"" metrics and subjective human opinion (is it similar and recognizable or not);
2) more accurate math measures with metrics like loss functions;
3) subjective human opinion (is it a nice looking handwritten font or not).","","Image Generation","Generating symbols and styling them","","0"
"732","5044","844176","04/10/2020 17:11:57","## Task Details
We could find math formulas for curves to draw each symbol.
It can be regression algorithms for fitting coefficients of the ""universal"" math formula (trigonometrical, exponential, power, etc.; maybe a combination of all these types of functions).
Example of a possible function type:
[Funny Functions. Wolfram Alpha](https://www.wolframalpha.com/input/?i=awesome+face+curve)

## Expected Submission
Steps (as I can imagine):
1) math curves for all symbols;
2) exploration for finding a ""universal form"" of math formula of symbol curves.

## Evaluation
1) exact formulas that reflect the symbol image in the coordinate plane;
2) a ""universal form"" of math formula of symbol curves with sets of coefficients for every symbol as a result.","","Symbol Curves","Math functions for ""writing"" letters in a coordinate plane","","1"
"824","5227","4879449","04/29/2020 03:23:14","California housing prices data analysis","","California Housing Prices","California Housing Prices","05/01/2020 00:00:00","28"
"2176","5227","4257848","09/21/2020 05:37:24","## Task Details
The goal of the analysis is to find the base line factor affecting the value of house holds in California. 

## Expected Submission
Output file with predictor variables and the predicted vs actual values. 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
The good solution will the be the minimum mean-absolute error value with proper model justification. 
### Further help
https://www.kaggle.comcalifornia-housing-prices","","Xgboost model for California house holds value predictions","","","11"
"4185","5669","7267323","04/25/2021 09:04:30","## Task Details
Data has to have consistent time-zone otherwise are useless.
Check records
Excel D2019-21 (GMT-4 Eastern US&Canada)
2019/07/01 0:00:00	India                           	Low Volatility Expected         	Nikkei Markit Manufacturing PMI 
                                                                                
and

https://www.investing.com/economic-calendar/ (GMT-4 Eastern US&Canada)
2019/07/01 1:00:00   	India                           	Low Volatility Expected         	Nikkei Markit Manufacturing PMI 

There is 1 hour shift - ERROR

and same records for 2020
Excel D2019-21 (GMT-4 Eastern US&Canada)
2020/07/01 1:00:00	India                           	Low Volatility Expected         	Nikkei Markit Manufacturing PMI 
                                                                                
and
https://www.investing.com/economic-calendar/ (GMT-4 Eastern US&Canada)
2020/07/01 1:00:00   	India                           	Low Volatility Expected         	Nikkei Markit Manufacturing PMI 

There is 0 hour shift in 2020 - OK


## Expected Submission
One timezone across data.

## Evaluation
Same time zone

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Wrong time zone","Changing time zone Eastern US&Canada to Central US&Canada","04/26/2021 23:59:00","0"
"1268","5778","907203","07/06/2020 13:38:38","## Task Details

## Expected Submission

## Evaluation

### Further help","","Clasification using Airbnb dataset","","11/30/2020 00:00:00","0"
"317","5839","2478334","12/24/2019 15:49:23","classification  nih-chest-xrays","","nih-chest-xrays","Projet ddeep learning","01/20/2020 00:00:00","17"
"759","5857","4845877","04/15/2020 19:07:10","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
Pattert project","","Patrarn","","05/04/2020 00:00:00","18"
"1401","5914","1150494","07/17/2020 12:08:23","## Task Details

Some Indian cities have alternative names that may appear in text.

It is common for data scientists to encounter this scenario while working with documents that are aged.

## Expected Submission

Can you write a kernal to replace all instances of city_alias with city_name in the document?

## Evaluation

A non exhaustive list is shared below:

```
allahabad &lt;-&gt; **prayagraj**
bareilly &lt;-&gt; bareli
bengaluru &lt;-&gt; bangalore
chennai &lt;-&gt; madras
coimbatore &lt;-&gt; kovai &lt;-&gt; koyamuthur &lt;-&gt; koyampattur
cuttack &lt;-&gt; kataka
gurgaon &lt;-&gt; **gurugram**
guwahati &lt;-&gt; gauhati &lt;-&gt; pragjyotishpura
hubballi &lt;-&gt; hubli
hyderabad &lt;-&gt; bhayanagar
indore &lt;-&gt; indhur
kanpur &lt;-&gt; cawnpore
kochi &lt;-&gt; cochin
kolkata &lt;-&gt; calcutta
kozhikode &lt;-&gt; kozhikkod &lt;-&gt; calicut
mangaluru &lt;-&gt; mangalore
meerut &lt;-&gt; meratha
mumbai &lt;-&gt; bombay
mysuru &lt;-&gt; mysore
puducherry &lt;-&gt; pondicherry
pune &lt;-&gt; poona
rajahmundry &lt;-&gt; rajamahendravaram &lt;-&gt; rajamahendri
shimla &lt;-&gt; simla
thane &lt;-&gt; thana
thiruvananthapuram &lt;-&gt; trivendrum &lt;-&gt; ananthapuri
trichy &lt;-&gt; tiruchirappalli &lt;-&gt; trichinopoly
vadodara &lt;-&gt; baroda
varanasi &lt;-&gt; benaras
visakhapatnam &lt;-&gt; vizag &lt;-&gt; waltair
```

## Resources

The list of geotaged cities occuring as categories in the dataset is available [here](https://gist.github.com/therohk/ca3a7c44d4241002922220f84483c17a).

It is convenient for apps requiring the data in json format for plotting.

Reference kernal: [here](https://www.kaggle.com/therohk/india-urban-category-fluctuations).

Trivia: In India, all latitides are north, all longitudes are east.
Trivia: Goa is only one within the list of categories that is not a city but a state.","","City Names and their Alternatives","Text modification and cleanup challenge","","3"
"277","6012","3882035","12/16/2019 10:51:04","## Task Details
What is the most minimum temperature that has ever been recorded?

## Expected Submission
A notebook showing the results of your solution.

## Evaluation
A clear representation of the results.","","The most min temp","","","25"
"278","6012","3882035","12/16/2019 10:52:38","## Task Details
What is the most maximum temperature that has ever been recorded.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
Clear results from the analysis.","","Most max temp","","","17"
"279","6012","3882035","12/16/2019 10:54:24","## Task Details
What is the largest amount of rainfall that has ever been recorded.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of the results.","","Largest amount of rainfall","","","11"
"2865","6012","5818649","12/05/2020 16:11:18","Would you please update this useful data , and enter new data for 2017 , 2018 , 2019, and 2020? Thank you","","Update data up to 2020","","","11"
"4038","6012","1685485","04/10/2021 20:36:14","Analyze how each feature behaves in a different location and find out max and min value of the feature per location","","Analyse features wrt locations","","","1"
"3737","6012","6580290","03/11/2021 19:11:22","Task Details
Cleaning dataset is the most challenging step of regression. Going through the steps is very important to achieve the desire decent dataset.

Expected Submission
In this task, we expected to clean dataset and deal with Multicollinearity and Outliers. Then Standardize the dataset and choose the best regression to predict the Rain Tomorrow.

Evaluation
The higher accuracy is the best result.","","Prediction Rain Tomorrow","Choose the best Regression","","4"
"3612","6012","4430211","02/27/2021 13:20:05","## Task Details
How many time occurred 3 days in a row of raining in each location.
Example: Albury [2008/12/13, 2008/12/14, 2008/12/15]

## Expected Submission
A noteboook with the solution

## Evaluation
Clear results.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","How many times has it rained three consecutive days?","","","3"
"3646","6012","3041627","03/02/2021 21:54:02","## Task Details

It could be very helpful to test the impact of exact geographic coordinates of locations on the outcome variable. For example, Sydney has a latitude `-33.865143` and longitude `151.209900`","","Add the exact geographic coordinates of the location","Latitude and Longitude","12/31/2021 23:59:00","2"
"3364","6012","6572258","02/01/2021 00:10:55","## Task Details
Main Task

## Expected Submission
A notebook detailing a model that gives the best accuracy.","","Predict next-day rain","","","20"
"2331","6087","5581528","10/05/2020 03:27:55","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Sir i want to code effectively","","Data analysis","Data","10/16/2020 23:59:00","17"
"3162","6152","5322588","01/09/2021 15:06:04","## Task Details
For Learning Purpose, you are expected to build:

Split data into feature and label
Split data into train and test
Mean
Variance
Covariance
Mean Squared Error (MSE)
Coefficients of Linear Regression (Based on training data)
Predict the label for testing data
Evaluate the model using MSE","","Apply Simple Linear Regression from scratch","Build everything from base","","0"
"861","7001","4922195","05/07/2020 21:00:55","## Task Details
I need the census tables by Zip code rather than Tract ID. I need a mapping of the Zip codes from Tract ID of the Census demographics. 

 

## Expected Submission
N/A

## Evaluation
N/A

### Further help
If you need additional inspiration, check out 
https://www.uszip.com/zip/85281","","Tract IDs to Zipcodes","Census Tract ID to Zipcode","06/25/2020 00:00:00","1"
"411","7448","844176","01/28/2020 09:23:52","## Description of Directions

1. To expand the database in all directions: the number of brands, products, and quantity of photos inside each brand or product type. The more material for research, so the more interesting experiments can be made. I consider it is possible to recognize such indicators as an individual style by machine learning algorithms at the level of 95-99% of human recognition for this indicator.

2. To supplement the base with some indicators related to customer preferences. For example, which products and styles cause the greatest number of likes or dislikes. We can take into account collections of different years to trace the influence of fashion on success at the moment and successful design ‚Äúfor centuries‚Äù (the one that will be popular 10 and 20 years later).

## Expected Submission
Users could apply any algorithms to reach 95-99% of human detection for brands (it should be based on many experiments with the same sets of photos and with thousands of human recognitions) or increase the level of algorithm classification in general.

Other expected results are the new datasets combined with this one and increasing the fields of experiments.

## Evaluation
Evaluations of task realizing are very easy in this case: what levels of recognition we could reach and what kind of interesting additional indicators we could collect.","","Expanding","The Main Task for Every Visual Data","","1"
"4519","7871","590653","05/28/2021 13:05:20","Which area of interest/expertise is in abundance in Pakistan and where we need more people?","","Which area of interest/expertise is in abundance in Pakistan and where we need more people?","","","0"
"4520","7871","590653","05/28/2021 13:05:48","How many professors we have in Data Sciences, Artificial Intelligence, or Machine Learning?","","How many professors we have in Data Sciences, Artificial Intelligence, or Machine Learning?","","","0"
"4521","7871","590653","05/28/2021 13:06:17","Which country and university hosted majority of our teachers?","","Which country and university hosted majority of our teachers?","","","0"
"4522","7871","590653","05/28/2021 13:06:38","Which research areas were most common in Pakistan?","","Which research areas were most common in Pakistan?","","","0"
"4523","7871","590653","05/28/2021 13:07:05","How does Pakistan Student to PhD Professor Ratio compare against rest of the world, especially with USA, India and China?","","How does Pakistan Student to PhD Professor Ratio compare against rest of the world?","","","0"
"4524","7871","590653","05/28/2021 13:07:30","Any visualization and patterns you can generate from this data","","Any visualization and patterns you can generate from this data?","","","1"
"589","8782","4000116","03/15/2020 17:36:05","## Task Details

Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Flowers","","","13"
"1220","8782","2551291","06/28/2020 19:52:34","## Task Details
The aim of this task is to identify which class of flower the image belongs to, by getting the maximum accuracy on the Test set.
The test set will be fixed at 20% of the entire dataset. The rest of the dataset can be used for training and validation and the split for those 2 can be done as per the user's discretion.

Participants will be allowed to manipulate the dataset for increasing the quality or quantity of images in the dataset but must make those datasets public and available to all before submission, to maintain a level playing field for all participants; so only submit when you are sure of your submission!


## Expected Submission
The submission for this task must be a notebook with the maximum accuracy you can achieve on the test set, the outputs must be visible in the notebook. The picking of images for each set must be done at random, and the random seed to be used to make sure that all participants get similar images in every set will be **43**. Submissions not using this value as the seed will not be counted.

## Evaluation
The solution notebooks will run and reviewed by the evaluators individually before assigning you a final score, on the notebook.

### Further help
If you need additional inspiration, check out this notebook for this task:
- https://www.kaggle.com/rishitchs/kernel4375adc82d


Good luck.","","Get the Highest Accuracy on the Test Set","Get the Highest Accuracy on a test set of 20% of the entire data set.","","17"
"2362","9232","5364587","10/07/2020 14:50:46","Types of anemia 
RBCs shapes","","Heamatology","Anemia","","8"
"487","9726","4575948","02/28/2020 12:12:51","## Task Details
Create a dataset

## Expected Submission
As soon as possible

## Evaluation
Fast","","Handwritten Cursiv Text Recognition system","","03/15/2020 00:00:00","11"
"26","9906","1314380","12/02/2019 17:57:22","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","October: Disability Awareness Month","Create and describe a plot that relates to ""October: Disability Awareness Month""","11/01/2020 00:00:00","1"
"4234","10100","2986787","04/30/2021 19:11:54","## Task Details
We need a balanced dataset to create a more stable sentiment analysis model.

## Expected Submission
Submission can be code, algorithm, logic, 3rd party libraries

## Evaluation
Simplicity is the only criterion since it rocks.","","Balance the Yelp Review Dataset","Balance the Positive and the Negative reviews in the dataset","","1"
"19","10128","1314380","12/02/2019 17:50:39","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","March: Women‚Äôs History Month","Create and describe a plot that relates to ""March: Women‚Äôs History Month""","04/01/2020 00:00:00","1"
"474","10449","1997853","02/23/2020 15:32:06","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluat","","monkey","monkey","02/24/2020 00:00:00","4"
"4566","10624","7486601","05/30/2021 17:42:16","Get Something From the Data.","","Get Insights from the Data","","","2"
"428","10832","3945997","02/04/2020 21:14:13","## Predict Bone Age from X-ray

## Expected Submission
N/A

## Evaluation
N/A","","Predict Bone Age from X-ray Images","","","5"
"1896","11073","1761512","08/30/2020 17:02:13","## Task Details
Create a clustering model using Python language to find how many similar complaints are there in relation to the same bank or service or product. This model will be used to assist banks in identifying the location and types of errors for resolution leading to increased customer satisfaction to drive revenue and profitability.","","Consumer Complaints","","","0"
"3330","11142","6459396","01/30/2021 08:13:13","Predicts whether HR will leave or not","","Logistic Regression Model on Why HR Leaving","","","17"
"2855","11309","5651445","12/04/2020 07:45:40","## Task Details
Do some analysis on the data set and find the most important airport .

## Expected Submission
submission expected before 8th December 2020. The output should be an airport which is the most important or the one which might become the most important in some time.

## Evaluation
Based on visualization of the data set , how you find the most important node.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Centrality Measures","Find the most important city using betweenness centrality and closeness centrality measures.","12/08/2020 23:59:00","0"
"4045","11616","6589914","04/12/2021 04:30:01","Using the ratings and movie counts determine the best actor of Bollywood ..","","BEST ACTOR OF BOLLYWOOD","","","0"
"3188","11657","6517819","01/14/2021 04:21:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. Titanic Dataset EDA to analyze data

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
Titanic Dataset different EDA
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
more the visualization, better it is.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Titanic Dataset EDA","Titanic Dataset EDA","","9"
"3200","11657","6528036","01/15/2021 10:26:42","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. Learning algorithms

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain? Learning algorithms

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? Learning algorithms by visualizing

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Learning algorithms","Learning algorithms","","5"
"1034","11657","4748969","06/02/2020 15:24:23","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Titanic_RadomForest","","","11"
"5011","12414","7840174","07/05/2021 14:14:30","calculating the average for each base system used by the county","","Average","Computer science","07/06/2021 23:59:00","0"
"3419","12713","1628913","02/06/2021 14:33:29","## Task Details
We want to know if weather events like wind, snow, extreme cold, or rain has an impact on scoring and betting lines/odds.

## Expected Submission
Users should submit a dataset that with fields for weather events, temperature, wind for the game dates in the spreadspoke_scores.csv list of NFL games since 1966. Home stadium is the location of the game. We have partial data already in our data set as well as games played in a dome (i.e., no weather event).

## Evaluation
Completeness of weather event data is the key.","","Add game weather info","","02/06/2025 23:59:00","1"
"3420","12713","1628913","02/06/2021 14:35:54","## Task Details
Can we gain insights on betting lines and odds from team's historic offense and defense stats such as yards gained, yards given up to an opponent, and turnovers.

## Expected Submission
Submission should include spreadspoke_scores.csv with team home/away offense yards and/or defense yards and turnovers by team.

## Evaluation
Completeness of data is key.","","Add yardage and turnover data","","02/06/2025 23:59:00","1"
"3421","12713","1628913","02/06/2021 14:37:58","## Task Details
Injured star players impact game outcomes as well as betting lines/odds.

## Expected Submission
Submit dataset to complement spreadspoke_scores.csv game data with either a yes/no field for a key player injured, if regular QB started the game or not, or a list of typical starting players that didn't play for that game.

## Evaluation
Completeness of data is key.","","Key player injury data","","02/07/2025 23:59:00","1"
"5009","13668","6332659","07/05/2021 13:13:52","Make a line graph to show the Sentences spoken by season from Season 1 to season 7.","","Words per Season","","","1"
"740","13720","4779637","04/11/2020 15:58:36","After performing Backward elimination and Multiple Linear regression with Significant Value=0.05, 
I got predictive variable as folowing:
1)Age
2)BMI
3)Children
4)Smoker
Please Tell me that am i Right or Wrong?","","Find Predictive Variables","","","83"
"696","13720","4362469","04/05/2020 17:56:35","This task is about
predict the cost on the basis of male and female.","","PREDICT THE DATA SET ON BASIC OF MALE AND FEMALE","","04/10/2020 00:00:00","90"
"6097","13797","2091150","09/16/2021 03:53:47","## Task Details
Need to find the model accuracy in order to understand which one is working verry good.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Model Accuracy","Best Model","12/16/2022 23:59:00","0"
"3199","13996","6484820","01/15/2021 09:18:32","‚ÄúÈ¢ÑÊµã‰øùÁïôÂÆ¢Êà∑ÁöÑË°å‰∏∫„ÄÇÊÇ®ÂèØ‰ª•ÂàÜÊûêÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂÆ¢Êà∑Êï∞ÊçÆÔºåÂπ∂ÂºÄÂèëÈíàÂØπÊÄßÁöÑÂÆ¢Êà∑‰øùÁïôÁ®ãÂ∫è„ÄÇ‚Äù [IBMÁ§∫‰æãÊï∞ÊçÆÈõÜ]
ÊØèË°å‰ª£Ë°®‰∏Ä‰∏™ÂÆ¢Êà∑ÔºåÊØèÂàóÂåÖÂê´Âú®ÂÖÉÊï∞ÊçÆÂàó‰∏≠ÊèèËø∞ÁöÑÂÆ¢Êà∑Â±ûÊÄß„ÄÇ

Êï∞ÊçÆÈõÜÂåÖÂê´ÊúâÂÖ≥‰ª•‰∏ã‰ø°ÊÅØÔºö

Âú®‰∏ä‰∏™ÊúàÂÜÖÁ¶ªÂºÄÁöÑÂÆ¢Êà∑‚ÄìËØ•ÂàóÁß∞‰∏∫‚ÄúÂÆ¢Êà∑ÊµÅÂ§±‚Äù
ÊØè‰∏™ÂÆ¢Êà∑Â∑≤Á≠æÁΩ≤ÁöÑÊúçÂä°-ÁîµËØùÔºåÂ§öÊù°Á∫øË∑ØÔºå‰∫íËÅîÁΩëÔºåÂú®Á∫øÂÆâÂÖ®ÔºåÂú®Á∫øÂ§á‰ªΩÔºåËÆæÂ§á‰øùÊä§ÔºåÊäÄÊúØÊîØÊåÅ‰ª•ÂèäÊµÅÁîµËßÜÂíåÁîµÂΩ±
ÂÆ¢Êà∑Â∏êÊà∑‰ø°ÊÅØ-‰ªñ‰ª¨Êàê‰∏∫ÂÆ¢Êà∑ÁöÑÊó∂Èó¥ÔºåÂêàÂêåÔºå‰ªòÊ¨æÊñπÂºèÔºåÊó†Á∫∏ÂåñË¥¶ÂçïÔºåÊØèÊúàË¥πÁî®ÂíåÊÄªË¥πÁî®
ÊúâÂÖ≥ÂÆ¢Êà∑ÁöÑ‰∫∫Âè£ÁªüËÆ°‰ø°ÊÅØ-ÊÄßÂà´ÔºåÂπ¥ÈæÑÊÆµ‰ª•Âèä‰ªñ‰ª¨ÊòØÂê¶Êúâ‰º¥‰æ£ÂíåÂèóÊäöÂÖª‰∫∫
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","??????","","","17"
"624","14295","4036682","03/24/2020 08:28:05","how to find the approval or not?","","APPROVAL????","","","0"
"287","14506","3882035","12/16/2019 21:07:24","## Task Details
Which city has the cheapest restaurants?

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your results. A comparison can be made with other cities.","","City with cheap restaurants","","","18"
"288","14506","3882035","12/16/2019 21:09:12","## Task Details
Which city has the most expensive restaurants.

## Expected Submission
A notebook with the results of your analysis.

## Evaluation
A clear representation of your results. A comparison can also be made with other cities.","","City with most expensive restaurants","","","1"
"289","14506","3882035","12/16/2019 21:12:21","## Task Details
Which city has restaurants that are reviewed as excellent by customers the most.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your analysis. Comparisons can also be made with other cities.","","City with excellent rating","","","4"
"290","14506","3882035","12/16/2019 21:14:27","## Task Details
Which city has restaurants that have the largest number of votes from customers.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
Clear representation of your results. Comparisons can also be made with other cities.","","City with largest number of votes","","","3"
"291","14506","3882035","12/16/2019 21:16:11","## Task Details
Which restaurant has the largest number of votes from the customers.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your results.","","Restaurant with largest number of votes","","","3"
"873","14701","5060497","05/10/2020 17:14:49","[The SNAP combined Facebook dataset can be downloaded from:](https://snap.stanford.edu/data/facebook_combined.txt.gz)
`import networkx as nx;
from random import random;
# Read data from the dataset, and create graph G_fb
G_fb = nx.read_edgelist(""facebook_combined.txt"", create_using = nx.Graph(), nodetype=int);
# Show the number of edges in G_fb
print(""edges = "" + str(G_fb.number_of_edges()));
# Show number of nodes in G_fb
print(""nodes = "" + str(G_fb.number_of_nodes()));
# TASK1. Now your task is to compute the probability whether there is an edge between two v
## edge_probab = ...
# TASK2. Compute the ACC (average clustering coefficient) of G_fb
# (consult the NetworkX manual or the video lecture for the correct function which does it)
## av_clust_coeff = ...
# Now we have to generate a random graph. First we initialize it
G_rand = nx.Graph();
# TASK3. generate edges in G_rand at random:
for i in range(0,k) :
 for j in range(0,i) :
 # Add an edge between vertices i and j, with probability edge_probab (as in G_fb)
 # ...

# Now we print out the number of edges and the ACC of the new graph
print(""rgraph_edges = "" + str(G_rand.number_of_edges()));
# av_clust_coeff = ...
print(""rgraph_acc = "" + str(av_clust_coeff));
# The results which should be submitted to the grader include the ACC of G_fb and of G_rand`","","Clustering data: real vs ransdom","calculate a particular graph parameter on a real social network dataset and compare it to the value of the same parameter on a randomly generated graph. The parameter is the average clustering coefficient (""ACC""):acc = nx.average_clustering(G)","05/13/2020 00:00:00","21"
"5731","14701","6224917","08/14/2021 09:22:14","choose best clustering algorithm for this dataset and train it on this dataset
It is better to write your reason of how you select the algorithm and evaluate your model with some of evaluate metrics such as  silhouette score or SSE or Davies-Bouldin Index or graph_based .","","Train a clustering algorithm on this dataset","choose best clustering algorithm for this dataset and train it on this dataset","","5"
"6","14872","2931338","11/27/2019 07:23:17","##**Task Details**

Using the supplied predictive variables (GRE score, TOEFL score, University Rating, etc) to predict the likelihood of admission of a  new candidate. 

##**Evaluation Criteria**
The best model should be the one that evaluates to have the lowest RMSE overall, and please indicate the error you get on validation set containing the last 100 observations. 

##**Expected Submission**

Please submit a Kernel where the final cell outputs the RMSE score on the final 100 observations.","","Predict Likelihood of Admission","","","95"
"437","14872","4451580","02/07/2020 08:13:35","#j# Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Amsy12","Islamic university of madinah","02/08/2020 00:00:00","4"
"459","14872","3268512","02/19/2020 00:12:09","GRE scores are obviously important, but how important? Is there a certain point where spending tens of hours to improve a few points not worth it?

Use machine learning techniques to plot a curve: GRE score vs. chance of admission. Post the RMSE for the machine learning model used.","","How important are GRE scores?","Use machine learning methods to find how important GRE scores are to final admission standards.","","20"
"17","15739","1314380","12/02/2019 17:47:57","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Jan: Human Trafficking Prevention Month","Create and describe a plot that relates to ""Jan: Human Trafficking Prevention Month""","02/01/2020 00:00:00","1"
"4525","15752","7435920","05/28/2021 13:08:20","# Identify number of requests for each hour and plot the same using line plot. Use the following instructions 

# Convert the data type of time column to datetime using pd.to_datetime() with appropriate time  format 
# Use resample function to identify number of requests per hour and plot the same using line plot 
# Interpret the chart and identify from which hour of the day, the number of page visits increases","","Identify number of requests for each hour and plot the same using line plot.","Use the following instructions","","0"
"988","16374","3503081","05/27/2020 20:05:36","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)Bsjjdhdjdjdjxjxsj","","Turjya","","","0"
"467","17810","4535098","02/21/2020 14:20:40","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
As a binary classifier, able to detect pneumonia using chest x-rays.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
Data set will have 100 images from the ‚Äúnormal‚Äù class and 100 images from the ‚Äúpneumonia‚Äù class. 
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
classifier is able to detect correctly base on given values.","","Classification model detect pneumonia","","02/27/2020 00:00:00","93"
"4505","17810","5291112","05/26/2021 22:46:39","## Task Details
Classification is the main task in this dataset but in order to bring a real help to doctors it is essential to give them tools to study our model. Then, i propose you to show what kind of information you can learn from your model and to display them in a fancy way.

## Expected Submission
Notebook with uncertainty measurement and interpretable tools as shap values or bayesian models.","","Interpretable tools and uncertainty measurement.","Go behind classification and propose interpretable tools and uncertainty measurement.","","5"
"3971","17810","7027389","04/05/2021 14:47:29","## Task Details
Here, activation map will tell which layer had the most impact on our model's performance.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Finding the activation Map and saliency model's performance evaluation","","","3"
"3025","17860","4137085","12/23/2020 22:58:30","## Task 1
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Irish dataset","Beginner","","22"
"5500","17860","4656934","08/02/2021 11:05:18","Remove target column and build clustering model.","","Clustering","","","1"
"6815","18009","4561355","11/10/2021 16:26:58","## Task Details
Predict class label using any classification model.","","Predict Class label","","","1"
"240","19712","3882035","12/15/2019 15:27:24","## Task Details
Which gender is mostly involved in accidents at these plants?

## Expected Submission
A notebook showing the results.

## Evaluation
A clear output showing the gender that is mostly affected at these plants.","","Gender mostly involved in accidents","","","12"
"241","19712","3882035","12/15/2019 15:32:20","## Task Details
Are third parties usually involved in these accidents or it is mainly the employees? 
## Expected Submission
A notebook showing the results of the analysis.
## Evaluation
A clear output of the results after the analysis.","","Third Parties Or Employees?","","","1"
"242","19712","3882035","12/15/2019 15:34:19","## Task Details
What usually causes these accidents?

## Expected Submission
A notebook showing the results of the analysis.

## Evaluation
A clear output of the results after the analysis.","","Main cause of accidents","","","1"
"6370","19911","2628358","10/15/2021 15:15:59","## Task Details
Weather Stations in Brazil sometimes get wrong data or can not collect the data so we get NA values. We have a clean\_na function that is self explanatory, fill NA values.   

## Expected Submission
Submit a notebook that implements a better clean\_na function. 

## Evaluation
Don`t have an evaluation, i want to test other clean\_na functions with different methods to gather information to fill NA values and use the best one. 

ps: i or we in discussions will think in a evaluation method, maybe remove some lines and replace with NA ones and compare with the method","","Fill NA values","Remake clean_na function","","0"
"6371","19911","2628358","10/15/2021 15:20:01","## Task Details
In metrics.py is a create_metrics function.   
The objective is make a better one, with better and/or metrics.   

## Expected Submission
Notebook that implements a create_metrics function.  

## Awesome Submission
Generally to regression problems we make some comparison between y_i and y_pred and the sequential error problem is not solved.   
If we forecast y_1 baddly y_2 will get a bigger error than it should get if we get y_1 good. So the Awesome Submission would be a create_metrics function that implements a evaluation metric that use the sequential error information to evaluate the forecast.   

## Evaluation
The Evaluation is get a better evaluation.","","Better Evaluation Metric","make a better create_metrics function","","0"
"6359","19911","2628358","10/15/2021 12:03:16","## Task Details
Time Series - Very Straightforward - Make a Forecast

## Expected Submission
A notebook that make a forecast.

## Good Submission
Would be great if you forecast a week forward but is not necessary

## Evaluation
Evaluate using create_metrics function","","Make a model","Forecast the next week rain","","0"
"4499","20710","5049158","05/26/2021 10:26:10","This task will help users to know the building of model which would predict house prices based upon new parameters","","Model Building","Pickle file","05/27/2021 23:59:00","4"
"3517","20817","4821962","02/16/2021 15:51:16","## Task Details
The story behind this task is simple: There are a lot of codes to read text from an image and to identify stamps in a document. However, I could not find any that would read text from a stamp. Stamps come in different shapes and sizes so its not always easy to simply ran an OCR on the image. 

## Expected Submission
Users should submit Notebooks.


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Identify & Read Text From Stamps","","","0"
"2324","21128","187301","10/04/2020 11:20:35","#  Predicting ad positioning
 
## Task Details
Placement of ads on a website is the primary problem for companies that operate on ad revenue. The position where the ad is placed plays a pivotal role in whether or not the ad will be clicked.

## Expected Submission
Create a notebook, which proposes the the best position for ads placement. 

## Evaluation
You need to predict the ad position so that it is clicked by the website users.
The total clicked ad positions can be used for evaluation of the model.","","Ad placement optimization","","","1"
"2375","21143","5868699","10/08/2020 04:20:33","## Task Details
I'm trying to predict house price using various parameters","","House Price Prediction","House Price","","1"
"370","21785","1587599","01/10/2020 03:47:01","ÏòÅÌôîÎ¶¨Î∑∞ Î∂ÄÏ†ï/Í∑ºÏ†ï ÌèâÍ∞Ä ÏòàÏ∏° / ÌÖêÏÑúÌîåÎ°ú Ïä§ÌÑ∞Îîî","","Popcorn","","01/31/2020 00:00:00","0"
"311","22001","3726555","12/22/2019 11:09:58","## Task Details
We will discuss about the performance of different algorithms as a regress-or","","KC House sale price prediction using ML techniques","Comparison of algorithms","","1"
"260","22219","3882035","12/15/2019 19:52:14","## Task Details
Figure out the taxi company that is usually used in Chicago.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your analysis. A comparison can also be made with other companies.","","Taxi company usually used in Chicago","","","5"
"261","22219","3882035","12/15/2019 19:55:13","## Task Details
What is the the mode of payment that is usually used in Chicago

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your analysis.","","Mode of payment usually used","","","8"
"3588","22268","1560061","02/24/2021 06:40:59","test","","Google BigQuery","","","0"
"6312","22268","8567579","10/11/2021 13:06:02","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
I am learning the basics of SQL 
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","SQL learning","","","0"
"283","22446","3882035","12/16/2019 19:35:55","## Task Details
Come up with a graph that is ranking the programming languages according to popularity.

## Expected Submission
A notebook with a clear visual representation.

## Evaluation
A clear visual representation of the results.","","Popular languages","","","0"
"256","22526","3882035","12/15/2019 18:30:53","## Task Details
Which major category crime is mostly committed in London?

## Expected Submission
A notebook showing the results.
## Evaluation
A clear representation of the results. A comparison can be made with other crimes.","","Major Category Crime","","","4"
"257","22526","3882035","12/15/2019 18:46:33","## Task Details
Figure out the minor category crime that takes place the most in London.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your results. A comparison can also be made with other crimes that are below the top minor category crime.","","Minor Category Crime","","","0"
"167","22741","1952004","12/09/2019 19:36:44","Using kernel to create a visualization of the distribution of the most common names used in the US. Creativity points count!!!","","Create visualization for the distributions","","","0"
"4972","23651","5156746","07/02/2021 15:27:24","## Task Details
Are heart attacks and skin infections related to ingrown toenails that cause pus to build up as plaque in the body, creating cavities and ulcers?

## Expected Submission
Toe Nail Fungus test
Skin damage before and after images
techniques used to remove the ingrown nails and debride the wound.

medications used orally or topically

## Evaluation
how the skin and nail grows back after cleaning

### Further help - YouTube Channels
-Podologia Integral
-DC Foot Doctor
-DR. Toe Bro
-Jaws Podiatry 
-The Meticulous Manicurist","","Acupressure Refloxology","Brain Paranasal Area","09/30/2021 23:59:00","0"
"2772","23777","6220528","11/25/2020 06:09:47","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

test","","identify cat and dog","identify cat and dog","11/26/2020 23:59:00","5"
"3467","24658","168670","02/11/2021 18:45:37","Exploratory data analysis to understand number of landmarks overall and by country/region. Is there more landmarks for a particular region?","","How many total landmarks are there","Bonus: Landmark by country or region","","0"
"416","24759","1782365","01/30/2020 09:34:40","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?                                  ## Better understanding of the universe.","","Unlimited","","","0"
"56","25491","1156831","12/07/2019 14:51:10","## Task Details

Are you able to solve the following tasks? 

* Characters with more dialogue in The Original Trilogy

* Most repeated words in The Original Trilogy

* Use wordclouds to visually represent the most repeated words

* Most repeated bigrams (and trigrams, if you want) in The Original Trilogy

* Network of bigrams (or trigrams). Visualize all of the relationships among words

* Most repeated words for each character

* Most relevant words for each character using [tf‚Äìidf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) ([`bind_tf_idf()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/bind_tf_idf) function)

* Categorize the words in a binary fashion into positive and negative categories using the [`bing`](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) lexicon ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Categorize the words using the [`nrc`](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) lexicon ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Categorize the words using the [`AFINN`](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Sentiment analysis for each character

* Repeat some of the previous tasks for each movie separately

In the Star Wars universe, the Sith (Darth Vader or Emperor Palpatine) are associated with negative feelings such as anger, fear, hate, etc. Conversely, the Jedi (Luke Skywalker or Yoda) teach its followers to not give in to feelings of anger toward other lifeforms, which would help them resist fear and prevent them from falling to the Dark Side of the Force. According to your sentiment analysis done previously, do you notice differences between the Dark Side characters and the Light Side characters? Explain your insights! üòä 

And remember, 

""*The greatest teacher, failure is.*"" ‚Äî Yoda

## Expected Submission

A notebook with relevant visualizations and insights. 

## Evaluation

Solution will be evaluated by the reaction of the community.","","Text Mining tasks","‚ÄúDo. Or do not. There is no try.‚Äù ‚Äî Yoda","","3"
"284","25883","3882035","12/16/2019 20:35:11","## Task Details
Are people who commit traffic violation usually drunk? This can be checked for using the alcohol column

## Expected Submission
A notebook with your solution.

## Evaluation
A clear analysis with clear results.","","Alcohol And Traffic Violation","","","1"
"285","25883","3882035","12/16/2019 20:38:25","## Task Details
How many traffic violations actually lead to fatality?

## Expected Submission
A notebook showing your solution.

## Evaluation
Clear results from the analysis.","","Fatality and Traffic Violation","","","1"
"286","25883","3882035","12/16/2019 20:52:53","## Task Details
Do traffic violations usually include Seat Belt Violations? This can be analyzed by looking at the belts column and figuring out how many ""yes"" occur and then compare it to the number of ""no"".

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
Clear comparison between yes and no to come up with a good conclusion.","","Belts and Traffic Violation","","","1"
"2930","26475","1900130","12/13/2020 15:18:44","## Task Details
Looking for inspiration on this dataset.

## Expected Submission
Except of simple classification, any further usage on this dataset, looking for inspiration.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Looking for inspiration on this dataset","","","0"
"212","26658","1203505","12/12/2019 20:12:04","## Task Details

I created [a kernel](https://www.kaggle.com/nityeshaga/how-indian-developers-differ) in which I compared Indian developers with the ones from the rest of the world across a wide range of attributes. In this analysis, [I found](https://www.kaggle.com/nityeshaga/how-indian-developers-differ#AI-opinions:) that Indian developers have a considerable different aptitude with regards to various aspects of AI like - 

* Automation of jobs
* Algorithms making important decisions
* AI surpassing human intelligence (""singularity"")
* Fairness in algorithmic versus human decisions

It would be interesting to see similar analysis done on groups created in different ways. 

## Expected Submission

An analysis of opinions about AI among different developer groups that allow us to understand our community's aptitude to this important future technology. 

[Here's my analysis of opinions amongst developers in India, US, UK, Germany and the whole world put together.](https://www.kaggle.com/nityeshaga/how-indian-developers-differ#AI-opinions:)

## Evaluation

Just make it comprehensive and interesting by taking as many different groups, across as many different criterias as possible. (for eg - Do Android developers have different views as compared to iOS developers?)","","How do the opinions about AI differ?","How do the opinions about AI differ among different developer groups based on age, developer jobs, education, frameworks, languages, salaries, etc.","","2"
"213","26658","1203505","12/12/2019 20:24:43","## Task Details

1 out of every 5 responder to this survey is a full-time student. Almost 68,000 people completed this survey. That means we have the responses of almost 13,000 students in this dataset. 

There were a total of 129 questions on the survey and on an average, it would take a person 30-minutes to respond to the entire survey. This gives us a unique opportunity to gain a deep understanding of how the future developers community is shaping up to be.

## Expected Submission

Submit a detailed analysis of student developer community. 

## Evaluation

Just make it pretty and interesting. All the best! :)","","What do the student developers think?","An analysis of views, opinions and tastes of future generation of developers","","2"
"214","26658","1203505","12/12/2019 20:31:54","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

I got the idea from this excellent analysis on [Predicting R vs. Python](https://www.kaggle.com/nanomathias/predicting-r-vs-python). 

I love VIM. I would love to see how it fares among the developer community. 

## Expected Submission

Who uses VIM and for what? What are their views and opinions on AI? And more..

## Evaluation

Just make it pretty and interesting. All the best! :)","","How Vim users differ from non-Vim users?","VIM vs. Emacs (or your favourite editor) war! :p","","2"
"215","26658","1203505","12/12/2019 20:49:55","## Task Details

Contributing to open-source probably helps people learn new programming skills. But in [my analysis of Indian developers](https://www.kaggle.com/nityeshaga/how-indian-developers-differ/comments#Communication-Tools:), I found that a considerable proportion of them do not consider it a part of their education.
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1203505%2Fb34b67c615e3b123ac477858b84b046c%2Fopen-source-learning.png?generation=1576183346704810&alt=media)

Contributing to open-source software doesn't just help the community, but it also helps the contributor in his/her personal growth. So, I wasn't sure what I could infer from this above finding. Maybe there are other groups of developers who feel this disconnect as well.

## Expected Submission

An analysis of open-source contributors and their learning habits. 

Who might be the people who find a disconnect between contributing to open-source and learning from the experience? It would be good if we could diagnose this problem and learn what can be the possible reasons for this? 

## Evaluation

Just make it pretty and interesting. All the best! :)","","Do people learn by contributing to open-source?","Contributing to open-source probably helps people learn new programming skills.","","1"
"6963","27352","8892049","11/29/2021 14:49:15","## Task Details
- use plt.subplot to create image with multiple plots;
- plt.subplot(5, 7, 14) creates grid with 5 rows and 7 columns, and ""activates"" cell number 14 (all next plt.* commands will be applied to activated cell);
- use loops to avoid copying the same drawing code;
- if needed, extract pixel processing to separate function;
- use` df.sample(10).iterrows() `to get 10 random rows from the data frame and iterate over them;

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?","","Display 10 random samples from MNIST dataset","","12/02/2021 23:59:00","1"
"2937","28454","1970003","12/14/2020 12:09:09","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF","","Linear_regression","","","0"
"175","28627","1156831","12/09/2019 21:52:41","## Task Details

Some ideas: 

* Most repeated words during the match

* Use wordclouds to visually represent the most repeated words

* Most repeated bigrams (and trigrams, if you want) during the match

* Network of bigrams (or trigrams). Visualize all of the relationships among words

* Analyze user attributes: followers, friends, favourites, etc. 

* Most frequent words in the users profile description 

* Categorize the words in a binary fashion into positive and negative categories using the [`bing`](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) lexicon ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Categorize the words using the [`nrc`](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) lexicon ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Categorize the words using the [`AFINN`](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) ([`get_sentiments()`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/get_sentiments) function)

* Time analysis. Tweets per minute, for example. In which moments of the match is there more activity?

* Twitter geolocation

* Cross-language differences 

* Sentiment analysis over time. For example, are there more positive/negative comments when a team score a goal? 

* Network analysis: graph theory, metrics and properties of the network, community detection, network visualization, etc.

## Expected Submission

A notebook with relevant visualizations and insights.

## Evaluation

Solution will be evaluated by the reaction of the community.","","2018 UEFA Champions League Final data set tasks","","","1"
"5907","28901","5581091","08/29/2021 08:35:37","In this task, you must select the appropriate features in this dataset based on the rules of feature engineering, then predict the value of the pass / fail column and present your model scores.

For more information about feature engineering, you can refer to the following [link](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114) or see this mini-course in [Kaggle](https://www.kaggle.com/learn/feature-engineering).","","Feature Engineering","","","1"
"5819","28924","8182991","08/20/2021 16:20:35","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
ajeeb","","YOuMusTbEajeeb","also ajeeb","","0"
"1017","29351","5205137","05/31/2020 15:34:11","implement Linear Discriminant Analysis Feature Reduction and Support Vector Machine Classifiers on Voice Clasification data set.

Apply the codes and save them in a seperate .py or .ipynb file DO NOT PUT THE CODE IN YOUR REPORT DOCUMENT, only present your output metrics as well as requested graphs and personal comments in the reprot.

what to do :
1. Linear Discriminant Analysis: 
# apply Linear Discriminant Analysis Feature Reduction with 10 most important components on the properly prepared and standard scaled features from the Voice classification data set with 30% test size for the split percentage, use the obtained reduced features to train,fit and predict on both Logistic Regression and Desicion Tree classifiers , report the accuracy percentages and use the traiend models to obtain the confusion matrices and the classification report on the test part of the data set, plot the confusion matrices as a graphs.

Tip: the labels are in text form convert them into binary numeric form in order to properly work on classifiers.

Tip: use the plot_confusion_matrix method from sklearn.metrics as it is the cleanest approach to output the confusion matrix . 

	
2. Support Vector Machine Classification:
# apply Support Vector Machine Classifiers with Linear Kernel,3rd degree Polynomial Kernel,RBF""Gaussian"" kernel on the feature reduced data obtained from the previous section, report the accuracy percentage and use the traiend models to obtain the confusion matrices and the classification report on the test part of the data set, plot the confusion matrices as a graphs.","","voioce.csv","","06/01/2020 00:00:00","0"
"2558","29843","1085380","10/28/2020 07:34:51","## Task Details
As part of the DS Jam session, EDA on IPL dataset needs to be performed.

## Expected Submission
The typical submission should be a report-style notebook containing your analysis and findings. Following is a sample notebook for reference: https://www.kaggle.com/jpmiller/some-best-practices-for-analytics-reporting

## Evaluation
Four main criteria for deciding top submissions:
- Presentation
- Storytelling
- Visualization
- Insight generation


### Further help
If you have any additional query, feel free to write back to gtribe@greyatom.com","","Present insights on one of the most popular cricket leagues","G-Tribe DS JAM","11/07/2020 23:59:00","2"
"2088","29843","4940214","09/13/2020 19:05:46","## Task Details
If you want to have a better feel about a match situation at any time of the match, you need to some more relevant columns
1. run scored
2. wicket fallen
3. run rate
4. required run rate
5. ball remaining
6. player reached 50
7. player reached 100

All of the above can be derived from the deliveries.csv

## Expected Submission
User can submit note book add above mentioned column either in a new data frame or add into the deliveries data frame.

## Evaluation
A faster process of creating the new columns is preferable

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Adding columns for match situations","example: run scored, wicket fallen","","6"
"835","30069","4994971","05/02/2020 19:59:14","1) How do you visualize price distribution of paid apps ?
2) How does the price distribution get affected by category ?
3) What about paid apps Vs Free apps ?
4) Are paid apps good enough ?
As the size of the app increases do they get pricier ?
How are the apps distributed category wise ? can we split by paid category ?","","tasks1","Activity","","9"
"2353","30292","5086195","10/07/2020 02:20:52","## Task Details
The data shows region and total volume. I would like to see which region has the most volume per year and overall. 

## Expected Submission
I would like to see the code solution and the visualization of the answer. 

## Evaluation
N/A

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Volume per Region","","10/30/2020 23:59:00","20"
"3524","30292","6214542","02/17/2021 15:33:52","## Task Details
Try using regression analysis to predict the future price of avocado.

## Expected Submission
Ipynb

## Evaluation
A high R^2 score.

### Further help
Check my notebook if you like: https://www.kaggle.com/ilyapozdnyakov/avocado-price-prediction-xgb-pca-prophet","","Avocado price prediction","Predict the price of avocado in the chosen region in 2019+","","16"
"4054","30292","6338326","04/12/2021 15:54:49","Explore the Dataset variables to see insights and derive aspects/directions of a good grasp about the seasonality and other factors.","","Avocado Production Exploration","","","1"
"4871","30292","5407110","06/21/2021 07:32:35","## Task Details
The data shows average price and total volume. Predict the consumption (sold) and price of a particular month (Also use other features too).

## Expected Submission
I would like to see the code solution and the visualization of the answer.

## Evaluation
N/A

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Prediction of Avocado price and Total Volume sold as per month.","","","1"
"4549","30292","7240266","05/29/2021 13:02:09","## Task Details
This task is to discover which state in the USA ate the most avocadoes in 2017. The reason for this task is to see which state loves avocadoes the most, and which one loves them the least. We will need to explore beyond the number of avocadoes sold each year, to see how many avocadoes per person are eaten. 

## Expected Submission
Please submit data on the avocado sales of each US state. The task can be done using datasets. 

## Evaluation
A good result would be to find the total number of avocadoes sold in each state, then find the average eaten per person. For this task, the average number of avocadoes per person should be calculated, because the population of each state varies.  

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","State By State Avocado Consumption","","","2"
"5933","30368","8247240","08/31/2021 05:53:23","01. Task Details
Every task has a story. Tell users what this task is all about and why you created it.
HDFC Ltd. is a leading organization in the home loan and home financing sector. The company realized an exponential growth in the net profit in spite of pandemic situation in the last FY 2020-21.The organization plans a broad level survey of all the existing customers. It also includes the feedback of the future potential customers in the customer survey report.
02. Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
The team is expected to submit a comprehensive survey report on the feedbacks of the current and the future customers. We prepared the database for the contact numbers, address of correspondence, question/answer item and the real feedback of all the stakeholders of the company.
The solution involves the analysis of feedback database of the survey report. This approach is efficient to take immediate decisions for the bank managers against the scope of improvements and the aspirational areas of HDFC Ltd.
03. Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
The solution is aiming to provide effective and efficient way for improvements and growth of the company's business. The analytical reports shows the tabular and the graphical trends and charts on  the real feedbacks received from the vital stakeholders. This report will provide the immense business opportunities in the future for the organization.
04. Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
The team recommend the execution and monitoring of the different action points given in the survey report. We ensure that the high quality analytical approach would make a sustainable growth for the company's business. The survey report helps the decision makers to take quick decisions to make benefits for the HDFC Ltd.","","To maintain database of the potential customers to enable growth in the sales revenue.","Networking and brand value improvement of the company.","09/01/2021 23:59:00","2"
"6664","30633","1571785","10/24/2021 10:10:02","Perform EDA, Data Visualizations, storytelling to extract acquisition trends.","","Tech Company Acquistion Trends","Analyse Common Patterns in acquistions","","4"
"1444","30667","5074078","07/21/2020 15:30:59","## Task Details
One of the most critical aspects of a machine learning project is the understanding of the main features that correlate to the attribute that we are trying to predict. The purpose of this task is to determine which features directly affects customer's satisfaction. You may use clustering methods, feature engineering, visualizations, or anything you need to get across.

## Expected Submission
Please submit a notebook containing your findings, preferably with good visualizations and summary of the results to better tell the story.","","Which features affects customers' satisfaction the most?","","","1"
"3127","30764","6462956","01/04/2021 17:49:02","If someone have any idea about better codes ways  ,can u explain me other ways to coding well.","","More Effective Codes","Codes","01/12/2021 23:59:00","0"
"6897","30975","6345165","11/22/2021 01:57:32","## Expected Submission
Giv The Answers with id and number","","answers","answers?","","0"
"379","30990","1383726","01/15/2020 05:50:07","## Task Detaails
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","house data","","01/31/2020 00:00:00","4"
"509","31029","4463191","03/05/2020 02:15:59","do by friday","","do by friday","","03/06/2020 00:00:00","22"
"1631","31050","5388294","08/08/2020 05:36:25","## Model options
Attempt fitting a random CNN and then move onto using a SOTA (or at least something recognized).","","Fit a model to the expression dataset","","","0"
"1632","31050","5388294","08/08/2020 05:37:08","## Web app to serve the model
Flask?","","Create a web app","","","0"
"28","31455","1314380","12/02/2019 17:59:47","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","December: Domestic Violence Prevention Month","Create and describe a plot that relates to ""December: Domestic Violence Prevention Month""","01/01/2021 00:00:00","1"
"339","32132","3296186","12/28/2019 03:00:07","not overfiter","","cv all","do u collect tis data","01/03/2020 00:00:00","8"
"1420","33080","4760409","07/19/2020 14:58:10","## Task Details
Predict the best  equation for Sell Price for the car using the any of available datasets 


## Expected Submission
- Equation for the the task 


## Evaluation
We are not judging anyone, your solution is always best in your perpective

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the Sell Price for the car","","","35"
"6134","33080","6612721","09/19/2021 20:56:39","## Task Details
The Objective - deeply analyze the dataset, by experimenting with a variety of ML models and technics
--- Main Tasks ---

1. Cleaning and organizing the data
2. Get an initial understanding of relations in the data 
3. Clustering ‚Äì for categorizing data, set ‚Äústrong‚Äù features","","Appling Clustering to the model","seeking patterns to be use for prediction","10/19/2021 23:59:00","4"
"3156","33129","6338158","01/08/2021 17:06:19","Try finding similar songs using attributes from the dataset","","Similar Songs","","","0"
"3498","33180","2100685","02/14/2021 14:33:23","Data Visualization of heart disease UCI without cleaning any data.","","Data Visualization","","","12"
"80","33180","910830","12/08/2019 06:09:42","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","9"
"81","33180","910830","12/08/2019 06:09:58","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","187"
"82","33180","910830","12/08/2019 06:10:13","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","34"
"83","33180","910830","12/08/2019 06:10:29","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","10"
"84","33180","910830","12/08/2019 06:10:45","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","7"
"85","33180","910830","12/08/2019 06:11:01","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","4"
"86","33180","910830","12/08/2019 06:11:17","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","46"
"87","33180","910830","12/08/2019 06:11:32","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","17"
"88","33180","910830","12/08/2019 06:12:50","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","10"
"1678","33180","4998275","08/12/2020 07:10:56","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
just knowledge
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Features Engineering","","","5"
"2428","33180","2586226","10/13/2020 18:04:22","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2586226%2Ff47a0e14dcbd150dd3bfc0f8269d8e79%2FListhata.JPG?generation=1602612250463336&alt=media)","","data_vs","","","1"
"2926","33672","3347419","12/13/2020 12:56:09","## Task Details
Data that Lending club provides has a few **formatting inconsistency** which has to be handled and Explored before starting a Model building process. Even after **train data on the best Machine learning algorithms, it gives poor performs**. That's a data exploration kind of crucial part for this dataset.

## Expected Submission
1. Your understanding of the dataset.
2. Clear explanation about the Method and Technique you used on the dataset.
2. Proper Cleaning of the Columns, checking proper data type, handling missing values. Prepare notebook which generates basic insights (visualization) from data. Save the final table in any format suitable.

## Evaluation
1. The method used to handle missing values
2. Understandability of Visualization
3. Insights on the dataset","","Data Cleaning and Exploration","Provide good understanding and visualization of dataset","","6"
"2927","33672","3347419","12/13/2020 13:01:30","## Task Details
Make a model to assess whether or not a new customer is likely to pay back the loan. You can use several different machine learning models.

## Expected Submission
A well explained notebook that accomplishes the task.

## Evaluation
After making the model try to answer the following questions:
1. Would you offer this person a loan?
2. Did this person actually end up paying back their loan?","","Predict Loan Defaulters","Predict if a future customer will pay back the loan","","1"
"1530","33975","4428938","07/29/2020 19:27:09","1. Comparing the population of each county vs the number of patients admitting into treatment programs
2. The average consumption of dependency substances per capita
3. The likelihood of patients readmitting","","Examining Chemical Dependency Treatment Programs in New York State","","","0"
"630","34662","4738241","03/25/2020 06:08:43","## Task Details
Classify fruits

## Expected Submission
Submit by April 1st 2020

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Classification of fruits","","03/31/2020 00:00:00","3"
"1382","34935","5420337","07/16/2020 13:00:18","Create a ML Model to group the customesrs so that the operator can recommend the plans to new customer based on his/her existing Customers

Would be Great if it contains graphs and figures for analysis purpose in future","","ML Model","ML Model for TeleCust1000t","","1"
"4709","36091","7624562","06/09/2021 22:21:22","I want to find a way to use this data to improve the utilization of a chiller.

But i couldn't underestand what does the column ID means. Is this months? hours?

Im waiting for your answer.

Randal","","Id column","","","0"
"2305","36610","5874063","10/01/2020 20:17:16","## Task Details
I'm trying to create a report or research that shows there was impact and covid do affect the out come of transportation.

## Expected Submission
dataset and how covid affect chicago transportation

## Evaluation
i will like a well explain result","","impact of covid 19 on chicago transportation","","","0"
"2085","36831","662076","09/13/2020 17:17:45","## Task Details
Total distance covered by Indian Railways?
What's the frequency of trains in major Stations?
How many trains Run daily?
What's the Maximum and minimum distance covered by Trains?

Many more such findings, which can help analyze  this dataset and understand about World's largest Rail Network. 

## Expected Submission
Users can submit notebooks (Jupyter or any other), no time limit.

## Evaluation
Purpose is to understand Indian Railways and how its operating without having modern equipment's. 
More findings and clear analysis would be helpful. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Analysis of World's largest Railway network with more than 13,000 trains, how it's functioning despite being huge number of trains and transporting everyday millions of passengers across all parts of India which is having 1.3 billion people, at least 75% population use Indian Railways. More than 7300 stations across India connecting each and every part of India. Let's find out hidden secrets of Amazing Railways of India.","","Indian Railways Analysis","How World's largest Rail network function with maximum number of passengers in the world. More than 7000 Stations and 1.3 billion people use it as their primary mode of transportation.","09/13/2021 23:59:00","0"
"2136","36831","5792539","09/17/2020 06:28:25","*****##* Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","London bike sharing dataset","Bike sharing dataset","10/30/2021 23:59:00","0"
"4589","36954","4663002","06/01/2021 07:46:13","1)Replace the null values (NA) of gender column with its mode or median and explain
why mode/median used to replace NA values
2) Replace the null values (NA) of tenure column (numerical variable) with its median,
and explain why mode/median used to replace NA values
3)Analysis based on gender of the users
‚óè What is composition of male and female users?
‚óè Which category of gender has more friends?
‚óè Which category of gender initiated more friendships?
‚óè What is the distribution of tenure across different categories of gender?
 4)Analysis based on the least active users on Facebook
‚óè How many users have no friends?
‚óè How many users did not like any posts?
‚óè How many users did not receive any likes?
5)Analysis based on the user accessibility (Mobile Devices vs. Web Devices)
‚óè What is the average number of posts liked by users (based on gender) through web vs. mobile devices?
‚óè What is the average number of likes received by users (based on gender) through web vs. mobile devices?","","Replacing Nan's","","06/02/2021 23:59:00","0"
"3435","37129","5168032","02/08/2021 11:28:48","Predict the labels of audio recordings.","","Classification on FSDD using Spectograms","","","0"
"6860","38300","8912643","11/17/2021 07:41:00","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission# 
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

test","","fingertest","fingertest","11/30/2021 23:59:00","0"
"4036","39214","396276","04/10/2021 16:30:57","## Task Details
You need to create a multi classifier that classifies events into one of the 3 classes (normal, dos, probe, r2l, u2r)
You can find more about those attack families online

## Attack families
You can find the mapping of each cyber attack to it's family type: http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types","","Build a multi-class cyber attack classifier","","","0"
"3947","39214","396276","04/03/2021 17:58:03","## Task Details

Build a binomial classifier which is able to differentiate between legitimate and malicous events","","Binomial classification of Legit and Malicious events","","","0"
"1106","39657","5219165","06/10/2020 11:41:50","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
FGDFG# Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What shFREEFEould the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","VVVFDFV","","","1"
"4000","41752","6521428","04/08/2021 04:47:25","The Health Department has developed an inspection report and scoring system. After conducting an inspection of the facility, the Health Inspector calculates a score based on the violations observed. Violations can fall into:high risk category: records specific violations that directly relate to the transmission of food borne illnesses, the adulteration of food products and the contamination of food-contact surfaces.moderate risk category: records specific violations that are of a moderate risk to the public health and safety.low risk category: records violations that are low risk or have no immediate risk to the public health and safety.The score card that will be issued by the inspector is maintained at the food establishment and is available to the public in this dataset.
San Francisco's LIVES restaurant inspection data leverages the LIVES Flattened Schema (https://goo.gl/c3nNvr), which is based on LIVES version 2.0, cited on Yelp's website (http://www.yelp.com/healthscores).","","SF Data Analysis","","","0"
"6028","42075","8296201","09/07/2021 09:00:39","## Task Det
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","ramans","data","09/08/2021 23:59:00","0"
"1642","42363","4760409","08/09/2020 03:46:25","## Task Details
Best model Suggested to predict correct possible .
## Expected Submission

If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Logistic Regression Model","","","6"
"1751","42363","3987580","08/19/2020 07:50:01","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","HR Analytics Case Study","","11/19/2021 23:59:00","3"
"1699","42674","3369225","08/14/2020 21:48:48","You've probably managed to verify which group of mall customers is the main target - firstly, these are people whose both annual income and spending score are high. Secondly, it might also be the group of people who don't earn much, but they are eager to spend a lot of money (maybe they are easily influenced by commercials and special offers?).

## Task Details
Try to describe the target group as precisely as possible to be able to present the most efficient marketing strategy:
* gender
* age
* income","","The specific characteristics of target group","","","17"
"1348","42674","1548979","07/13/2020 07:59:31","EDA
- Cluster similar customers together","","Find the hidden spending patterns","","","40"
"4772","42674","7618590","06/14/2021 14:32:14","Create segments from target Variable","","Create segments from target Variable","","06/15/2021 23:59:00","2"
"126","43005","1156831","12/09/2019 08:22:38","## Task Details

Use the [`spotifyr` package](https://www.rcharlie.com/spotifyr/index.html) and retrieve the entire discography of your favorite band from Spotify's Web API, along with audio features and track/album popularity metrics ([`get_artist_audio_features()`](https://www.rdocumentation.org/packages/spotifyr/versions/1.0.0/topics/get_artist_audio_features) function)

Remember that you need to set up a Spotify developer account [here](https://developer.spotify.com/dashboard/).

You can analyze some audio features such as valence, danceability, energy, loudness, speechiness, instrumentalness, etc. and popularity metrics. If you want more information about the metrics, please check [here](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/).

 Show your results! üé∂ 

## Examples 

Cool examples made with the `spotifyr` package:

* [Exploring the Spotify API with R: A tutorial for beginners, by a beginner](https://msmith7161.github.io/what-is-speechiness/), Mia Smith

* [Blue Christmas: A data-driven search for the most depressing Christmas song](https://caitlinhudon.com/2017/12/22/blue-christmas/), Caitlin Hudon

* [Sentiment analysis of musical taste: a cross-European comparison](http://paulelvers.com/post/emotionsineuropeanmusic/), Paul Elvers

* [Thom Yorke GIFs, spotifyr, and ggplot2](https://www.rcharlie.com/post/thom-yorke/), RCharlie

* [fitteR happieR](https://www.rcharlie.com/post/fitter-happier/), RCharlie

## Expected Submission

A notebook with relevant visualizations and insights.

## Evaluation
Solution will be evaluated by the reaction of the community.","","Analyze your favorite band!","","","2"
"267","43276","3882035","12/16/2019 10:08:24","## Task Details
Figure out the team that was mainly mentioned in the hashtags.

## Expected Submission
A notebook with your solution.

## Evaluation
Clear results from your analysis.","","Team mainly mentioned in Hashtags","","","3"
"268","43276","3882035","12/16/2019 10:12:23","## Task Details
How many tweets were there when the Final took place, i.e on the 15th of July 2018.

## Expected Submission
A notebook showing your solution.

## Evaluation
A clear representation of your solution. You can also compare with the number of tweets that were made on the previous match days.","","Number of tweets on the Final","","","0"
"269","43276","3882035","12/16/2019 10:16:25","## Task Details
Which country did the people that tweeted the most on the day of the Final(15th of July 2018) come from.

## Expected Submission
A notebook showing the results of your solution.

## Evaluation
A clear representation of your results.","","Country that tweeted the most","","","1"
"1772","44170","5558516","08/20/2020 18:38:58","## Task Details
using this data to learn more

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","LAPD Wilshire test","learning test","","0"
"640","44726","1157108","03/26/2020 07:34:50","game played, W/L, mpg, ppg, etc.","","add weekly stats","add weekly stats for each player of the week","","2"
"5149","46927","6570680","07/16/2021 15:48:39","Predicting house prices","","Predict price of houses","","07/16/2023 23:59:00","6"
"192","48737","2102373","12/11/2019 05:46:40","## Task Details
There are many theories about the influence of using social media on our mental well being. This dataset contains my self reported mood of a single individual and logs of twitter activity. Is there any relationship between them?

## Expected Submission
Notebooks with exploratory analysis and predictive modelling (cross-validation).

## Evaluation
I am interested in two things - out of sample prediction and insight. I am looking for robust models that can be interpreted.","","Predict my mood based on Twitter activity","How well social media activity can explain self reported mood?","","1"
"1620","49078","1160191","08/07/2020 07:39:30","## Task Details
It is interesting to notice that the average solar power is increasing over the years. But PV pannels are ageing and are collecting dust. Is the power meter overestimating over time? I have a second power meter and it lags a little bit behind (-100kW on 35000kW). Not enough to compensate the increase.
So is this global warming? 
## Expected Submission
Estimate the cumulative power of the next year. 
## Evaluation
Is your estimation within 1% of total power of one year?
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict solar power of next year.","","","1"
"2713","49864","5325101","11/17/2020 03:11:02","# Purpose
1) The relationship between the number of installations and the rating <br>
2) Analyzing the result of step 1 by dividing it into paid apps and free apps. <br>
3) Calculate the share by category, and calculate the average of the number of installations and ratings.","","Google Play Store Analytics","Google Play Store Analytics","","5"
"89","49864","910830","12/08/2019 06:13:30","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","2"
"90","49864","910830","12/08/2019 06:13:47","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","5"
"91","49864","910830","12/08/2019 06:14:04","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","25"
"92","49864","910830","12/08/2019 06:14:21","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","3"
"93","49864","910830","12/08/2019 06:14:43","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","1"
"94","49864","910830","12/08/2019 06:15:08","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","2"
"95","49864","910830","12/08/2019 06:16:08","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","2"
"96","49864","910830","12/08/2019 06:16:41","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","3"
"97","49864","910830","12/08/2019 06:16:58","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","22"
"271","49864","3882035","12/16/2019 10:27:00","## Task Details
Which application has the largest number of reviews?

## Expected Submission
A notebook showing your solution.

## Evaluation
A clear representation of your results. You can also come up with the top 10 list of apps that have many reviews.","","App with large number of reviews","","","14"
"272","49864","3882035","12/16/2019 10:28:30","## Task Details
Which application has the largest size.

## Expected Submission
A notebook showing the results of your solution.

## Evaluation
A clear representation of your results.","","App with the largest size","","","16"
"273","49864","3882035","12/16/2019 10:32:37","## Task Details
Which app has the largest number of installs.

## Expected Submission
A notebook showing the results of your solution.

## Evaluation
A clear representation of your results. You can also come up with the top 10 list of the most installed apps. These can be represented on a horizontal bar graph.","","App with the largest num of installs","","","6"
"274","49864","3882035","12/16/2019 10:35:09","## Task Details
How many apps are free and how many apps are paid for on the google play store.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
Clear results of the analysis.","","Paid vs Free","","","14"
"275","49864","3882035","12/16/2019 10:38:05","## Task Details
Which app has not been updated for a long time. This can be done by comparing the current date with the last date that the app was actually updated.

## Expected Submission
A notebook showing the results of your solution.

## Evaluation
Clear results from the analysis that you have made.","","App which hasn't been updated","","","6"
"276","49864","3882035","12/16/2019 10:40:20","## Task Details
What is the most popular category that has the largest number of installs.

## Expected Submission
A notebook showing the results of your solution.


## Evaluation
Clear results from the analysis. A comparison can also be made with other categories.","","Most popular category","","","78"
"1425","49864","4237854","07/20/2020 08:36:46","## Task Details
 For the years 2016, 2017, 2018 what are the category of apps that have got the most downloads.

## Expected Submission
A notebook showing the results of your solution.","","For the years 2016, 2017, 2018 what are the category of apps that have got the most downloads.","","","10"
"1147","49864","5287184","06/15/2020 14:03:04","## GPS
Every task has a story. Tell users what this task is all about and why you created it.

## list of google play store accessing GPS permission ,category wise 
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","google play store apps with GPS permission","app intelligence","06/18/2020 00:00:00","3"
"5449","49864","4656934","07/31/2021 17:51:17","Using this dataset:
""/kaggle/input/google-play-store-apps/googleplaystore.csv""
Build a Recommendation system to predict Similar Apps.

Use NearestNeighbor and CosineSimilarity sklearn Modules.
You can use any metric i.e, ""Manhattan"",'Cosine Similarity"" and ""Minkowski""

Examples:
Input1:
getRecommendedApps(app_names.loc[""Online Girls Chat""],get_similarity=True,recommend_apps=5)

Output1:
Similar Apps for ""Online Girls Chat:""

	App 	Similarity

| FREE VIDEO CHAT - LIVE VIDEO AND TEXT CHAT | 99.999001 |
| Live Girls Talk - Free Video Chat | 99.998562 |
| Girls Live Chat - Free Text & Video Chat | 99.998042 |
| Live Chat - Free Video Chat Rooms|   99.998042 |
| Girls Online Talk - Free Text and Video Chat | 	99.994701 |


 
Input2:
getRecommendedApps(app_names.loc[1029],get_similarity=True)

Output2:

	App 	Similarity
0 	Six Pack in 30 Days - Abs Workout 	99.995725
1 	Abs Workout - Burn Belly Fat with No Equipment 	99.993723
2 	Bike Computer - GPS Cycling Tracker 	99.823670
3 	Fitness Dance for Zum.ba Workout Exercise 	99.731602
4 	Headspace: Meditation & Mindfulness 	99.169275","","Android-App-Recommendation","","","1"
"6970","49864","6738955","11/30/2021 07:11:05","## Task Details
Find What is the most popular app with its relation with sentiment score and that finance app will be free .

## Expected Submission
Solution should contain dataframe of top 10 free finance apps with its sentiment score.","","Most Popular Finance App","Which is the most popular finance app and also It is free.","","1"
"6166","51737","8418757","09/23/2021 06:48:59","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
births and deaths","","birthanddeathdataset","birthsanddeaths","09/24/2021 23:59:00","0"
"3135","51982","3333090","01/05/2021 12:52:40","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Making the classifaction of weed vs crop in the soyabeans
detection of particular weed board_leaf","","Classification of Crop vs Weed by SVM Technique","Supervised Learning","01/10/2021 23:59:00","1"
"5515","52093","8035673","08/03/2021 02:11:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission&gt; 
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- Yes https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","The sociel science","Expiriment to life future","10/14/2022 23:59:00","0"
"281","52093","3882035","12/16/2019 11:18:55","## Task Details
Which author(s) has published the most articles.

## Expected Submission
A notebook with the solution.

## Evaluation
A clear representation of the solution.","","Author(s) with the most articles","","","0"
"366","52999","1923390","01/09/2020 10:14:40","## Task Details
NLP Practice session

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","nlp_practice","","01/11/2020 00:00:00","0"
"460","53450","929585","02/19/2020 08:09:37","","","search comarketing possibilities","","","0"
"4974","54339","5156746","07/02/2021 15:57:46","Acupressure Reflexology to clear the Brain Paranasal Area

Task Details -
Do Dormant Viruses or Plaque Build Up cause shingles in older Adults?

Are heart attacks and skin infections related to ingrown toenails that cause pus to build up as plaque in the body, creating acne, cyst, cavities and ulcers?

- Universalis Alopecia 
-Cicatricial Alopecia
-Lichen Planopilaris
-Androgenetic Alopechia
-Telogen
-Discoid Lupus
-Areta
-cytamegolo
-Porokeratosis


Expected Submission
Images of Toenails and Finger nails


Evaluation
how the skin and nail grows back after cleaning

Further help - YouTube Channels
-Podologia Integral
-DC Foot Doctor
-DR. Toe Bro
-Jaws Podiatry
-The Meticulous Manicurist","","Test of Leprosy","Leviticus 13","09/29/2021 23:59:00","1"
"5936","55098","8252106","08/31/2021 17:42:50","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- `https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198` (Top 50 Spotify Songs 2019)

- ``","","QuantumDecay Task","","","1"
"5792","55151","1204019","08/18/2021 17:35:30","## Task Details
After a customer purchases the product from the Olist Store, a seller is notified to fulfill that order. After the customer receives the product or the estimated delivery date is past, they receive an email satisfaction survey, where they can write down their shopping experience and jot down some comments.

Thus, this task consists of performing the sentiment analysis of the reviews.","","Sentiment Analysis","","","8"
"419","55151","4405334","01/30/2020 19:37:38","Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Teste Consulta","Teste","","22"
"205","55175","73737","12/11/2019 11:51:58","## Task Details
Given a user's session predict whether the user will buy.

## Expected Submission
The submissions should be mostly in notebooks, have clear code and explain the steps taken. 

## Evaluation
A good solution is one with a good `roc_auc_score` (don't forget this is an unbalanced dataset). Since the dataset is quite big, work can be done with samples only (both training and testing).","","Predict whether user will buy","","","0"
"6065","56485","7723234","09/12/2021 10:31:46","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Actionable insights help in making data informed decisions. In other words, they are some meaningful findings that result from analyzing the data
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
The users are required to submit visualize the graphs for each of the input parameters and from this data set analysis which of these columns are more prominent in categorizing the breast cancer data as malignant
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
Some clustering techniques can be used. And based on the well defined clusters formed the solutions can be arrived at
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Actionable insight","How can I generate Actionable Insight from Breast Cancer Prediction Dataset?","09/16/2021 23:59:00","0"
"4845","56596","7492077","06/19/2021 14:40:04","## Task Details.
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","FiveThirtyEight","","","3"
"6424","57315","8382087","10/21/2021 05:39:18","Try hypertuning after normal fiting of the model","","Hypertuning","","","1"
"6425","57315","8382087","10/21/2021 05:40:14","The accuracy should be above 80%","","Above 80%","","","1"
"2796","58951","733491","11/27/2020 09:55:14","## Task Details
Predict if two sentences have the same meaning. In other words, how similar they are.","","Duplicates identification","","","0"
"211","60591","769452","12/12/2019 08:37:40","## Task Details
Document columns (in Romanian now) to explain meaning in English

## Expected Submission
Update to dataset

## Evaluation
Complete columns documentation","","Document columns","Document columns (in Romanian now) to explain meaning in English","01/31/2020 00:00:00","0"
"744","64826","1024670","04/13/2020 05:25:31","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

The transcription tell us about the medical case , however the medical term that refer to the complete transcription is the medical specialty and that is more close to the standard  medical term . Our task is here to map correct medical specialty to the corresponding transcription.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

User need to solve the task using notebooks.
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

We primarily need notebook solution to the task.","","Coding  Medical specialty from Transcription","From Transcription predict what will be the correct Medical specialty","","6"
"6844","67095","3486296","11/15/2021 12:59:07","I am looking for datasets about air traffic. I can understand that you take that dataset from ""Bureau of Transportation"" but I need more information about data. Especially about data features.","","Can you provide more information for this dataset?","","","0"
"1173","70308","3633236","06/18/2020 15:35:40","can we get good fit","","winequality is it getting overfitting?","","","1"
"237","71509","1156831","12/15/2019 12:36:24","## Task Details

Does your city have an Open Data portal? Upload some interesting data sets and contribute to improving different aspects of the city by analysing and visualising data.

Examples of Open Data iniciatives: 

* [Open Data Barcelona](https://opendata-ajuntament.barcelona.cat/en)

* [Open Data Madrid](https://datos.madrid.es/portal/site/egob/)

* [Open Data Catalonia](https://analisi.transparenciacatalunya.cat/en/)

* [Open Data of the Port of Barcelona](https://datos.gob.es/en/catalogo?publisher_display_name=Autoridad+Portuaria+de+Barcelona)

* [Open Data TMB (Metropolitan Transport of Barcelona)](https://www.tmb.cat/en/about-tmb/tools-for-developers)

* [Open Data Germany](https://www.govdata.de/)

## Expected Submission

A notebook with relevant visualizations and insights.

## Evaluation

Solution will be evaluated by the reaction of the community.","","Open Data","I want to know your city!","","3"
"193","71736","2102373","12/11/2019 05:54:58","## Task Details
This dataset includes a set of brain maps in NIfTI format. Those represent brain activation. They were collected from different academic studies that might be biased in different ways.

## Expected Submission
Notebooks with exploratory analysis and outlier detection.

## Evaluation
There are a only a few maps and each has many data points. Creative approach to this problem will be appreciated!","","Find outlier brain maps","Which brain maps might not belong in this set?","","0"
"131","72505","727004","12/09/2019 14:48:26","Use this data for geo visualizations","","Use this data for geo visualizations","","","1"
"2804","74581","6259959","11/28/2020 05:34:48","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Modelling loan prediction using logistic regression model,,, this data is giving only one significant variable out of the rest of variable?, what can be done for it to give at least 5 significant variables?","","Loan prediction","","","3"
"2743","74977","5983505","11/22/2020 02:58:24","## Task Details
Predicting student performance with the demographic and socioeconomic information.

## Expected Submission
The solution should contain Notebook with prediction accuracy.
An informative solution should also include statistical testing and information about what contributes the prediction.
Of course the script should be easy to read and understand.

## Evaluation
See Expected Submission

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Performance prediction","","","136"
"2345","74977","5361263","10/05/2020 21:19:11","Gender Prediction","","Gender Prediction","","","30"
"280","74977","3882035","12/16/2019 11:06:02","## Task Details
Figure out if a correlation exists between the different attributes that are in the dataset.

For example:
Gender and reading score
Race and Math Score
Lunch and Writing Score

## Expected Submission
A notebook with a visual representation showing if a correlation exists between these different attributes.


## Evaluation
A clear visual representation of the analysis.","","Correlations between different attributes","","","72"
"1455","74977","5367545","07/22/2020 16:24:49","Let us see the performance of students by various parameters.","","Performance of Students In Exams","","07/23/2020 00:00:00","151"
"5553","74977","5421675","08/05/2021 04:46:04","If the student gets score more than 50 then he/she has passed the exam. 
Consider the exam is of score 300. 
If the student scores less than 50 in a subject then he/she failed that subject but if his/her overall score is more than 150 then consider him/her pass(considering he/she is weak/not interested in that subject).
So there will be new column adding the grades of all 3 subjects. And one more column which shows if the student passed the exam or not.","","Passing test","","","11"
"6696","74977","1620570","10/27/2021 19:24:56","Predicting the reading, writing, math Score for this dataset","","Predicting the reading, writing, math Score for this dataset","Predicting the reading, writing, math Score for this dataset","10/31/2021 23:59:00","13"
"718","75548","400819","04/09/2020 00:38:53","## Task Details
Screening through hours of surveillance videos can be very cumbersome and consuming for the authorities  

So, we need a framework that can screen through hours of streaming and shortlist the suspicious region from the whole data. And it can be used for live monitoring as well. 

This task is about classifying videos into criminal and normal activities. All the crimes will be clubbed into a single class called ""crime"" and Normal videos will in class ""0"" as before.


## Expected Submission
For each video predict the confidence score for ""crime"" class.
So,the submission.csv will have two column. First will have the name of the video and second will have the confidence for ""crime"" class prediction.

## Evaluation
The solution having the best precision and minimum loss value will get the top rank.","","Classify an activity as normal or criminal","Criminal Activity Localization","05/30/2020 00:00:00","1"
"3843","77361","6235047","03/23/2021 04:45:30","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.


What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Chi square test","Chi square test","03/27/2021 23:59:00","0"
"3221","78163","1013910","01/19/2021 00:59:17","Cleaning the data Bank of Baroda bank : Checking the amount of money","","Bank Deafulters","Cleaning the data Bank of Baroda bank : Checking the amount of money","01/22/2021 23:59:00","1"
"3872","80116","5865761","03/26/2021 15:59:27","## Task Details
Model will be trained in such a way that while giving any years of experience, model should return accurate or most closely results

## Expected Submission
Submission of MSE using different regression models

## Evaluation
On the basis of given salary

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the salary","Predict the salary for any given experience","03/31/2021 23:59:00","6"
"4179","80134","6985605","04/25/2021 01:48:53","~90% accuracy","","Honey Bee pollen","Thang","04/26/2021 23:59:00","1"
"1283","81794","4454045","07/07/2020 17:52:51","## Task Details
Submit kernels classifying garbage into their respective categories using deep learning methods.

## Expected Submission
The user should submit their kernel here.

## Evaluation
Kernels with a higher accuracy are preferred. Well-descriptive kernels are also preferred.","","Classify Garbage using Deep Learning","Using neural networks, classify garbage into various categories.","","6"
"4163","82373","990761","04/23/2021 14:34:32","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Use notebook
## Evaluation
F-1 stat, but review data first
also evaluate and further review through network methods

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","GTSRB classification for network analysis","","","1"
"5024","83417","7764030","07/06/2021 16:39:26","Study the relationship between goals a player scored and the shots he performed","","Assignement 2","","","0"
"155","85351","910830","12/09/2019 18:15:02","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","28"
"156","85351","910830","12/09/2019 18:15:29","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","13"
"157","85351","910830","12/09/2019 18:15:43","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","13"
"158","85351","910830","12/09/2019 18:16:00","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","23"
"159","85351","910830","12/09/2019 18:16:16","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","0"
"160","85351","910830","12/09/2019 18:16:32","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","0"
"161","85351","910830","12/09/2019 18:16:47","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","1"
"162","85351","910830","12/09/2019 18:17:04","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","0"
"163","85351","910830","12/09/2019 18:17:21","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","0"
"354","86474","1477377","01/07/2020 14:26:51","## Task Details
Get another city and record the pitfalls of just getting the data which the liberal govt has put Canada economy in Jeopardy  

https://climate.weather.gc.ca/historical_data/search_historic_data_e.html

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Collecting this raw data was horrible","Finding an honest answer is a political question","01/07/2023 00:00:00","0"
"415","86986","4365464","01/30/2020 08:51:57","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.","","cricket","match","03/20/2020 00:00:00","1"
"1230","87153","4671579","06/30/2020 17:16:24","Any transfer learning models or pretrained models can be used
1. Basic Summary of Model used to be displayed
2. Predictions by the Model on the test data
3. Take a random image from the dataset and display the predictions

You can refer to the submitted notebooks for inspiration!!
All the best!!","","Using Pre-trained models for predictions","Use models other than CNN for Image Classification","","20"
"185","90311","84413","12/10/2019 05:46:32","## Task Details
As of December 2018, this data set contains about 20 thousand airbnb listings in Amsterdam.  The purpose of this task is to predict the price of Amsterdam Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
2818, 59
3209, 160
20168, 80
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Amsterdam Airbnb Rental Prices","","","3"
"2498","91397","2047893","10/20/2020 18:53:35","## Task Details

The custom data contains 100000 observations in flightdelaytrain.

It is subset of 2015 Flight Delays and Cancellations available on kaggle (https://www.kaggle.com/usdot/flight-delays)

Here we try to do (Yes/No) prediction, if the departure would be delayed for 15 mins or more.

## Expected Submission

Sample Submission - Acceptable submission format. (.csv/.xlsx file with 68720 rows)

## Evaluation

F1-Score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Mini Flight Delay Prediction","","11/05/2020 23:59:00","0"
"522","92608","4539118","03/09/2020 19:44:23","Hey lovely people,

I created this task because I think there are even more appealing ways to visualise this data. As we know the zipcodes, it must be possible to translate the information into a map.

Submissions include a map that shows per province how much electricity (or gas) is used, or even per city/town. The solution should contain a map of the Netherlands and a legenda. Annual electricity (or gas) consumption in colour would be perfect. The solution should also be computed in R.","","Energy consumption of the Netherlands map","Making a map","03/19/2020 00:00:00","0"
"2087","92927","5698175","09/13/2020 18:22:38","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

test","","test kaggle","","09/23/2020 23:59:00","0"
"5179","96967","6900019","07/18/2021 07:22:04","## Task Details
With all the details of the accident available, try to estimate the total value of the claim

## Expected Submission
Your solution should contain a list of expected total claim values

## Evaluation
The main aim is to minimize the RMS error","","Claim prediction","How much is your claim worth?","","1"
"24","98582","1314380","12/02/2019 17:55:26","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","August: Mental Health Awareness Month","Create and describe a plot that relates to ""August: Mental Health Awareness Month""","09/01/2020 00:00:00","0"
"5226","101781","4656934","07/19/2021 18:31:45","EDA-on-Survey-Questions","","EDA-on-Survey-Questions","","","0"
"5227","101781","4656934","07/19/2021 18:35:16","EDA-on-survey-responses","","EDA-on-survey-responses","","","0"
"1074","102285","5011420","06/06/2020 12:05:51","find the prdiction of test data .
try to cross the value of 99 % precision.
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","prdiction on test data","","","0"
"184","104645","84413","12/10/2019 05:42:04","## Task Details
As of December 2018, this data set contains over 22 thousand airbnb listings in Melbourne.  The purpose of this task is to predict the price of Melbourne Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
9835, 60
10803, 35
12936, 159
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Melbourne Airbnb Rental Prices","","","1"
"124","104645","1213499","12/08/2019 17:52:34","## Task Details
There are different types of holiday properties that aimed to different target customers. I expect participants to create a clustering method to group them according to their characteristics.

## Expected Submission
A notebook that shows creativity of the groupings using unsupervised learning methods.


## Evaluation
Solution will be evaluated by reaction of the community.","","Unsupervised Learning Practice - Melbourne","Cluster Properties According to Their Characteristics","","0"
"6775","107537","8628331","11/07/2021 15:49:58","the R code for UniversalBank","","UniversalBank","","","1"
"443","107620","4413068","02/12/2020 05:46:56","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Emotion Detection From Speech","","02/29/2020 00:00:00","8"
"1687","108335","4456846","08/13/2020 09:51:57","## Task Details
This task will predict a view's occupation throught his film watching record.
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
users submit a pkl file saving a machine-learning model, and a csv file recording 
the test data and your predict results
## Evaluation
Try your best to make F1 score as high as possible","","Occupation and Age Prediction","","03/07/2021 00:00:00","2"
"2593","108707","761104","10/31/2020 08:15:18","## Task Details
- How do you define monthly performance of cinemas
- Predict next month or week of performance of each cinema

*notice this should be a formulated time series forecasting (mostly meta task!) , instead of forecasting sale of each cinema. Your solution  required more creativity*
 
#  
## Expected Submission
For each cinema prediction  next month or week performance (continuous or Ordinal) based on defined performance formula. 
# 
## Evaluation
Evaluation depends on your problem formulation  but the predictive model is time series and should follow the related methods and validation. Back test, cross val time-series split or perhaps walk forward!","","Define cinema performance and predict","","","1"
"2850","108707","761104","12/03/2020 23:12:13","## Task Details
Segmentation in marketing is one of the fundamental task which help marketers to know customer, product and services. cinema segmentation has the advantages to grade cinemas basically and improve it for movie distribution , budget planning, and so on.  
#  
## Expected Submission
Provided data doesn't include cinema details like demographic data, ownership, capacity and so on. But based on sale history (+ feature engineering) we should have better practical segmentation of cinemas. 

for each cinema with unique id predict its cluster based on your clustering algorithm(s)
# 
## Evaluation

Clustering quality

Once clustering is done, how well the clustering has performed can be quantified by a number of metrics. Ideal clustering is characterised by minimal intra cluster distance and maximal inter cluster distance.
There are majorly two types of measures to assess the clustering performance.

(i) Extrinsic Measures which require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure.

(ii) Intrinsic Measures that does not require ground truth labels. Some of the clustering performance measures are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc.

Trivially your evaluation is based on  Intrinsic Measures and i suggest start with :  
- Elbow method
- Silhouette analysis

There are advanced statistical methods like Gap statistic which we will explain in kernels. 


# 
### Further help

&gt;- https://scikit-learn.org/stable/modules/clustering.html
- https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a
- https://www.cc.gatech.edu/~isbell/reading/papers/berkhin02survey.pdf
- https://pypi.org/project/pyclustering/
- https://machinelearningmastery.com/clustering-algorithms-with-python/
- [My Notebook on Properties Segmentation](https://www.kaggle.com/arashnic/eda-and-clustering)","","Segment Cinemas","Apply and practice clustering algorithms","","1"
"6260","109196","3401050","10/05/2021 10:17:10","## Task Details
Check for the input mail if it is Spam or Ham !!!","","Check for the input mail if it is Spam or Ham !!!","","","2"
"174","109371","2326382","12/09/2019 21:19:44","## Task Details
This dataset contains all countries and their average alcohol consumption per capita. Find some correlations between alcohol consumption and *any* other metric that could be interesting. This could include: happiness, average income, car accidents, or anything you find!

## Expected Submission
Submit a dataset or kernel outlining a correlation between alcohol consumption and some other metric. 

## Evaluation
This will be subjective, but finding a metric that closely tracks the alcohol consumption of a country is the goal.","","Is drinking a lot good or bad?","","","0"
"683","110374","4530109","04/02/2020 09:39:19","I have been searching online for the detection of breath cycle in ecg signal and I came up with R-R interval in the ecg signal can determine the length of inspiration and expiration.Is there any other method to detect the inspiration and expiration in the given database?
or is is just 1:1 ratio of the inspiration and expiration 

using the same database any other method to detect breath phase
I will compare the results with the inspiration and expiration I have detected","","detection of inspiration and expiration in the given breath cycle","","04/15/2020 00:00:00","10"
"2637","111880","6082442","11/06/2020 14:13:56","This a interesting new data set, not the mainly used MNIST dataset.
I am interested how good scikit-learn can perform in classification of the images.

You should submit only solutions solved with scikit-learn, using Notebooks.
The solution should contain a good model with a high matching score.

A good solution is a score higher then 0.8","","Classification with scikit-learn","","","8"
"444","115231","1488076","02/12/2020 18:52:05","## Task Details
Submitted models classify individual pieces with a great accuracy, however the logic to generate FEN strings is still hard-coded. 

## Expected Submission
Submit a model, able not only to classify individual pieces, but also to generate FEN strings from a pixel values of the image.

## Evaluation
A good solution would be a model, able to map pixel values of image to a FEN string representation with a great accuracy and as fewer trainable parameters as possible.","","Build a model able to learn how to generate FEN","","","2"
"168","117566","1156831","12/09/2019 20:09:07","## Task Details

It's hard to believe, but Luka Doncic is playing even better than in his first season, making history numerous times over in the first six weeks or so of this campaign. For example, he has just passed Michael Jordan (Dec 9, 2019) for most consecutive games with 20 points, 5 rebounds and 5 assists since the NBA/ABA Merger.

The combination of team victories and stellar individual performances have propelled Luka Doncic to the top of the MVP ladder of some analysts. Incredible!

Would you be able to analyze Luka Doncic's statistics for the current 2019-20 NBA Season? Use data from [Basketball Reference](https://www.basketball-reference.com/).

## Expected Submission

A notebook with relevant visualizations and insights.


## Evaluation

Solution will be evaluated by the reaction of the community.","","Luka Doncic 2019-20 NBA Season","","","2"
"3775","120316","6563551","03/14/2021 17:20:59","## Task Details
Movies are an art and as any art, it is subjective weather people like it or not, but unlike other arts, these require huge amounts of people and money to achieve which means they have many more variables which can influence their enjoyment which can be measured with Machine Learning.

## Expected Submission
A machine learning algorithm that can predict a movie's ratings

## Evaluation
Which algorithm gets the most accurate predictions

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict a movie's user ratings based on objective data","","","1"
"407","120660","479879","01/27/2020 08:05:01","## Task Details
Explore the BigQuery Python API to explore Litecoin crypto blockchain 

## References 
1.  BigQuery API Kaggle reference: https://www.kaggle.com/sohier/beyond-queries-exploring-the-bigquery-api
1. Google analysis of crytocurrencies: https://cloud.google.com/blog/products/data-analytics/introducing-six-new-cryptocurrencies-in-bigquery-public-datasets-and-how-to-analyze-them 

## Expected Submission
Not formalized 

## Evaluation
Ability to extract Litecoin blockchain data, etc.","","Explore BigQuery and Litecoin crypto blockchain","Using Python BigQuery API","","2"
"408","130081","4272627","01/27/2020 12:05:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","????????","","03/01/2020 00:00:00","1"
"4446","130171","7461133","05/19/2021 23:18:00","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

df dfg dfsfd","","cbbcvbxc","cvbbcv","","0"
"6939","131138","8238846","11/26/2021 12:47:36","*I have used machine learning algorithms such as Random forest, k mean cluster to calculate ""remaining useful life"" and collected data from here by means of datasets and could not able calculate the ""remaining useful life"" for my project*## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predictive maintenance of centrifugal water pump by using data driven Approach","Machine learning algorithms","11/30/2021 23:59:00","0"
"2906","133976","6361803","12/11/2020 06:10:37","I need a project about car price prediction using ml algorithms techniques
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Car price prediction using ml techniques","","","0"
"588","134715","3630854","03/15/2020 17:10:13","Use various Algorithms to increase the performance of the prediction of the sentiment expressed in the review.","","Sentiment Analysis","","12/31/2020 00:00:00","27"
"6195","136082","6907725","09/26/2021 02:17:34","## Task Details
Task is to create a matplotlib/seaborn visualisation of the given data

## Expected Submission
Carry out data cleaning processes
Analyse the data
Create visualisations

## Evaluation
Provides a comprehensive understanding of key information derived from the data

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Creating a visual representation of data","","","0"
"2662","137031","6061208","11/09/2020 18:04:01","## Task Details
With this dataset, you are asked to determine a few things regarding the waiters' tips. You will need to answer the following:

1. What days of the week pay the most in tips?
2. What gender pays the most in tips?
3. Do smokers or non-smokers pay more in tips?
4. Does the total bill price affect the tip the waiter will get?
5. Does family size affect the tip the waiter will get?
6. What time of the day generates the most tips?

Be sure to show visualizations of these answers as well!","","Gathering Information","","","6"
"6276","145424","8382087","10/07/2021 17:29:58","Lets see who comes up with a better model with high accuracy.
Proper visualization techniques should be used
Proper analysis of data should be there","","Accuracy greater than 95%","","","0"
"453","145545","2777345","02/18/2020 00:24:17","## Task Details
I'm working on a graduate project in Deep Learning for Drug Discovery using Molecular dataset.
if you are interested in it, please text me on khbheri@gmail.com","","Dataset for Molecular CNN for Drug Discovery","Molecular convolutional neural network for Drug Discovery","02/29/2020 00:00:00","1"
"3179","148152","6515349","01/13/2021 10:03:36","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
g
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","fghjfghjkl","","","0"
"5410","148715","7526686","07/29/2021 17:11:24","Can you please update this list sir?","","Update","","","0"
"2650","153420","6090409","11/09/2020 02:13:05","## Task Details
everyone  should love and enjoy the songs through the heart.
## Expected Submission
it is better to submit as document for the other corrections.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
please search the all top songs in Telugu which was released from last 2 years. 
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","top 20 songs in Telugu","melodious","12/31/2020 23:59:00","5"
"2975","153581","6101913","12/18/2020 07:56:41","## Task Details
Eager to understand the typology of who and why people chatted over Bitcoin, I took this dataset as a challenge to solve.

## Expected Submission
Can you beat me, correct me, feedback me over the data science approach : 
**=&gt; is bitcoin made of tweets???**

## Evaluation
Here is the methodology and steps i have taken
please improve

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6101913%2F1fff9b6713e738e97d4987424203b33e%2FProject.jpg?generation=1608278024025975&alt=media)


**=&gt; There should be a link over time periods between the chatter (and therefore which topics/keywords) and the price variation on a weekly lag.
**
GOOD LUCK


### Further help
The notebooks can be downloaded and run over here 
https://www.kaggle.com/leticehs/nlp1-part1-dataset-cut
https://www.kaggle.com/leticehs/nlp-bitcoin-analysis-2-preprocessing
https://www.kaggle.com/leticehs/nlp3-bitcoin-features-supervised-models

If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analyzing the influence of the twitter chat over Bitcoin price - NLP","","","3"
"4412","160647","988579","05/17/2021 08:05:22","Classify the images as a male or female.","","Male/Female Classification","","","1"
"742","162048","4861339","04/12/2020 02:53:13","## Task Details
Compare the Rate of Change in House Price from 2010 to 2020 for a range of Irish town. Generate some insightful visualization to display this data.

## Expected Submission
22nd of April","","Ireland Property Price","SAS_Project","04/22/2020 00:00:00","1"
"425","165566","2639087","02/03/2020 08:57:26","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
efdewdfewfdwefew
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","I need to complete ir","rfgr","02/14/2020 00:00:00","6"
"3495","165566","6288608","02/14/2021 10:44:12","Predict a brain tumor from a snapshot from a dataset","","tumor_detection","","","10"
"3046","168733","5849106","12/26/2020 12:43:55","L&T Vehicle Loan Default Prediction","","L&T Vehicle Loan Default Prediction","L&T Vehicle Loan Default Prediction","","1"
"2082","168831","1929307","09/13/2020 05:49:59","Should you use Race of a person - to predict his Salary ?","","Income Classification","Predict whether a given person will have Salary more than $50K","","1"
"626","169483","4732146","03/24/2020 13:19:59","## Task Details
For Machine learning home work

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","PredictionDrivingStyle","","","3"
"4229","171352","5597979","04/30/2021 05:22:35","The data set used here has 2 columns, one column is the date and the other column relates to the consumption percentage. It includes the data from Dec 31, 1984 to Dec 31, 2017. For predicting the consumption of electricity in the coming future.","","Using Time Series Analysis Electric Production","","","4"
"190","174300","1272482","12/11/2019 05:32:37","## Task Details
Horses or Humans is a dataset of 300√ó300 images. Use this dataset to classify images using transfer learning","","Classify images using transfer learning","","","0"
"191","174300","1272482","12/11/2019 05:39:09","## Task Details
For understanding how our deep model is able to classify the input image, we need to understand how our model sees the input image by looking at the output of its intermediate layers. By doing so, we are able to learn more about the working of these layers. The main aim of this task is visualize intermediate layers of different type of network.","","Visualize network intermediate layers","","","1"
"382","174469","2411547","01/17/2020 09:36:50","## Task Details
Skin cancer is one of the leading causes of cancer-related deaths on this planet. Often it can be prevented by going regularly to the dermatologist. Unfortunately, this is often neglected by various people until it's too late.
Let's create some machine learning models that might help to diagnose early stages of melanoma.

## Expected Submission
Users should submit a notebook, which is contains a model that is trained on the training data, and tested the accuracy and AUC on the test data.

## Evaluation
As the dataset is balanced, I believe that Accuracy would be a good measure for this task. Also Area Under Curve (AUC) can be regarded.","","Predict Malignant and Benign Skin Moles","","","3"
"383","174469","2411547","01/17/2020 09:43:26","## Task Details
Let's imagine we have an application, where our machine learning model is deployed. What happens if a user wants to predict a picture of his/her cat? How can the model detect, that it is not a picture of a skin mole, but an outlier?

## Expected Submission
Users should submit their solution of classifying skin moles from other various random things. The difficulty is, that no training data is provided for other than skin moles. Hence, AutoEncoder or VAE seem to be a good approach

## Evaluation
I don't have any evaluation metric yet.","","Detect Pictures of Skin Moles from Other things","Is it a skin mole or a cat?","","2"
"384","174469","2411547","01/17/2020 09:46:55","## Task Details
The current pictures of skin moles also contain a lot of other content, such has hair and other things that might not be relevant for prediction. Can we train machine learning models, that learn to cluster the skin moles, such that a classifier could be run only on such area?

## Expected Submission
Users are expected to submit their notebooks, where they are able to separate the skin mole in the picture from the rest.

## Evaluation
No evaluation metric fixed yet. Human intuition, I guess?","","Skin Mole segmentation","Unsupervised learning challenge","","4"
"121","174999","727004","12/08/2019 13:22:14","Use these features and get a high score!","","Use these features and get a high score!","","","1"
"132","174999","727004","12/09/2019 14:50:24","Use feature selection approaches and select the best features","","Select best features","","","1"
"133","174999","727004","12/09/2019 14:51:11","Use these features and create good EDA to analyze the data.","","Make good visualizations!","","","0"
"5033","177084","7855595","07/07/2021 14:45:59","escape room clue","","Escape Room","","07/08/2021 23:59:00","0"
"4086","181273","7199121","04/16/2021 08:02:45","## Task Detailsdddd
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","fffeddfwf","fffffffffff","04/23/2021 23:59:00","1"
"1182","181839","696006","06/20/2020 19:10:59","## Task Details
Build a classification model to classify the driver behaviors

## Expected Submission
-- Nil --

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help","","Driver Behavior Classification","Build a classification model to classify the driver behaviors","","0"
"2910","186172","6293676","12/11/2020 20:06:21","## Task Details
Take picture from mobile device

## Expected Submission
Detect eye diseases

## Evaluation
Advice

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Remote diagnostic","AAproject2","12/18/2020 23:59:00","1"
"1272","187466","4750044","07/06/2020 19:48:06","Could you please define every column of the dataset? There are some that I can't distinguish of what it's about. For example: the column ""Response"" tells who have responded to a question (if I understood), but which is the question?","","Define the meaning of each column","For more usability of the dataset","","17"
"6282","189386","6981725","10/08/2021 10:13:28","Test Task","","Test Task","Test Task","10/09/2021 23:59:00","1"
"6303","193934","8560413","10/10/2021 23:56:11","## Task Details
Face recognition evaluation
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Face Recognition","","10/11/2021 23:59:00","0"
"200","199387","3086688","12/11/2019 09:33:23","## Task Details
Exploratory data analysis

## Expected Submission
Kernel/ Notebook

## Evaluation
Code Ethics and infographics","","EDA- :Python","","","12"
"189","199387","1272482","12/11/2019 05:27:25","## Task Details
This is a countrywide traffic accident dataset, which covers 49 states of the United States. The purpose of this task is to visualize the dataset states wise.","","Visualize US Accidents Dataset","","","71"
"353","199387","2044079","01/07/2020 09:22:25","## Task Details
Predict the location of the accident","","Predict the location of the accident","","","23"
"338","199387","4256072","12/27/2019 11:53:52","give ur best shot spam filter!","","banmepls","banme","01/24/2020 00:00:00","1"
"221","199387","3882035","12/14/2019 12:47:24","## Task Details
Which US state has the highest number of accidents, and a description of the accidents that usually occur in that state.

## Expected Submission
A notebook that shows the state with the highest number of accidents and the main cause of the accidents in that state.

## Evaluation
All states should be ranked with the first one being the state with the highest accidents. This is done for better comparisons and to provide a more detailed solution.","","The state that has the highest number of accidents","","","44"
"222","199387","3882035","12/14/2019 12:54:28","## Task Details
Figure out the time that accidents usually occur in the US. This can be done by using the ""start time"" and ""end time"" columns. 

## Expected Submission
A notebook that outputs the time that accidents usually occur.

## Evaluation
A proper ranking of the times that accidents occur. The time that accidents usually occur should be at the top of the list together with the number of times accidents occurred during that specific time. Other times at lower ranks should follow. This is done to provide a more detailed solution.","","At what time do accidents usually occur in the US","","","24"
"449","199387","1429656","02/14/2020 03:27:48","## Task Details
Examine the relationship between accident severity and other accident information such as time, weather, and location.","","Factors Affecting Accident Severity","","","11"
"169","204491","1156831","12/09/2019 20:31:14","## Task Details

Compare other NBA basketball players. Some ideas:

* Dirk Nowitzki vs Larry Bird

* Kareem Abdul-Jabbar vs Wilt Chamberlain vs Shaquille O‚ÄôNeal

* Magic Johnson vs John Stockton

* Stephen Curry vs Ray Allen vs Klay Thompson (3-point shots analysis)

* Giannis Antetokounmpo vs Lebron James

* Chris Paul vs Kyrie Irving

* Joel Embiid vs Nikola Jokic

* Paul George vs James Harden

* Kevin Durant vs Kawhi Leonard

* Luka Doncic vs Trae Young

* Dwight Howard vs Shaquille O‚ÄôNeal üòÇ 

* Alex Caruso vs Michael Jordan üî•  (just kidding)  

...and many other possibilities! Use data from [Basketball Reference](https://www.basketball-reference.com/). Explain your insights! üòä 

## Expected Submission

A notebook with relevant visualizations and insights.

## Evaluation

Solution will be evaluated by the reaction of the community.","","NBA player comparisons","","","3"
"203","205069","769452","12/11/2019 10:07:07","## Task Details
Original data has columns names in Romanian. Add English translations for the columns names, in separate files.

## Expected Submission
Submit additional data files, with English equivalents for the original Romanian translations.

## Evaluation
Complete and accurate English translations for the columns names.","","Add English translations","Add English translations for the columns names","01/31/2020 00:00:00","0"
"204","205069","769452","12/11/2019 10:11:15","## Task Details
Add Romanian territorial administrative units information,  including Geo JSON or another format for Geographical information. These are municipalities or neighborhoods, the upper territorial level being the county (Romanian: ""judet"").

## Expected Submission
Additional data files in the dataset.

## Evaluation
Accurate data, to cover for all territorial administrative units of Romania. Ideally, provide a Kernel showing the distribution of votes (for a certain hour) at territorial administrative units level.","","Add administrative units information","Add Romanian administrative units information,  including Geo JSON or another format for Geographical information","01/31/2020 00:00:00","0"
"6020","209295","7892775","09/06/2021 12:24:15","Build a model that can be used to differentiate healthy people from people
having Parkinson‚Äôs disease.
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Parkinson‚Äôs disease.","Build a model that can be used to differentiate healthy people from people having Parkinson‚Äôs disease.","09/13/2021 23:59:00","0"
"1161","209905","4245544","06/17/2020 14:16:30","Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","house_price","niu","","0"
"819","225833","4962301","04/27/2020 02:22:59","lightgbm+xgboost","","machinehack-used cars sales price","lightgbm+xgboost","04/28/2020 00:00:00","1"
"3028","229732","2693578","12/24/2020 04:26:19","## Task Details

Try to create a CNN based network which has the ability to distinguish
 between the topics using the data set provided.
1. Try to create a binary model choosing any 2 topics
2. Once you are comfortable with above solution, try to create a multiclass classification problem choosing anywhere between 3 to 5 topics or higher if you can crack it.

## Expected Submission
Solution can be solved using notebooks and the solution should take in a .txt file as input and predict the topic of the submitted file. Also in case of multiclass classification print out the probability across all the labels

## Evaluation
Solution is : 
Satisfactory -- Accuracy 70% to 80%
Good            -- Accuracy 80% to 85%
Very Good   -- Accuracy 85% to 90%
Excellent     -- Accuracy 90% to 97%
State of the Art -- 97% to 99.9999%

### Further help
Read the below article to get an headstart.

https://towardsdatascience.com/text-classification-in-keras-part-1-a-simple-reuters-news-classifier-9558d34d01d3","","Text Classification","Creating a text classification CNN network using keras and tensorflow","12/31/2021 23:59:00","0"
"262","230350","3882035","12/15/2019 20:04:23","## Task Details
Which state in Brazil has the largest number of gas stations analyzed.

## Expected Submission
A notebook showing the results of your analysis.

## Evaluation
A clear representation of your results. Comparisons can also be made with other states.","","State with largest number of gas stations","","","9"
"263","230350","3882035","12/15/2019 20:43:08","## Task Details
Figure out the state with the minimum price of gas.

## Expected Submission
A notebook showing the solution of your analysis.

## Evaluation
A clear representation of the state with the lowest price for gas. A comparison can be made with other states.","","State with the min price of gas","","","0"
"264","230350","3882035","12/15/2019 21:17:44","## Task Details
Figure out the state with the maximum price of gas in Brazil.

## Expected Submission
A notebook showing your solution.

## Evaluation
A clear representation of your results. A comparison can be made with other states.","","State with max price of gas","","","0"
"265","230350","3882035","12/15/2019 21:20:27","## Task Details
Figure out the most common gas that is sold in Brazil.

## Expected Submission
A notebook with the results of your analysis.

## Evaluation
A clear representation of your results together with comparisons with other gases.","","Most common product","","","1"
"282","231310","3882035","12/16/2019 11:21:33","## Task Details
Which book received the highest text reviews.

## Expected Submission
A notebook with the solution.

## Evaluation
A clear representation of the results of your analysis.","","Book with the highest text reviews.","","","15"
"134","234668","727004","12/09/2019 14:51:48","Create an interesting EDA on these features.","","Make EDA on this data","","","0"
"135","234668","727004","12/09/2019 14:52:31","Use any feature selection approaches and select the best features.","","Select best features","","12/10/2019 00:00:00","0"
"3201","235864","5581490","01/15/2021 11:40:11","Present your most interesting findings in creative visualizations","","Data Cleaning & EDA","","","0"
"352","236410","642765","01/07/2020 09:13:06","## Introduction
The main purpose of the data set's recording is to be able to model the stator and rotor temperatures of a PMSM in real-time. Due to the intricate structure of an electric traction drive, direct measurement with thermal sensors is not possible for rotor temperatures, and even in case of the stator temperatures, sensor outage or even just deterioration can't be administered properly without redundant modeling. In addition, precise thermal modeling gets more and more important with the rising relevance of functional safety.
 
Your task is to design a model with appropriate feature engineering, that estimates four target temperatures in a causal manner (no future values should be considered for actual predictions). In order to maintain real-time capability, model sizes should be as small as possible. Note that temperature estimation in production will be deployed on best-cost hardware of traction drives in an automotive environment, where lean computation and lightweight implementation is key.

## Details
### Target features: 
- stator_yoke
- stator_winding
- stator_tooth
- pm

### Input features:
- The remaining features exclusive torque.

Torque is a quantity, which is not reliably measurable in field applications, such that this feature shall be omitted in this task.

### Cross-validation
Profile no. 65 and 72 denote the test set, the rest is for training.

## Expected Submission
Submit a notebook, where the complete pipeline from pre-processing, feature-engineering, model training, and prediction on profiles 65 and 72 is conducted.

## Evaluation
A good solution does
- causal predictions (contemplate only past and current values)
- have a small model size (in terms of amount of parameters and lightweight execution)
- predict stator_* and pm features with a very small **MSE** and a small maximum absolute deviation

## Extra
An additional requirement could be to predict higher temperatures more accurate than low temperatures and have over-estimates more common than under-estimates in order to ensure avoidance of over-heating. These are secondary though due to the more difficult comparability.","","Estimate Important Component Temperatures","Avoid overheating at any time","","21"
"996","239296","860951","05/28/2020 13:51:28","## Task Details
Find the body mass index (BMI) of each character, and find which character has the highest.","","Body Mass Index (BMI)","","","0"
"6199","239789","5752234","09/26/2021 15:35:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.vvv

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","do linear regression","","09/27/2021 23:59:00","0"
"702","240307","4825666","04/06/2020 19:20:01","Immediately follow any all direct commands from. 
Commander
Livingston
Branson.
Ellison.
Smolder","","Task master","Captain Jorgan","04/12/2020 00:00:00","0"
"703","240307","4825666","04/06/2020 19:41:06","Responsible for pairing individuals with teachers. developers. National spokesman . companies. Developers and other parties that upon joining produce best results. Proper placement. Qualified individuals earn double National average. Members and long time affiliates only requirement is communication with those they favor. Lol..goalis to create top positions for our people. Especially those whom during the obama administration did go to scol schools and have met requirements","","Social development and economical solution specialist","Seeds","04/30/2020 00:00:00","0"
"704","240307","4825666","04/06/2020 19:45:05","## Taskuh an","","Talent scout","Scott Smith","07/01/2020 00:00:00","0"
"705","240307","4825666","04/06/2020 20:05:19","United Embassy Base director
United nations Embassy coordinator
United states Embassy network staffing coordinator
United nations connection base line integrator


I need one of my girls ( or boys) in each Embassy. The chosen should communicate efficiently with VIP' s and effectively.... Communication is key here...

Free housing ( apartment) as well as vehicle and clothing shall be provided.
We provide our people with the top positions as well as above average pay.","","Conference planning scheduling coordinator","Mrs Brown..miss swilling....Mrs Hensley...Mrs. Justine...Mrs Galveston. Mrs. Spain. Miss. Argulis","","0"
"520","240417","4501560","03/08/2020 16:12:14","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

Crime committed by teenagers .

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

Police record of the criminals in teenage.
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Regression","","Theft by teenagers","age &lt; 19 ; age &gt; 12","03/15/2020 00:00:00","2"
"477","242863","2291692","02/24/2020 16:05:53","Cobalah untuk menemukan informasi-informasi yang bermanfaat dari data ini","","Temukan Informasi-Informasi yang berguna dari data","","","1"
"3627","251878","6051648","03/01/2021 14:33:26","Data Analysis of Population Growth between different countries. Ideally USA and China","","Countries Population","A comparison of USA and China Population's Growth","","0"
"4704","255092","7206642","06/09/2021 15:21:00","## Task Details
Figure out the person who has won more fights by Submission

## Expected Submission
Notebook

## Evaluation
the statistics do not lie","","Most Submission Wins","","","1"
"207","255092","1789087","12/11/2019 15:17:43","## Task Details
I would like to challenge you to predict if red has won or not in 2019 fights. Please exclude 2019 winner data and use it as a validation dataset. How to split test and train dataset is up to your choice.

## Expected Submission

CSV file consists of player names, dates and the Red_wins + your f1 score(since you already have the validation set at the end of the task you could calculate your f1 score by yourself)
R_fighter, B_fighter,date, r_winner:
sample:
Henry Cejudo;Marlon Moraes;2019-06-08;1
r_winner is 1 if red wins it is 0 when blue wins


## Evaluation
The evaluation metric would be the f1 score.
I hope you will enjoy it.","","Guess who has won in 2019","Can you predict the winner based on historical data","01/21/2020 00:00:00","4"
"235","255092","3882035","12/14/2019 17:40:25","## Task Details
In this task, you have to figure out the person who has won more fights in the past 5 years. 

## Expected Submission
A notebook showing your solution.

## Evaluation
A clear explanation of the solution, and also comparisons can be made with other fighters who are after this person.","","The person who has won most fights","","","1"
"236","255092","3882035","12/14/2019 17:58:26","## Task Details
Figure out the person who has won more fights by KO/TKO.

## Expected Submission
A notebook showing your solution.

## Evaluation
A clear explanation of your solution. A comparison can be made between that fighter and other fighters who are after him.","","Person who has won more fights by KO/TKO","","","1"
"4049","255093","2508581","04/12/2021 07:04:29","## Task Details
The core of every business is retaining customers and having them stick to the products. Try to find relations in the variables to determine how to improve the quality of the business.

## Evaluation
A prescription to the aforementioned issue.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Finding new ways to improve the business","","","1"
"136","258168","1952004","12/09/2019 18:01:19","## Task Details
It's Christmas! I would love to know the alcohol content of each drink so that I know which countries are the most lavish on alcohol usage and check against their happiness level in https://admin.kaggle.com/marcospessotto/happiness-and-alcohol-consumption

## Expected Submission
A dataset with two extra columns with the overall alcohol content (%) for the drink and the actual volume of 100% alcohol (ie. typical unit of the drink made multiplied by the overall alcohol content of the drink)","","Add column with the alcohol content","","","1"
"137","263093","910830","12/09/2019 18:05:16","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","0"
"138","263093","910830","12/09/2019 18:05:43","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","0"
"139","263093","910830","12/09/2019 18:06:00","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","0"
"140","263093","910830","12/09/2019 18:06:18","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset.
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","0"
"141","263093","910830","12/09/2019 18:06:41","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights.
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","0"
"142","263093","910830","12/09/2019 18:07:00","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","0"
"143","263093","910830","12/09/2019 18:07:21","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","0"
"144","263093","910830","12/09/2019 18:09:06","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","0"
"145","263093","910830","12/09/2019 18:10:10","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","0"
"22","263093","1314380","12/02/2019 17:53:25","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","June: LBGT Pride Month","Create and describe a plot that relates to ""June: LBGT Pride Month""","07/01/2020 00:00:00","1"
"6147","263888","2067080","09/21/2021 12:30:08","## Task Details
Using SHAP/LIME, curate a story to tell it to the end user.

SHAP and LIME are two ways to get insights from your model.

No deadline","","Model Explainability","","","1"
"181","268833","84413","12/10/2019 05:29:19","## Task Details
As of August 2019, this data set contains almost 50 thousand airbnb listings in NYC.  The purpose of this task is to predict the price of NYC Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
2539, 149
2595, 225
3647, 150
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.  Given the limited number of variables in this dataset, accurate predictions will be difficult.","","Predict NYC Airbnb Rental Prices","","","84"
"2712","268833","5325101","11/17/2020 01:55:56","* The purpose of the analysis is as follows.
1) Compare prices by region by looking at Airbnb on a New York City map.
2) Analyze whether there is a difference in price by room type.","","Data analytics using Airbnb data","Airbnb newyorkcity data","","31"
"315","271144","4239908","12/23/2019 00:38:56","A task to create a dataset of africa crisis country","","dataset","task","01/08/2020 00:00:00","6"
"5367","284040","682869","07/27/2021 08:39:55","## Task Details
- Sample Data
time stamp based
80-20 split based

Apply Multiple classification algorithms 
1. Decision Trees
2. MLP
3. XGBoost

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

Methods & Results comparison table

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Recall, F1-Score, FPR, FNR","","ML Experiments","Study Elliptic Dataset Classification using ML","07/28/2021 23:59:00","0"
"5368","284040","682869","07/27/2021 08:42:50","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

Explore Graph Neural Networks on Elliptic Dataset to find hidden relationship patterns  


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

Model Performance and Test Results

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Recall, F1-Scores","","Graph Nets on Elliptic Dataset to Classify Illicit Transactions","","07/28/2021 23:59:00","0"
"3133","284285","1962508","01/05/2021 10:50:44","## Task Details
To learn to apply BERT for Intent Classification

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Intent Classification","","01/06/2021 23:59:00","0"
"336","293841","3136470","12/26/2019 08:54:14","## Task Details
 Predict the popularity.

## Expected Submission
One can use the features associated with the popularity and with the help of them you can predict the popularity and can highlight what is the most dominant features responsible for popularity

## Evaluation
One can use linear regression for the prediction.","","Prediction","","","8"
"223","293841","3882035","12/14/2019 13:27:45","## Task Details
Figure out if there is a relationship between the top 10 genres and top 10 artists. Do these top artists actually belong to the top genres?

## Expected Submission
A notebook showing the relationship between the top 10 artists and the top 10 genres.

## Evaluation
A clear visualization showing the correlation between the artists and the genres.","","Relationship between top genres and artists","","","6"
"198","293841","2370491","12/11/2019 09:16:56","## Task Details
Additional files should be added to this dataset with the top 50 songs from previous years. This would allow temporal analysis of music tastes.

## Expected Submission
.csv files should be submitted which could be added to this dataset. The files should use a format consistent with the 2019 such that temporal analysis can easily be performed.

## Evaluation
A good solution will feature several years of data and be in a format consistent with the current dataset.","","Top 50 Spotify Songs - previous years","","","75"
"196","293841","2370491","12/11/2019 09:02:25","## Task Details
It will be interesting to see what the top songs share in common (in terms of the features which can easily be  quantified). In this task you should use different clustering algorithms to cluster the top Spotify songs in 2019 by their numerical features (excluding popularity), this should be presented with an interactive visualisation and the results should be supported by a textual summary of any insights which can be drawn. Primarily you should be looking to discover if the songs cluster in different genres and from this can you identify the top music genres in 2019?

## Expected Submission
The submission should be a notebook and should include interactive visualisations of the clustering. 

## Evaluation
A good solution will present one or more novel insights from performing the clustering.","","Can you identify different genres?","Clustering with interactive visualisations","","26"
"1176","293841","4824095","06/19/2020 18:01:19","I have to study Regression, principal component analysis, clustering, anova with r studio. I need help

I need Rstudio (no phyton etc pls) code and comments.




thanx","","Regression, principal component analysis, clustering, anova with r studio","","06/22/2020 00:00:00","1"
"701","294995","2557253","04/06/2020 14:57:16","what is the origin data of the data?


## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","what is the origin data of the data?","","","0"
"1281","296210","5346399","07/07/2020 13:18:26","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5346399%2Ff07e9c7d9ed4e80f6fc9976e82f79a3e%2FIMG_20200707_174330.jpg?generation=1594127864857676&alt=media)
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5346399%2F7fb9aa014a69fa102a899f82e946a529%2FIMG_20200707_174330.jpg?generation=1594127903026307&alt=media)","","Power consumption of Germany","","","1"
"692","298063","2858272","04/04/2020 11:59:48","## Task Details
The VAR model in our Kernel can be significantly improved.

## Expected Submission
Submit a notebook with an improved VAR model.

## Evaluation
Improve the MAPE in our our example:
- 2 bedrooms 1.027963025803897
- 3 bedrooms 2.3523955523069557
- 4 bedrooms 8.347488048618313
- 5 bedrooms 7.229671356483708","","Improve the provided VAR model","How can you improve the VAR solution in our Kernel?","","0"
"862","309887","4897815","05/08/2020 10:16:24","histogram of age, differentiate using drug, using faceting introduce happy_sad group

plot density of mem_score_before, differentiate using drug, using faceting introduce happy_sad group

draw scatter plot on mem_score_before vs mem_score_after, using factor happy_sad_group

use geom_bar on any feature along with fill() if using R","","GGPLOT","","","4"
"173","311962","2326382","12/09/2019 21:00:45","## Task Details
Given a list of dietary restrictions, find the top recipes that fit the bill. Dietary restrictions can include, but is not limited to, ""lactose intolerance"", ""gluten intolerance"", ""vegan"", ""vegetarian"", ""pescatarian"".

## Expected Submission
A kernel that returns a list of recipes that match the criteria above. You will need to be able to convert/classify each restriction into a form that can be used to query against the dataset.

Example input:
```
input_1 = [""vegetarian""'],
input_2 = [""gluten intolerance"", ""pescatarian""]
```

Example output:
```
output_1 = [38798, 'i can t believe it s spinach', 30, 5, 3]
output_2 = [87098, 'homemade vegetable soup from a can', 15, 7, 1]
```

## Evaluation
Try to maximize recipe rating and minimize recipe duration while fitting *all* dietary requirements.","","I have a unique diet","","","3"
"164","311962","1952004","12/09/2019 19:00:22","## Task Details
I'm hungry and I've got some left-over, but currently unknown ingredients, (ex: 'winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt') in my fridge. I want to find the recipe that use the most (if not all) the ingredients in my fridge with fewest additional ingredients to buy, with the least amount of effort in time to prep/cook.

## Expected Submission
A kernel that takes an array of arrays of ingredients in my fridge and outputs the recommended dish information.
Each inner array is the ingredients that I have for one dish.
Expected output is an array of arrays of [recipeId,  recipeName, prepTimeInMinutes, numberOfFridgeItemUsed, numberOfAdditionalItemsNeeded]:

Example input:
```
input = [
 ['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'butter', 'olive oil', 'salt'], 
['low sodium chicken broth', 'tomatoes', 'zucchini', 'potatoes', 'wax beans', 'green beans', 'carrots'], 
['spinach',  'garlic powder', 'soft breadcrumbs', 'oregano', 'onion']
 ]
```

Example output:
```
output = [ 
[137739, 'arriba baked winter squash mexican style', 60, 7, 0],
[87098, 'homemade vegetable soup from a can', 15, 7, 1],
[38798, 'i can t believe it s spinach', 30, 5, 3]
 ]

```

## Evaluation
An average score of recipes will be calculated. Each recipe score is calculated as:
```each recipe score = numberOfFridgeItemUsed ^ 60 / prepTimeInMinutes) - numberOfAdditionalItemsNeeded ^ (prepTimeInMinutes / 15)```

Note: I'm willing to leave some of my items in the fridge for my next meal üòÉ","","Hungry with my fridge!","Kernel to find the recipe that use the items in my fridge","01/31/2020 00:00:00","21"
"62","316056","910830","12/08/2019 05:42:17","This task relates to the background understanding of the problem statement. 
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset. 
Outcome of this task will be understanding of the attributes of the dataset.","","Background analysis","","","8"
"63","316056","910830","12/08/2019 05:44:38","This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.","","Data visualization","","","12"
"64","316056","910830","12/08/2019 05:46:38","This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task. 
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Data cleaning","","","1"
"65","316056","910830","12/08/2019 05:50:21","This task relates to the feature engineering aspect of the analysis. The requirement is to come up with novel features based on the functional understanding of the dataset. 
It is important to keep in mind to avoid correlated features during this process. Each feature should only improve the information contained in the dataset.","","Feature engineering","","","0"
"66","316056","910830","12/08/2019 05:52:41","Every analysis leads to insights generation and this task is aimed to generate unique and visually understandable insights. 
The requirement is to expose the hidden story inside the dataset using tables and graphs. Bonus points to those who can create an interactive dashboard.","","Insights generation","","","0"
"67","316056","910830","12/08/2019 05:54:58","This task relates to the model creation aspect of the analysis. The requirement is to create a baseline model and measure the baseline score of the model performance. This will be the benchmark of the final model that will be created.","","Model creation - baseline","","","0"
"68","316056","910830","12/08/2019 05:56:28","This task relates to the final model creation aspect of the analysis. The requirement is to build the final model with the highest performance scores. Additionally this model should beat the baseline model scores. Submission should include the scores on the validation set.","","Model creation - Final","","","0"
"69","316056","910830","12/08/2019 05:58:45","This will be the final stage where the model needs to be tested against the test set and the scores need to be noted. It is important to keep in mind not to create a model which overfits on the training set but performs poorly on the test set. Additionally the model needs to predict the values on the test set and the same needs to be submitted.","","Model test and submission","","","2"
"70","316056","910830","12/08/2019 06:00:26","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end to end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","[BONUS] - Create an application","","","2"
"197","318093","2370491","12/11/2019 09:11:14","## Task Details
In this task you should investigate the different mobile game developers. The task is to identify which developers make the most well received games but for minimal cost. The analyse should highlight developers who publish games with a high cost and include in-app purchases and compare these to developers offering free game downloads. Ideally the solution will identify both developers who charge little for their games and develop excellent games to those who charge a lot an offer poor quality games. Care should be given to statistical relevance (I.e., no solid conclusions can be drawn about a developer with one game with one user review).

## Expected Submission
The task should be a notebook where the analysis and findings are presented.

## Evaluation
A good submission will highlight predatory developers (making high cost, low quality games) and also allow a reader to explore the different developers ratings and cost models.","","Which developers give the most bang for your buck?","","","1"
"1","318093","998023","11/05/2019 20:30:00","### Task Details
This dataset has a lot of data about mobile apps, including the rating. What we don't know is when and why an app will get a good rating.

The goal of this task is to train a model that can predict the average user rating of an app. Bonus points if that model isn't just a black box, but can explain why that is. Feel free to use any means for this explanation (correlations, general EDA, model weights, ...).

### Expected Submission
Kaggle Notebooks containing:
- A quick explanation what type of model you're training and any thoughts about your approach
- Steps to actually train the model
- An evaluation of accuracy using **mean absolute error**
- Bonus: Anything you can do to explain which factors are most important (feature selection, EDA, ...)

Feel free to use and join any additional data source beyond the data contained in this dataset.

### Evaluation
Evaluation will be by **mean absolute error**. But subjective bonus points for a good explanation!","","Explain user ratings","Create a notebook that explains which games get high ratings","","14"
"2","318093","2931338","11/05/2019 22:59:25","Using the first 15,000 rows of the dataset as training data, and the balance 2,000 as test data try to predict the average rating of each game and how many ratings that game will get. 

This task requires a Kernel for submission!","","Predict highly rated games","Modeling Challenge","01/27/2019 00:00:00","23"
"57","327959","1213499","12/07/2019 19:43:32","## Task Details
In addition to maximizing accuracy, it is also important to explain why your model decided in a given way. For this reason, the task is to create a classification model with integrated gradients as [here](https://cloud.google.com/ml-engine/docs/ai-explanations/overview)

## Expected Submission
Users are expected to submit a clearly documented kernel.

## Evaluation
A good solution would be a neat explanation of the model behaviour that a non-technical audience can understand.","","Classification With Feature Attributions","","","3"
"1931","337422","3088399","09/03/2020 12:48:18","Build a recommendor system to recommend movies on the basis of the movie genre.","","Recommendation","","","0"
"627","340585","1203797","03/24/2020 17:04:00","## Task Details ##

A list of over 600 Python packages can be hard to digest. It's unlikely that every Kaggle user is going to care about every package. It seems like some kind of unsupervised clustering on this dataset could be useful to most of us as Kaggle notebook users.

## Submissions ##

Break the list of packages in the dataset into sections of some kind. A ""Table of Contents"" that lets Kaggle users quickly find the types of Python packages that they're interested in.","","Create a Table of Contents for Python Packages in Kaggle Notebooks","","","0"
"5328","349053","4656934","07/25/2021 13:20:32","Predict Yearly Amount Spent","","predict Yearly Amount Spent","","","1"
"5329","349053","4656934","07/25/2021 13:22:17","Predict time spent on Website.
Predict time spent on App.","","Time spent on Website vs Time spent on App","","","1"
"2680","351753","3731066","11/11/2020 23:49:38","Run new graphs of yearly fatalities.","","Graphs of yearly fatalities","","","0"
"122","359079","1213499","12/08/2019 17:41:42","## Task Details
There are different types of holiday properties that aimed to different target customers. I expect participants to create a clustering method to group them according to their characteristics.

## Expected Submission
A notebook that shows creativity of the groupings using unsupervised learning methods.

## Evaluation
Solution will be evaluated by reaction of the community.","","Unsupervised Learning Practice","Cluster Properties According to Their Characteristics","","4"
"183","359079","84413","12/10/2019 05:39:00","## Task Details
As of August 2019, this data set contains nearly 8 thousand airbnb listings in Singapore.  The purpose of this task is to predict the price of Singapore Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

```
id, price
49091, 83
50646, 81
56334, 69
...
```

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Singapore Airbnb Rental Prices","","","6"
"3738","360179","4876210","03/11/2021 19:24:13","## Task Details
You have to predict Players Ratings using the features present in the data set.

## Expected Submission
Create a notebook having a model that predicts players ratings .

## Evaluation
Use Mean Absolute Error to determine the accuracy of predictions.

### Further help
If you need additional inspiration, check out these existing  tasks:
https://www.kaggle.com/prahladmehandiratta/fifa20-predicting-players-rating/edit","","Predicting Players Ratings","","","2"
"59","367098","1213499","12/07/2019 20:12:58","## Task Details
Aim of this task is to visualize breweries on a map with showing number of breweries in each state.

## Expected Submission
Users expected to submit a kernel with a map that shows breweries in each state. 

## Evaluation
A good solution would be a nice visualization of the breweries. Creativity will count if other data sources added (e.g. income of the state, number of young people etc.)","","Visualize Breweries in USA","","","2"
"25","368072","1314380","12/02/2019 17:56:26","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","September: Hispanic Heritage Month","Create and describe a plot that relates to ""September: Hispanic Heritage Month""","10/01/2020 00:00:00","0"
"758","369392","3488717","04/15/2020 17:54:06","## Task Details
Unsupervised learning for sentiment analysis
## Expected Submission
Append a new column with the sentiment score","","Sentiment Analysis","","","0"
"427","369392","3488717","02/04/2020 13:53:02","## Task Details
Carry out basic EDA on the dataset

## Expected Submission
Visualisations and text summaries

## Evaluation
Eye-catching and indepth","","Exploratory Data Analysis","Exploratory data analysis","","0"
"700","371948","2858272","04/06/2020 00:53:49","## Task Details
We are looking for new and creative ways to visualise this data.

## Expected Submission
Matplotlib or Plotly visualisation of this data.

## Evaluation
At least 3 but not more than 5 distinctive features on the graph, plot, table summary.","","Visualise this data in at least 5 different ways","","","0"
"1792","372507","3593360","08/22/2020 15:02:09","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Let us analyze the data in order to deduce some insights from it. These insights will help the common man as well as the decision-makers in the Saudi Arabia

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Task for the Data","","10/22/2020 23:59:00","1"
"238","372519","425370","12/15/2019 15:19:40","## Task Details
Does gender affect who gets searched during a stop?

## Expected Submission
A good EDA with different kinds of plot to understand the data","","Does gender affect who gets searched in a stop?","","","1"
"239","372519","425370","12/15/2019 15:24:37","Do men or women speed more often?","","Do men or women speed more often?","","","0"
"2776","375151","3957760","11/25/2020 15:13:53","https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","my first trial, fitness project","","","6"
"34","376235","998023","12/05/2019 04:54:28","## Task Details
Governments are spending a lot of money on military, the idea behind it is to make citizens feel save. The question is, is this really the case? Does spending these large amounts really translate into a feeling of safety and actual safety.

## Expected Submission
Notebooks. This task is mainly about finding other related datasets which explain how safe people in a certain country feel (bonus: and how save they really are). Use whatever statistics you can find and join them with the data in this dataset to figure if the countries spending them most really make their citizens (feel save). Looking mostly for visual explanations here, so charts, tables, heatmaps or whatever you can use to visualize the relationship.

## Evaluation
This is highly subjective, but I'd say explainability counts over accuracy here, e.g. the solution should be easy to comprehend and to get a quick idea, rather than to be perfectly optimized for an optimal fit.","","Does military spend pay off?","Find proof in any data","","5"
"2725","376847","3073807","11/18/2020 21:30:50","## Task Details
Predict the day ahead (next 24 hours) of energy demand in Spain using past demand, prices, and weather data (wind speed/direction, temperature, location, rain etc).

## Expected Submission
Submit a notebook predicting the total energy load for each hour of the next 24 hours for each day in 2018. Use the Hourly energy demand generation and weather datasets and free to add data if you have it. Your solution will be 365 days where each day has 24 hourly predictions (i.e. 8760 total predictions).

## Evaluation
Results are evaluated on mean absolute error in MegaWatts (MW).

### Further help
Here is an example notebook for inspiration:
- https://www.kaggle.com/nicholasjhana/univariate-time-series-forecasting-with-keras (Univariate Time Series Forecasting With Keras)","","Predict Spain's Total Daily Energy Demand","","","3"
"454","379764","1364892","02/18/2020 03:20:34","Implementing StyleGAN or BigGAN for this dataset","","Implementing StyleGAN or BigGAN","","","1"
"1707","383055","1761512","08/15/2020 17:51:39","## Task Details
There are **26 features given** out of them these **3 are mostly advertised** in the advertisements.
How these 3 mostly advertised features-&gt; Carbody(Sedan,Hatchback), engine size and peakRPM **is related** to the ***price*** 

## Expected Submission
Use different algorithms to solve don't directly start with XGboost or Random forest classifier.
It would be good to have iterative process first by using basic algo then the more refined one.","","Only 3 features are mostly used in the advertisement are they  affecting the price .**(In Python)**","How Carbody(Sedan,Hatchback), engine size and peakRPM is related to the price (In Python)","","18"
"6208","383055","5752234","09/27/2021 12:39:10","my first one","","REgression","My first task at kaggle","","8"
"973","387909","3265895","05/26/2020 07:00:43","## Task Details
Learn object detection","","Object Detection Learning","","","0"
"4809","391127","7712143","06/17/2021 11:36:12","EDA and Prediction","","EDA and Prediction","","06/18/2021 23:59:00","1"
"344","393245","3581271","12/29/2019 09:15:52","## Task Details
As seen from dataset, there are teams that produces winning series of games.
The goal is to determine, which factors from previous games influence team's performance (winning games in a row).","","Basics for teams' series (winning streaks)","","","0"
"6336","395541","8286016","10/13/2021 22:03:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
 DELEVERING FOODITEMS IN FOOD DESERTS UNDER 8 MILES 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
DIVIDE FOOD ITEMS
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?THE FOOD ACCORDING TO THEIR WEIGHT AND LOCATION
DIVIDING 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","FOOD FILE","","10/16/2021 23:59:00","0"
"1462","397875","5491491","07/23/2020 06:23:13","Create a model of KMeans Clustering with 3 clusters and use elbow method","","KMeans Model","K Means Clustering","09/30/2020 00:00:00","0"
"120","405647","727004","12/08/2019 13:21:53","Use these features and get a high score!","","Use these features and get a high score!","","","0"
"441","408487","5309","02/10/2020 06:05:52","Create a topic classification benchmark using bert multilingual and compare it to german bert.","","Topic classification using BERT","","","1"
"220","408499","2980413","12/14/2019 12:46:22","## Task Details
 Trend of Accidents .  % of accidents - increase or decrease per 1000 vehicles .

## Expected Submission
Solution should explore trend over the years.

## Evaluation
Good story telling & good visualizations will be evaluated better.","","Trend of accidents","","","0"
"45","408523","1690611","12/06/2019 04:48:06","## Task Details
Handling missing value and EDA is an art of data science and this dataset has multiple columns with missing value problems. In my research paper, I used the elimination of rows and columns and replacing other missing values with the mode of the entire column.<br> But that's, not the best way to solve it I want to see how you are going to solve these problems along with EDA. <br>There are various research papers discussing different techniques for the imputation of missing values or handling missing values. Go through those papers, understand them and implement them. Also please do mention the name of the paper, authors, and link to it if you use any research paper to solve this problem. This would also help other data scientists to understand your technique and gain knowledge about different techniques to handle missing values in dataset. Show off your data science skills to solve them. 
Link to my paper - [https://goo.gl/qiunPq](https://goo.gl/qiunPq)

## Evaluation
The data Studied with a good Evaluation of EDA would always be good. Moreover, a data cleaning technique that leads to higher accuracy,  and precision and recall score or lease root mean squares error and mean absolute error would always be considered better.","","Study research papers for data pre-processing","Study different research papers that deals with how to handle missing values or imputation of missing values and implement them with this dataset.","","5"
"46","408580","727004","12/06/2019 05:26:18","## Task Details
Tasks is a new feature for datasets. Let's see what can be done.

## Expected Submission
This dataset contains handwritten digits, which are different from the well-known MNIST dataset. I challenge you to make EDA, possibly compare this dataset to MNIST and build some good DL model. Bonus points for comparing model performance on this dataset and MNIST dataset.

## Evaluation
I suppose when we create tasks, we can't set some rewards, so it won't make sense to set some strict evaluation criteria. Just make something interesting and have fun :)","","A test task: make EDA and a good model","This is one of the first tasks on Kaggle","12/18/2019 00:00:00","1"
"118","408580","727004","12/08/2019 13:14:52","Do something","","Second test task","","","1"
"394","411512","1961669","01/23/2020 09:12:36","check out the dataset

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","EDA the ecomm data","","","11"
"206","412894","769452","12/11/2019 11:54:52","## Task Details
Update the Leaderboard data so that this reflect the current status

## Expected Submission
Submit updates of the Leaderboard data

## Evaluation
As recent as possible is the best","","Update the Leaderboard data","Update the Leaderboard data so that this reflect the current status","12/12/2019 00:00:00","1"
"305","414435","734966","12/19/2019 07:52:08","## Task Details
Segment out each building instance from drone imagery.

## Expected Submission
Submit the task on a Jupyter Notebook or on a Colab Notebook with a publicly accessible link added. There is no hard deadline regarding minor changes but a baseline would be encouraged before the said deadline.
## Evaluation
Evaluations will largely depend on the Dice Score metric.
Learn more about Dice at this [link](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou/276144#276144).","","Instance Segmentation for Buildings","Perform segmentation on buildings. Create masks with individual building instances .","03/01/2020 00:00:00","1"
"5519","414522","8054375","08/03/2021 10:52:58","Add to
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Image charr","day first","08/31/2021 23:59:00","1"
"50","414695","1213499","12/06/2019 14:31:16","** Task Details**
While this data have customer reviews in free text format on airlines, those reviews do not have any topic. Aim of this task is classifying them according to topic. Possible topics are:
- Ground service
- Food and Beverage
- Cabin Service
- Entertainment
- Seat Comfort

Note that one review could belong to more than one topic since a customer can mention about more than one issue in a review.

**It is forbidden to use fields other than customer review.**

**Expected Submission**
Since free text fields are not categorized yet, this task is aimed to classify free text fields. Solution should contain a notebook that explains approach(es) to classify free text reviews.

**Evaluation**
Originality of the approach 
Readability of the document

Note: Since free text fields are not labelled, other than manual checks we can not determine accuracy of the classification at this point.","","Classify Customer Reviews by Topic via Free Text","","","0"
"51","414695","1213499","12/06/2019 18:16:04","**Task Details**

Customers leave genuine reviews to the [Airlinequality](https://www.Airlinequality.com) website by verifying their trips. By using features at the dataset, this task requires you to predict whether a customer recommends an airline or not. Detailed information on features can be found at the data's page.

**Expected Submission**

Participants should submit their predictions with a notebook. Solutions should contain their approach to the problem. 

**Evaluation**
Selected metric is logarithmic loss here. Since, many of the observations are pretty straightforward to predict, small difference between competitors expected.","","Binary Classification Challenge","Can you predict whether a customer recommends an airline or not?","","0"
"308","414929","807182","12/20/2019 22:08:45","## Task Details
Visualisation is the key to any issue that can be solve with Data!
Let's try to create a very basic visualisations (bar, line, pie chart) that provide some information of datasets.
Mostly on these two, ListOfTalukas.csv and StateLevelWinners.csv

## Expected Submission
Please submit a notebook (R/ Python) with grate visualisations!

## Evaluation
More the insights provided from visualisations better!

This notebook could be use as starting point, https://www.kaggle.com/kerneler/starter-paani-foundation-s-satyamev-3c8d1ae0-4","","#1. Let's create some Basic Visualisations.","Create basic Visualisations for ListOfTalukas.csv and StateLevelWinners.csv","","1"
"314","414929","807182","12/22/2019 22:40:16","## Task Details
Find more relevant dataset that will provide meaning full insights to enrich WaterCup dataset.
Some of the sample dataset from Kaggle Datasets are here,
1. https://www.kaggle.com/bombatkarvivek/rdir-2011-27-maharashtra
2. https://www.kaggle.com/rajanand/rainfall-in-india
3. https://www.kaggle.com/bombatkarvivek/longitude-and-latitude-of-places-in-maharashtra
4. https://www.kaggle.com/bombatkarvivek/detail-list-of-all-villages-44146-in-maharashtra

But let's not restrict our creativity for only these and feel free to inport datasets from other external sources!

## Expected Submission
Please submit the notebooks with grate isights that have derived from joining the multiple datasets.
Some ideas could be,
a. WRT #1 from above mention dataset, District wise ratio of villages participating in competition and trend over the years.
b. WRT #2 from above mention dataset, trend in rainfall in the regions and their relationship to the villages participating in the water-cup competition.   

## Evaluation
Relevance of the insights from the datasets and data preprocessing in the notebook are worthy to note here.","","#2. Enrich database by joining with Other dataset!","Find more relevant dataset that will provide meaning full insights to enrich WaterCup dataset.","","1"
"348","414929","807182","01/01/2020 20:26:52","## Task Details
Aim is to find method that will identify the correct pair of District-Taluka-Village among different datasets.

In Task#2, we realise there are quite a lot of time we have names of the places typed differently in different datasets. That leads us to creating a mapping of names manually, something like this:
https://www.kaggle.com/bombatkarvivek/paani-foundation-s-water-cup-eda-with-geopandas
`
_df_ListOfTalukas = _df_ListOfTalukas.replace('Ahmednagar','Ahmadnagar') \ . 
                                        .replace('Buldhana','Buldana') \   
                                        .replace('Sangli','Sangali') \  
                                        .replace('Nashik','Nasik')  
`

Of course this is not way to go with bigger datasets and more granular mapping! 

## Expected Submission
Some of the possible solution could be:
- Fuzzy matching:   
https://towardsdatascience.com/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49 .  
https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536
- DL:  
https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Auto-EM.pdf .  
http://pages.cs.wisc.edu/~anhai/papers1/deepmatcher-sigmod18.pdf

## Evaluation
as always, Innovative and robust solution!

Note, the other dataset I used for matching is here, https://www.kaggle.com/bombatkarvivek/mh-villages-v2w2","","#3. Entity Resolution: District-Taluka-Village.","Let's find the robust solution of typical problem Entity resolution for District-Taluka-Village","","0"
"194","417790","1558197","12/11/2019 08:46:52","## Task Details
There is interesting data about  price in the dataset. To take the best out of it, it is important to make the algorithms know they are float values.

## Expected Submission
A tutorial as a notebook that explains how to do.

## Evaluation
Don't worry about it, it is just a way to try this new task feature ;-P","","Convert prices in float values","","","0"
"226","418778","3882035","12/14/2019 15:25:26","## Task Details
Figure out the top 10 teams according to the number of games won.

## Expected Submission
A notebook showing the top 10 teams according to the number of games won.

## Evaluation
A clear representation of the top 10 teams which may be in the form of a graph.","","Top 10 teams","","","7"
"227","418778","3882035","12/14/2019 15:31:35","## Task Details
Figure out the top 10 teams that have the highest Adjusted Offensive Efficiency

## Expected Submission
A notebook showing the top 10 teams that have the highest Adjusted Offensive Efficiency.


## Evaluation
Clear representation of the top 10 teams which may be in the form of a graph.","","Top 10 Highest AOE","","","3"
"228","418778","3882035","12/14/2019 15:33:50","## Task Details
Figure out the top 10 teams that have the highest Adjusted Defensive Efficiency.

## Expected Submission
A notebook showing the top 10 teams that have the highest Adjusted Defensive Efficiency.

## Evaluation
Clear representation of the top 10 teams which may be in the form of a graph.","","Top 10 Highest ADE","","","1"
"661","418778","2192630","03/31/2020 04:08:35","## Task Details
Try and find the team with the biggest case for 2020 NCAA Champion. With the season coming to an abrupt ending, we should try and find the team that would've run the table.

## Expected Submission
This task is pretty open ended and fun to do I think. A great submission would be using the previous seasons' data (cbb.csv) to come up with this year's champion and make your argument. Use whatever you think would be good to prove your point (graphs or machine learning models).

## Evaluation
A good solution would have support behind it and make sense in the context of this season. This task isn't super easy to judge since the season didn't finish, but I am excited to see the submissions and arguments given. This is mainly for fun, so have some fun and pick your champion.","","2020 March Madness Champion","Let's find the 2020 March Madness Champion","","2"
"224","419811","3882035","12/14/2019 14:32:58","## Task Details
Visualize the number of cars that passed a particular node. This can be in the form of a bar graph whereby the x-axis represents the nodes and the y-axis represents the number of cars that passed through that day.

## Expected Submission
A notebook with a clear visual representation of the number of cars that passed a node.

## Evaluation
Clear visual representation.","","Visualization of num of cars that passed a node","","","6"
"225","419811","3882035","12/14/2019 14:47:20","## Task Details
List the nodes that had the maximum number of cars pass through them, and then from those nodes figure out which ones provided a minimum amount of time to let cars pass through.

## Expected Submission
A notebook showing the nodes that were more time efficient when a lot of cars passed through them.

## Evaluation
A ranking showing the nodes that had a lot of cars pass through them, and the time it took for the cars to pass through them from the minimum amount to the maximum amount.","","Nodes that provided a minimum amt of time","","","1"
"23","420712","1314380","12/02/2019 17:54:31","### Task Details
Create a notebook that contains at least one plot and write at least one paragraph that explains the data.  You should use the dataset that we have provided for at least one of your plots but we encourage you to join with additional datasets as well.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","July: Indigenous History Month","Create and describe a plot that relates to ""July: Indigenous History Month""","08/01/2020 00:00:00","0"
"53","421699","3012786","12/07/2019 08:52:36","## Task Details
I thought it'd be possible to work with Folium or smth with maps. However I didn't suceed.

## Expected Submission
To solve the task in this case is by using a Notebook. The solution should contain a map.

## Evaluation
Any solution with a map is a good solution. Better maps, better solutions? However, if I got any map, will do.","","How to work with digital altimetric data?","i.e. geom(point) column and others (not null) in this Dataset.","","0"
"54","422953","3012786","12/07/2019 12:11:20","## Task Details
I¬¥ve already seen OH and Label Encoder codes to deal with categorical columns. But they are applied to ML (with train, valid). I looked both in micro course and in competitions.  

## Expected Submission
Users should submit anything encoding . They solve the task using Notebooks. The solution should contain any script to encode, so that I could work with categorical features. 

## Evaluation
A good solution has codes that enable categorical attributes be used. The simpler 
 the better. Understandable codes that can be used in non-ML Datasets without writing a lot of lines.","","OH in non- ML Datasets.","OH Codes when you don't have train/test files.","","0"
"1284","423204","4454045","07/07/2020 17:55:33","## Task Details
Submit kernels classifying gemstones into their respective categories using deep learning methods.

## Expected Submission
The user should submit their kernel here.

## Evaluation
Kernels with a higher accuracy are always great for submissions. Well-descriptive kernels are preferred. Kernels with EDA are also welcomed.","","Classify Gemstones using Deep Learning","","","3"
"1096","423585","2785832","06/08/2020 23:24:41","Need to clean Contents text data and embedded HTML tags","","Data cleaning in Content","","","2"
"4839","423609","7113245","06/19/2021 06:13:43","1.Plotting of Date Vs Closing Price
2. Plotting of Value Change of the company‚Äôs share price with respect to the initial days of Covid i.e, on the last week of March.
3. Plotting of Percentage Change of a company‚Äôs share price with respect to the initial days of Covid i.e, on the last week of March.
4. Plotting of Company‚Äôs Closing price, 9 Day Moving Average and 50 day Moving Average.
5. Volume traded for different companies since covid from March,2020.
6. Finding the relation b/w Volume traded and the percentage of deliverable in the volume happened.
7. Finding the pattern of percentage difference daily for all the stock data frames.
8. Plotted the frequency of daily returns.
9. Plotted the company‚Äôs percentage change and the closing price of the particular day in a same plot.
10. Plotted the company‚Äôs percentage change and the Volume traded on that particular day in a same plot.
11. Plotted the company‚Äôs Exponentially Moving Average in the Same plot with different alpha
12. Plotted all the company‚Äôs pairplot which resembles all the possible plots w.r.to columns in the data frame","","NIFTY 50 Stocks Analysis: Pharmaceutical Industry","Dr. Reddy's, Cipla, Sun Pharma","","3"
"4793","423609","7113245","06/16/2021 15:27:46","1) Here you can find various plots across several industries in post covid world from March 2020
2) Lot of the code should be modified","","NIFTY 50 STOCK ANALYSIS IN POST COVID WORLD","Want to look at the plots across different industries","","0"
"4454","424014","3925987","05/21/2021 07:44:09","## Task Details

It has been more than one year since I started publishing this dataset, and doing a weekly update to share a [tag cloud search](https://robertolofaro.com/ECBSpeech) 

Therefore, decided that now I have enough material (more than 3000 items- speeches, interviews, etc) to apply machine learning on.

Following the prior task on the same dataset (issued on 2021-03-12 [#FridayDataSetTask01 : Identify correlations between communication and events](http://robertolofaro/DataSetTask_20210312)), this task is again about NLP.

In this case, it is a text visualization task.

The dataset contains speeches, interviews, etc. posted online since 1997 (and updated quarterly- the next update is due in June 2021).

The task is simple but can be deliver more: use the information with the ""frequencies"" side of the dataset to show the evolution across time.

Do not add further complexity (e.g. selections, etc), but just show the evolution as a result or dictionary (your choice).

Further tasks will expand on this.

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Currently my weekly update is done manually (each Sunday), due to the varying format of the source material (sometimes text, sometimes, PDF presentations, sometimes lists)- but I would like to explore scraping option to automate the process

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask06 : Identify trends in ECB communication","following communication threads across time","","0"
"3740","424014","3925987","03/12/2021 07:16:15","## Task Details

It has been more than one year since I started publishing this dataset, and doing a weekly update to share a [tag cloud search](https://robertolofaro.com/ECBSpeech) 

Therefore, decided that now I have enough material (more than 3000 items- speeches, interviews, etc) to apply machine learning on

The aim is to 
1. first, find correlations between communication and events, i.e. how communication follows or pre-empts events, but using only material within the dataset
2. find other data-oriented datasets that could show evolution; as the ECB is a European Union institution but anyway within a global context, and my focus now is integrating non-economic/financial factors within corporate reporting, I would like to use at least two other datasets I already posted:
2. a) [UN SDG EU 27 - 7 KPIs on how EU is faring on the 2030 sustainability targets](https://www.kaggle.com/robertolofaro/sdgeu-datamart-sample)
2. b) [Selected indicators from World Bank](https://www.kaggle.com/robertolofaro/selected-indicators-from-world-bank-20002019)

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Currently my weekly update is done manually (each Sunday), due to the varying format of the source material (sometimes text, sometimes, PDF presentations, sometimes lists)- but I would like to explore scraping option to automate the process

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","Identify correlations between communication and events","a cross-dataset analysis","06/30/2021 23:59:00","0"
"210","425182","106953","12/12/2019 02:47:32","## Task Details
Extract as many insights as you can about the candidates

## Expected Submission
A Kaggle kernel in any language will do. Some questions to answer :
1. How has candidate contributions changed over time?
1. How do candidate contributions differ between individual & PACs?
1. Which candidates get the most or least contributions & how does that change over time?
1. Any other questions related to this topic that someone wants to answer.

## Evaluation
1. It should answer all the above questions + plus some more & give a fairly complete picture of candidate contributions over these years.","","Analyze the candidates data","","12/31/2019 00:00:00","0"
"1416","426827","3396171","07/19/2020 05:40:29","## Task Details
Credit score cards are a common risk control method in the financial industry. It uses personal information and data submitted by credit card applicants to predict the probability of future defaults and credit card borrowings. The bank is able to decide whether to issue a credit card to the applicant. Credit scores can objectively quantify the magnitude of risk.
&nbsp;
Generally speaking, credit score cards are based on historical data. Once encountering large economic fluctuations. Past models may lose their original predictive power. Logistic model is a common method for credit scoring. Because Logistic is suitable for binary classification tasks and can calculate the coefficients of each feature. In order to facilitate understanding and operation, the score card will multiply the logistic regression coefficient by a certain value (such as 100) and round it.
&nbsp;
At present, with the development of machine learning algorithms. More predictive methods such as Boosting, Random Forest, and Support Vector Machines have been introduced into credit card scoring. However, these methods often do not have good transparency. It may be difficult to provide customers and regulators with a reason for rejection or acceptance.


## Expected Submission
Your high-quality notebooks.

## Evaluation

Build a machine learning model to predict if an applicant is 'good' or 'bad' client, different from other tasks, the definition of 'good' or 'bad' is not given. You should use some techique, such as [vintage analysis](https://www.listendata.com/2019/09/credit-risk-vintage-analysis.html) to construct you label. Also, unbalance data problem is a big problem in this task. 

### Further help

Detailed data explanation is at 
https://www.kaggle.com/rikdifos/credit-card-approval-prediction
If you have some questions about data explanation, please leave your comment at this discussion post:
https://www.kaggle.com/rikdifos/credit-card-approval-prediction/discussion/119320
I will answer ASAP.

Related data : [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)
Related competition: [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)","","Credit Card Approval Prediction","Predict the risk each applicant defaulting credit card loans","","43"
"781","426888","4830551","04/18/2020 14:54:32","Create Funnel in Tableau","","Create Funnel in Tableau","","","0"
"2038","426943","3496887","09/10/2020 08:09:38","This dataset include images which needs to be classified and based on the prediction you need to segment images based on their category since you can't train any model because no such labeled data is present so use any pretrained model and classify those images and save them in different folders on the basis of there class.","","Image Segmentation","","","0"
"596","427052","2656674","03/17/2020 22:07:01","Cross-Market Analysis ‚Äì Perform Association/correlations between product sales.

Identifying Customer Requirements ‚Äì Identify the best products for different customer segments.

Customer Profiling ‚Äì Determine what kind of people prefer to buy what kind of products. Any group behavior based on geographical region or product segment is  to be analyzed.

Key Business Questions to be answered by this project are following:
Is a customer most likely to buy another item in the same or different  product segment while finalizing one particular item purchase?

Is there any buying preference pattern by geographical region ?","","key Business Questions","","","0"
"195","427847","2913628","12/11/2019 08:47:35","## Task Details
There is an algorithm to count full moon (syzygy) exact date and time in the past/future. Check [wiki Lunar calendar](https://en.wikipedia.org/wiki/Lunar_calendar).

## Expected Submission
Need to extend this dataset with all full moons occurrences as Day of the week | Date | Time in AD

| Weekday | Date            | Time        |
|---------|-----------------|-------------|
| Monday  | 1 January 0001  | 11:11:11 pm |
| ...     | ...             | ....        |
| Sunday  | 1 December 1889 | 11:11:11 pm |

*this table is just a fake data to understand that is needed üåú 

## Evaluation
Library [PyEphem](https://rhodesmill.org/pyephem/) surely can help.","","Full moons occurrences (datetime) in AD","exact date and time of syzygy in AD","","0"
"9","429504","998023","11/29/2019 04:25:31","### Task Details
Random generators should generate truly random numbers. But how do you do this, when you have a machine that is completely deterministic? There's a whole field in computer science trying to figure out that exact question.

### Expected Submission
Anything really! A notebook, slide deck dataset, a sentence. Whatever convinces me.

### Evaluation
This is highly subjective. Whatever convinces me :)","","Find patterns in randomness","Can you crack np.random.rand()","","0"
"6807","430721","7777187","11/10/2021 01:37:31","## Task Details
Based on the 100 images labeled dataset try to build a classifier","","Eye Disorder Classification","Classifying eye disorder images","","2"
"345","430832","4271829","01/01/2020 02:06:31","Mapping out potential areas of potholes around the capital city","","Water born illness","","","4"
"199","430861","3086688","12/11/2019 09:22:41","## Task Details
In the foreign exchange market, the smallest changes in international affairs and economics can have a huge effect on the conditions of a financial transaction, sometimes resulting in gigantic losses without the involved parties even being aware of it.

## Expected Submission
Predict the end-of-month exchange rate with given variables: market and economic news data.

The data set contains a training set, test set, and text feature folders.

## Evaluation 
R2 score","","Forex Algorithm Challenge","","","0"
"201","430861","3086688","12/11/2019 09:35:15","## Task Details
In the foreign exchange market, the smallest changes in international affairs and economics can have a huge effect on the conditions of a financial transaction, sometimes resulting in gigantic losses without the involved parties even being aware of it.

## Expected Submission
Predict the end-of-month exchange rate with given variables: market and economic news data.

The data set contains: training set, test set, and text features folders.

## Evaluation
R2 score","","Forex fluctuation rate prediction","","","0"
"117","430934","2566546","12/08/2019 08:44:37","## Task Details
Develop a Sales Strategy by using this datasets.

## Expected Submission
Using Kaggle Notebooks.

## Evaluation
Number of Votes.

## Timeline
Ongoing.","","Develop a Sales Strategy","","","8"
"11","431181","1314380","12/02/2019 17:31:35","### Task Details
Use the Open Elections dataset to map voting patterns in a specific state or precinct.  Discuss.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation

Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Map and discuss voting patterns.","Use open election data to learn about local politics.","","2"
"12","431181","1314380","12/02/2019 17:35:15","### Task Details
Create structured CSV files from PDF documents.  Use the the PDF source files from the Open Elections dataset (folders containing the word ""source"") to create structured CSV files that contain the same data in an easier-to-use format.  Compare the accuracy to the official .CSV files that were generated using Tabula software in the official Open Elections dataset (folders containing the word ""data"").  Save your results in the output of your notebook.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Create CSV files from PDF files.","Develop tools to help researchers learn from open election data.","","9"
"13","431181","1314380","12/02/2019 17:36:51","### Task Details
Use the Open Elections datasets to identify which states or precincts had the largest changes in voting patterns from one election to the next.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Identify the largest changes in voting patterns.","Use open election data to learn about local politics.","","2"
"14","431181","1314380","12/02/2019 17:38:32","### Task Details
Use the Open Elections datasets to identify which states or precincts have the smallest differences between the numbers of Republican vs Democrat voters.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Identify precincts with equal numbers of R vs D.","Use open election data to learn about local politics.","","2"
"15","431181","1314380","12/02/2019 17:39:58","### Task Details
Use the Open Elections datasets to identify which states or precincts are most accepting of 3rd party candidates

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Identify precincts that vote for 3rd parties.","Use open election data to learn about local politics.","","4"
"16","431181","1314380","12/02/2019 17:41:03","### Task Details
Use the Open Elections datasets to identify which states or precincts have the most homogenous voting populations.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Identify homogenous voting populations.","Use open election data to learn about local politics.","","4"
"480","431244","610759","02/25/2020 05:21:00","1. Create a kernal
2. establish connection with data
3. get big function to run cleanly - There is an issue with the all_of() function from tidyselect.  For some reason it it's being recognized.","","Add kernel for big formatting function","","02/28/2020 00:00:00","0"
"119","431305","727004","12/08/2019 13:21:18","Use the files and get a high score!","","Use the files and get a high score!","","","1"
"127","431831","1746215","12/09/2019 14:13:55","Analyze and plot a graph to visualize this relation.","","How outside temp was related to inside temp ?","","","8"
"128","431831","1746215","12/09/2019 14:16:11","Perform EDA and visualize using plots...","","variance of temp for inside - outside room temp ?","What was the variance of temperature for inside and outside room temperature?","","6"
"129","431831","1746215","12/09/2019 14:18:05","Can you use any regressive models or Fancy time series models like :  ARIMA, SARIMA, ARIMAX, etc on this dataset and achieve a notable accuracy.","","Predict the next scenario?","Can you use Time Series Forecast algo to predict the next scenario?","","3"
"130","431831","1746215","12/09/2019 14:20:22","This climate need to be protected.","","Climate on verge of RISK ?","any warning signals for climate disaster ?","","2"
"49","432524","2130163","12/06/2019 11:56:02","## Task Details

Make the catalogue of the text, video, photos, emojis, and mixtures by the UserId, Username, and reception. Created by the UserId are a starting point and timestamp_ms is the first table, then distribution based on the nested JSON with the help of Parsers. While after every 4 to 5 collector there will be the index for the further classes. And It should flow the Diagram which depends upon the coder.

## Expected Submission

Each table to have a key to make a tree over the root table with super, primary, secondary, foreign, and left, right, full or outer joint. In the end, there should be at least 5 to 7 tables to make machine learning code and make sure about the mixture elements(photo, video, text) is one of them. Thus, It is connected to the UserId from the first data it looks like a data mining. For further, the coder should not mix the User with others to make a prediction in the range of minimal accuracy based on user use.

## Evaluation

Whatever the content is pulled that also be categorized in the forum of the JISAW competition. And also make the tree of the cases which you have to split the data over the climbing the tweets.

The Class Diagram, Sequential, State, Activity, and use case has used accurately to the data formation. That all data which we create is not affect the other data. There will be computerization to others, it is very friendly dataset, and for education purpose.","","For Class Diagram split the table into 25 tables","Parsing the json context from the entity.","03/01/2022 00:00:00","2"
"3340","432559","3548389","01/30/2021 15:41:44","## Task Details
hi all
help me to find solution ,,,,,,","","classification of normal and abnormal gait using deep learning","human locomotion","","0"
"229","432906","3882035","12/14/2019 16:00:26","## Task Details
Figure out the top 10 cheapest listings which are available.

## Expected Submission
A notebook showing the top 10 available cheapest listings.

## Evaluation
A clear representation of the top 10 cheapest listings.","","Top 10 Cheapest Listing","","","0"
"116","434238","1571785","12/08/2019 07:23:48","Netflix is known for its strong recommendation engines. They use a mix of content-based and collaborative filtering models to recommend tv shows and movies. In this task, one can create a recommendation engine based on text/description similarity techniques.","","What to watch on Netflix ?","Find similar movies / tv shows using text similarity techniques","","212"
"123","434238","1213499","12/08/2019 17:45:56","## Task Details
Before watching something on Netflix, I always check shows IMDB score. Here Kaggler's are expected to merge this dataset with IMDB scores of them and visualize top shows. Other insights would also be interesting such as country of origin of the shows, their ratings etc.

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
A good solution would be a creative kernel that shows ratings of the shows on Netflix. If you want to go beyond,  you can even make a dashboard on it.","","Show me the Ratings","Which shows are the best?","","82"
"741","434238","4784502","04/11/2020 16:52:52","## Task Details
The mission of this task to the best rated movies in netfilx.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","How to find the best rated Movies in Netflix","Best Movies","","69"
"2447","434238","4835558","10/16/2020 06:13:09","Task Details :

**Recommendation system is required in subscription-based OTG platforms. **
Recommended engine generally in three types 1.content Based recommended engine 2.collaborative recommender engine and 3.hybrid recommended engine
Expected Submission

With the help of this particular data set you have to build a recommended engine. And your recommended engine will return maximum 10 movies name if an user search for a particular movie.
Evaluation

Recommended engine must return 5 movie names and maximum it can return 10 movie names if an user search for a particular movie. This recommender engine should not give suggestion in between 1 to 4 and 6 to 10 it have to return 5 movie names for 10 movie names.

### Further help
If you need additional inspiration, check out these : 
- https://www.kaggle.com/sankha1998/collaborative-book-recommendation-system
- https://www.kaggle.com/sankha1998/collaborative-movie-recommendation-system
- https://www.kaggle.com/sankha1998/content-based-movie-reommendation-system","","Movie Recommendation System","","","199"
"3782","434238","5288885","03/15/2021 16:25:09","**Task Details:**
As mentioned above, a number of columns in this dataset have comma separated values, which makes it difficult to find how many titles an actor or actress appeared in or how many titles a director has filmed.

**Expected Submission:**
Cleanse the comma separated values into tables/dataframes for unique actors, directors, countries, and genres that can be linked back to the original dataset via the ""show_id"" field. There are a variety of tools and software that can accomplish this.

Once the data has been cleansed feel free to include some counting functions to find the most popular actors, actresses, and directors.

**Further help:**
If you need additional inspiration or want to glimpse a potential solution, check out [this version](https://www.kaggle.com/jackkerschner/netflix-movies-and-tv-shows-cleansed) of the dataset","","Top Actors/Actresses, Directors, Genres, and Countries","A number of columns in this dataset have comma separated values, which makes it difficult to find the most popular actors/actresses or directors.","","22"
"6236","434238","1571785","10/02/2021 08:21:09","Comparative analysis can be performed for different platforms, in order to get the strategies, content, and other analysis. 
Dataset in exact same format can be found here - https://www.kaggle.com/shivamb/disney-movies-and-tv-shows","","Compare Netflix and Disney+","compare the content, insights, trends from two platforms","","7"
"5556","436801","926694","08/05/2021 10:50:20","## Task Details
Height and Weight prediction using just images.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predicting Height and Weight of a person from Image","","08/31/2021 23:59:00","0"
"347","438065","2655356","01/01/2020 17:38:47","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

**The following task should contain list of top 5 ISO standard products with a published status and with price_CF to number of pages ratio greater than its mean**

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

**Submissions via notebooks. The users are required to submit the following list of products list along with visualization of the status of the various products**

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

**Where proper care is taken towards visualizing and making the correct list of products**","","Top 5 ISO standard products","","","0"
"230","439105","3882035","12/14/2019 16:25:50","## Task Details
Check if white noise exists in the time series. A time series is white noise If it has a sequence of random numbers that cannot be predicted. If white noise exists, it means a good model which will not need further improvements can actually be developed. The error forecasts produced by this model will be white noise and this white noise cannot be predicted meaning the model does not need further improvements.

## Expected Submission
A notebook showing the results after checking for white noise.

## Evaluation
A clear representation that shows whether white noise exists or not. The white noise time series can also be compared to the original time series.","","Check For White Noise","","","5"
"231","439105","3882035","12/14/2019 16:33:47","## Task Details
Check if stationarity exists in the time series. Stationarity means that the statistical properties of a process generating a time series do not change over time. This will mean that future values can easily be predicted. Stationarity can be tested for using the dickey-fuller-test.

## Expected Submission
A notebook showing whether stationarity actually exists in the time series or not.

## Evaluation
A clear explanation of whether stationarity exists or not after the analysis.","","Check For Stationarity","","","6"
"232","439105","3882035","12/14/2019 16:43:54","## Task Details
Seasonality means cycles that repeat regularly over time. Identifying and removing seasonality can produce a clear relationship between the input and output variables. In this task, you have to detect seasonality by using decomposition. Decomposition is splitting the time series into 3 effects which are:
1.) Trend
2.)Seasonal
3.)Residual

Using these, you can then figure out if seasonality actually exists or not.

## Expected Submission
A notebook with clear visualizations showing if seasonality exists or not.

## Evaluation
A clear explanation if seasonality exists in the time series or not. If it exists, how can it be removed?","","Check For Seasonality","","","2"
"233","439105","3882035","12/14/2019 16:47:51","## Task Details
Autocorrelation is the similarity between a time series and a lagged version of itself. In this task, you have to detect if it actually exists in the time series.

## Expected Submission
A notebook with clear visualizations showing whether autocorrelation exists or not.

## Evaluation
Clear visualizations followed by explanations of whether autocorrelation exists or not.","","Check for AutoCorrelation","","","1"
"234","439105","3882035","12/14/2019 16:54:48","## Task Details
Partial Autocorrelation means producing a direct relationship between a time series and a lagged version of itself. In this task, you have to visualize the Partial AutoCorrelation of the time series by using the Partial Autocorrelation Function.

## Expected Submission
A notebook showing a clear visualization of the Partial AutoCorrelation of the time series.

## Evaluation
A clear visualization of the Partial AutoCorrelation together with some explanation.","","Visualize Partial Autocorrelation of time series","","","1"
"360","440088","3699007","01/08/2020 12:12:23","## Task Details
Last year I created a Christmas Carol Generator based on code from Cholett's boook. But I'm not 100% satisfied with an outcome. 
Here is my code: https://www.kaggle.com/wojciech1103/christmas-carol-generator
 I hope together we can create a proper Christmas Carols Generator and we can get proper lyrics.

## Evaluation
Dataset is in Polish so it can be challenging for non-polish speakers to evaluate it. But it can also be something new for you to try and create something you can't understand. :D  
I will evaluate your solution ""by hand"" by analyzing context and sense of created lyrics.","","Christmas Carols Generator","","","0"
"361","440088","3699007","01/08/2020 12:15:06","## Task Details
I'm really curious what Kaggle community could squeeze out of this dataset. 
I'm not very familiar with technics for text analyzing and NLP so I hope I can learn a few things from your notebooks.","","Christmas Carols EDA","","","0"
"209","440279","2876392","12/11/2019 21:53:19","## Task Details
I'd like the dataset to have all D1 teams in it, currently it has about 50% of the teams and I can do about 2 teams an hour. Anyone else that is passionate about college football can collaborate to improve the dataset with more teams.

Here is a list of the teams that are currently in the dataset: 

## Expected Submission
I'll make an Kernal that I will link with my R code to show my process for getting all the data, this should be a good starting point to work with. Any help will be appreciated.

Teams Included:

1. BYU (Ind)
2. Clemson (ACC)
3. Colorado (PAC-12)
4. Georgia Tech (ACC)
5. Kansas State (Big 12)
6. Michigan State (Big 10)
7. Nebraska (Big 10)
8. Notre Dame (Ind)
9. Rutgers (Big 10)
10. Texas A&M (SEC)
11. Virginia (ACC)
12. Washington (PAC-12)
13. Wisconsin (Big 10)
14. Oklahoma (Big-12)
15. Penn State (Big-10)
16. UCLA (PAC-12)
17. WVU (Big-12)
18. Florida State (ACC)
19. Ole Miss (SEC)
20. Arkansas (SEC)
21. Boise State (MWC) 
22. Nevada (MWC) 
23. Cinncinnati (AAC) 
24. Syracuse (ACC) 
25. NC State (ACC)
26. Iowa State (Big-12)
27. Baylor (Big-12) 
28. Indiana (Big-10)  
29. Marshall (CUSA) 
30. Western Kentucky (CUSA) 
31. Northern Illinois (Mid-American) 
32. Toledo (Mid-American) 
33. Troy (Mid-American)
34. SMU (American)
35. Alabama (SEC) 
36. Arizonia (Pac-12) 
37. Arkansas State (Sun Belt) 
38. Ball State (MAC) 
39. Boston College (ACC) 
40. Buffalo (MAC) 
41. California (PAC-12) 
42. FIU (C-USA) 
43. Georgia State (Sun Belt) 
44. Hawaii (MWC) 
45. Illinois (Big 10) 
46. Kansas (Big 12) 
47. Kent State (MAC) 
48. Louisiana (Sun Belt) 
49. Louisiana Tech (C-USA) 
50. Memphis (American) 
51. Miami - OH (MAC) 
52. Middle Tennessee (C-USA) 
53. Missouri (SEC) 
54. New Mexico (MWC) 
55. Northwestern (Big Ten) 
56. Ohio (MAC) 
57. Oregon State (PAC-12) 
58. San Diago State (MWC) 
59. South Alabama (American) 
60. Southern Miss (c-usa) 
61. UCF (American) 
62. UMass (Independent) 
63. USC (Pac-12) 
64. UTEP (C-USA) 

The task can be broken down into the following subtasks for each team:
- Scrape year information from Wikipedia
- Manual Imputing Tailgating, New Coach, Conference and Stadium Capacity
- Combine with the weather data from nearby weather station

## Evaluation
The best submission will have all the variables for the data including:
Date, Team, Time, Opponent, Rank, Site, TV, Result, Attendance, Current Wins, Current Losses, Stadium Capacity, Fill Rate, New Coach, Tailgating, PRCP, SNOW, SNWD, TMAX, TMIN, Opponent_Rank, Conference, Year, Month, Day","","Add Additional teams to the dataset","","","0"
"7037","441111","2039816","12/09/2021 18:17:33","This is just an example for testing task removal..","","Example Task","Sample","","0"
"217","442139","2184901","12/13/2019 06:39:35","Task:
This is a binary classification problem where you need to predict whether an ad buy will lead to a netgain.

Data Description:
Train.csv : 26049 x 12 [including headers] : training data set

Test.csv : 6514 x 11 [including headers] : test data set

sample_submission.csv : example for submission format of Results.csv 

Data                           Data Description

id                              Unique id for each row

ratings                      Metric out of 1 which represents how much of the targeted 
                                  demographic watched the advertisement

airlocation              Country of origin

airtime                    Time when the advertisement was aired

average_runtime   Minutes per week the advertisement was aired

targeted_sex          Sex that was mainly targeted for the advertisement

genre                      The type of advertisement

industry                  The industry to which the product belonged

economic_status   The economic health during which the show aired

relationship_status  The relationship status of the most responsive customers to 
                                   the advertisement

expensive                 A general measure of how expensive the product or service is 
                                  that the ad is discussing.

money_back_guarantee     Whether or not the product offers a refund in the case 
                                             of customer dissatisfaction.

netgain [target]               Whether the ad will incur a gain or loss when sold","","Predict the ad's success","","","2"
"208","442565","263892","12/11/2019 19:37:57","## Task Details
Use the provided data to train a machine learning model that learns how to predict the target column.

## Expected Submission
A notebook that accomplishes the task.

## Evaluation
The objective is to maximise the number of correct decisions and as a result, we're after maximising accuracy.","","Predict which customer group is worth targeting","","","1"
"6231","443056","8473885","10/01/2021 17:43:10","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. predict

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","prediction","","","1"
"442","447704","2533892","02/12/2020 05:11:07","I created this data set with the intent to develop an accurate  zero cost means by which  Autism could be detected by analysis of a facial images.To date I have used CNNs that have reached 95.0 accuracy on the test set. I really want a solution that is on the order of 98% or better. Perhaps 95% is the best that can be done with this data set because since it was collected via internet searches it may contain some level of errors. At any rate with the expertise  out there I was hoping some one would develop a more accurate kernel but to date no one has tried. If the accuracy can be improved I would post this to a website that would enable parents to submit a facial photo of their child and get back an accurate probability estimate for the presence of Autism. From that they could proceed to a full clinical diagnosis. I ask that you talented AI experts see if you can beat the 95% limit.","","Detect Autism with 98% accuracy","Use the Autism data set to detect Autism","","6"
"296","447911","473521","12/17/2019 05:49:16","## Task Details
It's common that you must work with unclean data (though not so much in Kaggle competitions). This task requires you to identify any problems in the data such as missing time periods, or outliers in the data.

## Expected Submission
A submission should be in notebook form giving a good explanation of any problems with the data, and what we could do to fix them.

## Evaluation
A good solution will find as many problems with the data as possible with the best techniques applied to clean the data.","","Clean Data","Identify problems with the dataset and make it more usable.","","1"
"297","447911","473521","12/17/2019 05:51:25","## Task Details
It's common when working on Kaggle competitions to get data that has been randomised or processed in some way. This dataset is no different. This task requires you to look past the hidden walls and try to find out what is hidden.

## Expected Submission
The submission should be a notebook revealing as much information as possible about the data. Some examples could be: explanation for which city is which, or what products are related.

## Evaluation
The best solution will make a good justification.","","Demystify the Data","Can you find the hidden secrets?","","0"
"298","447911","473521","12/17/2019 05:55:24","## Task Details
Timeseries forecasting is an important part of many businesses and ecommerce is not excluded. In this task you should try to predict the future sales for this company based on the data. One fun library to get started with is Facebook's Prophet.

## Expected Submission
The submission should be a notebook detailing the expected performance in 2020. 

## Evaluation
A good solution will have detailed predictions (e.g. daily not yearly, each product). It would also have a good estimate of the error of the prediction. This would include a validation against some portion of the data.","","Predict the Future","Can you gaze forward?","","0"
"300","449681","2483879","12/18/2019 23:54:53","The main task of this dataset is to classify Spam messages using either the original message, the BERT-Embeddings or a combination of the two","","Classify Spam messages","","","2"
"301","449681","2483879","12/18/2019 23:56:08","Study if BERT-Embeddings perform better for classification than just using TF-IDF on the original message.

You can also study if using both approaches yields better results.","","Does BERT-Embeddings work better than TF-IDF?","","","3"
"302","449681","2483879","12/18/2019 23:57:29","Derive a way to check what are the most common words for each class and how much do they influence in the classification task","","Most common words from Spam/Ham messages?","","","3"
"303","449681","2483879","12/18/2019 23:58:50","Study if there exists some Spam messages that you cannot correctly classify. If so determine what makes them distinct from other Spam messages that you can classify","","Spam messages that can't be correctly classified","","","2"
"306","449681","2483879","12/19/2019 10:25:31","During the encoding of the messages using BERT to create this dataset, punctuation and english stopwords were stripped from the messages befor encoding.

Study if this beneficial or harmful for classification. Are false positives/negatives happenning due to this fact?","","Does stopword removal mess up BERT's performance?","","","3"
"304","450143","2796256","12/19/2019 07:43:00","## IPL 2020 Qualifiers

Predict which teams will Qualify in IPL 2020

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","IPL 2020 Qualifiers","Predict which teams will Qualify in IPL 2020","05/11/2020 00:00:00","1"
"309","452308","1484847","12/21/2019 15:50:27","This includes a list of people with their salaries and job titles. Something is unambiguously wrong in how this data is formatted. Review each column and propose a change, reject rows, or reject the entire dataset, based on that error.","","101-Census","What's wrong with this dataset?","12/31/2020 00:00:00","0"
"724","452462","2809468","04/09/2020 21:06:46","## Task Details
Can we predict the cause of airplane accidents based on various attributes in the data?

## Expected Submission
Airline and Accident_cause.","","Can we predict the cause of accidents?","","","1"
"310","452468","769452","12/22/2019 10:22:21","## Task Details
Create Kernel to showcase the use of Haar Cascade resources

## Expected Submission
Kernel

## Evaluation
The kernel will highlight usability of the data","","Create Kernel to showcase dataset features","Create Kernel to showcase the use of Haar Cascade resources","01/31/2020 00:00:00","3"
"1153","455045","1314380","06/16/2020 16:01:17","## Task Details

What percentage of news articles are about entertainment?  Business?

Calculate the percentage of news articles that are on the topics of business and entertainment.

## Expected Submission

A Kaggle notebook.

## Evaluation

Is the result accurate?
Is the code easy to read and reuse?","","What percentage of news articles are about entertainment?  Business?","Calculate the percentage of news articles that are on the topics of business and entertainment.","06/18/2020 00:00:00","0"
"757","455645","4248073","04/15/2020 16:10:05","## Task Details
Create 2/3 segments/clusters based on the customer's profile features.

## Expected Submission
a csv file with two columns, Name & Cluster ID-1,2,3 etc.","","Segmentation","Create 2/3 segments/clusters based on the customer's profile features.","","1"
"1366","457124","1788308","07/15/2020 00:38:01","## Task Details
Classification or etc.

## Expected Submission
- soon

## Evaluation
- soon

### Further help
- soon","","Exoplanets classification","Kepler Telescope data","01/01/2030 00:00:00","1"
"4210","457396","7161615","04/28/2021 10:19:05","There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.

Many corporate organizations are yet to see the impact of climate change at organizational level. That the impact of Climate change on Organization's Human Resource can affect their revenues and other metrics.

This problem is an attempt to study the relationship between average global temperature and employees of an organization.","","Impact of Climate change on Organization's Human Resource","","04/29/2021 23:59:00","0"
"340","458359","1158131","12/28/2019 05:25:09","## Task Details
A fintech company disbursed car loans to customers. As loan aged some customers turned bad while others perfomed good. With the given dataset extract insight which explains characteristics of a customer. This dataset is very good for doing EDA and Feature engineering. So apply your creativity and find as much insight as you can.

## Expected Submission
The solution should contain Python code or notebook.

## Evaluation
Solution is judged on EDA and Feature engineering work.","","Find insights and explain characteristics","","","1"
"343","458893","3595464","12/29/2019 02:50:04","## Task Details
This is a task at finding some interesting insights into our dataset. 
See the mean of goals for each team.

## Expected Submission
Exist goals difference between teams in different seasons?

## Evaluation
Lets begin with grouping matches by goal difference between home and away side in the same season.","","Matches by goal difference in the same league","Group matches by goal difference.","12/01/2020 00:00:00","1"
"1006","459013","4756289","05/29/2020 14:45:14","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","depression_id","","","2"
"3867","460235","3925987","03/26/2021 08:51:14","## Task Details

As part of my https://robertolofaro.com/datademocracy, in January 2020 released a sample dashboard on EU Member States status vs. UN SDG.

Specifically, I had selected few that are related to the themes I am writing on (digital transformation, innovation, cultural and organizational change).

But, actually, had used the material for few articles since late November 2019, e.g.

* [2019-11-26 just few doubts on #national and #local challenges in #Italy and #Turin - how well we are coping with them - #industry #development #SDG #smartcity](http://robertolofaro.com/index.php?page=381)
* [2019-12-30 Assessing systemically: the data side of #European #Union #integration within #digital #transformation](http://robertolofaro.com/index.php?page=391)

The dataset is small- purposedfully: if you need details, you can visit the source website (see the [documentation of this dataset](https://www.kaggle.com/robertolofaro/sdgeu-datamart-sample)).

The aim is to 
1. identify correlations between KPIs (e.g. look at the discussion about the KPI IT_USE_ii99 within the 2019-12-30 article listed above)
2. find supporting datasets that are not within those that I already posted online
3. identify further tasks (or subtasks)
4. considering that the KPIs should be updated yearly until 2030, define and implement a pipeline that automates as much as possible activities (e.g. from scraping to cleaning/transformation to aggregation and presentation).

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask02 : Identify correlations between KPIs on UN SDGs for EU 27","finding clustering of countries across KPIs","","0"
"1756","461566","4587023","08/19/2020 14:32:13","Bom dia, qual √© o tipo de ativo coletado ?","","D√∫vida sobre o tipo de ativo","","","0"
"1347","462077","3203431","07/13/2020 07:49:29","## Task Details
We need to add more dataset details because this is a very good and important dataset from the Fama French Data library and it is important that we go ahead and add those changes to increase its usability so that more people can utilize it.","","Add more Dataset Details","","","1"
"376","463916","4188155","01/13/2020 07:06:56","# seperate column for country","","Seperate players country wise","Seperate players country wise","","13"
"365","464405","1314380","01/09/2020 03:10:30","## Task Details

Which metros have the highest median price for a single family home?
What are the top 10 most expensive metros in the USA?

## Expected Submission

A notebook and a plot that answers the question.

## Evaluation

Accuracy of answer, quality of documentation, and reusability of code","","What are the 10 most expensive metros in the USA?","Which metros have the highest median price for a single family home?","","3"
"369","464405","1314380","01/10/2020 02:21:04","## Task Details

Which metros have the highest (rent : home value) ratio for a 2 bedroom unit?  What are the top ten cities where the rent is high compared to the value of the home?

## Expected Submission

A notebook containing a plot that answers the question.

## Evaluation

Accuracy, documentation, reusability of code","","Which metros have the highest (rent : home value)?","Median 2 bedroom rent divided by median 2 bedroom home value","","8"
"373","465931","2856867","01/12/2020 18:28:51","## Task Details
There is need to prepare good data exploration on the current dataset. Feel free in your experiments.","","Data exploration (EDA)","","","0"
"374","465931","2856867","01/12/2020 18:30:37","## Task Details
Build a model with good regression results on the current dataset.

## Evaluation
Get the better mae metric.","","Solve the regression problem","","","0"
"351","467000","1477377","01/07/2020 00:05:03","## Task Details
In the final cluster.  I;d like to know which groups of interests are for each groups of people.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","In the final cluster","In the final cluster","","1"
"5409","467062","7526686","07/29/2021 16:34:52","Can you please update the list sirüôè","","Request","","","0"
"1001","467291","1148841","05/29/2020 12:18:51","kblanquera found out that there are some issues in the matching between books and authors.

The source code must be debugged and a new version of the dataset must be created.","","Fix authors mismatch","","","0"
"1076","467291","1148841","06/06/2020 14:56:31","Add rating breakdown","","Add rating breakdown","","","0"
"2313","468218","5320059","10/03/2020 08:13:33","## Task Details
1.  Use the pitstop data to calculate the time wasted in the pitstop and adjust the pitstop wasted time in the driver lap timings and come up with a new pole positions and see which teams can benefit from optimizing the pitstop effectively.
2. Use the data provided above and come up with the feature that can be strongly correlated to win/loss in the race.
## Expected Submission
Primary task should be focused more and the secondary task results can be used to optimize the results of the first task.

## Evaluation
Minimum deviation from the actual data for pole position for the test data. 
Participants should keep in min that there are no standards to define which team wins after the adjusted pitstop timings, it is purely theoritical.

##for further reference
Visit Formula 1 website:  https://www.formula1.com/","","Strategy formulating feature","","","15"
"367","468229","3808416","01/09/2020 15:56:50","## Task Details
Want to work on cutting edge machine learning applied to real-time manufacturing process control? [Liveline Technologies](https://www.liveline.tech) is developing a novel approach, and we have systems running live production today. Show us your ML chops by predicting output characteristics from various sensor input data!

## Expected Submission
You should submit a notebook that illustrates your approach to predicting output sequences and generates a validation score.

## Evaluation
The primary outputs of interest are from the first process stage (see notes on dataset). Outputs consist of 15 dimensional measurements taken each second. There are 3 evaluation criteria for models:

**1. Accuracy.**   Defined by RMSE of predicted vs observed outputs over the prediction horizon.
**2. Prediction horizon.**   How far ahead can you predict with reasonable accuracy? 30 time steps? 60? 120?
**3. Number of outputs.**   Some of the 15 output measurements are easier to predict than others. How many can you predict?

Bonus points if you can predict the 15 output measurements from the second stage... it's much more challenging!

## Hints
Signal processing, feature generation, and other transformations may be very helpful...","","Output sequence prediction","Recruiting challenge from Liveline Technologies","","0"
"363","468361","4190967","01/09/2020 01:57:11","## Detalhes da Tarefa
Existe uma teoria mistica atr√°s da aleatoriedade? H√° mais n√∫meros mais prov√°veis de sair do que outros? J√° saiu alguma vez uma sequ√™ncia de numeros da mesma dezena? Porque tantos jogos se acumulam e ningu√©m ganha?


## Brincando, estudando e se divertindo
N√£o queremos chegar a lugar nenhum. Mas penso que podemos nos divertir muito aprendendo mais sobre Data Science e essas teorias.","","Calcular dist√¢ncia e testar combina√ß√µes","Testar novos m√©todos de intera√ß√£o","01/31/2020 00:00:00","0"
"355","468367","796185","01/07/2020 22:00:41","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the—ã—ã solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","???????? ??? ??????","","03/30/2020 00:00:00","0"
"357","468425","2568266","01/08/2020 11:31:37","## Task Details
I suspect that the assignation of id_number is not random.

## Expected Submission
We expect a kernel capable of extract some information of id_distribution.

## Evaluation
Any lead that indicates that the id_distribution is not uniform, or random is a huge step.","","What info is hiding the id_number distribution?","","","2"
"358","468425","2568266","01/08/2020 11:34:11","## Task Details
Some data of the 2.5km checkpoint is missing, but this people have finished the race nevertheless.

## Expected Submission & Evaluation
Any insight on determining if this NaN indicates cheating and not an error data would be awesome!","","Did some people cheat in the race?","","","0"
"359","468425","2568266","01/08/2020 11:38:20","## Task Details
Some would say that young people have more energy than older people. But they are also less disciplined. Could it be that younger people started the race with a faster pace, but got slowlier during the race than older people?

## Expected Submission & Evaluation
Any insight in favor of this hypothesis (or refuting it) is highly appreciated.","","Are younger people more erratic?","","","0"
"2227","468445","1309432","09/25/2020 02:25:04","## I need your help!","","test task!","","","0"
"362","469529","612672","01/08/2020 22:25:33","## Task Details
I want to test the task feature 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","my task","","01/15/2020 00:00:00","0"
"371","472357","3334414","01/11/2020 14:41:25","## Task Details
Plot a visualization showing the difference in call quality via State. Even better if you can create different visuals to show the change in quality over time.

## Expected Submission
Submission can include a kernel which uses the dataset to create a visualization.

## Helpful resources
You can try this [mini course](https://www.kaggle.com/alexisbcook/your-first-map) to learn how to plot data in geographic maps.","","Call quality map","Plot a map showing overall call quality in India, by State","","0"
"377","473216","2226962","01/13/2020 20:36:32","* predicting the future results of marketing companies based on available statistics and, accordingly, formulating recommendations for such companies in the future.

* building a profile of a consumer of banking services (deposits).","","Predicting future results","build model with best ROU_AUC score","","1"
"375","473642","1640261","01/12/2020 18:44:33","## Task Details
Every task has a story. Tell users what this task is all about and why you created it..

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","My test task","","","0"
"381","476283","3833504","01/15/2020 20:15:44","Source the average data scientist salary (maybe from Glassdoor/Kaggle survey) and try and find places with high Data Science salaries/low cost of living. Would help anyone looking for somewhere to work!","","Add Average Data Scientist Salary","","","20"
"380","477308","2198463","01/15/2020 16:55:56","## Task Details
Need to make (NN) and use this dataset to classify new sentences to it's corresponding sentiments.","","Sentiment Analysis using ANN","","01/25/2020 00:00:00","0"
"385","479954","2226962","01/17/2020 19:17:27","using the data to predict whether the patient will fall ill and indicate the causes that most affect the disease","","Predict patient illness","","","7"
"713","480074","3740202","04/08/2020 13:34:36","## Task Details
Check which regions are most affected by weather events in the USA and make weather forecasts based on data information.","","Prevendo eventos clim√°ticos - EUA","Check which regions are most affected by weather events in the United States and predict weather events.","","1"
"872","481889","5047132","05/10/2020 10:43:13","we are try design model which can differentiate gender by photo

## Expected Submission
provide much accurate model

## Evaluation
by verifying image either male or female","","man women classification","using CNN","","1"
"388","484552","4284047","01/22/2020 05:51:46","Country wise runs in cricket","","Runs scored by country","Country wise runs in cricket","01/31/2020 00:00:00","1"
"397","485425","453212","01/24/2020 03:48:16","## Task Details
In my project, I had three success criteria: Days profitable, days better than the S&P500, and home run (will it triple in value during the first year on the market). Are these the best ways to look at this problem? Is there some other way to pick successful stocks that can be measured better? 

## Expected Submission
A kernel (preferably a notebook) detailing why a success criterion is more efficent f for investing than the ones created above. 

## Evaluation
Be creative, this is an open-ended problem that I'm sure financial institutes around the world would be interested in hearing different view points from.","","New Success Criteria","","","0"
"6763","486282","5647653","11/05/2021 11:22:57","EDA on Countries participated, disciplines, events.","","EDA on Countries participated, disciplines, events.","","","1"
"392","486699","4285820","01/23/2020 00:22:33","## Question
What else do people ages 19-24 who've broken bones have in common with each other?

## Submission and Evaluation
This is just for fun! Do what you want.","","Commonalities Task #1","What else do people ages 19-24 who've broken bones have in common with each other?","","1"
"393","486699","4285820","01/23/2020 00:23:03","## Question
What else do people who've both ran a marathon and had braces have in common?

## Submission and Evaluation
This is just for fun! Do what you want.","","Commonalities Task #2","What else do people who've both ran a marathon and had braces have in common?","","1"
"395","487438","2226962","01/23/2020 16:03:29","Use any NN for classification over 5 classes of coins.","","Classification task with NN","","","0"
"414","487592","370586","01/28/2020 19:03:45","**This task is for beginners!**

## Task Details
Select rows 13, 42 and 77 of columns datetime, temp and cnt.

## Expected Submission
Your own public notebook based on this dataset.","","2 - Select some columns and rows","","","1"
"399","487592","370586","01/24/2020 05:47:17","**This task is for beginners!**

## Task Details
Create your own notebook based of this dataset and make it public for the community to see. 
Go to Kernels -&gt; Your Work -&gt; Click on your notebook -&gt; Sharing -&gt; Set Privacy to Public -&gt; Save

## Expected Submission
Your own public notebook based on this dataset.","","1 - Create your own notebook and make it public","","","1"
"396","487786","453212","01/24/2020 03:43:34","## Task Details
Low-level languages are less and less popular in the data science community, is there a term (in this dataset or outside) that is most related to that? 

## Expected Submission
Produce a kernel (preferably notebook) explaining your reasoning and why you think this claim is valid or not. 

## Evaluation
Like all data science, results are disputable, do your best to refute or back the claim and see what the rest of the community has to say, and remember to have fun learning!","","The downfall of low level languages?","","","0"
"403","488167","1630013","01/25/2020 07:25:26","## Task Details
* Make the prediction of Category using Description or title of video
* EDA

## Evaluation
F1-score","","Prediction of Category","Make the prediction of Category using Description or title of video","","1"
"3726","488316","2658406","03/10/2021 16:30:49","## Task Details
Sitting in 2021, being a PF fan , I miss them so much. So I thought let`s try to recreate them. I know it is not possible to recreate the greats but atleast we can try. Using AI we can do two things viz.
1) create original lyrics using nlp from their existing ones.
2) Give tunes to these 'AI created lyrics' using AI(of course).

## Expected Submission
Generate original songs which gives the vibe of pink floyd. There is no metrics for evaluation so tune your model until it satisfy you xD.","","Lyrics Generation","Use your machine to write lyrics like Gilmour, Waters and Barrett. ( NLP)","","0"
"3387","488403","2337246","02/03/2021 11:09:47","You have to forecast the energy demand","","Forecasting","","","0"
"3388","488403","2337246","02/03/2021 11:12:06","Get some insights out of the data and present it in beautiful visualization.","","Insights","","","0"
"404","488693","2382518","01/25/2020 07:26:30","visualize the data based on location which has more pollution level","","visualize the data which with respect to location","","","0"
"1409","783964","767","07/17/2020 23:44:16","## Task Details
This dataset is created for Phase 3 of the 3rd Deep Learning and AI Summer/Winter School (DLAI3). More details about the challenge can be found at https://www.kaggle.com/c/dlai3/.

Medical image analysis has continually been an area of prominent and growing importance, in both the research and application aspects. However, it is important to realize that the algorithms, methodology, as well as the source of data, need to be strictly scrutinized. Phase 3 of the DLAI3 Hackathon is a challenge that provides a mixed dataset of publicly available COVID-19 chest x-ray images from various sources, a pneumonia dataset obtained from a study on children, partially verified thorax CXR images of adults from NIH, and unverified no-finding images from NIH. See below for the sources of these data.

The participants are challenged to obtain good performance in their modeling in the Public and Private Leaderboards, as well as to be able to interpret their results in form of a full paper submission to CSBio2020 under CHAT-2020 (www.csbio.org/2020/).

Please cite this dataset as follows: Jonathan H. Chan, ‚ÄúDLAI3 Hackathon Phase3 COVID-19 CXR Challenge.‚Äù Kaggle, doi: 10.34740/KAGGLE/DSV/1347344.

## Expected Submission
For those interested to compete against others, look for instructions under Discussions at https://www.kaggle.com/c/dlai3/.

In general, feel free to submit a notebook that outlines the data preprocessing, model building and evaluation. You may use this dataset plus any other data you have available. 

## Evaluation
This is typical of a real-life dataset that is imbalanced. Be aware of the different sources. For the DLAI3 Phase 3 challenge, the evaluation will be macro F1 average.

### Further help
A similar but smaller dataset was released earlier at https://www.kaggle.com/tawsifurrahman/covid19-radiography-database. Some kernels are available there for starters.","","Multi-class Chest X-Ray (CXR) Modeling","","11/18/2020 00:00:00","0"
"1749","836676","8503","08/19/2020 01:53:28","## Task Details
1. Load the data from the CSV files
2. Explore each dataset - columns, counts, basic stats
3. Understand the domain context and explore underlying patterns in the data
4. Explore the data and try to answer questions like - 
- What is the mean value of daily yield?
- What is the total irradiation per day?
- What is the max ambient and module temperature?
- How many inverters are there for each plant? 
- What is the maximum/minimum amount of DC/AC Power generated in a time interval/day?
- Which inverter (source_key) has produced maximum DC/AC power? 
- Rank the inverters based on the DC/AC power they produce
- Is there any missing data?

You might have to pre-process the data to allow for some of the analysis (hint: date and time)

## Expected Submission
Submit your notebook with all your work.

## Evaluation
You will be evaluated on how detailed and comprehensive is your exploration.","","Descriptive analytics","Initial exploration of the dataset","","69"
"1823","836676","8503","08/25/2020 02:16:59","## Task Details
Create a fresh notebook with visualizations that help understand the data and underlying patterns. 

1. Start with graphs that explain the patterns for attributes independent of other variables. These will usually be tracked as changes of attributes against DATE_TIME, DATE, or TIME. Examples - how is DC or AC Power changing as time goes by? how is irradiation changing as time goes by? how are ambient and module temperature changing as time goes by? how does yield change as time goes by? Explore plotting variables against different granularities of DATE_TIME and which is the best option for each variable.

2. Plot two variables against each other to discover degree of correlation between them. Try out different variable pairs - ambient and module temperature, DC and AC Power, Irradiation and module/ambient temperature, irradiation and DC/AC Power. Can you find different ways of visualizing the above relationships?

## Expected Submission
Submit your notebook with all your work.

## Evaluation
As always, you will be evaluated how thorough is your work. There are extra marks for creativity on this one - the more interesting and varied your graphs the more points you get.","","Visualization and further exploration","Employ different visualization techniques to understand the data and underlying patterns","","34"
"1824","836676","8503","08/25/2020 02:26:54","## Task Details
This is probably the most important skill for a Data Scientist - the ability to communicate the story. You have worked hard over the last two tasks for understand the data and discover patterns and relationships. It is time to put all that to good use. 

Find one story worth telling based on all your work. Create a notebook that walks the viewer through the entire story, one step at a time. 

A few tips - 
- Pick an interesting conclusion that you want to arrive at
- Build a logical progression from loading and pre-processing data to showing minor observations along the way and eventually building up to the grand finale
- Substantiate your argument with data along the way (you are a data scientist not just a story teller :))
- Every good story has some key elements - characters, setting, plot, complication and solution, try to build as many of them as you can.
- Deliver for the aha moment! Give the user an insight that can potentially impact the business. If the viewer doesn't get that then they won't appreciate your effort.


## Expected Submission
Submit your notebook with all your work.

## Evaluation
You will be graded on - 
- how engaging is your story?
- how well did you substantiate it with data?
- how interesting was the conclusion/key insight?
- how good were your visualizations?","","Tell a story","","","11"
"1932","836676","8503","09/03/2020 14:26:17","## Task Details
Please submit your competition entries to this task.

## Expected Submission
Your notebooks with your work that you will be presenting to the expert panel.

## Evaluation
You will be evaluated on the following criteria - 
1. Thoroughness of your data analysis
2. Quality of data visualization
3. Understanding of tech as well as domain
4. Are you providing actionable insights?
5. Softer aspects - flow of the storyline, confidence in your outcomes, ability to justify your analysis and answer questions","","Competition","","","13"
"1969","783223","18463","09/06/2020 08:53:32","Forecast Rail Usage by Country until 2050Using this data and external data, forecast number of passengers per country per year.

Data comes from https://en.wikipedia.org/wiki/List_of_countries_by_rail_usage
Overview
Almost 10,000 billion freight tonne-kilometres are travelled around the world. Roughly one quarter of these are travelled in the United States, another quarter in China, and a third in Russia. Of the 3,000bn passenger-kilometres travelled across the world, 1,346bn of these are travelled solely in China. The average Swiss person travels 2,430 km by train each year, almost 500 more than the average Japanese person (the Japanese having the second-highest average kilometres travelled per passenger in the world).

In 2014, there were around 1 million kilometres of railway in the world (a decrease of 3% compared to 2013). Of this, 350,000 km were in Europe and mainly used for passenger service, 370,000 km were in North America and mainly used for freight, and 230,000 km were in Asia and used for both freight and passenger service.In America and Europe, there are many low cost airlines and motorways which compete with rail for passenger traffic, while Asia has seen a large growth in high-speed rail with 257bn pkm representing 72% of total world high-speed rail passenger traffic.","","Forecast Rail Usage by Country until 2050","Forecast Rail Usage by Country until 2050","12/31/2021 23:59:00","0"
"1997","783764","18463","09/07/2020 06:44:18","Forecast number of medals by country for next Winter Olympic Games
Cross-country skiing has been contested at the Winter Olympic Games since the first Winter Games in 1924 in Chamonix, France. The women's events were first contested at the 1952 Winter Olympics.","","Forecast number of medals by country for next Winter Olympic Games","Forecast number of medals by country for next Winter Olympic Games","12/31/2026 23:59:00","0"
"1458","793371","18463","07/22/2020 18:13:05","√† partir des donn√©es du dataset mais pas seulement, analyser les articles qui traitent du covid19","","Analyser l'historique de publications Covid19","√† partir des donn√©es du dataset mais pas seulement","09/30/2020 00:00:00","0"
"1459","793385","18463","07/22/2020 18:30:17","Documentation du rapport sur la mobilit√© au format CSV
Cet ensemble de donn√©es est con√ßu pour contribuer √† att√©nuer l'impact du COVID-19. Il ne doit pas √™tre utilis√© √† des fins de diagnostic, pronostic ni traitement m√©dical. Il n'est pas non plus destin√© √† servir de guide pour organiser des d√©placements personnels.

Les donn√©es pr√©sentent l'√©volution de la fr√©quentation de lieux tels que les magasins d'alimentation et les parcs dans chaque zone g√©ographique. Pour savoir comment utiliser cet ensemble de donn√©es dans le cadre de votre travail, consultez l'aide concernant les rapports sur la mobilit√© de la communaut√©.

Sachant que la pr√©cision de la position et la d√©finition des cat√©gories de lieux varient d'une r√©gion √† l'autre, nous ne recommandons pas d'utiliser ces donn√©es pour comparer les variations entre des pays ou entre des r√©gions aux caract√©ristiques diff√©rentes (par exemple, une zone rurale et une zone urbaine).

Nous excluons toute r√©gion ou cat√©gorie de l'ensemble de donn√©es si nous ne disposons pas de suffisamment de donn√©es pertinentes d'un point de vue statistique. Pour en savoir plus sur la mani√®re dont nous calculons ces tendances tout en prot√©geant la vie priv√©e, lisez la section √Ä propos de ces donn√©es ci-dessous.

Cat√©gories de lieux
Alimentation et pharmacies
Tendances de mobilit√© dans les lieux tels que les march√©s alimentaires, les magasins d'alimentation, les march√©s de producteurs, les √©piceries fines, les parapharmacies et les pharmacies.

Parcs
Tendances de mobilit√© dans les lieux tels que les parcs municipaux, les parcs nationaux, les plages publiques, les marinas, les parcs canins, les places et les jardins publics.

Arr√™ts transp. en commun
Tendances de mobilit√© dans les lieux tels que les infrastructures de transports en commun (par exemple, les stations de m√©tro, les arr√™ts de bus, les gares routi√®res et les gares ferroviaires).

Commerces et loisirs
Tendances de mobilit√© dans les lieux tels que les restaurants, les caf√©s, les centres commerciaux, les parcs √† th√®me, les mus√©es, les biblioth√®ques et les cin√©mas.

Lieux de r√©sidence
Tendances de mobilit√© dans les lieux d'habitation.

Lieux de travail
Tendances de mobilit√© dans les lieux de travail.","","Show with graphics the impact of COVID19 on Mobility","","12/31/2020 00:00:00","1"
"1466","794826","18463","07/23/2020 14:01:05","A partir des donn√©es d√©crites ici : https://www.data.gouv.fr/fr/datasets/r/05f5389a-8465-46f9-b9f2-5d6d6552c4a5

Estimer l'√©volution des PTZ pour la fin de l'ann√©e 2020 et 2021","","Forecast PTZ for next year","","09/30/2020 00:00:00","1"
"1259","748584","18463","07/04/2020 21:20:37","## Task Details
Compare countries by happiness and other human metrics

## Expected Submission
A notebook with your insights

## Evaluation
Graphics, EDA and explainations

### Further help
Read previous report here
https://worldhappiness.report/","","Compare countries by happiness and other human metrics","by continent, by GPD","","10"
"1968","857070","18463","09/06/2020 08:42:03","Using this history and external data, forecast world Top Manufacturers


Top 10 Largest Manufacturing Companies in the World 2020, Global Manufacturing Industry Factsheet
What is the Current State of the Global Manufacturing Industry?
The top 10 largest manufacturing companies in the world continue to lead the way for the global manufacturing industry in revenue and growth. The leading companies operate in many sectors including automotive, electronics, medical and technology. Relationships are now global, as companies all over the world are looking for different characteristics that are enticing at a B2B level including the efficiency of manufacturing and production output, environmental policies, and pro-business practices.

This fact sheet is to assist business professionals, investors, and people in general that are interested in developing relations with global clients including:

 

Discover who the top 10 largest manufacturing companies in the world are?
What are some manufacturing industry stats and insights to leverage in 2020?
Which countries have the best manufacturing output?
What is the largest manufacturing industry in the world?
Insights into the global manufacturing sector performance
 

BizVibe is already helping the top manufacturing companies in the world connect. Connect and track the latest news and insights from these companies.","","Forecasting World Top 10 Manufacturers until 2030","Forecasting World Top 10 Manufacturers until 2030","12/31/2021 23:59:00","1"
"1971","857073","18463","09/06/2020 10:45:40","Forecast Mobile Cell Subscriptions by Country

Mobile cellular subscriptions (per 100 people)
International Telecommunication Union, World Telecommunication/ICT Development Report and database.

This statistic shows the top countries by number of mobile cellular subscriptions in 2018. In China, there were more than 1.64 billion mobile cellular subscriptions in use in 2018.","","Forecast Mobile Cell Subscriptions by Country","Forecast Mobile Cell Subscriptions by Country","12/31/2021 23:59:00","0"
"1970","857078","18463","09/06/2020 10:37:42","Long-term interest rates refer to government bonds maturing in ten years. Rates are mainly determined by the price charged by the lender, the risk from the borrower and the fall in the capital value. Long-term interest rates are generally averages of daily rates, measured as a percentage. These interest rates are implied by the prices at which the government bonds are traded on financial markets, not the interest rates at which the loans were issued. In all cases, they refer to bonds whose capital repayment is guaranteed by governments. Long-term interest rates are one of the determinants of business investment. Low long-term interest rates encourage investment in new equipment and high interest rates discourage it. Investment is, in turn, a major source of economic growth.","","Forecast Long Term Interest by Country until 2030","Forecast Long Term Interest by Country until 2030","12/31/2021 23:59:00","0"
"1963","859298","18463","09/05/2020 18:35:39","Parking signs recognition and localization aim to extract and digitize accurate on-street parking restrictions. Today‚Äôs visual data collection, annotation, and analysis practices are still costly, prone to error, and cumbersome as performed manually. While online street-level imagery databases contain updated panoramic images of all signs, their potential for understanding on-street parking restrictions at scale has not been fully explored. The key benefit of these databases is that once the parking signs are detected, accurate geographic coordinates of the detected signs can be automatically determined and visualized within the same platform. This paper evaluates the application of a computer vision-based method for parking signs recognition from street-level imagery aimed at facilitating the parking in dense cities. The method extracts images and leverages a sliding window mechanism to detect potential candidates for parking signs. A Histogram of Oriented Gradients for each candidate is formed and trained. Hard negative mining approach is then used at final stage to remove the remaining false positives. The potential detections of images from multiple viewpoints are then combined to locate the position of the signs on a map. Experimental results with high accuracy demonstrate the potential of leveraging street-level images and provide a viable solution for digitizing at scale all parking signs to help drivers understand parking rules and avoid fines.","","Automated Recognition and Localization of Parking Signs Using Street-Level Imagery","Automated Recognition and Localization of Parking Signs Using Street-Level Imagery","12/31/2020 23:59:00","0"
"1962","859525","18463","09/05/2020 18:29:06","This dataset contains 3,984 medical sentences extracted from PubMed abstracts and relationships between discrete medical terms were annotated. This dataset focuses primarily on ‚Äútreat‚Äù and ‚Äúcause‚Äù relationships, with 1,043 sentences containing treatment relations and 1,787 containing causal ones.

Human-in-the-loop annotators were given two different terms (such as ‚ÄúLewy Body Dementia‚Äù and ‚ÄúWell-formed Visual Hallucinations‚Äù) and were asked to mark the relationship between those terms (in this case ‚ÄúLewy Body Dementia causes Well-Formed Visual Hallucinations).

This corpus has been referenced in the following papers:

Anca Dumitrache, Lora Aroyo, Chris Welty: CrowdTruth Measures for Language Ambiguity: The Case of Medical Relation Extraction. LD4IE at ISWC 2015.
Anca Dumitrache, Lora Aroyo, Chris Welty: Achieving Expert-Level Annotation Quality with CrowdTruth: The Case of Medical Relation Extraction. BDM2I at ISWC 2015.
 

The input data for this job are sentences from medical publications, medical relations, and a pair of key terms from that sentence.","","Create model to classify relation between term1 and term2","Create model to classify relation between term1 and term2","12/31/2020 23:59:00","0"
"1972","859862","18463","09/06/2020 10:50:30","Forecast Cherries Producing Countries until 2030

You can use this data and external data. Thank to share external data into forum

This is a list of countries by cherry production from the years 2016 to 2018, based on data from the Food and Agriculture Organization Corporate Statistical Database. The estimated total world production for 2018 was 4,076,944 metric tonnes, increasing by 12.9% from 3,611,283 tonnes in 2017.","","Forecast Cherries Producing Countries until 2030","Forecast Cherries Producing Countries until 2030","12/31/2021 23:59:00","0"
"1975","860836","18463","09/06/2020 14:50:11","Find most important features for 16 Personality Factors Test


This is a free online meaure of Cattell's 16 personality factors.

Introduction
In his explorations of personality, British psychologist Raymond Cattell found that variations in human personality could be best explained by a model that has sixteen variables (personality traits), using a statisical procedure known as factor analysis. Following this discovery he went on to create and promote the 16PF Questionnaire. This test uses a public domain scales from the Internation Personality Item Pool to measure the same traits.

Procedure
This personality test consists of 164 statements about yourself, for each indicate how accurate it is on the scale of (1) disagree (2) slightly disagree (3) niether agree nor disagree (4) slightly agree (5) agree. It will take most people around ten minutes to complete.","","Find most important features for 16 Personality Factors Test","Find most important features for 16 Personality Factors Test","12/31/2020 23:59:00","0"
"2014","860871","18463","09/08/2020 13:46:18","Timeseries Most Popular websites until 2030

Alexa ranks websites based on a combined measure of page views and unique site users, and creates a list of most popular websites based on this ranking time-averaged over three-month periods. Only the site's highest-level domain is recorded, aggregating any subdomains.","","Timeseries Most Popular websites until 2030","Timeseries Most Popular websites until 2030","12/31/2021 23:59:00","0"
"1976","860876","18463","09/06/2020 14:57:25","Find relevant features using 73k Answers to the Machivallianism Test

In the field of personality psychology, Machiavellianism is a personality trait centered on manipulativeness, callousness, and indifference to morality.[1] Though unrelated to the historical figure or his works, the trait is named after the political philosophy of Niccol√≤ Machiavelli, as psychologists Richard Christie and Florence Geis used edited and truncated statements inspired by his works to study variations in human behaviors.[2][3][4] Their Mach IV test, a 20-question, Likert-scale personality survey, became the standard self-assessment tool and scale of the Machiavellianism construct. Those who score high on the scale (High Machs) are more likely to have a high level of deceitfulness and a unempathetic temperament.[5]

It is one of the dark triad traits, along with narcissism and psychopathy.

From Wikipedia page : https://en.wikipedia.org/wiki/Machiavellianism_(psychology)","","Find relevant features using 73k Answers to the Machivallianism Test","Find relevant features using 73k Answers to the Machivallianism Test","12/31/2021 23:59:00","1"
"2037","860921","18463","09/10/2020 06:40:27","https://openpsychometrics.org/tests/RSE.php
This is an free online version of the Rosenberg Self Esteem Scale.

Validity
This scale is the most widely used measure of self esteem for research purposes but it is NOT a diagnostic aid for any for any psychological issues of states. If you are worried that your self esteem may reflect poor mental health please consult your doctor. The scale has been used in more than one hundred research projects.

Because the concept of self esteem is one most people should be familar with, this test will proably not tell you anything you do not allready know. You should have a pretty good grasp of your results just by asking yourself the question, ""do I have low self esteem?"" The scale can however give you a better picture of your state in relation to other people. Your results will also include a little bit more about the relationship between self esteem and life outcomes.

Procedure
The scale consists of ten statements that you could possibly apply to you that you must rate on how much you agree with each. The items should be answered quickly without overthinking, your first inclination is what you should put down.

Participation
In addition to being offered for public education purposes, this survey is being used as part of a research project and your answers will be recorded. By starting this test you are agreeing to have any data you enter used for research.","","Analyze 48k Answers to the Rosenberg Self-Esteem Scale","Analyze 48k Answers to the Rosenberg Self-Esteem Scale","","0"
"1977","860980","18463","09/06/2020 16:13:38","Analyze 52k Answers to the Experiences in Close Relationships Scale.


This is an online version of the Experiences in Close Relationships Scale, a test of attachment style.

Introduction
Attachment style is how an individual behaves in relationships with other.

The ECR was created in 1998 by Kelly Brennan, Catherine Clark and Phillip Shaver. It groups people into four different categories on the basis of scores along two scales.

Procedure
The inventory consists of thirty six that must be rated on how characteristic they are of the subject. The test should not take most people more than four minutes.
https://openpsychometrics.org/tests/ECR.php","","Analyze 52k Answers to the Experiences in Close Relationships Scale.","Analyze 52k Answers to the Experiences in Close Relationships Scale.","12/31/2021 23:59:00","0"
"1978","860981","18463","09/06/2020 16:29:52","Analyze 18k Answers to the Multidimensional Sexual Self-Concept Questionnaire.
This is an interactive version of the Multidimensional Sexual Self-Concept Questionnaire.

Introduction
Sexual self-concept is a persons own view of their sexual behaviours and actions.

The MSSCQ was created by Dr. William E. Snell, Jr. in 1995 for the general study of sexuality. It measures along twenty separate scales.

Procedure
The inventory consists of one hundred statements that must be rated on how characteristic they are of the subject. The test should not take most people more than eight minutes.","","Analyze 18k Answers to the Multidimensional Sexual Self-Concept Questionnaire.","Analyze 18k Answers to the Multidimensional Sexual Self-Concept Questionnaire.","12/31/2020 23:59:00","0"
"1981","861000","18463","09/06/2020 16:32:10","Analyze 318k Answers for Open Sex Role Inventory.


Open Sex-Role Inventory
This is an interactive personality test measuring masculinity and femininity (or gendered personality traits) modeled on the Bem Sex-Role Inventory.

Background
In the 1970s Sandra Bem developed the Bem Sex-Role Inventory to challenge the view the masculinity and femininity were polar opposites and that a masculinity-femininity not matching your gender was a sign of poor mental health. Bem thought that it was possible to be both masculine and feminine at the same time and that this was the healthiest psychological state. The Open Sex Role Inventory was developed as open source, modernized measure of masculinity and femininity. The documentation of its development can be found here.


Test Instructions
The test has 22 statements of opinion that you must rate on a seven point scale of how much you agree with each. It should take most people 4-6 minutes to complete.


Participation
This test is provided for educational and entertainment use only. It should not be used as psychological advice of any kind and comes without any guarantee of accuracy or fitness for any particular purpose. Also, your responses may be recorded and anonymously used for research or otherwise distributed.
https://openpsychometrics.org/tests/OSRI/","","Analyze 318k Answers for Open Sex Role Inventory.","Analyze 318k Answers for Open Sex Role Inventory.","12/31/2020 23:59:00","0"
"1984","861152","18463","09/06/2020 19:11:24","Find most important features in cholesterol

Cholesterol treated as the class attribute.

As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction using instance-based learning with encoding length selection. In Progress in Connectionist-Based Information Systems. Singapore: Springer-Verlag.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Publication Request: &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; This file describes the contents of the heart-disease directory.

This directory contains 4 databases concerning heart disease diagnosis.
All attributes are numeric-valued.  The data was collected from the
four following locations:
 
  1. Cleveland Clinic Foundation (cleveland.data)
  2. Hungarian Institute of Cardiology, Budapest (hungarian.data)
  3. V.A. Medical Center, Long Beach, CA (long-beach-va.data)
  4. University Hospital, Zurich, Switzerland (switzerland.data)
 
Each database has the same instance format.  While the databases have 76
raw attributes, only 14 of them are actually used.  Thus I've taken the
liberty of making 2 copies of each database: one with all the attributes
and 1 with the 14 attributes actually used in past experiments.
 
The authors of the databases have requested:
 
   ...that any publications resulting from the use of the data include the 
   names of the principal investigator responsible for the data collection
   at each institution.  They would be:
 
    1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
    2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
    3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
    4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:
       Robert Detrano, M.D., Ph.D.
 
Thanks in advance for abiding by this request.
 
David Aha
July 22, 1988
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;","","Find most important features in cholesterol","Find most important features in cholesterol","12/31/2021 23:59:00","1"
"1985","861160","18463","09/06/2020 19:17:39","Find relevant features to sleep quality and quantity.

Data from StatLib (ftp stat.cmu.edu/datasets)

Data from which conclusions were drawn in the article ""Sleep in Mammals: Ecological and Constitutional Correlates"" by Allison, T. and Cicchetti, D. (1976), Science, November 12, vol. 194, pp. 732-734. Includes brain and body weight, life span, gestation time, time sleeping, and predation and danger indices for 62 mammals.

Variables below (from left to right) for Mammals Data Set:

species of animal

body weight in kg

brain weight in g

slow wave (""nondreaming"") sleep (hrs/day)

paradoxical (""dreaming"") sleep (hrs/day)

total sleep (hrs/day) (sum of slow wave and paradoxical sleep)

maximum life span (years)

gestation time (days)

predation index (1-5) 1 = minimum (least likely to be preyed upon) 5 = maximum (most likely to be preyed upon)

sleep exposure index (1-5) 1 = least exposed (e.g. animal sleeps in a well-protected den) 5 = most exposed

overall danger index (1-5) (based on the above two indices and other information) 1 = least danger (from other animals) 5 = most danger (from other animals)

Note: Missing values denoted by -999.0

For more details, see

Allison, Truett and Cicchetti, Domenic V. (1976), ""Sleep in Mammals: Ecological and Constitutional Correlates"", Science, November 12, vol. 194, pp. 732-734.

The above data set can be freely used for non-commercial purposes and can be freely distributed (permission in writing obtained from Dr. Truett Allison).

Submitted by Roger Johnson rwjohnso@silver.sdsmt.edu

Total sleep treated as the class attribute. Attributes for slow wave and paradoxical sleep have been deleted. (The animal's name has also been deleted.)","","Find relevant features to sleep quality and quantity","Find relevant features to sleep quality and quantity","12/31/2021 23:59:00","0"
"1986","861167","18463","09/06/2020 19:25:58","Create a model to supervised class variable

All nominal attributes and instances with missing values are deleted. Price treated as the class attribute.

As used by Kilpatrick, D. & Cameron-Jones, M. (1998). Numeric prediction using instance-based learning with encoding length selection. In Progress in Connectionist-Based Information Systems. Singapore: Springer-Verlag.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Title: 1985 Auto Imports Database

Source Information: -- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu) -- Date: 19 May 1987 -- Sources: 1) 1985 Model Import Car and Truck Specifications, 1985 Ward's Automotive Yearbook. 2) Personal Auto Manuals, Insurance Services Office, 160 Water Street, New York, NY 10038 3) Insurance Collision Report, Insurance Institute for Highway Safety, Watergate 600, Washington, DC 20037

Past Usage: -- Kibler,~D., Aha,~D.~W., & Albert,~M. (1989). Instance-based prediction of real-valued attributes. {it Computational Intelligence}, {it 5}, 51--57. -- Predicted price of car using all numeric and Boolean attributes -- Method: an instance-based learning (IBL) algorithm derived from a localized k-nearest neighbor algorithm. Compared with a linear regression prediction...so all instances with missing attribute values were discarded. This resulted with a training set of 159 instances, which was also used as a test set (minus the actual instance during testing). -- Results: Percent Average Deviation Error of Prediction from Actual -- 11.84% for the IBL algorithm -- 14.12% for the resulting linear regression equation

Relevant Information: -- Description This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.

The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.

-- Note: Several of the attributes in the database could be used as a ""class"" attribute.

Number of Instances: 205

Number of Attributes: 26 total -- 15 continuous -- 1 integer -- 10 nominal

Attribute Information:
Attribute: Attribute Range: ------------------ -----------------------------------------------
symboling: -3, -2, -1, 0, 1, 2, 3.
normalized-losses: continuous from 65 to 256.
make: alfa-romero, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo
fuel-type: diesel, gas.
aspiration: std, turbo.
num-of-doors: four, two.
body-style: hardtop, wagon, sedan, hatchback, convertible.
drive-wheels: 4wd, fwd, rwd.
engine-location: front, rear.
wheel-base: continuous from 86.6 120.9.
length: continuous from 141.1 to 208.1.
width: continuous from 60.3 to 72.3.
height: continuous from 47.8 to 59.8.
curb-weight: continuous from 1488 to 4066.
engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.
num-of-cylinders: eight, five, four, six, three, twelve, two.
engine-size: continuous from 61 to 326.
fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.
bore: continuous from 2.54 to 3.94.
stroke: continuous from 2.07 to 4.17.
compression-ratio: continuous from 7 to 23.
horsepower: continuous from 48 to 288.
peak-rpm: continuous from 4150 to 6600.
city-mpg: continuous from 13 to 49.
highway-mpg: continuous from 16 to 54.
price: continuous from 5118 to 45400.

Missing Attribute Values: (denoted by ""?"") Attribute #: Number of instances missing a value:
41
2
4
4
2
2
4%","","Create a model to supervised class variable","Create a model to supervised class variable","12/31/2021 23:59:00","0"
"1988","861175","18463","09/06/2020 19:39:57","Find most relevant features in quakes event
Dataset from Smoothing Methods in Statistics (ftp stat.cmu.edu/datasets)


Simonoff, J.S. (1996). Smoothing Methods in Statistics. New York: Springer-Verlag.","","Find most relevant features in quakes event","Find most relevant features in quakes event","12/31/2021 23:59:00","0"
"1989","861188","18463","09/06/2020 19:45:26","Data from StatLib (ftp stat.cmu.edu/datasets)

These data are those collected in a cloud-seeding experiment in Tasmania between mid-1964 and January 1971. Their analysis, using regression techniques and permutation tests, is discussed in:

   Miller, A.J., Shaw, D.E., Veitch, L.G. & Smith, E.J. (1979).
   `Analyzing the results of a cloud-seeding experiment in Tasmania',
   Communications in Statistics - Theory & Methods, vol.A8(10),
   1017-1047.
The rainfalls are period rainfalls in inches. TE and TW are the east and west target areas respectively, while NC, SC and NWC are the corresponding rainfalls in the north, south and north-west control areas respectively. S = seeded, U = unseeded.

Rain in eastern target region is being treated as the class attribute. (Attribute for rain in the western target region has been deleted.)","","Find relevant features in this cloud dataset","Find relevant features in this cloud dataset","12/31/2021 23:59:00","0"
"1990","861197","18463","09/06/2020 19:52:02","Find relevant features in Tree Growth

The trimming of trees under distribution lines on city streets and in rural areas is a major problem and expense for electrical utilities. Such operations are routinely performed at intervals of one to eight years depending upon the individual species growth rate and the amount of clearance required. Ontario Hydro trims about 500,000 trees per year at a cost of about $25 per tree.

Much effort has been spent in developing chemicals for the horticultural industry to retard the growth of woody and herbaceous plants. Recently, a group of new growth regulators was introduced which was shown to be effective in controlling the growth of trees without producing noticeable injury symptoms. In this group are PP 333 ( common name paclobutrazol) (2RS, 3RS - 1 -(4-chlorophenyl) - 4,4 - dimethyl - 2 - (1,2,4-triazol-l-yl) pentan - 3- ol and EL-500 (common name flurprimidol and composition alpha - (1-methylethyl) - alpha - [4-(trifluromethoxyl) phenyl] - 5- pyrimidine - methanol). Both EL-500 and PP-333 have been reported to control excessive sprout growth in a number of species when applied as a foliar spray, as a soil drench, or by trunk injection. Sprout length is a function of both the number of internodes and the length of the individual internodes in the sprout. While there have been many reports that both PP 333 and EL-500 cause a reduction in the length of internodes formed in sprouts on woody plants treated with the growth regulators, there has been but one report that EL-500 application to apple trees resulted in a reduction of the number of internodes formed per sprout","","Find relevant features in Tree Growth","Find relevant features in Tree Growth","12/31/2021 23:59:00","0"
"1991","861200","18463","09/06/2020 20:05:00","Find most relevant features in Commerce Reviews

Dataset creator and donator: Zhi Liu, e-mail: liuzhi8673 '@' gmail.com, institution: National Engineering Research Center for E-Learning, Hubei Wuhan, China

Data Set Information:

dataset are derived from the customers reviews in Amazon Commerce Website for authorship identification. Most previous studies conducted the identification experiments for two to ten authors. But in the online context, reviews to be identified usually have more potential authors, and normally classification algorithms are not adapted to large number of target classes. To examine the robustness of classification algorithms, we identified 50 of the most active users (represented by a unique ID and username) who frequently posted reviews in these newsgroups. The number of reviews we collected for each author is 30.

Attribute Information:

attribution includes authors' linguistic style such as usage of digit, punctuation, words and sentences' length and usage frequency of words and so on","","Find most relevant features in Commerce Reviews","Find most relevant features in Commerce Reviews","12/31/2020 23:59:00","0"
"1995","861641","18463","09/07/2020 06:37:26","Generate fun dialog from DialogGPT

This repository contains the source code and trained model for a large-scale pretrained dialogue response generation model. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.

The repository is based on huggingface pytorch-transformer and OpenAI GPT-2, containing data extraction script, model training code and pretrained small (117M) medium (345M) and large (762M) model checkpoint.

The model is trained on 147M multi-turn dialogue from Reddit discussion thread. The largest model can be trained in several hours on a 8 V100 machines (however this is not required), with distributed training and FP16 option.

The include script can be used to reproduce the results of DSTC-7 grounded dialogue generation challenge and a 6k multi-reference dataset created from Reddit data.

Project webpage: https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/

ArXiv paper: https://arxiv.org/abs/1911.00536","","Generate fun dialog from DialogGPT","Generate fun dialog from DialogGPT","12/31/2021 23:59:00","0"
"2007","862993","18463","09/08/2020 06:14:46","COVID-19 image dataset collection 
In the context of a COVID-19 pandemic, we want to improve prognostic predictions to triage and manage patient care. Data is the first step to developing any diagnostic/prognostic tool. While there exist large public datasets of more typical chest X-rays from the NIH [Wang 2017], Spain [Bustos 2019], Stanford [Irvin 2019], MIT [Johnson 2019] and Indiana University [Demner-Fushman 2016], there is no collection of COVID-19 chest X-rays or CT scans designed to be used for computational analysis.

The 2019 novel coronavirus (COVID-19) presents several unique features Fang, 2020 and Ai 2020. While the diagnosis is confirmed using polymerase chain reaction (PCR), infected patients with pneumonia may present on chest X-ray and computed tomography (CT) images with a pattern that is only moderately characteristic for the human eye Ng, 2020. In late January, a Chinese team published a paper detailing the clinical and paraclinical features of COVID-19. They reported that patients present abnormalities in chest CT images with most having bilateral involvement Huang 2020. Bilateral multiple lobular and subsegmental areas of consolidation constitute the typical findings in chest CT images of intensive care unit (ICU) patients on admission Huang 2020. In comparison, non-ICU patients show bilateral ground-glass opacity and subsegmental areas of consolidation in their chest CT images Huang 2020. In these patients, later chest CT images display bilateral ground-glass opacity with resolved consolidation Huang 2020.

Goal
Our goal is to use these images to develop AI based approaches to predict and understand the infection. Our group will work to release these models using our open source Chester AI Radiology Assistant platform.

The tasks are as follows using chest X-ray or CT (preference for X-ray) as input to predict these tasks:

Healthy vs Pneumonia (prototype already implemented Chester with ~74% AUC, validation study here)

Bacterial vs Viral vs COVID-19 Pneumonia (not relevant enough for the clinical workflows)

Prognostic/severity predictions (survival, need for intubation, need for supplemental oxygen)
https://github.com/ieee8023/covid-chestxray-dataset","","COVID-19 image dataset collection","COVID-19 image dataset collection","12/31/2021 23:59:00","0"
"2006","863426","18463","09/08/2020 06:11:43","MosMedData Chest CT Scans Challenge

Scientific research on the possibility of using clinical decision support systems in the Moscow healthcare system with data analysis based on advanced innovative technologies","","MosMedData Chest CT Scans Challenge","MosMedData Chest CT Scans Challenge","12/31/2021 23:59:00","0"
"1422","787828","23583","07/20/2020 03:40:12","## Task Details
Is there a correlation between energy consumption and events such as holidays, university events, social events, etc. Can these events be scraped from various web-based sources?

## Expected Submission
Submissions for this task should investigate:
- Which parts of the data sets have highly abnormal behavior
- Whether there are data sources online anywhere that might explain these types of behavior
- Whether external data sources can correlate or predict energy behavior in buildings in general
- Create an analysis that seeks to correlate this behavior with the data using visualizations and models

## Possible online data sources
- Twitter
- Holiday schedules
- Current events feeds
- News feeds
- Sports calenders","","Can we use social media or other data sources to predict weird behavior in buildings?","Twitter, facebook, holiday schedules, etc?","","3"
"2544","787828","23583","10/27/2020 03:23:50","## Task Details
Everyone knows that time-series data from building energy and automation systems are messy. Meters and sensors fail, have huge spikes, and other unreasonable behavior due to various glitches in the system. Buildings can have periods of 'weird' behavior in which the building is being used in a completely different way from 'normal'. Equipment can fail or there can be outages of energy due to events or natural disasters.

Identifying these periods of anomalous behavior is important for numerous reasons:
- Finding anomalies may result in the identification of energy-saving opportunities for a building/buildings
- Removing anomalous behavior from training data may result in improved short and long term ML prediction/regression
- Separating anomalous behavior gives a sense of the behavior of certain operating modes of the building

There are two types of meter data files in this data set -- those with the suffix `_clean.csv` and those without. The ones that are *clean* have already been put through a process of cleaning that is defined in the documentation for each of those files. Those without the suffix are considered the *raw* data and you can try to apply your own cleaning techniques to see how to find weird/anomalous behavior.

There are numerous outlier detection methods out there! Some of the examples of Python packages for this task are:
- https://pyod.readthedocs.io/en/latest/
- https://pypi.org/project/anomatools/
- https://github.com/linkedin/luminol
- https://github.com/TDAmeritrade/stumpy

## Expected Submission
Users should submit a single notebook that describes the setup, processing, and results of their analysis of the BDG2 project that illustrates a solution for outlier removal and/or anomaly detection. Users should give details on the advantages and disadvantages of the approach. The single notebook should have references to other publicly shared notebooks from the person/team who submitted that may provide intermediate analysis steps that aren't contained in the submitted notebook.

The analysis should include a lot of documentation to provide clarity of the techniques and approaches used. The submission can use code and ideas from other Kaggle notebooks/submissions as long as credit is given in the form of links and written documentation, but it must be clear what innovation has been added to previous efforts from others.

This repository contains time-series data files that have not been cleaned that can be used for this task - i.e: the files **without** the `_clean.csv` suffix. Comparison of the proposed anomaly detection approach with the cleaned files might be a good way to measure differences. 

## Evaluation
Solutions will be evaluated based on the Clarity, Content, and Creativity criteria outlined in the BPS5229 course in Week 8.","","What are good ways to find anomalous behaviour in building meter data?","Out of the numerous options out there, which ones are good for buildings?","11/12/2021 00:00:00","3"
"4483","787828","5497243","05/24/2021 23:03:46","### Task Details

What recommendations can be made based on the datasets? 

### Expected Submission

Submissions for this task should investigate:

- Which parts of the data sets have highly abnormal behavior
- Create an analysis that seeks to correlate this behavior with the data using visualizations and models
- How do energy companies plan production based on behaviour and trends 

### Possible online data sources

- Energy companies 
- Current events feeds
- News feeds","","What recommendations can be made based on the datasets?","","","2"
"6304","787828","23583","10/11/2021 02:42:09","## Task Details
OK, so we have an open data set of building meter data. It's not big, but not small. We can apply all kinds of data science techniques and stuff -- but *why?* Why would anyone who runs a company that wants to make money care about the use of these data?

This task prompts you to create an example Kaggle notebook that outlines a potential use case of the BDG2 data set that would be helpful to a profit-driven company. This use case could be within the building industry, but that isn't required. 

Ideally, this use case would include the following sections:
1. A introduction and discussion of the situation in this business that has a **problem** in need of a solution that can be solved through the use of meter data. 
2. A proposal of a use of the hourly meter data, like those in this dataset that could be used to provide a **solution** to that problem. This solution can be very practical and implementable on real buildings immediately tomorrow, or it could be more theoretical and could depend on the market transforming in certain ways to make it possible in the medium or long term. You can imagine scenarios that use other data sources beyond the BDG2 dataset - and you're welcome to even use those datasets in your solution.
3. A set of data visualizations that give a simple example of the problem and solution using data from the BDG2 data set. This is what we call a *toy example* as your goal is to show step-by-step how your idea works on real data. This example could even be done on a single meter. You are welcome to use any type of analysis method -- all the way from human-driven data visual analytics to advanced machine learning/prediction/regression/classification. 
4. A conclusion and discussion on how your solution could scale across the building stock -- this is a massive challenge for some techniques and easier for others. If reasonable, you can try to show your technique/idea across many or all buildings/meters in the BDG2 dataset as an example of scalability.

Some generic ideas that could provide a starting point:
- Companies need to make decisions about renovations of their buildings. Sometimes they need to decide what kind of renovations would be best or which buildings are the best candidates for renovation. Using prediction can assist in the *measurement and verification* process for determining energy savings due to implemented measures.
- Companies often want to identify the behavior of the way occupants use buildings or the way operators manage buildings. They want to understand how the building is being used so that they can implement technologies or interventions that improve the energy performance, or health/safety of the occupants. Sometimes the behavior of buildings by occupants is related to things like airborne disease control - ie: the density of occupants in the spaces.
- Companies sometimes want to identify the best targets for their products. They want to fine-tune their marketing and advertising to maximize their customer acquisition or retention. Understanding how a building uses energy might be an emerging means of informing those decisions.
- Companies who work in the smart grid area might want to use meter data to get a sense of how renewable energy sources such as solar and wind can be better integrated into the grid. They might want to provide consulting to utilities to help them avoid the need to build expensive electricity generation plants that would only be needed during high peak periods.

## Expected Submission
Each submission will be a Kaggle notebook submitted to this task. It should be documented very well using the text cells in addition to the code that processes the data.

## Evaluation
The solutions will be evaluated based on the following criteria:

Content:
- The content should be compelling to someone who is interested in the answers to the objectives
- Convince your audience that your scope is appropriate and interesting and the decisions your team made were good ones
- Your team is providing evidence to back up the decisions made
- There are no ‚Äòkey word‚Äô or specific things I‚Äôm looking for in marking or evaluating your project ‚Äì it's all about the evidence you give to show that your structure, logic, and results are done compelling or not

Clarity:
- Simple, step-by-step explanation of the sections
- Good use of diagrams, graphics, and data visualization
- You can download, modify and reupload graphics with arrows and annotations to explain concepts even further than what you can do using just Python/pandas
- The documentation and explanation of the process is detailed enough to clearly explain the notebook on its own

Creativity:
- Create an out-of-the-box idea by finding something interesting in the data set or exploring different business models
- Create interesting and clever ways of explaining complex things and visualizing data","","How can building meter data and/or prediction models be used in practice?","Make a toy example of a use case, business model, or other showcase of hourly time-series meter data from buildings!","","1"
"5939","787828","23583","09/01/2021 01:38:53","## Task Details
We all have a sense of how buildings use energy because we live and work in buildings. We know that when it gets hotter outside, there is often a cooling system that ramps up to keep us comfortable. We know that when the sun goes down, we need lights to allow us to see properly. Some buildings are highly influenced by the humans inside them who turn on devices such as TV's or computers during certain time-frames. In commercial buildings, there are big systems that might be on a certain schedule in which they turn on in the morning and turn off at night.

The goal of this task is to prototype ways to extract signals from the data that indicate which factors are the main sources of driving energy demand in each of the buildings. There are numerous examples of detecting and extracting weather influence in the literature, but other influences might be more challenging. 

Since the data set has little ground truth (such as submeters that specifically measure a type of energy), this task includes a certain amount of exploratory analysis and key assumptions made by the data scientists who work on it. Supervised and unsupervised ML techniques might be important tools to help dissect this problem, but they are not required in the solution.

## Expected Submission
Users should submit a single notebook that describes the setup, processing, and results of their method of detecting and visualizing the factors that influence energy consumption. The single notebook should have references to other publicly shared notebooks from the person/team who submitted that may provide intermediate analysis steps that aren't contained in the submitted notebook.

The analysis should include a lot of documentation to provide clarity of the techniques and approaches used. The submission can use code and ideas from other Kaggle notebooks/submissions as long as credit is given in the form of links and written documentation, but it must be clear what innovation has been added to previous efforts from others.

## Relevant Literature
Section 2 of [this open-access paper](https://www.mdpi.com/2504-4990/1/3/56/htm) gives a high level understanding of what factors influence building energy consumption as well as references that can be further explored.

## Evaluation
Solutions will be evaluated based on the Clarity, Content, and Creativity criteria outlined in the BPS5229 course in Week 8.","","What are the factors that influence building energy consumption and how can we identify them?","Weather? Schedules? Humans? Systems? Seasons? What is it?!","11/26/2021 23:59:00","3"
"1433","789868","23583","07/21/2020 03:28:23","## Task Details
Researchers who explore subjective thermal perception in buildings can ask occupants basically the same question in several different ways: they can ask whether the person feels comfortable, whether they perceive a cooling or heating sensation, or whether they would prefer for the systems in the building to produce more heating or cooling. A comparison of the differences between these ways of asking the question of whether you are ""hot"" or ""cold"" deserves an analysis.

## Expected Submission
Create kernels that analyze the various aspects of the difference between the variables related to asking *how* a person feels thermal comfort","","Can we compare Thermal Comfort vs. Sensation vs. Preference?","What is the best way to ask people how they feel?","","2"
"1461","789868","23583","07/23/2020 00:42:33","## Task Details
There is a lot of recent discussion on whether thermal comfort is really taking into consideration the needs of all occupants. There have even been popular news outlets who have covered stories on sexism in office temperatures from the [New York Times](https://www.nytimes.com/2018/08/28/nyregion/office-temperature-sexist-nixon-cuomo.html), [the New Yorker](https://www.newyorker.com/tech/annals-of-technology/is-your-thermostat-sexist), and [Time Magazine](https://time.com/5592353/office-temperature-study/). The question related to this task is to piece together evidence from the ASHRAE Thermal Comfort Database II to show how factors such as sex, age, metabolism, and other factors influence comfort. 

This task can likely be done with exploratory analysis to characterize the differences as well as metrics or transformations of the data (from literature or created by the team) to detect an influence of factors. Supervised and unsupervised ML techniques might be important tools to help dissect this problem, but they are not required in the submission.

## Expected Submission
Users should submit a single notebook that describes the setup, processing, and results of their method of identifying which human factors influence thermal comfort. The single notebook should have references to other publicly shared notebooks from the person/team who submitted that may provide intermediate analysis steps that aren't contained in the submitted notebook.

The analysis should include a lot of documentation to provide clarity of the techniques and approaches used. The submission can use code and ideas from other Kaggle notebooks/submissions as long as credit is given in the form of links and written documentation, but it must be clear what innovation has been added to previous efforts from others.

## Relevant Literature
A great overview of recent work to analyze these differences can be found [in this recent publication](https://www.researchgate.net/publication/324809583_Individual_Difference_in_Thermal_Comfort_A_Literature_Review).

## Evaluation
Solutions will be evaluated based on the Clarity, Content, and Creativity criteria outlined in the BPS5229 course in Week 8.","","How do human and personal factors influence thermal comfort perception?","Do age, sex, metabolism, and other human factors influence how you feel comfort?","","3"
"5983","789868","23583","09/03/2021 02:52:19","## Task Details
The concept of [acclimatization](https://en.wikipedia.org/wiki/Acclimatization) might be linked to how people feel in the built environment thermally. For example, if someone grows up in a hot, humid climate (like Singapore), then they might be more sensitive to colder temperatures causing them to report discomfort differently than people from cold climates. Or maybe they like cold temperatures more since it provides relief from their climate. 

The goal of this task is to analyze the data set to determine whether where the person is located has an influence on their tendency to have comfort preferences that are different than other locations.

This task can likely be done with exploratory analysis to characterize the differences as well as metrics or transformations of the data (from literature or created by the team) to detect an influence of factors. Supervised and unsupervised ML techniques might be important tools to help dissect this problem, but they are not required in the submission.

## Expected Submission
Users should submit a single notebook that describes the setup, processing, and results of their method of identifying which human factors influence thermal comfort. The single notebook should have references to other publicly shared notebooks from the person/team who submitted that may provide intermediate analysis steps that aren't contained in the submitted notebook.

The analysis should include a lot of documentation to provide clarity of the techniques and approaches used. The submission can use code and ideas from other Kaggle notebooks/submissions as long as credit is given in the form of links and written documentation, but it must be clear what innovation has been added to previous efforts from others.

## Evaluation
Solutions will be evaluated based on the Clarity, Content, and Creativity criteria outlined in the BPS5229 course in Week 8.","","How does geographic location influence thermal comfort perception?","Does where you live full-time influence what responses you give to surveys about thermal comfort?","","1"
"2213","889822","40408","09/23/2020 15:43:10","## Task Details
Inactivating a gene can have detrimental effects on the metabolic network which is made of thousands of reactions and compounds. Which metabolic pathways are affected by knocking out a gene is a basic question. With comprehensive metabolomics, we can obtain a snap-shot of hundreds of metabolic pathways and their dys-regulations when a gene is knocked out. 

## Expected Submission
Users can submit a list of metabolic pathways, modules, sets, individual chemicals and reactions which they think were disturbed when a gene was inactivated. Results can be in any format. 

## Evaluation
We will check the consistency in identified biological entities and pathways to compare the results submitted by various users. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Which metabolic pathways are disturbed in a knockout strain ?","","","1"
"1953","858360","49346","09/05/2020 08:18:10","There is some weirdness with fastprogress, and some things like learner.summary() don't work. Need to get versions figured out - I think fastcore/fastai may be a bit ahead?","","Fix fastprogress","","","0"
"792","527438","66167","04/21/2020 21:16:50","## Task Details
We will focus on improving balanced accuracy using Automated Feature Engineering

## Expected Submission
Every notebook that uses Automated Feature Engineering and improve current balanced accuracy (80.6%) . Classifier (RandomForest) or its parameters should stay the same to allow fair comparison

### Further help
https://www.kaggle.com/snassimr/baseline-model-and-improvement-with-featuretools 
References are in the notebook above","","Improve Performance on Investing Program Type Prediction Dataset","With featuretools or similar tool for Automated Feature Engineering","10/30/2020 00:00:00","0"
"593","494724","2234678","03/16/2020 17:21:22","## Task Details
Looking at China's growth rate over time, we can clearly see that they were able to curb the growth rate of the virus. Over the last few weeks, different countries have performed different forms of mitigation to do the same: ban large gatherings, close schools, stop incoming flights, put cities in lockdown, etc.

The task is to evaluate the effectiveness of mitigation by trying to see if a correlation can discovered between the different types of mitigation and the growth rate of confirmed cases. What measures seem to work and which not? Which ones are the most effective?

Keep in mind there are numerous factors which might affect the grow rate (e.g. country's general hygiene, population density, how much time they had to prepare) so not all countries can be compared easily. China being an obvious outlier since it was country 0.","","Correlation between growth rate and types of mitigation across countries","","","50"
"508","494724","998023","03/04/2020 23:24:06","## Task Details
The outbreak of Covid-19 is developing into a major international crisis, and it's starting to influence important aspects of daily life. For example:
- Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
- Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether. 
- Grocery stores: In highly affected areas, people are starting to stock up on essential goods.

A strong model that predicts how the virus could spread across different countries and regions may be able to help mitigation efforts. The goal of this task is to build a model that predicts the progression of the virus throughout March 2020.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model. 

With this model, you should produce a table in the following format for all future days of March (similar to `covid_19_data.csv`)
- `ObservationDate`: Observation date in mm/dd/yyyy
- `Province/State`: Province or State
- `Country/Region`: Country or region
- `Confirmed`: Cumulative number of confirmed cases
- `Deaths`: Cumulative number of deaths cases
- `Recovered`: Cumulative number of recovered cases


The notebook should be well documented and contain:

1. Any steps you're taking to prepare the data, including references to external data sources
2. Training of your model
3. The table mentioned above
4. An evaluation of your table against the real data. Let's keep it simple and measure Mean Absolute Error.

## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

At the end of March, we'll look at the top 5 upvoted submissions and holistically award the best solution. We‚Äôll announce and recognize March‚Äôs Kaggle Tasks Award Winner on our Twitter account at the beginning of April, sharing an official certificate of accomplishment.

Our team will be looking at:

1. **Accuracy** - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?
2. **Data Preparation** - How well was the data analyzed prior to feeding it into the model? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.
3. **Documentation** - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.

This task is not strictly about getting the best submission score. Since it involves data that changes in real-time, the most accurate solution would only require uploading the real historic data on the last day of March, but that‚Äôs besides the spirit of this Task. We‚Äôre looking for genuine approaches to building models on a real problem that can serve as learning examples for our community","","Predict the Spreading of Coronavirus","Can we help mitigate the secondary effects of covid-19 by predicting its spread","03/31/2020 00:00:00","225"
"514","494724","4309042","03/06/2020 11:38:43","## Task Details
Predict how people are going to recover (assuming no cure was invented) based on old recovery records.

## Expected Submission
Time series data, of what will happen in the future, recovery records, infection records, death records ... etc.

## Evaluation
Only the future will contain the actual results (please note any results that happens in the future after a cure is found won't be used for evaluation, we only consider normal recovery, death & infection rates, assuming countries follow exactly the same policies they have been following


Additional points, for models that can adopt to new measures taken by governments to contain the epidemic, and if those models still predict the numbers with lower error

MSE is the evaluation metric","","Predict how the epidemic will end","If your model suggests it will end","","124"
"620","494724","4654705","03/23/2020 04:38:47","Can We Correlate weather conditions and Corona virus Spread through Data?Can We confirm  this  hypothesis that spread of corona virus is slow down by  warm weather.

We will take different cities  weather reports  date wise and no of patients 
on that date.

 












## Task Details
we have to get data  regarding count of   infected peoples  of particular city date wise with temperature and then analyse it with humidity perception and wind 
may be these factors contribute a little  but one can not ignore its effects 

## Expected Submission
complete data set of corona virus is required 

## Evaluation
having detailed analysis will make us to predict further spread of corona virus","","Can We Correlate weather conditions and Corona virus Spread through Data?","Can We confirm  this hypothesis that spread of corona virus slow down in warm weather","04/06/2020 00:00:00","41"
"641","494724","1867237","03/26/2020 15:36:38","Can someone create an India specific prediction based on the current mitigation scenarios taking place in the country.
Final output should also predict how the epidemic will end if at all it ends.","","India Specific prediction","","","26"
"825","494724","4321234","04/29/2020 11:07:35","The transmission rate is also an important factor towards a disease. We'll need you to find out the transmission rate.","","Transmission Rate Over Time","Predict the rate of transmission over time","","9"
"5482","494724","6188271","08/01/2021 14:44:20","## Task Details
Need you to visualization number of confirmed , death or recovered cases in countries with different date.

## Expected Submission
the solution should be contain charts and plot the relation between Cases in countries with date.

## Evaluation
submission is better than another?
- should the output plot be animated that be video or gif image.
- be like this animation:

<code><img src=""https://github.com/MhmdSyd/Bar_Chart_Race_Gif/blob/main/COVID_Sub.gif?raw=true""></code>

### Further help
- you should use ""covid_19_data.csv"" dataset.
-","","COVID-19 | Visualization","visualization the number of cases  by Country with date","","1"
"631","557629","605616","03/25/2020 21:41:39","## Task Details
1. Prediction the number of new cases each day from March 26th to April 30th when the country is under lockdown
2.  Prediction the number of new cases each day from March 26th to April 30th in a hypothetical situation where there is no lockdown.

## Expected Submission
The solution should contain your assumptions, a CSV file with date in column 1, affected patients, new patients, new fatality (deaths) in the next three column. The solution should also contain the notebook or the code that was used to predict the output. Any other datasets used must be submitted. 

## Evaluation
The good solution will hold in line with the real life data.","","Predict how well can the 21 day lockdown perform in containing spread of the virus?","India in under 21 day lockdown to contain the spread of coronavirus","","121"
"633","557629","605616","03/25/2020 22:37:25","## Task Details
We need to understand how the lockdown will effect different sectors of the economy to provide the maximum assistance to the vulnerable sectors. 

Example set of sectors that should be focussed on (but not limited to) - 
1. Urban formal economy
2. Urban informal economy
3. Rural economy

Example subset of sectors that should be focussed on (but not limited to) - 

1. E-commerce
2. Travel and tourism
3. Retailers
4. Agriculture
5. Hospitals and medical services
6. Education
7. Banking and Insurance
8. Petroleum products and by-products
9. MSME Sector
10. Informal workers
(Add other sectors)

## Expected Submission
1. The key performance indicators are the total revenue per sector, total number of  people/households effected by state/UT, revenue per capita (+/-), total loss/gain in the sector, recovery time (time taken to cover the gap by 50%),

2. Visualizations

3. Notebooks to reproduce the results

## Evaluation
No Evaluation. Evaluation is subjective. Open for suggestions","","Who are the most affected due to the lockdown?","Which are the most affected sectors of the society due to the lockdown?","","45"
"634","557629","605616","03/25/2020 22:54:57","## Task Details
Predict of potential emergencies in case the crisis goes out of hand.
The key questions to be solved are 
1. In case of adverse situations where should the essential medical kits go? In what proportion?
2. Where should the testing kits go? 
3. After the 21 days of lockdown at which place the probability of spread of virus is highest. Which regions will have the lowest. At which places the lockdown must be revoked.

## Expected Submission
Key performance indicators -
1. Number of fatalities
2. Number of patients
(please add other KPIs)
 
Key parameters to track are -
1. Shortage of N95 masks per doctor/medical personnel.
2. Number of tests per 1000 suspected individuals
3. Number of tests per patient tested positive in the region.

Other parameters
1. Number of doctors per 100 patients
2. Number of medical kits per 100 patients
3. Number of testing kits per 


## Evaluation
Subjective. No evaluation criteria. Please suggest","","Prediction of potential emergencies","Prediction of potential emergencies in case the situation goes out of hand.","","10"
"650","557629","605616","03/28/2020 18:51:56","## Task Details
It is a well known fact that the virus spread takes an exponential distribution. It is a well established fact that social distancing can [flatten the curve (The bell curve)](https://www.weforum.org/agenda/2020/03/exponential-economist-epidemiological-curve-coronavirus-covid19). It is necessary to flatten the curve so much so that healthcare infrastructure in the country can manage the peak number of patients at any given time.

The dataset contains the details of number of beds and hospitals (statewise distribution can be found [here](https://www.kaggle.com/dheerajmpai/hospitals-and-beds-in-india)). The database also contains individual patient data. (for real time data access [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml)). 

Updates : Indian Railways is allotting train compartments as isolation wards. Database of Indian railways can be found [here](https://data.gov.in/keywords/indian-railways)

## Expected Submission

1. Find the maximum capacity of patients each state can sustain. This will decide how much we need to ""flatten the curve"".

Preferred format: A notebook to reproduce the results. A csv file with the states in rows and dates in each column. Each entry showing the maximum capacity on that specific day in that specific state. Feel free to make changes

2. Analyze the patient data and predict the coronavirus ""patient"" curve. A csv file with states in rows. Columns must contain dates. Each entry containing the number of patients.

3. If possible : Add the number of isolated and critical patients separately.

## Evaluation
Subjective.","","Flattening the curve","How much should India flatten the curve so that the medical infrastructure can manage the outbreak","","15"
"651","557629","605616","03/28/2020 20:07:22","## Task Details
""Test, test, test"" was what WHO said to prevent spread of Covid-19. 

Covid-19 takes some time to show symptoms. In some cases there are [no symptoms](https://www.weforum.org/agenda/2020/03/people-with-mild-or-no-symptoms-could-be-spreading-covid-19/) at all. Early testing and isolating the coronavirus patients can prevent spread of disease.

Not all countries have the same testing capacities. India as of now has tested upto 25,000 suspect. Even developed nations like US had the capacity to test only 7000[ suspects per day](https://www.npr.org/sections/health-shots/2020/03/18/817768723/u-s-coronavirus-testing-starts-to-ramp-up-but-still-lags) (until 14th March). 

## Using algorithms to speed up testing

Pooling is a simple algorithm that can speed up testing ([here](https://drive.google.com/file/d/1tTfnultRwlYExroAbT4n-4pwN0Hr_liH/view?usp=sharing), [here](https://www.israel21c.org/israelis-introduce-method-for-accelerated-covid-19-testing/)). In layman‚Äôs term samples of a group of individuals are mixed together and tested at once. If the test turns out negative we know none of the individual is infected. If it turns out to be positive then we divide the original group into two and repeat the procedure.

There can be multiple ways to do that. It can also be done in two steps. Test a group. If found positive then test each individual samples. It can be done in three steps. In fact, we can come up with a affective coding algorithm to shuffle up multiple individual samples and find the cases in one go. Or any other creative way.

In a region with low probability of infection these techniques can drastically reduce the number of required testing. 

Aspects that needs to be taken under consideration while analysing
1. Please keep the real life situation in mind. Explicitly mention the assumptions. For example, testing takes time. A suspect cannot wait for 100 test Sequential testing only delays the result. Also, avoid cases where you test 1 billion at one shot.
2. Keep your algorithm as simple as possible. It should be explained easily to a paramedic.
3. Compare your results with the existing solutions and other proposed solutions. Explain the advantages. (In terms of time taken or the number of tests for the population)
 
## Expected Submission

1. Notebook that contains the code. Along with the details that show your method is better than the other.


## Evaluation
Subjective. Lesser number of tests and lesser time to determine the makes better solution. In the methodology explained above its a trade off.","","Finding faster techniques to test Covid 19","Use better algorithmic techniques to mass test Covid 19","","23"
"875","557629","879504","05/11/2020 12:59:31","## Task Details
Create a Trend of progression and update the results

## Expected Submission
Submit Notebooks , GIF etc.. 

## Evaluation
Humans are good till now in making cognitive decisions through visualizations. so its an alternative approach to represent a data through visualization  

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysing Trend Graph","","05/15/2020 00:00:00","3"
"865","557629","4568636","05/09/2020 04:42:04","1.  You Analysis should cover discriptive statistics and any two ML algorithm (linear or logistics Regression, Decission tree or Random forest).
2.import the data set to WAMP server and write any 10 SQL enquiries to extract data about covid19 cases like State wise ,district wise ,city wise,age wise ,gender wise.","","Covid 19 dataset","Analysis dataset","05/11/2020 00:00:00","14"
"923","557629","5113808","05/18/2020 07:11:41","## Task Details
Government planning

## Expected Submission
Submit anything logical and factual according to data, extrapolate the current trends

## Evaluation
Should match with real life data

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Migration trends for employment purpose","laborers shifting to farms for employment and sustainability","05/28/2020 00:00:00","1"
"1032","557629","5116483","06/02/2020 10:51:47","## Task Details
Let us create a bar chart race to see how the confirmed cases developed in each state across the dates.
Use the covid_19_india.csv file.

## Expected Submission
Submit a video/gif file with the cases rising since beginning of March.

## Evaluation
The bar chart race should have the date, state name, cases per state,  total cases across the country.","","COVID19 India - Bar Chart Race","Create a bar chart race with the number of confirmed cases per states","","11"
"4161","557629","5587303","04/23/2021 06:00:50","COVID-19 has impacted the World a lot in many fields. When the cases were rising, everyone was hoping for the best to have a vaccination. Many Nations tried to develop Vaccines and few of them were successful. This Dataset is about the COVID Vaccination details in INDIA.

## Task Details
Following are the inspirations to work in this dataset : 

1. How many states have fully vaccinated?
2. What is the number of positive cases rising in India?
3. Which states are more affected by the virus?
4. Analyze to show insights regarding coronavirus.

## Expected Submission
Solve the task primarily using Notebooks or Datasets. 

## What should the solution contain?
Details of states, vaccination, and positive cases based on the analysis in INDIA.

## Evaluation
Use strong EDA techniques and predictions as per your understanding, and reach the best insights. (Feel free to try something new every time.)

## Key Areas
Data Cleaning, Data Preparation, Exploratory Data Analysis.","","COVID-19 Vaccine Analyzing","EDA and Predictions","05/23/2022 23:59:00","8"
"1060","694560","71388","06/05/2020 08:00:07","## Task Details
Global warming is the ongoing rise of the average temperature of the Earth's climate system and has been demonstrated by direct temperature measurements and by measurements of various effects of the warming - Wikipedia

Task is to analyze the rise of temperature over time in different parts of the world

## Expected Submission
A notebook with details on
1. How much is the temperature increase in different parts of the world over time?
2. Which cities are seeing a rapid increase in temperature over time?","","Analyze the rise of temperature over time","","","24"
"4676","644702","7616007","06/08/2021 05:34:46","## NIFTY 50 Trends
By analyzing  nifty 50 trends  one can say the forecast of behavior of share market. One can also predict  effect of pendamic.

# Dividend 
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Nifty50","","","1"
"2211","867408","77920","09/23/2020 13:52:43","## Task Details
Sometimes people are interested in quick summary, not whole article

## Expected Submission
The solution should be a machine/deep learning code to summarize a given long article to short but complete summary. A notebook would work as well.

## Evaluation
shorter the better, should cover all the important topics of the article.

### Further help","","summarize a news article","summarize a long news article using AI models","01/01/2021 23:59:00","0"
"1426","788423","84462","07/20/2020 10:40:11","It would be very useful to develop a kernel that attributes a classification model's prediction to parts of the picture. In our case, the picture is the spectrogram of the vibration recording. Wood-feeding sounds manifest themselves as impulsive sounds (i.e. crack of fibers). A model interpretability technique would prove that an infestation decision is based on detected impulses. Recordings from trees located in urban environments are noisy.","","Class activation visualization","","","0"
"2167","881764","108364","09/19/2020 13:32:59","## Task Details
This dataset is made for Instance Segmentation Task.","","Instance Segmentation","","","2"
"2527","636600","113389","10/24/2020 07:23:53","## Context
Joe Biden is the democratic candidate for US Presidency 2020.

## Submission
A notebook demonstrating insights of Joe Biden's tweets and associated sentiments across the years.

## Evaluation
Sentiment score prediction and corresponding learnings. It'd be a good idea to compare against Donald Trump's tweets: https://www.kaggle.com/austinreese/trump-tweets","","Joe Biden sentiment analysis","","11/03/2020 23:59:00","2"
"1217","630055","113389","06/28/2020 12:38:35","## Details
COVID-19 has led to severe lockdowns in India. Not only did it improve the air quality of India across cities, it might even lead to a permanent change in AQI levels with a lot of industries and processes getting restructured and reworked on based on the impact of COVID-19.

## Submission
A notebook quantifying the impact of COVID-19 during lockdown period in India as well as continuing impact post-lockdown.

## Evaluation
Be descriptive. Choose relevant metrics. Quantify impact. Tell a story. Write good code.

### Example
@parulpandey 's notebook: https://www.kaggle.com/parulpandey/breathe-india-covid-19-effect-on-pollution is a great example notebook and a good place to start.","","Impact of COVID-19 on AQI in India","","12/31/2020 00:00:00","9"
"1877","630055","522718","08/29/2020 14:26:05","I got the motivation for this task from the #ChaiEDA group that @init27 has started.  The group meets weekly.  Group is active and engaged and I've had a lot of fun participating so far.  Details can be found here: https://twitter.com/bhutanisanyam1/status/1290323765499330572?s=20
Please join if you can! 

The task below is completely fictional.  I made it up to give the members of the #ChaiEDA group something specific to focus on.  I hope it does not offend anyone.  Intention is to provide direction and purpose to the #ChaiEDA group. 

## Task Details
Unfortunately, your uncle's wife recently died due to severe air pollution.  Your uncle is super rich and he misses his wife very much.  He calls you, knowing that you love data.  You pick up and he immediately says, ""In 2 weeks, tell me which city to invest in to improve air pollution for the long run.  You'll have the $ for 3 years.  Then, if you are successful, I will give same amount for 2 more cities.  Please provide a rough plan to convince me of which geographical city / area to focus on, why that city, briefly on how you will make improvements, and how you will track progress.  Thanks.""  Then he says that he hopes your family is doing well and that he will call back in 15 days and then abruptly hangs up before you can say anything. 

## Expected Submission
Choose your top city and two more cities.  
Your uncle wants to move fast.  Lucky for you, your uncle loves data and visuals.  He wants the visuals to tell the story.  He also doesn't like to read too much.


## Evaluation
Whoever has the most upvotes for this task will win. 

Have fun and enjoy! 
Thanks, David

(Submission Deadline is TBD at the moment)","","Where to deploy resources in India to combat air pollution?","Your super rich uncle has $ and motivation to combat air pollution.  What will you do?","","13"
"2600","630055","4485681","11/01/2020 09:02:47","## Task Details
Every year delhi population curses Punjab and Haryana farmers of practising stubble burning which according to them and everyone else is the major reason of pollution. But people tend to overlook the fact that population of delhi is also participating in elevating the levels of AQI. Some such factors are the number of vehicles, crackers in Diwali, lots of industries present in suburbs and many more. 
now finally when people are sitting at home and there are less number of vehicles on street and hopefully people celebrating a relatively greener diwali a lot of factors can be cut down and we can actually find out to what extent Punjab and Haryana are responsible for this problem. 

## Expected Submission
using these databases and any other databases that you can find from Kaggle try to find out the change in AQI and all the reasons associated with any sudden shift in AQI. Try to reason out the actual problems that plaguing Delhi. Try to come up with some implementable solutions for the same","","FINDING OUT CAUSES OF POLLUTION","find other datasets also","","4"
"1183","680672","1790018","06/20/2020 21:57:34","## Goal
The competition is aimed at articulating insights from the Interviews with ML Heroes. Check the judging criteria for complete details.

## Expected Submission
The submission should be a Kernel that is original work, the notebook should ideally talk about a particular theme/story instead of simple EDA and shouldn‚Äôt follow a voting ring pattern.

## Judging Criteria:

The scoring will be based on points out of 100, based on 5 criteria highlighted below. 
Every participant will be evaluated out of 5 points by the 4 judges allowing the maximum score achieved to be 100. (5 points * 5 criteria * 4 judges)

Winners will be chosen based on the highest score achieved overall along with special recognition to participants with the highest score achieved in individual criterion.

Final LB will be published on the 1 year anniversary of the Podcast.

- **Presentation**: How well is the notebook written in terms of code quality, text description, and grammar?
- **Storytelling**: Is there a natural flow of the story that connects various points?
- **Visualizations**: Are the visualizations appealing, understandable, and aligned with description?
- **Insights**: Are the insights relevant, useful, and actionable?
- **Innovation**: How novel and creative are the ideas and approaches?

### Resources:

- Workshops will be announced soon, please keep an eye out [here](https://www.kaggle.com/rohanrao/chai-time-data-science/discussion/156137)
- [Tutorial on how to explore the data](https://www.kaggle.com/parulpandey/how-to-explore-the-ctds-show-data) by @parulpandey
- @abhishek will be sharing a video on how to use the transcripts very soon","","Competition Submissions","Submit your kernel(s) here for evaluation","07/16/2020 00:00:00","15"
"1474","680672","1790018","07/24/2020 11:49:37","## Goal
Kernels articulating interesting or actionable insights from The [Chai Time Data Science Show](https://chaitimedatascience.com/) are invited.

## Expected Submission
The submission should be a Kernel that is original work, the notebook should ideally talk about a particular theme/story.

Please check the [Winning solutions](https://www.kaggle.com/init27/ctds-show-milestones) for the (now closed) Kaggle contest

## Criteria of Evaluation:

The judging criteria will remain based on the Kaggle Contest run earlier:

- **Presentation**: How well is the notebook written in terms of code quality, text description, and grammar?
- **Storytelling**: Is there a natural flow of the story that connects various points?
- **Visualizations**: Are the visualizations appealing, understandable, and aligned with description?
- **Insights**: Are the insights relevant, useful, and actionable?
- **Innovation**: How novel and creative are the ideas and approaches?

## Resources:

- Please explore the [Kernels Tab](https://www.kaggle.com/rohanrao/chai-time-data-science/kernels) for a large number of submissions that have been posted for the Kaggle Contest that was run during June 2020

## Prizes:

Selected Kernel authors from every month, based on the kernels submitted will be provided an option to claim:
- An Amazon gift voucher
- Shoutout to their work in an episode of CTDS.Show
- CTDS.Show merchandise (Cup)","","CTDS.Show Monthly Contest","Open Monthly contest for discovering insights based on CTDS.Show","","4"
"605","548503","178864","03/18/2020 16:54:21","## Task Details
Headline generation is a part of summarization task. Summarization is a very nice feature to have in many applications. The idea of this Task is to play around with SotA approaches in headline generation with this cryptonews dataset. 

## Expected Submission
Generated titles need to be created for the validation set, ""First 30 words baseline"" [Notebook](https://www.kaggle.com/kashnitsky/cryptonews-headline-generation-first-30-words) for details. 

## Evaluation
Average between ROUGE-1, ROUGE-2, and ROUGE-L scores, F1-variant, see the ""First 30 words baseline"" [Notebook](https://www.kaggle.com/kashnitsky/cryptonews-headline-generation-first-30-words) for details.","","Headline generation for cryptonews","Summarizing the content of an article generating a header","05/15/2020 00:00:00","0"
"1358","640429","230058","07/14/2020 11:48:41","## Task Details
Can you build a text classifier that can detect whether a given snippet of source code is written in Java or Python or C++ ?

## Expected Submission
You can solve this task using Kaggle Notebooks. 

## Evaluation
Try different evaluation metrics like AUC ROC or F1 score. Which one is better suited for this multi-class classification problem ?!","","Build a classifier to identiy the programming language from the source text","Text Classification","","2"
"1360","640429","230058","07/14/2020 11:53:46","## Task Details
Will functional languages form a common cluster, or will you see more similarities between C/ C++ & Java syntax ? Find out how different languages can be clustered in meaningful ways.

## Expected Submission
This task should be solved using Kaggle Notebooks.

## Evaluation
A good solution will be self evident with the kind of clusters it creates. If you can cluster by their programming paradigm or their typed system or their style of evaluation, then that model is clearly insightful.","","Can you cluster programming languages into meaningful groups ?","Clustering","","0"
"870","641649","252874","05/10/2020 01:21:35","## Task Details
Is it possible to co-relate studies being conducted on different drug regimes with the ones being postulated for the covid-19 virus?
Can it help in reducing the number of false positive signals  

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Co-relate it with other Covid-19 studies","","","0"
"944","667002","252874","05/21/2020 17:39:08","What is the demography of a typical migrant worker?
Which states contribute have a larger population of migrant workers?
Can their educational, sex and other characteristics help us understand them better?","","Identify the demographic of migrant workers","","","0"
"1036","689642","252874","06/02/2020 18:06:50","Can you use the dataset to identify patterns. 
Is it possible to quantify their impact on the **ground**","","Identify patterns in fake information","","","0"
"1300","765093","252874","07/09/2020 07:04:58","The dataset contains images from the Oxford and Paris datasets. 
You can use it to train models for landmark recognition and retrieval.","","Create landmark recognition & retrieval models","","","0"
"2248","893809","254481","09/26/2020 07:32:57","## Task Details
Analyze approvals based on region and sections of society
## Expected Submission
1.which state received most approval ?
2.which state's approval was least disbursed ?
3.which section of society (minority, small business,women owned) received most approval?
4.which section of society's approval was least disbursed ?
5. Is there any correlation b/w 2 and 4 ?

## Evaluation
Post the analysis of correlation between 2 and 4.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analyze approvals based on region and sections of society","Analyze approvals based on region and sections of society","01/26/2024 23:59:00","0"
"1232","749853","273127","07/01/2020 07:51:10","## Task Details
A piece of narrative text communicates to a reader about events, places, and persons. Automated extraction of action description from a narrative text is a challenging task. Development of an appropriate network model is an important step but knowledge representation, using syntactic and semantic aspects forming features, is critical for extraction of acceptable action description.

## Expected Submission
1. Develop an algorithm to extract  syntactic cues from the text
2.Develop an algorithm to extract  semantic labels
3.Develop an algorithm for co-reference resolution for the extracted syntactic and semantic information 
4.Identify the features by integrating syntactic, semantic, and co-reference information

5.Develop a knowledge representation algorithm using the features identified above 
6.Identify threshold patterns for joint probability distribution.
7.Develop a Bayesian network model.
8.Develop a reasoning scheme for inferring action description.
9.Analyse the results and comment on the impact of representation on inference and learning algorithm.

## Evaluation","","Extract syntactic cues and semantic labels from a narrative text","","","2"
"1428","788945","285393","07/20/2020 15:33:35","## Task Details
Can you help understand the user behavior based on their activities ? 

* Frequency of submissions to competitions 
* How talkative they are on discussion forums 
* Do they only participate in competitions ? 

A starter kernel to [explore user activity is provided here ](https://www.kaggle.com/tomtillo/top-ranked-kaggler-daily-activity-eda)

## Evaluation
A good visualization on the user behavior would be the starting point in understanding the user traits","","Create user personas based on their activity on Kaggle","What kinda of kaggle user are you ?","","0"
"1495","801648","285393","07/27/2020 12:39:34","##  What would be the best metric to define the most volatile day for NIFTY ? 

## Expected Submission
A daily chart to show the difference between the most volatile and the least volatile day","","Most volatile day for NIFTY INDEX","What was the most volatile day","","0"
"1533","806977","285393","07/30/2020 11:27:34","### More ideas to ponder & visualize
#### Job Type/ Title
* What is the salary growth for other Job Titles ?  ( Did they show a steady increase or stayed stagnant ? )
* What job-role did have the highest progression in % terms / absolute terms ? 
* Which Job title/ job type grew the maximum over the years ? 
* What job title has the highest Benefits : Base pay ratio ? 

#### Individual 
* Who had the higest pay in the years 2011- 2019 ? 
* Who received the most benefits ?
* Who had the highest Overtime pay ? [](http://)

#### Part-Time v/s Full Time Employees 
* Was there a difference in the pay between PT ( Part-Time ) v/s FT ( Full Time ) employees ? 
* Which job title had the highest PT:FT ratio ?","","More Ideas to Ponder and Visualize","Can you come up with more insights from the salary data ?","","0"
"3476","776281","294675","02/12/2021 15:03:34","## Task Details

Explore and visualize the hand probabilities within the game of Texas Holdem No Limit Poker


## Expected Submission

Create an EDA analysis and visualization exploring the probability of winning or making specific hands at each stage of the game, and the possible range of hands your opponent might have.

## Evaluation

Can you use this analysis to help win at a game of poker?","","Poker Hand Probabilities","","","1"
"3477","571368","294675","02/12/2021 15:08:35","http://prize.hutter1.net/

**Compress the 1GB file enwik9 to less than the current record of about 116MB**

Being able to compress well is closely related to intelligence as explained below. While intelligence is a slippery concept, file sizes are hard numbers. Wikipedia is an extensive snapshot of Human Knowledge. If you can compress the first 1GB of Wikipedia better than your predecessors, your (de)compressor likely has to be smart(er). The intention of this prize is to encourage development of intelligent compressors/programs as a path to AGI.
Interview with Lex Fridman (26.Feb'20) ([Video](https://www.youtube.com/watch?v=E1AxVXt2Gv4), [Audio](https://lexfridman.com/marcus-hutter), [Tweet](https://twitter.com/lexfridman/status/1232726312206393344))

## The Task

Losslessly compress the 1GB file enwik9 to less than 116MB. More precisely:

- Create a Linux or Windows compressor comp.exe of size S1 that compresses enwik9 to archive.exe of size S2 such that S:=S1+S2 &lt; L := 116'673'681 = [previous record](http://prize.hutter1.net/#prev).
- If run, archive.exe produces (without input from other sources) a 10^9 byte file that is identical to enwik9.
- If we can verify your claim, you are eligible for a prize of [500'000‚Ç¨√ó(1-S/L)](http://prize.hutter1.net/hfaq.htm#money). Minimum claim is 5'000‚Ç¨ (1% improvement).
- Restrictions: Must run in ‚â≤100 hours using a single CPU core and &lt;10GB RAM and &lt;100GB HDD on our [test machine](http://browser.primatelabs.com/v4/cpu/145066).

Remark: You can download the zipped version [enwik9.zip](http://mattmahoney.net/dc/enwik9.zip) of enwik9 here (‚âà300MB). Please find more details including constraints and relaxations at http://prize.hutter1.net/hrules.htm.

## Motivation

This compression contest is motivated by the fact that being able to compress well is closely related to acting intelligently, thus reducing the slippery concept of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which is intrinsically difficult (many researchers live from analyzing data and finding compact models). So compressors beating the current ""dumb"" compressors need to be smart(er). Since the prize wants to stimulate developing ""universally"" smart compressors, we need a ""universal"" corpus of data. Arguably the online encyclopedia Wikipedia is a good snapshot of the Human World Knowledge. So the ultimate compressor of it should ""understand"" all human knowledge, i.e. be really smart. enwik9 is a hopefully representative 1GB extract from Wikipedia.","","Hutter Prize for Compressing Human Knowledge - 500'000‚Ç¨","Compress the 1GB file enwik9 to less than the current record of about 116MB","","0"
"1560","812457","313391","08/02/2020 19:06:03","## Task Details
When we work in real world its difficult to keep track of complaints and often complaints are organised manually. Can we use unsupervised learning and identify what a complaints is talking about from complaints text
## Expected Submission
A Notebook which clearly explains the organisation and on what basis it was done

## Evaluation
We already have product and issue in columns. We can evaluate our unsupervised model based on those columns

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Topi Model","Use topic model to better organize ccomplaints","","1"
"2432","903533","313391","10/14/2020 06:02:38","## Task Details
The most common problem for Audio streaming websites is how to recommend relevant songs and content to users. The songs are to be recommended based on user preference.

## Expected Submission
The expected results would return a playlist when a user enters a list of his favorite songs
Also, you can create algorithms to find similar songs based on playlist data

## Evaluation
Evaluation metrics is yet to be decided. But a good results will have similar songs in terms of genre, similar artist and etc

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Songs Recommendation Engine","Write a algorithm to identify similar songs and do playlist creation","12/31/2020 23:59:00","0"
"2094","872890","323048","09/14/2020 11:53:23","With Week 1 approaching this Saturday, Who would your Ideal 11 be? 

Share a notebook based analysis, get those players on your team and in the process, win a bigger prize -- Kaggle Medals !!","","Who is your Ideal 11 for Week 1?","","","0"
"1202","736742","324241","06/25/2020 10:36:29","I still have to get proper date data for the tournaments :
- get the week number / year to convert into monthes
- eventually add a start_date and end_date, based on days","","Improving dates data","","","1"
"2241","890250","328423","09/25/2020 22:01:29","For this task, you start with splitting the dataset into Pre and Post pandemic samples. (See [dataset descriptive statistics](https://datastudio.google.com/reporting/45aa1d0c-1a24-47df-9cd0-69fc140f3911).) Next, run a covariance analysis on the dataset variables*. To account for false-positive errors you should use a tight ( &gt; .95) confidence intervals. Last, run cluster analysis on the significantly different variables.

[Dataset variables data-type and description.](https://www.kaggle.com/zipo1328423/covid19-rorschach-test-dataset/discussion/186980)
 
   * For the array variables - Determinants, Content, and SpecialScores, in blend responses that contain more than 1 code, each code should be tally separately.

**Task Completion**
1. Upload a clean data file, with the extracted array codes, and the [data cleaning specification](https://www.kaggle.com/zipo1328423/covid19-rorschach-test-dataset/discussion/186687).
2. Present covariance results in tables and graphs.
3. Specify the clustering method of the significantly changed variables and present their clustering results.
4. Provide an initial exploratory ML model and present its results.
*  All the task final notebooks should be uploaded and completely functional on Kaggle.","","COVID-19 Rorschach test dataset task","Responses to the Rorschach test before and after COVID-19","","4"
"6914","497018","8256815","11/23/2021 16:18:14","java test","","java test","java test","12/07/2021 23:59:00","0"
"1498","697683","360751","07/27/2020 18:38:28","## Background
Finding relevant insights on this important issue is no easy task. This dataset brings together data of several different types to enable more complete analysis. Kaggle has announced an upcoming effort to create several challenges including analytic challenges. This is a good chance to get a head start.
https://www.kaggle.com/general/165766

## The Task
Use several data sources together to answer (or partially answer) a question that people care about. For instance:
 - What can we say about militarizing police? Does it make police safer? Does it lead to more citizen deaths? Does it reduce crime?
- How do police forces compare across important dimensions? What groupings might exist such as ""high budget - high arrests - shoots people"", or ""high budget - low arrests - low violence"". How does that correlate with community safety?

## Expected Submission
The typical submission is a report-style notebook containing your analysis and findings. I describe my idea of what that means in this notebook: https://www.kaggle.com/jpmiller/some-best-practices-for-analytics-reporting

## Evaluation
My expectation is that Kaggle will evaluate submissions as part of a formal analytics challenge. I'm guessing it will include other datasets as well. I'm glad to provide feedback at this stage to anyone who directly asks.","","Present insights on police violence and racial equity.","","","4"
"2445","870589","360751","10/16/2020 02:03:28","**Background**
Finding relevant insights on this important issue is no easy task. This dataset is 1 of 3 that bring together several different types of data to enable more complete analysis. Kaggle has announced an upcoming effort to create several challenges including analytic challenges. https://www.kaggle.com/general/165766 This is a good chance to get a head start.

*Update:* it has been several months since Kaggle's announcement and they haven't yet started any challenges. The most I can offer is that for the accepted answer - provided it meets a minimum standard - I will offer a recommendation/testimonial for you to your next prospective employer. If you're not changing jobs, I could provide information supporting a raise or promotion at your current job, or promote your work on twitter and linkedin. i could also write a note to your friends/family/spouse/significant other explaining why it's important to spend so many hours on Kaggle :) !

**The Task**
Use several data sources together to answer (or partially answer) a question that people care about. For instance:

 - What can we say about militarizing police? Does it make police safer? Does it lead to more citizen deaths? Does it reduce crime?
 - How do police department policies and union contracts combine to affect police violence? 
 - In terms of financing: what groupings might exist such as ""high budget - high arrests - shoots people"", or ""high budget - low arrests - low violence"". How does that correlate with community safety?

**Expected Submission**
The typical submission is a report-style notebook containing your analysis and findings. I describe my idea of what that means in this notebook: https://www.kaggle.com/jpmiller/some-best-practices-for-analytics-reporting

**Evaluation**
My hope is that Kaggle will evaluate submissions as part of a formal analytics challenge in the near future. I'm guessing it will include other datasets as well. If that doesn't happen (or even if it does), I'll choose an accepted answer based on the recommendations listed in the above link. 

Thank you for supporting a worthwhile cause.","","Present insights on public policy and police violence","Data science for good","","2"
"1811","843201","372183","08/24/2020 08:42:59","From the given dataset, Find the country with the least price for 1GB data.","","Find the country with the least price for 1GB Data","","","1"
"849","637189","378285","05/06/2020 04:35:40","## Task Details

Build Classification model to Classify Credit worthy and Risky Customers

## Expected Submission

What should users submit? 

Should they solve the task primarily using Notebooks 
Solution contain - 
           Effective classification of train / test split 
           Classifier model with Accuracy 
           Ensemble Model
           Explainability of classifier model with LIME / SHAPE ( Model explanation 
          library )

## Evaluation

Classification model should Good Accuracy 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
-  https://www.kaggle.com/praveengovi/credit-risk-analysis-beginner-s-guide","","Identify Risky Customer in Bank","Build Classification model to Classify Credit worthy and Risky Customers","05/17/2020 00:00:00","1"
"954","562468","4671579","05/23/2020 12:56:03","Better Accuracies on the predictions will be considered","","Prediction Accuracy of Chest X-Ray images","","","16"
"1961","555917","3754812","09/05/2020 17:36:22","To predict cases using COVID 19 Dataset.","","COVID 19 prediction","MAchine Learning","09/30/2020 23:59:00","0"
"1555","811058","402009","08/01/2020 21:08:50","## Task Details
An important part of understanding dialog is recognizing the entities mentioned

## Expected Submission
Notebook that produces annotations for podcast transcripts, including entities and reference spans.

## Evaluation
Submissions should report number of entities and references found, as well as how many of these entities correspond to those found in publicly available knowledge bases (e.g. ConceptNet - https://conceptnet.io/).

The higher the number of and proportion of recognizable entities, the better your solution.","","Identify entity references in podcasts","Named entity recognition and co-reference resolution in dialog","","1"
"2044","867875","410158","09/11/2020 01:10:34","## Task Details
Compare different sectors and find out which sector has a very lower mean salary and give a suggestion.

## Expected Submission
Visualization and summary based notebook is enough.","","Which sector needs a support from the Ontario government?","","","0"
"2060","868846","410158","09/12/2020 01:45:31","## Task Details
You can collect the content from the original article by using some web scraping and classify the links into 3 levels. Or else you can use the title to classify the level as well.

L1 - Basic Machine Learning Articles
L2 - Intermedial Machine Learning Articles
L3 - Expert level Machine Learning Articles

## Expected Submission
A notebook with some visuals and tables is enough.","","Classify the Links","","","0"
"2118","875911","410158","09/16/2020 05:43:08","## Task Details
Find which country Indians migrated the most and also find the top 3 countries which attracted Indians the most from 1980 to 2000.

## Expected Submission
Just a simple notebook with a visualization or table is enough to convey the message.","","Where did Indians migrate the most?","","","3"
"1861","848760","417337","08/28/2020 15:30:34","It would be valuable to combine this dataset with other county-level demographic datsets that can be found on Kaggle.","","Combine with County Demographic Data","","","12"
"1355","774369","462084","07/14/2020 01:21:14","## Task Details
Try to classify chess squares.

## Evaluation
Percentage of correctly classified chess squares.","","Chess Squares Classification","","","0"
"433","501019","1952004","02/04/2020 21:39:01","## Task Details
It might seem to be intuitive that bikeshare users would travel in the reverse direction in the evening than in the morning. But how true is that? If that's the case, would it reduce the value of convenience of bikeshares being available everywhere, and should they just get their own bikes? Do we also need a crew to pick up and restore all the bikes to initial distributions across the city before the next morning?

## Expected Submission
1. Find out what percentage of bikes are returned to its initial location?
2. How many bikeshare trips usually visit more than just start/end points, but visited some other points before the end of the day?","","Do bikeshare go round trip or one way?","","","16"
"658","577497","543708","03/29/2020 21:50:31","## Task Details
A lot of news articles regarding the recent coronavirus pandemic / covid-19 outbreak can be retrieved with the provided notebook from the Coronavirus subreddit.

## Expected Submission
There are claim annotated datasets available. It would be of interest to classify the sentences in the news articles whether they are a claim or not as a first step.

## Evaluation
This is either an unsupervised task or it applies transfer learning approaches form models trained on other datasets.","","Claim Detection","Can we detect claims in news articles regarding the coronavirus pandemic / covid-19 outbreak?","04/03/2020 00:00:00","1"
"776","594491","545810","04/16/2020 18:37:06","## Task Details
I want to know vulnerable age, what range of age is concentrating the rate of death people.

## Evaluation
Percentage condensed","","Concentrated death age","I want to know vulnerable age, what range of age is concentrating the rate of death people.","","0"
"726","584020","593313","04/10/2020 07:39:12","## Task Details
Try to not overfit depending on images sources 

## Expected Submission
Notebook with evaluation on a test set

## Evaluation
Accuracy , precision, recall, F1","","Train a COVID Classifier","","","1"
"662","568947","4599378","03/31/2020 12:25:30","Please update the data set","","Update the data set","","","0"
"1643","822553","612429","08/09/2020 05:24:22","## Task Details
Predict paper's title from its abstract 

## Expected Submission
Solve the task primarily using Notebooks 

## Evaluation
BLUE Score 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
-  https://www.kaggle.com/Cornell-University/arxiv (arXiv Dataset)","","Titles prediction","Predict paper's title from its abstract","12/31/2020 00:00:00","0"
"2061","821580","793761","09/12/2020 02:37:20","## Task Details
Uncover the most relevant topics associated with soccer, such as subjects, teams, players and much more.

## Suggested Methods
- UMAP/PCA/t-SNE on tf-idf or on pretrained word embeddings (such as glove and word2vec or language models (such as bert).
- LDA

## Resources
- [Basic example](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)","","Soccer Topic Modeling","","","0"
"493","533939","618942","02/29/2020 11:58:04","## Task Details
Determine the meter reading on the image

## Expected Submission
Train the neural network to find the meter reading on the image

## Evaluation
Metric: accuracy","","Determine the meter reading on the image","","","3"
"5595","714507","6224917","08/07/2021 06:13:01","## Task Details
Create a model for determination 'tempo' . 
In this task you must split data set into train set and test set then calculate accuracy score.

## Expected Submission
You must submit your code in a Jupyter notebook.
(In the last cell you must print accuracy score.)","","prediction 'tempo'","","","1"
"1941","856362","663396","09/04/2020 13:50:56","## Task Details
You need to highlight newsgroups. Each news item can be in several groups. For example, a high level topic like ""sport"" and a low level topic like ""Olympic Games 2020"".

## Evaluation
You can use rubric, subrubric and tags columns for self-testing.

### Further help
To complete the task, you may need the following libraries:
https://radimrehurek.com/gensim/
https://github.com/bigartm/bigartm
https://scikit-learn.org/stable/modules/clustering.html with tf-idf or bert features","","News topic modeling","","","0"
"1419","783883","666918","07/19/2020 14:41:51","## Hidden features

There are 10 special features hidden in the data. All 10 features are equally strong and together they can boost AUC by about 0.2 (from 0.7+ to 0.9+).

Publish EDA about how to find the hidden features and improve baseline model score.","","Find 10 special features hidden in data","","","1"
"5511","567482","6128454","08/02/2021 21:06:42","## Task Details
DS Learning Task 1

## Expected Submission
EDA of the Customer Churn on a bank data set

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA FOR BANK CUSTOMER CHURN","DS Learning Task 1","","0"
"1115","707827","694097","06/11/2020 14:29:50","Inflation data and currency exchange data can be used to adjust gross collections.","","Adjust gross collections for inflation","","","1"
"1116","707827","694097","06/11/2020 14:34:00","Lots of countries have high movie viewership but fewer collections due to the purchasing power of parity. Also, there are countries where gross collections increased significantly over time. What are these countries?","","Potential film markets","","","0"
"1089","695272","694097","06/08/2020 02:46:36","For example, if the commentary is *back of a length on middle, and pulled to Krunal at deep midwicket.*
The output should be of three different types shown as below
Bowler Tag - *back of a length*, *middle*
Batsman Tag - *pulled*
Fielder Tag - *Krunal*, *deep midwicket*","","Tagging every delivery in a cricket match","","","0"
"2264","895611","695758","09/27/2020 13:03:00","## Task Details

Since there are 100s of areas where a govt can focus in improving a country, it would be great to identify the top 5 areas where the govt of a developing nation should focus to grow towards a developed nation.

A definition of developed nation can be found in this [link](https://www.investopedia.com/terms/d/developed-economy.asp)

Can such 5 top areas of focus be identified?

## Expected Submission
Submission can be a notebook indicating the top 5 areas and supporting analysis","","Identify top 5 indicators developing nation govt should focus to progress towards a developed nation","","","0"
"1845","847165","700017","08/27/2020 09:56:26","You are given data of total (aggregated) power consumption, and the TV power consumption of three households. Each household has data for one day.

Task 1: Using TV instantaneous power consumption data, identify the
times when the TV is ‚ÄúON‚Äù. Notice that some TVs may have standby
modes.

Task 2: Using all the data given, design a classifier to identify times when
the TV is ‚ÄúON‚Äù. You may also want to train and test your designed
classifier. The trained classifier should not take TV instantaneous power as
input.

Please provide a script which attempts task 1 and 2. Please do not
provide a script without explanations.","","Time Series Labeling and Classification","","","0"
"511","529501","701762","03/05/2020 19:53:40","Determine possible spread zones","","Detect risk zones","","","0"
"499","529501","1178552","03/03/2020 07:12:04","## Task Details
practise

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","corona virus","exploratory analysis","","2"
"2010","863847","718310","09/08/2020 11:00:38","Exploring the ability of Multi-Lingual BERT on Kannada Language.","","Sentiment Analysis using Multi-Lingual BERT","sentiment analysis on Kannada Language","","1"
"1453","792929","726232","07/22/2020 14:33:01","## Task Details
The most useful application of this dataset would be predicting future outbreaks of EEE or WNV, if possible.","","Predicting future outbreaks","","","0"
"1821","843921","732424","08/24/2020 22:19:19","## Task Details
Prepare data visualization for this dataset. Use different types of charts in your solution.","","Data visualization","","","5"
"2125","843921","732424","09/16/2020 14:18:37","## Task Details
You need to clean data and prepare features for modeling. You can also use different methods of feature engineering.","","Feature engineering","","","0"
"2157","843921","1313949","09/18/2020 22:08:30","## Task Details
Create notebook with visualization of basketball field and players on it","","Basketball field visualization","","","0"
"1925","843921","732424","09/02/2020 22:22:07","## Task Details
Use ```Salary``` column as a target for regression task and predict salary for players. Split data into train/test sets with 20% for test set and random state=0 for splitting.

## Expected Submission
Notebook with model that predicts salary for players.","","Predict salary for players","","","10"
"2144","878510","732424","09/17/2020 22:13:34","## Task Details
Using playoff 2020 start history predict the winner of the season","","Predict a champion of NBA 2020 season","","","1"
"1764","838882","754781","08/20/2020 14:15:37","## Task Details
Define an algorithm that can sense the relevance of a team composition.

## Evaluation
What metric can be defined to sort a good composition from a bad one ?","","The best composition","","","0"
"6316","838882","6466279","10/11/2021 19:04:54","## Task Details
In career mode we need to find best young players with limited prices.
So lets find the best young players at Fifa22 dataset

## Expected Submission
Max value of total team release clause must be under the 30M Euro.

## Evaluation
Best solution must be under the 30M Euro and highest potential sum.","","Finding Best Youth Players","","","1"
"1212","742030","754781","06/27/2020 18:20:03","## Task Details
Try to generate your own sample of abstract images using this dataset and get the most amazing art gallery.

You can get started from <a href=""https://www.kaggle.com/getting-started/150948"">this topic</a> on GANs as reference

## Information
The painter Vassily Kandinsky is considered the founder of abstract art. He painted his first abstract watercolour Untitled in 1913.

![](https://upload.wikimedia.org/wikipedia/commons/6/69/Kandinsky_Sans_titre_1910-1913_MAM.gif)","","Generate your own abstract piece of art using GANs","Deep Learning is the new Vassily Kandinsky","","1"
"1191","733418","754781","06/22/2020 22:38:01","Blending...","","Ensemble","ResNet, EfficienNetB3 and VGG16","","0"
"689","585296","754781","04/03/2020 17:17:45","## Task Details
Describe and clean the dataset. Draw some graphs to make your data speak.

## Expected Submission
Whatever the choice you make, these are never bad if they are justified.","","Exploratory Data Analysis","","","0"
"690","585296","754781","04/03/2020 17:21:20","## Task Details
Can you find the statistical dependancy between:
1.  ""specialit√©"" and ""sexe""
2.  ""cheveux"" and ""salaire""
3.  ""exp"" and ""note""

## Statistical Tests
Different statistical tests for each category is expected","","Descriptive statistics","","","0"
"691","585296","754781","04/03/2020 17:22:59","## Task Details
Create a model that can predict whether a candidate is hired. What are the most important variables of your model ?
Describe the metric used.","","Machine Learning","","","0"
"981","609089","754781","05/26/2020 22:49:14","## Task Details
* Topic modeling, what is good, what is wrong with the beta
* Are players satisfied with the game ? Can it have a higher success than CS:GO, its biggest competitor ?
* EDA","","Topic Modeling, sentiment extraction, EDA","Get what could be improved by devs before the release in June","","0"
"782","587566","769452","04/18/2020 14:56:33","Add country level data for quarantines, isolated and ICU patients","","Add country level data","Add country level data for quarantines, isolated and ICU patients","04/20/2020 00:00:00","0"
"2226","587566","769452","09/24/2020 20:37:40","Use Notebooks to study the evolution in time of tests data","","Check the test data evolution","Verify the evolution in time of tests data","10/24/2020 23:59:00","0"
"539","544069","769452","03/13/2020 06:47:51","## Task Details
Unify the various countries variations; ex: Holy See & Vatican City, Macau & Macao SAR, South Korea and Republic of Korea and Korea, South.

## Expected Submission
Database with unified countries names

## Evaluation
Timeseries for each country need to be continous","","Merge the Country names variations","","03/14/2020 00:00:00","1"
"603","544069","769452","03/18/2020 13:12:17","## Task Details
Currently there are not much US data analysis currently.

## Expected Submission
Kernels highlighting the evolution of cases in US.

## Evaluation
Showing time evolution of the cases (Confirmed, Recovered, Deaths) in US.","","Analyze US data","Produce an analysis of US data.","02/10/2021 00:00:00","1"
"2390","698950","769452","10/09/2020 09:45:42","## Task Details
Study how unemployment evolved in the last 10 years in Europe.

## Expected Submission
Notebook or RMarkdown report.","","How unemployment evolved in last 10 years?","","02/09/2021 00:00:00","0"
"1743","698950","769452","08/18/2020 16:25:18","## Task Details

Can you spot patterns of unemployment across different European countries? Is unemployment different for young people or for a certain sex?

How the COVID-19 outbreak changed unemployment patterns in various countries?

## Expected Submission
Use Notebooks to state your findings.

## Evaluation
Provide graphs, analysis, discuss the results.","","Patterns for unemployment in different European countries","","09/18/2020 00:00:00","1"
"1695","698950","769452","08/14/2020 07:59:57","## Task Details
Use European unemployment data to try to understand if younger people are more severely hit by unemployment during COVID-19. By following the trends of unemployment by age and country, try to understand if and where young people are more affected by unemployment due to job market shrinking in the first months on current pandemic.

## Expected Submission
Use exploratory data analysis, graphs and, if possible, animations, to show the unemployment trends across Europe.","","Are young people more severely hit by unemployment during COVID-19?","","09/14/2020 00:00:00","11"
"1505","798386","769452","07/28/2020 12:08:45","## Task Details
Create Notebooks to follow the geographical distribution of tweets.","","Geographical distribution of tweets","Who, from where is tweeting about covid19?","08/31/2020 00:00:00","28"
"1506","798386","769452","07/28/2020 12:10:27","## Task Details
Create Notebooks for sentiment analysis from Covid-19 tweets","","Tweets sentiment analysis","Sentiment analysis for tweets about Covid-19","08/31/2020 00:00:00","18"
"1681","798386","769452","08/12/2020 15:40:17","Use animated maps to show evolution of number of tweets per day in different parts of the World

The animation should show aggregated volumes at day or even hour level.","","Create a Notebook to show animation with geographical distribution of tweets","Animated trends","08/26/2020 00:00:00","6"
"1694","798386","769452","08/14/2020 07:19:41","## Task Details

With the available data, analyze the trends of COVID-19 subject in the collected tweets. Follow both the time and spatial distribution.

## Expected Submission
Create a Notebook to show this trends.

## Evaluation
Use graphs and preferably animation to show the trends in time/space. Maps are welcomed.","","Dynamic in time and space of the tweets","Analyze the dynamic of COVID-19 subject (temporal, spatial)","09/14/2020 00:00:00","3"
"1750","829497","769452","08/19/2020 05:52:00","## Task Details
Explore the BBC YouTube Channel data.

## Expected Submission
Use Notebooks, Python scripts, R Markdown reports, R scripts for your analysis.

## Evaluation
Capture the time evolution of subjects, study the type of videos uploaded over time, look to the impact of videos over time; the successful submission should bring a fresh perspective to this data interpretation.","","Explore subjects on BBC YouTube Channel","","09/19/2020 23:59:00","0"
"1701","831652","769452","08/15/2020 08:59:04","## Task Details

This task is about applying  text analysis techniques to the Kaggle YouTube channel videos titles and description.

## Expected Submission

Submit your analysis work through a Kaggle Notebook.

Understand what are the topics discussed in these channels videos, from the video titles and descriptions. 

Check the time trends (how these subjects change in time).


## Evaluation
The best submission should capture the important topics in the Kaggle YouTube channel and how these change in time.","","Videos titles and description text analysis","Apply NLP techniques to the Kaggle YouTube channel videos titles and description","09/15/2020 00:00:00","0"
"2360","807771","769452","10/07/2020 13:47:21","## Task Details
Perform Sentiment Analysis on Trump Tweets.

## Expected Submission
It is expected you will use a RMarkdown report of a Jupyter Notebook to show your results.

## Evaluation
Most comprehensive analysis will be selected.","","Sentiment Analysis for Trump Tweets","","10/28/2020 23:59:00","0"
"2361","807771","769452","10/07/2020 13:49:14","## Task Details
Explore the tweets to understand the frequency, the topics, the dynamics of responses and impact.

## Expected Submission
Use a Notebook or a RMarkdown report for your analysis.

## Evaluation
Analyze the entire data, look to each aspect of the data.","","Exploratory Data Analysis for Trump Tweets","","10/29/2020 23:59:00","0"
"3277","816649","769452","01/26/2021 13:55:14","## Task Details

Most of the solutions to this problem - that is, train a model to classify Chinese MNIST digits images - used Tensorflow, PyTorch to build a Deep Learning network for the model.

Try to build a solution using RAPIDS library.","","Chinese MNIST:  Model using RAPIDS","Can you create a predictive model for Chinese MNIST classes using RAPIDS?","04/25/2021 00:00:00","0"
"2409","816649","769452","10/11/2020 19:22:57","## Task Details
Review the models used for solving the Chinese MNIST Challenge

## Expected Submission
Use a Notebook to introduce your study","","Model review for Chinese MNIST","","04/15/2021 00:00:00","1"
"1645","816649","769452","08/09/2020 09:03:19","## Task Details

Implement a classifier using PyTorch and GPUs for Chinese MNIST

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Ffae77a81c057fe419de60f5e2b20be25%2Fchinese_mnist_profile_small.png?generation=1596963542354014&alt=media)
 
## Expected Submission

Submit your Notebook.

## Evaluation
Obtain at least 0.90 accuracy.","","Implement a classifier using PyTorch and GPUs","Implement a classifier for Chinese numbers characters using PyTorch and GPUs","10/30/2020 00:00:00","1"
"1640","816649","769452","08/08/2020 20:21:11","#### Task description

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Ffae77a81c057fe419de60f5e2b20be25%2Fchinese_mnist_profile_small.png?generation=1596963542354014&alt=media)
 
Implement a classifier for Chinese MNIST (Chinese numerical characters) using Tensorflow and TPUs.

#### How to submit

Prepare a Notebook or a script with a model that gives a solution with at least 0.9 accuracy for the validation.","","Chinese MNIST: Implement a classifier using Tensorflow & TPU","Implement an image classifier for Chinese MNIST using Tensorflow and TPUs","04/15/2021 00:00:00","1"
"1674","816649","769452","08/11/2020 17:01:34","## Task Details

Implement a classifier using Tensorflow and GPUs for Chinese MNIST

![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Ffae77a81c057fe419de60f5e2b20be25%2Fchinese_mnist_profile_small.png?generation=1596963542354014&alt=media)
 
## Expected Submission

Submit your Notebook.

## Evaluation
Obtain at least 0.90 accuracy.","","Implement a classifier using Tensorflow/Keras/GPU","Implement a classifier using Tensorflow/Keras/GPU for Chinese MNIST","10/30/2020 00:00:00","0"
"2272","895647","769452","09/28/2020 14:49:32","## Task Details
Perform exploratory data analysis on the vote presence data.

## Expected Submission
Use a Notebook or a RMarkdown report to visualize your analysis.","","Exploratory data analysis","","10/28/2020 23:59:00","0"
"2273","895647","769452","09/28/2020 14:52:49","## Task Details
Every election is different, but some elections days are really special.
There are voting stations with an excessive percent of voters on special lists, or a very large number of votes using mobile vote.
Try to identify anomalies in vote distribution across municipalities or voting stations, unexpected distribution on sex/age and any voting presence statistics that stands out.

## Expected Submission
Use a Notebook or a RMarkdown report to show your results.","","Vote presence anomalies","Detect various vote presence anomalies","10/28/2020 23:59:00","0"
"2080","870460","769452","09/12/2020 19:35:57","You can code in Python or R, you can use Notebooks or R Markdown reports. Try to grasp the differences between various countries in EU.","","Compare absorption rates for SF in EU between different countries","","10/15/2020 23:59:00","0"
"1893","850950","769452","08/30/2020 12:25:13","## Task Details
Analyze the evolution of internet connections in European Union countries to understand the evolution in time, distribution per countries, which countries have high percents of internet access and which have lower access, focus on internet connection type as well as household type.

## Expected Submission
Create a Notebook or a R Markdown report to communicate your findings.

## Evaluation
Present in clear, concise form your data cleaning and analysis process, your findings, your future steps.","","Analyze the evolution of internet connections in European Union Countries","","09/30/2020 23:59:00","0"
"2307","902905","769452","10/02/2020 12:21:36","## Task Details
Process the text transcript of the presidential debate 2020 to understand the topics discussed by the speakers

## Expected Submission
Prepare a Notebook that present your approach and also showcase the results.

## Evaluation
Let the best documented and most beautifully presented results be the winner!","","Topic Modelling for USA Presidential Debate 2020","","11/02/2020 23:59:00","0"
"2308","902905","769452","10/02/2020 12:23:22","## Task Details
Use your knowledge of NLP techniques to process the data of the Presidential debate 2020 to explore, summarize, understand the topics discussed.


## Expected Submission
Present your results in a Notebook","","Apply Natural Language Processing techniques to analyze the debate","","11/02/2020 23:59:00","0"
"2296","900870","769452","09/30/2020 22:47:51","## Task Details
Explore the presidential debate video comments data to follow the sentiments of commentators, identify the topics, understand who is supporting each of the two candidates.

## Expected Submission
Prepare your submission using a Notebook or a RMarkdown report.","","Explore the presidential debate video comments data","","11/01/2020 23:59:00","0"
"2175","872755","4422823","09/21/2020 03:29:32","Must include insights gathered and reasons for inferences.","","Visualise Emission Data and tell a Story","","","3"
"2090","872382","793761","09/14/2020 05:03:37","## Task Details
Explore aspects such as success/failure, number of casualties and wounded as well geographical distributions and time frame of terrorism events.","","Explore Terrorism Events","","","0"
"2091","872382","793761","09/14/2020 05:13:44","## Task Details
Predict the outcome of the terrorism event using the remaining variables.

## Notes
It is suggested to binarize the column `Terrorist logistical success` for this task.","","Predict Outcome of Terrorism Event","","","0"
"2111","875173","793761","09/15/2020 17:30:16","## Task Details
Using tabular data and textual similarity, suggested possible matches

## Suggested Methods
- Cosine Similarity / Pearson correlation on top of Language model dense vectors
- UMAP/PCA/t-SNE


## Extra
Bonus points for creating visualizations that depict the cluster of profiles within the app!","","Lover Recommendation with Unsupervised Learning","","","0"
"1903","852142","793761","08/31/2020 09:10:27","## Task Details
Predict the damage class caused by a tsunami.
Note that there are many possible labels: Deaths, Houses Destroyed, Cost estimate, but once you choose one, don't use the others as input variables.

## Suggested Metrics
Note: Labels are available as classes.
- AUROC
- PRAUC
- Accuracy
- F1 Score","","Predict Damage Caused by Tsunami","","","4"
"2013","863914","793761","09/08/2020 12:34:48","## Task Details
Explore suicide rates and their associated trends, as well as the effects of infrastructure and personnel on the suicide rates.","","Explore Suicide Rates","","","0"
"2000","862213","793761","09/07/2020 12:47:15","## Task Details
Using both text and a few other variables (platform, votes, date) predict the score that a particular review will give to the game.

## Suggested metrics
- MAE","","Predict Review Scores","","","0"
"2271","897156","793761","09/28/2020 10:20:05","## Task Details
Using the review text, predict the rating

## Suggested Metrics:
- MAE
- RMSE","","Hotel Review Rating Prediction","","","17"
"1568","813158","793761","08/03/2020 04:14:40","## Task Details
Unravel the demographics, means of control and other variables associated with 
human trafficking.","","Explore Human Trafficking","","","0"
"1538","807599","793761","07/30/2020 19:51:57","## Task Details
Predict divorce, and uncover they key factors associated with it.","","Divorce Prediction","","","4"
"1670","825603","793761","08/11/2020 08:00:59","## Task Details
Create a detection model that detects the presence of people, helmet and head in the images provided.

## Suggested Metrics
- IoU per class
- Dice coef. per class
- Accuracy per class (classification only)","","Safety Helmet Detection","","","6"
"1621","820188","793761","08/07/2020 07:44:53","## Task Details
Create a detection model to classify instances of leaves and regress their localization.

## Suggested Metrics
- IoU
- mAP
- mAR","","Leaf Type Detection","","","0"
"1651","823840","793761","08/10/2020 05:20:08","## Task Details
Conduct an exploratory data analysis of schools in the US

## Reference Questions
- How does public and private schools differ?
- What is the geographical distribution of schools?","","Explore Schools","","","0"
"1722","834656","793761","08/17/2020 09:04:46","## Task Details
Using survey and demographic variables, predict the likelihood of a person having autism.

## Suggested Metrics
- AUROC
- PRAUC
- F1 Score
- Precision and Recall

Accuracy is not recommended, as their is substantial class imbalance","","Predict Autism","","","0"
"1851","847621","793761","08/27/2020 16:53:48","## Task Details
Using the provided files, apply transfer learning from the pretrained weights to create a model for object detection or instance segmentation.","","Create a Detection / Instance Segmentation Model","","","0"
"1875","849724","793761","08/29/2020 11:31:02","## Task Details
Create a classification model to identify leukemia cells from cell images

## Suggested Metrics
- AUROC
- PRAUC
- Accuracy
- F1 Score","","Leukemia Cell Image Classification","","","3"
"1400","782434","793761","07/17/2020 11:53:24","## Task Details
Explore jobs by company rating, salary, location and industry!","","Find the best jobs for business analyst roles","","","0"
"1378","778775","793761","07/16/2020 04:42:34","# Task Details
Find the jobs with the highest paid salary, in top rated companies!","","Find best jobs by Salary, Company Rating and Location","","","0"
"1356","775009","793761","07/14/2020 08:55:22","## Task Details
Find the jobs with the highest paid salary, in top rated companies!","","Find best jobs by Salary, Company Rating and Location","","","33"
"1586","775009","1761512","08/04/2020 13:54:35","## Task Details
Good rating of a company is present because of plethora of reasons so I am just wondering,
Is good rating is correlated with the salary offered ?

## Expected Submission
Show correlation between various features using a heatmap.","","Is there correlation between rating and salary offered by the company ?(in Python)","(Please provide submission in Python)","","7"
"1410","784336","793761","07/18/2020 05:19:19","## Task Details
Explore best jobs by rating, salary, location and any other variable you deem fit.","","Find optimal data engineer jobs","","","0"
"1781","784753","793761","08/21/2020 10:58:07","## Task Details
Create a segmentation model to segment liver from the CT images

## Suggested Metrics
- Dice Coefficient
- IoU","","Liver Segmentation","","","1"
"1782","784753","793761","08/21/2020 10:59:00","## Task Details
Create a segmentation model to segment liver **tumor lesions** from the CT images

## Suggested Metrics
- Dice Coefficient
- IoU

Furthermore, It is suggested to first segment the liver, isolate it, and then create a tumor lesions segmentation model.","","Liver Tumor Segmentation","","","1"
"1245","736953","793761","07/02/2020 22:18:11","## Task Details
Analyze the time series per media, forecast revenues for the music industry or per media. Check how music streaming changed the industry.","","Explore Music Sales","","","0"
"1227","736968","793761","06/30/2020 02:02:40","## Task Details
Explore prices and their variance across US states","","Explore Auto Insurance Rates","","","0"
"1198","734964","793761","06/24/2020 00:03:40","## Task Details
Create a detection model to find tomatoes within images.

## Suggested metrics:
IoU","","Tomato Detection","","","2"
"1264","756770","793761","07/05/2020 23:32:50","## Task Details
Using data of latitude, longitude and size related variables (bed, total workforce) explore the geographical distribution of hospitals","","Explore Hospital Locations and Capacity","","","0"
"1314","767686","793761","07/10/2020 12:40:22","## Task Details
Create a segmentation model to classify pixels that belong to the liver or to liver lesions.

## Suggested Metrics
- Dice coefficient","","Liver tumor segmentation","","","1"
"1329","769463","793761","07/11/2020 22:24:07","## Task Details
Create a segmentation model to classify pixels belong to liver or liver tumor

## Suggested Metrics
- Dice coefficient","","Liver Tumor Segmentation","","","0"
"1073","696528","793761","06/06/2020 06:39:12","## Task Details
Using time series regression, predicted rare pepe prices.","","Predict Rare Pepe Prices","","","0"
"1079","697471","793761","06/06/2020 16:10:52","## Task Details
Predict the price of a medicine given its type, exclusivity, and other variables.","","Price prediction","","","0"
"1071","696484","793761","06/06/2020 05:02:02","## Task Details
Explore how many indicators vary across the globe, which countries are more alike and different.","","Explore Educational Indicators","","","0"
"1072","696484","793761","06/06/2020 05:03:40","## Task Details
Using indicators of educational access, completion, literacy, teachers, population, expenditures and others, predict the learning outcomes of a country's educational system.","","Predict Learning Outcomes","","","7"
"1047","692736","793761","06/04/2020 09:31:23","## Task Details
Create and evaluate a segmentation model for segmenting vessel tissue from background.

## Suggested Metrics
- Dice coefficient
- AUROC on probabilistic predictions","","Lung Vessel Segmentation","","","0"
"1050","693767","793761","06/04/2020 20:16:16","## Task Details
Explore the memes and have fun","","""eXpLoRaToRy DaTa AnAlYsIs"" - Explore the memes","","","1"
"1051","693767","793761","06/04/2020 20:17:02","## Task Details
Create an image classification model to differentiate Doom from Animal Crossing content.","","Doom or Animal Crossing?","","","17"
"1031","688896","793761","06/02/2020 10:24:57","## Task Details
Explore how mobility has changed due to the pandemic

This dataset is very suited as a variable for other models.","","Explore Mobility Behavior","","","0"
"1082","688943","793761","06/07/2020 04:20:37","## Task Details
Create a detection model to localize ships from aerial images.

## Suggested metrics:
- IoU","","Detect Ships","","","0"
"1020","686454","793761","06/01/2020 03:46:10","## Task Details
Create a detection model to generate bounding boxes depicting the location of license plates.

## Suggested Metrics
- IoU
- mAP

## Optional
You try a OCR on top of the regressed boxes, in order to capture the plate number!","","License Plate Detection","","","10"
"1022","686757","793761","06/01/2020 08:13:52","## Task Details
Explore any topic deemed relevant by yourself, here's a few suggestions:
- Racial distribution
- Geographical distribution
- Manner of death,
- Lethal Force and Mental health","","Exploratory Data Analysis","","","2"
"1083","687125","793761","06/07/2020 05:04:18","## Task Details
Detect sheeps from images!

## Suggested metrics:
- IoU","","Sheep Detection","","","0"
"1077","687132","793761","06/06/2020 15:22:08","## Task Details
Create a detection model to localize fruits within the images.

## Suggested metrics
- IoU","","Fruit Detection","","","0"
"1078","687187","793761","06/06/2020 15:32:10","## Task Details
Help ensure safety by detecting helmets

## Suggested Metrics
- IoU","","Helmet Detection","","","0"
"990","679322","793761","05/28/2020 05:35:11","## Task Details
Classify the evidence of any of the 9 classes (8 + Normal).

## Suggested Metrics
- Categorical Accuracy
- AUROC for each class","","Skin Image Classification","","","1"
"1002","681741","793761","05/29/2020 12:31:40","## Task Details
From an image, try to predict how many upvotes it will receive.
Don't forget to use the metadata file! Age of the post matters a lot!","","Image Regression of Upvotes","","","0"
"1003","681741","793761","05/29/2020 12:33:20","""Exploratory Data Analysis""
Browse the images, improve your day a little bit, get your hopes on humanity up","","Chill, Relax and Enjoy the Images - ""Exploratory Data Analysis""","The perfect dataset doesn't exis.....","","0"
"1098","702682","793761","06/09/2020 06:42:14","## Task Details
Find the racoons!
Create an object detection model.

## Suggested Metrics:
- IoU","","Racoon Detection","","","0"
"1121","702697","793761","06/11/2020 19:09:24","## Task Details
Create an object detection for detecting handguns in images.","","Handgun Detection","","","0"
"1099","702771","793761","06/09/2020 07:43:47","## Task Details
Create a detection model to localize potholes and improve road safety!

## Suggested metrics
- IoU","","Pothole detection","","","0"
"1119","708417","793761","06/11/2020 17:44:40","## Task Details
From the review text, predict the score given to the game!","","Game review score prediction","","","0"
"1120","708417","793761","06/11/2020 17:45:03","## Task Details
Uncover the most liked and disliked aspects of the game!","","Topic Modeling","","","0"
"1164","723034","793761","06/17/2020 20:08:09","## Task Details
Using a generative model of your choice, create new emotes!","","Generate New Emotes","","","0"
"1177","727551","793761","06/20/2020 01:55:28","## Task Details
Create a model to assess the likelihood of a death by heart failure event.
This can be used to help hospitals in assessing the severity of patients with cardiovascular diseases.","","Predict Heart Failure","","","300"
"3070","727551","1832976","12/29/2020 18:25:25","## Task Details
Create Hyper Parameter Tuning Using any 2 Supervised Algorithm with Roc Curve and all Evaluation Metrrics","","Logistic Regression and Random Forest with HYper Parameter Tuning","","12/31/2020 23:59:00","14"
"3523","727551","6668882","02/17/2021 14:56:05","analyze the association of what causes a heart diseaheart disease? While are different associated factors with heart disease; we can simply name some of the attributes given; for instance, High cholesterol, weight, smoking, and so on. 

Based on this dataset, what are the probabilities that high cholesterol could be a major cause? 

Lastly, Analyze the data and find the strongest attribute that causes heart disease.","","Heart Failure prediction","","02/18/2021 23:59:00","20"
"4904","727551","1393578","06/24/2021 09:25:17","This dataset contains [censored data](https://en.wikipedia.org/wiki/Censoring_(statistics)) and should be analyzed accordingly. Each observation in the dataset contains information about whether the heart failure patients had died before the follow-up period indicated by the ""time"" column in the data set. Thus, ""DEATH_EVENT""=0 does *not* mean that the patient did not die, only that the patient had not died by the time of the follow-up.

Survival analysis was developed to estimate the life spans of individuals based on data where the outcome was censored by a follow-up duration. It is a method to answer questions such as ""How likely is a person to die within a certain time period?""

This dataset was first presented by two Pakistani researchers, where the results were analyzed using an appropriate Cox proportional hazards model. See the original research here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181001

It has subsequently been analyzed in machine learning context, ignoring the censoring problem. The aim of this task is to get submissions where the survival part is dealt with.

There exists a range of methods to estimate survival models. By far, the most used one in health science is the [Cox proportional hazards model](https://en.wikipedia.org/wiki/Proportional_hazards_model). A notable python package for the purpose is [lifelines](https://lifelines.readthedocs.io/).

# Completing the task
To complete the task, submit a notebook that uses survival models to predict heart failure death. Good submission will include:
- plots of the survival function
- quality metrics such as log-likelihood, concordance index, partial AIC depending on the model used.","","Predict heart failure using survival modeling rather than classification","","","13"
"891","646598","793761","05/13/2020 17:24:35","Its a bunch of open ended questions:
- What are the best free courses by subject?
- What are the most popular courses?
- What are the most engaging courses?
- How are courses related?
- Which courses offer the best cost benefit?","","Explore Free and Paid Courses","","","21"
"890","650614","793761","05/13/2020 17:20:56","# Summary
We have two different classes and their bounding boxes.
You can create a model for object detection to find the localization of the pets within the image!

# Architecture Suggestions
- EfficientDet
- RetinaNet
- MaskRCNN
- FasterRCNN","","Find the good boy","Create a detection model for dogs and cats","","0"
"1208","614679","793761","06/26/2020 03:26:34","## Task Details
Same as digit recognition, but with medical imaging classes instead!","","Image Classification","","","0"
"947","667852","793761","05/22/2020 07:07:49","Create a model to classify images at each category.","","Image Classification","Classify if image belongs to Dog, Cat or Wildlife","","1"
"948","667852","793761","05/22/2020 07:09:40","This dataset was originally proposed for a GAN.
Replicate away!
https://github.com/clovaai/stargan-v2","","Create a General Adversarial Neural Network","","","1"
"949","667889","793761","05/22/2020 07:30:50","## Task Details
Detect mask, no mask and incorrectly worn masks

## Suggested Metrics
- Jaccard Index, also known as IoU
- mAP (@IoU 0.3, 0.5, 0.7)
- mAR (@IoU 0.3, 0.5, 0.7)","","Mask Detection","","","42"
"896","652790","793761","05/14/2020 04:12:37","## Summary
Many companies received loans and grants as a financial aid to remain in business amidst the COVID-19 pandemic.

Even companies with a history of environmental, employment, tax issues received funding.
We can explore how the financial reliefs were distributed among industries as well as companies with different issue histories.","","Explore Grant Distribution","","","0"
"957","671145","793761","05/24/2020 01:19:23","## Task Details
Uncover the main jobs at risk of automation, bearing in mind how many people are at risk","","Explore Jobs at Risk of Automation","","","0"
"958","671172","793761","05/24/2020 02:04:11","## Task Details
Apply object detection models to localize road signs within an image

## Suggested Metrics
- Jaccard Index, aka, IoU (intersection over union)","","Detection of Road Signs","","","1"
"967","674071","793761","05/25/2020 13:22:23","## Task Details
Classify patient as healthy or COVID-19 infected","","COVID-19 Classification","","","0"
"968","674071","793761","05/25/2020 13:22:53","## Task Details
Segment COVID-19 radiological findings","","COVID-19 Segmentation","","","1"
"1205","676086","793761","06/25/2020 19:30:05","## Task Details
Explore the performance of each video used in the official youtube channel of cyberpunk 2077.","","Video Statistics Exploration","","","0"
"1206","676086","793761","06/25/2020 19:30:55","## Task Details
Understand the sentiment as well as the topics relevant to uncover the likes, dislikes and expectations about the game!","","Topic Modeling, Sentiment Analysis on Comments","","","1"
"809","561256","793761","04/23/2020 13:22:54","## Purpose
Segmentation of radiological findings can shed light into how severe the situation of the patient is, allowing for appropriate care.

## Sugested Metrics
- Dice Coefficient
- IoU (aka Jaccard Index)","","COVID-19 Infection Segmentation","","","7"
"810","561256","793761","04/23/2020 13:24:46","## Purpose:
Lung segmentation is often the first step to increase the performance of any other supervised model, as it takes away the noise created by regions of no interest for COVID-19.

## Sugested Metric:
- Dice Coefficient
- IoU (aka Jaccard Index)","","Lung Segmentation","","","8"
"811","561256","793761","04/23/2020 13:26:08","## Purpose:
Find evidence that the current image **could** present evidence of COVID-19 infection.
Note that the radiological findings of COVID are not exclusive to this disease.

## Sugested Metrics:
- AUROC
- F1-Score","","COVID-19 Classification","","","38"
"6256","596420","2456628","10/04/2021 20:01:55","Analisar e experimentar algumas abordagens de segmenta√ß√£o de tumor
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Explore some segmentation approach","try some codes to segment brain tumor","12/31/2021 23:59:00","0"
"1052","559783","793761","06/04/2020 20:21:15","## Task Details
Classify if the xray contains evidence of COVID-19 or not","","COVID-19 Image Classification","","","1"
"534","544037","794313","03/12/2020 06:09:50","Can a model be trained over this dataset to answer about the top events from 2018 in India ?","","Question Answering","","","0"
"5652","729475","7463110","08/10/2021 07:07:42","Identify which food is being eaten by listening to the sounds","","Classify Eating Sounds","","","0"
"1895","851218","820667","08/30/2020 15:58:25","Here's an obvious one - who is the best at making predictions?","","Find the best predictor","","","1"
"2714","841692","1102232","11/17/2020 06:39:00","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","train-task","20201117-1","","1"
"1165","723240","833623","06/18/2020 00:08:53","## Task Details

In my career I had a project to build a garage detection system, but unfortunately (as with many projects) the project never went forward. I was excited about the project, so I decided to try and replicate the project using images on Google. I thought this would also be a great opportunity to work on semi-supervised learning techniques and collaborate with the data science community on the topic!

**In this task, only a small faction of the images are labeled for training and holdout, and there are many more unlabeled images, hence the need for semi-supervised learning!  Use semi-supervised learning or supervised learning to train a classifier that performs best on holdout!**

Kaggle has a lot of competitions, but not many that really REQUIRE semi-supervised learning. This dataset and ""unofficial challenge"" is in spirit of semi-supervised learning education and experimentation.

## Expected Submission
Please use the following file to score your final model:

`image_labels_holdout.csv`

Performance will be based on AUC. Please use the following code to compute your holdout AUC score.

```
fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)
metrics.auc(fpr, tpr)
# 0.75
```
In order to have an unofficial learderboard, I ask that you use the following format to title your notebooks and scripts.

`[GDUC - 0.0000] Whatever You Want`

For example ""[GDUC - 0.9417] Self-training Baseline"". This will allow people to sort by notebook name and see the highest scoring notebooks for this ""unofficial challenge""!

## Evaluation

Performance will be based on AUC. Please use the following code to compute your holdout AUC score.

```
fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)
metrics.auc(fpr, tpr)
# 0.75
```

### Further help

Here are come excellent resources on semi-supervised learning:

http://www.cs.cmu.edu/~10701/slides/17_SSL.pdf
https://ruder.io/semi-supervised/
https://www.molgen.mpg.de/3659531/MITPress--SemiSupervised-Learning.pdf
http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf","","Garage Detection Unofficial SSL Challenge","Garage Detection Unofficial Semi-Supervised Learning Challenge","","0"
"1156","680446","844176","06/17/2020 07:58:44","99.9%
I'm serious.","","Maximum Accuracy","Minimum of Computations","","0"
"1350","771253","844176","07/13/2020 18:13:19","The database is intended to simplify training exercises at the initial level in the application of classification algorithms.
In my personal opinion, recognition of graphic objects of this type is impossible with the accuracy of 99-100 percent due to their schematic character (in general, the human mind will also experience difficulties in many cases, what volunteer artists wanted to portray in seconds: a banana or a boomerang :) ) Achieving 95-96 percent will undoubtedly be an excellent result for 50 classes (there are 340 in the original dataset).
Stable success is possible because there are a lot of graphic objects connected with the same word objects in human minds in different countries, nations, religions.","","95-96% accuracy","Sketch Recognition","","0"
"1326","759809","844176","07/11/2020 13:52:31","## Task Details
1) A general understanding of the ability for algorithms to classify horse breeds using only images: is it enough information or it needs to add something else (weights, heights, etc.).
2) Overcoming by algorithms the level of human abilities in this sphere (with measurement of the prediction accuracy).

## Expected Submission
1) Applying different algorithms and comparing results.
2) 85-90% accuracy of test predictions.

## Evaluation
Stable results for many experiments mean the success in reaching our goals

## Further help
Connecting with classification results for other biospecies with the same algoritms.","","Recognition of biospecies","Horse Breeds' Classification","","0"
"1609","785141","844176","08/06/2020 06:18:40","## Task Details
Create synthetic datasets with real and symbolic images and classify them by objects.
Human minds have no problem with this task. What about algorithms?

## Expected Submission
Successful classification with data of 1000-2000 images at the level 75-85% of accuracy.

## Evaluation
What makes a good solution? I think it's a well-balanced synthetic data of images and stable results of classification by neural networks.","","Mixing symbolic & real images","Data classification with different types of pixel images","","0"
"1552","785141","844176","08/01/2020 16:41:34","## Task Details
There is a huge field for experiments here.
1) Generate images based on the whole dataset.
2) Draw sketches, pictograms, and contours of objects separately with different models of neural networks.
3) Experiment with labeled and unlabeled bases for image generation.

## Expected Submission
Visually successful results.

## Evaluation
Pleasant look of objects from the point of human minds.","","Pictogram Generation","Applying Neural Networks","","0"
"2114","871622","844176","09/15/2020 20:40:30","1) Find the optimal structure for classification and/or reproducing of decorative artificial elements.
2) Try to build ""cyclic learning"" of algorithms: generate sets of patterns with ordinary mathematical functions, train neural networks on them, compare with images classified and/or generated by algorithms... And so on, until an aesthetically acceptable result will not have been achieved.","","Training neural networks on artificial images","Cyclic Learning","","0"
"2209","885386","851469","09/23/2020 11:13:47","There are more than 8 years stock price data is available in this data. Hope it will help to predict.

Dear Data Scientist keep predicting....","","Forecasting Dhaka Stock Exchange Price","Forecast stock price for next months","","1"
"941","621198","853926","05/20/2020 18:51:41","## Task Details
The first 125 columns are real parts of the radio frequency signals, the second 125 columns are imaginary parts of the radio frequency signals, the last column is the lable (1-10).

## Expected Submission
Split the dataset for training and testing. We expect higher than 80% accuracy on testing dataset

## Evaluation


### Further help","","Classification of 10 radio frequency signal medium","complexed valued classification problem","","0"
"814","561412","4770430","04/23/2020 23:31:36","Using Prophet forecasting death toll","","Canada Covid19 Death Prediction","Using Prophet forecasting death toll","","0"
"798","561412","3793733","04/22/2020 06:51:23","## Task Details
There are lots of Deep Learning architectures that can be used to predict time series data. Let's find the best one!

## Expected Submission
Users should submit their model and the results in a comparable manner (e.g. relative metrics, like RMSE divided by mean of test data).

## Evaluation
A good solution is a model with a reasonably low RMSE in both absolute and relative terms.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Comparison of Deep Learning architectures to predict confirmed cases","","","0"
"799","561412","4806321","04/22/2020 18:14:33","Use a notebook to load the data, use other data if needed.

Analyze what is the average temperature that promotes the growth of virus. How the increase of confirmed cases triggers in the countries where the average temperature is conducible to the growth of the virus. 

Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","How temperature of the countries is related to rate of transmission of Coronavirus?","","04/23/2020 00:00:00","0"
"803","561412","836830","04/22/2020 21:52:50","Predict Daily Indian Confirmed Cases.","","Predict Daily Indian Confirmed Cases","","","0"
"804","561412","4715365","04/23/2020 02:07:33","1. Visualizations of the confirmed cases for the U.S, China and India and the stock indexes.
2. Basic prediction of the cases. 
3. Visualization of stock indexes during the period of outbreak and predictions for US. index.


Note: Dataset for stocks is taken from yahoo finance.","","Economic meltdown visualizations and analysis","Visualizations of the confirmed cases for U.S, China and India. Basic prediction of the cases. Visualization of stock indexes and predictions for US.","","1"
"805","561412","4717056","04/23/2020 02:55:47","## Task Details
Visualization of confirmed cases in the world and in US
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Confirmed_cases_visualization_US","","","0"
"806","561412","4737255","04/23/2020 03:33:31","Prediction for next few days for Ontario, Canada","","Prediction for next few days","","","0"
"807","561412","3019716","04/23/2020 03:41:41","To explore the spread of coronavirus globally

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","COVID-19 Visualisation and Prediction","","","0"
"648","561412","857460","03/28/2020 13:44:31","## Task Details
The most fundamental question in a pandemic: how many more cases will we see tomorrow?

## Expected Submission
Use a notebook to load the data, use other data if needed. Make a prediction about the confirmed cases tomorrow, and the next days, for the next two weeks.
Show results of testing your model on a running basis, for example, train on Jan-Feb data, then do daily,weekly predictions of the future for March where we know the answer. Then show a plot of the predictions going forward using today's data.

Optionally you can also break this down per country.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Predict Daily Global Confirmed Cases","","","4"
"649","561412","857460","03/28/2020 13:45:55","## Task Details
The saddest question in a pandemic: how many more deaths will we see tomorrow?

## Expected Submission
Use a notebook to load the data, use other data if needed. Make a prediction about the deaths from COVID-19 cases tomorrow, and the next days, for the next two weeks.
Show results of testing your model on a running basis, for example, train on Jan-Feb data, then do daily, weekly predictions of the future for March where we know the answer. Then show a plot of the predictions going forward using today's data.

Optionally you can also break this down per country.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Predict Daily Global Deaths","","","3"
"746","561412","3539533","04/14/2020 05:10:09","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Predict Daily the United State Confirmed Cases","","04/22/2020 00:00:00","2"
"747","561412","3539533","04/14/2020 05:10:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Predict Daily the United State Deaths","","04/22/2020 00:00:00","2"
"793","561412","4805739","04/21/2020 22:10:15","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict Daily Canada Confirmed Cases","","","0"
"796","561412","4729379","04/22/2020 04:02:10","## Task Details
For people who live in the UK, knowing the COVID-19 situation of the UK is important.

## Expected Submission
It should be a notebook and contains the prediction method for past and future UK COVID-19 cases. The cases could be any type: confirmed, death, etc.

## Evaluation
The more accurate the prediction fits the reality COVID-19 confirmed cases, the better the prediction.","","Predict Daily UK Cases","","","0"
"777","561412","4812981","04/17/2020 02:40:22","## Task Details
Predict the daily deaths in Canada

## Expected Submission
15 days future predictions




### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict Daily Deaths in Canada","","","3"
"783","561412","857460","04/18/2020 17:18:18","## Task Details
The growth in confirmed cases and deaths in any country or region is about more than the number of infected, density and timing. It is also about what policies are in place such as school closures, social distancing, etc. The [howsmyflattening.ca](https://howsmyflattening.ca) site has created their own kaggle [HOW'S MY FLATTENING COVID19 DATASET](https://www.kaggle.com/howsmyflattening/covid19-challenges) which has a few files that can help address this task
- governmentresponse.csv
- npi_canada.csv
- npi_usa.csv

## Expected Submission
Use these csvs along with any other needed data to come up with improved predictions using a model of spread interventions in each country or region.","","Predict Growth in Cases Using Government Policies and Interventions","","04/30/2020 00:00:00","2"
"784","561412","4728740","04/18/2020 23:32:45","## Task Details
Recently, the confirmed cases in the Region of Waterloo is increasing rapidly. I just want to use what I learned to predict the number of confirmed cases in the feature","","Prediction of cumulative cases in Region of Waterloo","","04/22/2020 00:00:00","2"
"761","561412","4716376","04/16/2020 00:08:52","## Task Details
Predicting the number of cases sounds like a regression task rather than a classification task as we learned in class 

## Expected Submission
Users should submit a kaggle notebook with models trained on the first half of the dataset (initial period), and have them predict the number of expected cases. 

## Evaluation
A good comparison shows different regression algorithms and compares them visually/numerically.","","Comparison of Different Regression Algorithms","SVM vs Polynomial vs Bayesian","04/22/2020 00:00:00","2"
"1567","620916","863388","08/03/2020 03:20:32","## Task Details
This task was created for data scientists and data experts across the globe. This data provides a sample of the resume data provided in the dataset.

## Expected Submission
A full-blown analysis of the dataset has to be submitted to us. It has to blow our minds and use the dataset to its full potential. 

## Evaluation
The task submitted will be evaluated by our data engineers and we will get back to you with different prizes and goodies that you can avail on https://app.datastock.shop/","","Detailed Analysis Of Indeed Resume Data","An Analysis of the data provided","08/31/2020 00:00:00","0"
"909","655010","863388","05/15/2020 08:00:43","## Task Details
This task was designed to make people come up with different analyses for this dataset and we will feature the best analysis on our social media pages.

## Expected Submission
A detailed analysis of this dataset must be submitted. The task can be completed in a notebook and we will be happy to upload the same here.  The solution should contain a detailed step by step analysis of the product data that is available in this dataset.

## Evaluation
The evaluation will depend on the best submission. We will run it with our tech team once and see if it is good or not.

### Further help
If you need additional inspiration,  check out [Datastock](https://datastock.shop/) for further queries reach out to us at marketing@promptcloud.com","","Analysis of Products","A Full detailed analysis of the product data that is available in this dataset","06/10/2020 00:00:00","0"
"925","660744","863388","05/18/2020 12:35:08","## Task Details
This task is being created so that users can come up with quantifiable analysis for this dataset.

## Expected Submission
The users must submit a detailed and explanatory analysis of the dataset

## Evaluation
The evaluation process is simple, we will send the submissions to our tech team and they will evaluate the following.","","Job Listing Naukri","All Job Listings on Naukri","05/31/2020 00:00:00","0"
"963","673277","863388","05/25/2020 06:43:05","## Task Details
This task was created keeping solely in mind the analysts who will love to analyze such data for various purposes.

## Expected Submission
Users must submit a digital analysis for this dataset.

## Evaluation
The evaluation will be based on the best dataset that is reviewed by us from our tech department.","","Jobs On Monster","All Job Listings On Monster","05/31/2020 00:00:00","0"
"974","675696","863388","05/26/2020 08:18:16","## Task Details
This task has been created with the hope that users can analyze the product data that is available to them.

## Expected Submission
A thorough analysis of the dataset is required for us to perform an evaluation.

## Evaluation
The dataset will be given to our tech team for analysis and we will get back with the results once they have been obtained.","","Product Listing on Amazon","All product Listing on Amazon","05/31/2020 00:00:00","0"
"1091","700411","863388","06/08/2020 06:24:00","## Task Details
This dataset task was created keeping in mind the budding coders and tech geeks to analyze and interpret this sample dataset.

## Expected Submission
The users must submit a detailed analysis of this dataset.

## Evaluation
The results will be evaluated by our tech team.","","Amazon Details Analysis","A detailed analysis on the amazon dataset","06/30/2020 00:00:00","0"
"1092","700459","863388","06/08/2020 06:47:00","## Task Details
This task is created for the analysis and educational part of all job data on simplyhired

## Expected Submission
Users must submit  detailed analysis of the job data.

## Evaluation
Our tech team will evaluate the same after the submissions are done.","","Job Analysis of SimplyHired","A detailed job analysis of SimplyHired.com","06/30/2020 00:00:00","0"
"1144","716530","863388","06/15/2020 10:48:24","## Task Details
This task was created to make a detailed analysis of all the data that is provided here.","","Amazon Job Analysis","A detailed analysis of job data from amazon","06/30/2020 00:00:00","0"
"1145","717159","863388","06/15/2020 10:54:57","## Task Details
This task was created for the main purpose of analysis and detailed explanation of the dataset. 

## Expected Submission
Users should submit a detailed study of the task at hand and analyze the same for various purposes. 

## Evaluation
A good solution is something that can blow the minds of our tech team and the engineers who came up with these datasets.","","Analysis of Naukri Job Data","A detailed analysis of Naukri Data","06/30/2020 00:00:00","13"
"1112","707666","863388","06/11/2020 09:18:24","## Task Details
This dataset was created for the main purpose of analysis. 

## Expected Submission
A detailed analysis of this dataset
## Evaluation
The analysis submitted by the people, our tech team will evaluate the same and get back with the results","","Job Analysis of SimplyHired Job Data","A Detailed analysis of Job Data","06/30/2020 00:00:00","0"
"1361","775354","863388","07/14/2020 12:40:48","## Task Details
This task was created keeping in mind the analysts and researchers who want quality data daily for various purposes.

## Expected Submission
The users must submit a full review and analysis of the dataset and submit it to us. We have something special for you if you blow our minds.

## Evaluation
A good solution is something that blows our minds.","","Analysis of Flipkart Products","An analysis of all products that are available in Flipkart","07/31/2020 00:00:00","0"
"1363","775484","863388","07/14/2020 13:12:31","## Task Details
This task was created keeping in mind the analysts and researchers who want quality data daily for various purposes.

## Expected Submission
The users must submit a full review and analysis of the dataset and submit it to us. We have something special for you if you blow our minds.

## Evaluation
A good solution is something that blows our minds.","","Monster Job Listings","An Analysis of the Monster Dataset","07/31/2020 00:00:00","0"
"1373","777138","863388","07/15/2020 10:39:46","## Task Details
This task is being created for analysts and researchers across the globe. We love data and we love the exciting things that we can do with data

## Expected Submission
Users submit a full analysis of the dataset that is available to them. You can use this dataset to come up with an analysis of the same service.


## Evaluation
A good solution is something that blows our engineer's minds. It is a solution that will help us understand your perception on the data that is provided.","","Analysis of Amazon India Products","A full analysis of Amazon India Products India","07/31/2020 00:00:00","1"
"1359","774768","863388","07/14/2020 11:50:04","## Task Details
This task is being created keeping in mind the analysts and data researchers across the globe who want raw and quality data.

## Expected Submission
Submit a proper analysis that can be used, various users. We want all our users to use this dataset to their advantage. The analysis must be proper and worthy.

## Evaluation
A solution is something that blows our engineer's minds. We are always looking for something funky and fun to do with data.","","Job Analysis","This dataset contains job listings","07/31/2020 00:00:00","0"
"1464","790247","863388","07/23/2020 10:59:25","## Task Details
This task was created with the main purpose of getting an analysis done for the dataset

## Expected Submission
Users must submit a large analysis based on this dataset and what they used this data for. 

## Evaluation
A solution is something that blows our engineers minds. A solution must be effective and worthy. We will take a look at the same and get back with further details.","","Analysis of MakeMyTrip","This analysis is for the analysis of this dataset","07/31/2020 00:00:00","0"
"1192","733837","863388","06/23/2020 07:14:19","## Task Details
This task has been created to make the detailed analysis of the autoscout data that has been crawled from the website. 

## Expected Submission
Users have to submit a detailed analysis for the same data that has been provided by us. Users have to find the unique data from the sample data file and submit it to us. 

## Evaluation
A great solution is something that blows our minds away. Our tech team will be given the results and they will analyze and get back to us with the results. Once that is done we will let our users know.","","Autoscout Analysis","A detailed Analysis of Autoscout Data","06/30/2020 00:00:00","0"
"1720","834453","863388","08/17/2020 06:35:11","## Task Details
This task is being created by our teams at PromptCloud and DataStock. This task is created for various analytic purposes.   

## Expected Submission
Users must submit an analysis of the dataset. It has to be thoroughly checked and analyzed analytically. 

## Evaluation
A user evaluation of the dataset that is available to you. It has to be an accurate evaluation of the same.","","A CareerBuilder Job Dataset UK","An analysis of Job Dataset from CareerBuilder.","08/31/2020 00:00:00","0"
"1721","834520","863388","08/17/2020 07:15:02","## Task Details
This task was created by our teams keeping in mind the researchers and scientists for analyzing the data.

## Expected Submission
Users should submit a full-blown analysis of the dataset of the job listings that is available. 

## Evaluation
A good solution is something that will blow our minds. It is for various different purposes.","","An Analysis of Seek Australia Job Dataset","A full analysis of the dataset from Seek Australia","08/31/2020 00:00:00","0"
"1655","824284","863388","08/10/2020 11:13:23","## Task Details
This task is being created by us for all the data scientists and researchers across the world. 

## Expected Submission
users must submit a detailed analysis of this dataset. It can be anything that you infer from this sample dataset. 

## Evaluation
The analysis submitted will be analyzed by our engineers and the winner will get discounts on the datasets of your choice.","","Analysis of Job Data From Monster","A full analysis of job data from this dataset","08/31/2020 00:00:00","0"
"1659","824407","863388","08/10/2020 13:04:39","## Task Details
This task was created by our in house teams at PromptCloud and DataStock.

## Expected Submission
Users must submit a full-blown analysis of this job listing dataset. A detailed inference from this dataset.

## Evaluation
A good solution is something that will make our engineers think about their profession. Submit and avail discounts on all datasets of your choice.","","SimplyHired Job Listing Dataset 2019","An analysis of job listing from this dataset","08/31/2020 00:00:00","0"
"1619","820137","863388","08/07/2020 07:14:28","## Task Details
This task was created keeping in mind the data scientist who use this dataset for various purposes. 

## Expected Submission
The users must submit a full analysis of this dataset and they have to submit it to us within the time limit.

## Evaluation
A good solution is something that blows our minds out.","","Job Posting Dataset From Indeed","A Full analysis of Job Posting","08/31/2020 00:00:00","0"
"1570","813111","863388","08/03/2020 04:20:45","## Task Details
This task was created keeping in mind all the data scientists and researchers across the globe.

## Expected Submission
Users should submit a full-blown analysis from using this dataset. 

## Evaluation
The analysis will be evaluated by our data engineers and give us the results after which we will get in touch with you and get back to you with offers on https://app.datastock.shop/","","An Analysis of the Real Estate Data From Trulia","A full blown analysis of the dataset","08/31/2020 00:00:00","0"
"1611","818970","863388","08/06/2020 12:56:09","## Task Details
This task is being created keeping in mind the data scientists and data researchers across the globe 

## Expected Submission
Users must submit a full-blown analysis on the product listing dataset that will be useful for us.

## Evaluation
Something that blows our engineer's minds away.","","Product Listing Amazon Australia","An Analysis of Product Listing","08/31/2020 00:00:00","0"
"2123","876479","863388","09/16/2020 13:19:56","## Task Details
This task is being created for all the data scientists and data enthusiasts across the globe. 

## Expected Submission
The users must submit a full-blown data analysis of this dataset. 

## Evaluation
A solution that makes us go wild is the solution that we are looking for.","","Product Reviews Dataset","An Analysis of The Dataset","09/30/2020 23:59:00","0"
"2237","892727","863388","09/25/2020 12:39:10","## Task Details
This task was created for all the analysts and researchers across the globe. 

## Expected Submission
Users must submit a detailed analysis of the dataset that they infer from. 

## Evaluation
What makes a good solution? A solution that blows our minds.","","Walmart Analysis of Product Data","An analysis of Walmart product data","09/30/2020 23:59:00","0"
"2232","892567","863388","09/25/2020 10:10:16","## Task Details
This task was created for all the analysts across the world who want job data for analysis and predictions

## Expected Submission
Users should submit a full-blown analysis of the data that they have discovered and present it in an understandable format. 

## Evaluation
A good solution is something that will blow our engineer's minds. It is something that is very delicate and analysis will help all of us.","","Naukri Analysis","A detailed analysis of the data that is present in this sample","09/30/2020 23:59:00","0"
"2234","892593","863388","09/25/2020 11:25:00","## Task Details
This task was created for various reasons. We require a full analysis of the dataset that we have here. 

## Expected Submission
Users must submit a great analysis of the sample dataset that is available to them here. 

## Evaluation
A great analysis can down in history. We are looking for the best analysis of this dataset. There's a 20% discount on the dataset if you guys submit the best one. Hurry up.","","Monster USA Analysis","Monster USA Dataset","09/30/2020 23:59:00","0"
"2238","893143","863388","09/25/2020 18:52:00","## Task Details
This task was created by our in house teams at PromptCloud and DataStock. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that they have here. 
 
## Evaluation
A user submission that will be the best analysis of this dataset.","","Amazon.com Analysis","A full analysis of the dataset","09/30/2020 23:59:00","0"
"2290","900659","863388","09/30/2020 17:42:53","## Task Details
This task was created keeping in mind the data warriors and data scientists across the world. 

## Expected Submission
A detailed analysis of the amazon produces details that are present in this dataset. 

## Evaluation
What makes a good solution? A full and detailed analysis of the sample dataset that we have here.","","An Analysis of Amazon USA","A full blown analysis of amazon USA","10/31/2020 23:59:00","0"
"2168","881873","874280","09/19/2020 14:58:32","## Task Details

Your work is to build a predictive model which can identify a pattern in these variables and suggest that a hack is going to happen so that the cybersecurity can somehow stop it before it actually happens. 

## Expected Submission
You have to predict the column: ""MALICIOUS OFFENSE. The submission should follow the format of sample_submission.csv

## Evaluation
score = recall_score(actual_values, predicted_values)

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict if the Server will be hacked","","","7"
"612","551536","886142","03/21/2020 04:27:50","## Task Details
Do more tweets about coronavirus indicate more infections? 

Do more tweets about coronavirus indicate greater public awareness and seriousness and lead to a greater public acceptance of measures to limit spread, such as social distancing, and therefore fewer infections? 

Are people tweeting about particular symptoms? 

## Expected Submission
Ideally, a solution will be in the form of notebooks, to explore the data and explain conclusions.","","Coronavirus Tweets vs Infections","Are tweets indicative of infection rates?","","1"
"1064","582631","263098","06/06/2020 00:18:26","## Task Details
Can we predict the impact of various social interventions (including social distancing requirements, masks, school and business closures, opening of testing clinics, contact tracing, availability of serological testing, etc.) and public attitudes/sentiment on the spread of COVID-19 through populations?

As an extension, can we estimate the impact if various measures were lifted at different time intervals? What is the expected impact on effective social distance?  What is the impact on disease spread (Ro) and illness?

We are looking for insights into reliable predictors of the rate of spread of disease through a population, which do not need to be constrained to the specific factors outlined above.


## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 1: Impact of interventions on curbing the impact of Covid-19","","06/27/2020 00:00:00","16"
"1065","582631","263098","06/06/2020 00:21:46","## Task Details
Can we predict the impact on health infrastructure and resources of the Covid-19 spread?  Alternatively, can we predict the severity of illness from COVID-19 in a population based on availability of / access to healthcare resources?

For example, how does population demographic/risk correlate with infrastructure capacity?  How has health resource limitations affected adoption of digital health technologies?  How do these vary across countries/sub-populations within a country?


## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 2: Health infrastructure and resource availability","","06/27/2020 00:00:00","8"
"1066","582631","263098","06/06/2020 00:22:40","## Task Details
What are the risk factors associated with the severity of illness from Covid-19 infection?  For example, profile of patients and lifestyle, social, co-morbidities, genetics, viral strain, etc. influencers ‚Äì how do these affect the probability of requiring hospitalization, ICU, ventilator, mortality?

How does risk vary with healthcare professionals?  What is the impact of availability & implementation of PPE on their risk?

## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 3: Predicting illness severity in a particular patient or demographic","","06/27/2020 00:00:00","33"
"1067","582631","263098","06/06/2020 00:23:28","## Task Details
How has Covid-19 affected non-COVID-related healthcare availability (e.g. for cancer, cardiovascular disease, dialysis, etc. patients)?

Related questions we would also be interesting in hearing about include:

How can we restore/maintain effective non-Covid-related healthcare services?

Has the re-introduction of other services led to an increased risk of infection to these patients?

Are we seeing a shift in the way care is being delivered for these patients (e.g. fewer hospital /more clinic, home, or remote monitoring solutions, etc.)?


## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 4: Unintended consequences from health resource re-allocation toward COVID-19","","06/27/2020 00:00:00","3"
"1068","582631","263098","06/06/2020 00:24:06","## Task Details
Can we predict changes in demand for mental health services and how can we ensure access? (by region, social/economic/demographic factors, etc).  Are there signs of shifts in mental health challenges across demographies, whether improvements or declines, as a result of COVID-19 and the various measures implement to contain the pandemic?

## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 5: Mental health impact and support services","","06/27/2020 00:00:00","18"
"1069","582631","263098","06/06/2020 00:24:54","## Task Details
Can we take any of the other task questions and localize projections to sub-regions within a country (e.g. province, state, county, city, etc)?  Where is there sufficient data available to make this possible?

How do we take some of the provided models/solutions and convert them into visually appealing and meaningful insights for communication to a broad audience?  How would you communicate these insights to different audiences, such as Governments, Health leaders, Businesses, Community leaders, or the broader population?


## Evaluation
1. Scientific rigor
a. Is the solution evidence based? I.e. does it leverage robust data?
2. Scientific model/strategy
a. Did the solution employ a robust scientific method?
3. Unique and novel insight
a. Does the solution identify information (new data, features, insights etc) that is yet to be ‚Äúuncovered?‚Äù
4. Market Translation and Applicability
a. Does the solution resolve an existing market need for either an individual, health institution or policy maker?
5. Speed to market
a. Does it apply to an existing product vision such as a self assessment tool or policy decision-making tool?
6. Longevity of solution in market
a. Is the solution one that could be used in various markets through time?
7. Ability of user to collaborate and contribute to other solutions within the Kaggle community
a. Did the user provide expertise and or resources in the form of datasets or models to their fellow Kaggle members?

We also ask that **every notebook submission start with a short abstract outlining how you have addressed each of the above criteria in a clear and concise manner.** This is to ensure our multi-stakeholder (patients, healthcare practitioners, industry, academics) coalition leaders are able to quickly advance and provide their expertise to further refine your solutions and build with you.","","Task 6: Other EDA, Visualization and Insights","","06/27/2020 00:00:00","8"
"670","582631","4774428","04/01/2020 23:45:16","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations are at risk of contracting COVID-19?","","","190"
"671","582631","4774428","04/01/2020 23:46:41","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations have contracted COVID-19 and require ventilators?","And when does this occur in their disease course?","","18"
"672","582631","4774428","04/01/2020 23:48:26","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations have contracted COVID-19 who require the ICU?","And when does this occur in their disease course?","","17"
"673","582631","4774428","04/01/2020 23:49:12","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which patient populations pass away from COVID-19?","","","32"
"674","582631","4774428","04/01/2020 23:51:14","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations assessed should stay home and which should see an HCP?","","","17"
"675","582631","4774428","04/01/2020 23:51:47","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations of clinicians and patients require protective equipment?","","","13"
"676","582631","4774428","04/01/2020 23:52:18","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Which populations of clinicians are most likely to contract COVID-19?","","","15"
"677","582631","4774428","04/01/2020 23:53:46","Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The tasks (**task tab link**) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 

Evaluation 
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
Wednesday, April 22nd
Wednesday, May 6th
Wednesday, May 20th","","[CLOSED] How are patterns of care changing for current patients (i.e. cancer patients)?","Specifically, how are screening, diagnosis, treatment, and follow-up changing for current patients?","","15"
"678","582631","4774428","04/01/2020 23:58:42","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] What is the change in turnaround time for routine lab values for oncology patients?","Specifically, what is the change in turnaround time for routine lab values, including relevant biomarkers, throughout the care continuum?","","17"
"679","582631","4774428","04/01/2020 23:59:50","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] What is the incidence of infection with coronavirus among cancer patients?","And, is disease severity or trajectory different among these patients vs those not diagnosed with cancer?","","36"
"680","582631","4774428","04/02/2020 00:00:48","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] Are hospital resources being diverted from providing oncology care?","Which ones and by how much?","","18"
"681","582631","4774428","04/02/2020 00:02:37","## Task Details
The Roche Data Science Coalition is a group of like-minded public and private organizations with a common mission and vision to bring actionable intelligence to patients, frontline healthcare providers, institutions, supply chains, and government. The [tasks](https://www.kaggle.com/roche-data-science-coalition/uncover/tasks) associated with this dataset were developed and evaluated by global frontline healthcare providers, hospitals, suppliers, and policy makers. They represent key research questions where insights developed by the Kaggle community can be most impactful in the areas of at-risk population evaluation and capacity management. 


## Evaluation
One submission will be identified as the best response to the research question posed in this task on the timeline outlined below. That submission will be marked as the ‚Äúaccepted solution‚Äù to that task, and will be reevaluated by the next deadline against the new research contributed to that task. 

Submissions will be reviewed on a rolling basis, so participants are encouraged to work publicly and collaboratively to accelerate the research available for each task. 

## Timeline
The Roche Data Science Coalition will be evaluating solutions and surfacing this research to experts on the following schedule: 
- Wednesday, April 22nd
- Wednesday, May 6th
- Wednesday, May 20th","","[CLOSED] How is the implementation of existing strategies affecting the rates of COVID-19 infection?","Strategies including social distancing, use of PPE, etc.","","30"
"879","641389","929585","05/13/2020 11:51:17","Can you find the botanical names of plants in the text ?
Can you find the 'root - leaf - flower' words
Can you find a languagecorpus that best fits the manuscript","","Find significant words, language corps","can you find botanical names","","0"
"2200","885427","942387","09/22/2020 19:29:56","## Task Details
Create a model to predict the price of a vehicle

## Expected Submission
Solution should contain a h5 file, pickled scalers and an csv of prediction on the test set.

## Evaluation
I believe RMSE would be the best metric for evaluation. Please plot a residuals graph along with the RMSE score.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the price of a vehicle","Predict the price of a vehicle using only the brand name, model name, make_year, mileage, state, owner count and trim name","","2"
"5715","817870","1901368","08/13/2021 14:09:05","test test test test test test test 

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","test test test test test test test test test test","test test test test test test test test test","","0"
"739","585591","970405","04/11/2020 12:18:27","## Task Details
We need graphs to show the growth of the data according to the dates with the data.","","The graphs to show the growth of number of cases reported and the numbe people who are dead.","","","0"
"2715","501056","998023","11/17/2020 22:40:45","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. 12

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Test tast","Testing","11/19/2020 23:59:00","1"
"3292","499772","6608589","01/28/2021 01:54:58","Checking this place

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Sorry this is a test","A test only","01/29/2021 00:00:00","1"
"429","500872","998023","02/04/2020 21:29:49","## Task Details
The dataset shows the stock price over the last ten years. Over the last ~month or so, Tesla skyrocketed to over $900. How come? Is there any explanation we can derive from the chart itself? Ir is this all just driven by the news?

## Expected Submission
A notebook with a visual / textual explanation.

## Evaluation
This is highly subjective, but whatever looks most convincing.","","Why did Tesla just spike?","Is there a technical explanation or is it all driven by news?","","17"
"457","500885","4491668","02/18/2020 12:54:17","## Task Details
Research about topic


## Expected Submission
I have to classification of data, then take note detail all step in here 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
I don't know","","Gooogle_Trends_Data","Version 1","02/19/2020 00:00:00","4"
"1622","612177","4435864","08/07/2020 09:42:37","## Task Details
This task is curiosity-driven to find out whether it would be possible to train NLP model to generate article title given abstract.","","Title prediction by the abstract","","06/15/2040 00:00:00","20"
"1628","612177","3746802","08/07/2020 19:18:14","## Task Details
The problem is to write a survey paper with proper title and annotations which describes the related methodologies from a specific domain. 

## Expected Submission
The solution should contain the notebook which will generate a text/pdf/other document types for the survey paper.","","Creating a Survey from related article and papers","Automate the survey paper creation","08/08/2030 00:00:00","8"
"1757","612177","80046","08/19/2020 17:58:17","## Task Details
A classic task in document management is document classification. In other words, given a document can we assign it to a general topic? What if a paper can have multiple topics associated with it? It would be useful when for example building a recommender system of categories for a paper based on its content.

As such, this can be framed as a [multi label classification problem](https://en.wikipedia.org/wiki/Multi-label_classification). Given an abstract what topics/tags can we confidently assign to it without human intervention.

### Set up
For those who would like to compare model results, you can follow below instructions to prepare the test_set. Or just download it [from here](https://www.dropbox.com/s/j5bxdmdeyow45f3/validation.json?dl=0).

To be able to compare scores we will make following assumptions:
- We will be using the abstracts for validation data published on or after **2019-01-01**. 
- We will use the **same seeds** for train and test split and sampling.
- We will only focus on predicting the **general category**, ex.g. math instead of math.CS.

Use code below to sample and split into train and test. You can refer to [this notebook for examples](https://www.kaggle.com/kobakhit/eda-and-multi-label-classification-for-arxiv#Competition:-Prepare-data-for-multi-label-classification).
```python
import dask.bag as db
import json

SEED = 4 # bounces

docs = db.read_text('../input/arxiv/arxiv-metadata-oai-snapshot-2020-08-14.json').map(json.loads)

# get only necessary fields
trim = lambda x: {'id': x['id'],
                  'title': x['title'],
                  'category':x['categories'].split(' '),
                  'abstract':x['abstract']}
# filter for papers published on or after 2019-01-01
columns = ['id','category','abstract']
docs_df = (docs
             .filter(lambda x: int(get_latest_version(x).split(' ')[3]) &gt; 2018)
             .map(trim)
             .compute())

# convert to pandas
docs_df = pd.DataFrame(docs_df)

# add general category. we are going to use as our target variable
docs_df['general_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x])

# shuffle or sample and keep columns that have at least 1 positive example
sample_df = df.sample(frac = 1, random_state = SEED)
keep = sample_df.iloc[:,2:].apply(sum) &gt; 1
sample_df = pd.concat([sample_df.iloc[:,:2],sample_df.iloc[:,2:].iloc[:,keep.values]], axis = 1)

categories = sample_df.columns[2:]

# removed categories
print('Removed following categories from training : {}'.format(str(keep[~keep].index.to_list())))

# split into train and test
train, test = train_test_split(sample_df, random_state=SEED, test_size=0.15, shuffle=True)

X_train = train.abstract
X_test = test.abstract

print(X_train.shape)
print(X_test.shape)
```

## Expected Submission
A list of documents and assigned topics. It can be framed as a **multi-label classification problem** and the submission will look like this:

| documnet id | topics |
| --- | --- |
| 01 | ['computer science','math'] |
| 02 | ['physics'] |
| ... | ... |




## Evaluation
Use [this test set](https://www.dropbox.com/s/j5bxdmdeyow45f3/validation.json?dl=0) or code from previous section. The two metrics I would recommend are 
- Hamming [Score](https://stats.stackexchange.com/questions/233275/multilabel-classification-metrics-on-scikit)/[Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html)
- Exact Match Ratio (`accuracy_score` in `sklearn.metrics`)

Currently the benchmark is ([from here](https://www.kaggle.com/kobakhit/eda-and-multi-label-classification-for-arxiv#Competition:-Prepare-data-for-multi-label-classification))

| Metric |  Value|
|--|--|
| Hamming Loss |  0.0263|
| Hamming Score|  0.7718|
| Accuracy |  0.6265|

To get those scores you can use below code
```python
from sklearn.metrics import accuracy_score, hamming_loss

# to evaluate the multi label classification model use below metrics
print('Hamming loss : {}'.format(hamming_loss(y_true,y_pred)))
print('Accuracy : {}'.format(accuracy_score(y_true,y_pred)))
```

```python
def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):
    '''
    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case
    http://stackoverflow.com/q/32239577/395857
    '''
    acc_list = []
    for i in range(y_true.shape[0]):
        set_true = set( np.where(y_true[i])[0] )
        set_pred = set( np.where(y_pred[i])[0] )
        #print('\nset_true: {0}'.format(set_true))
        #print('set_pred: {0}'.format(set_pred))
        tmp_a = None
        if len(set_true) == 0 and len(set_pred) == 0:
            tmp_a = 1
        else:
            tmp_a = len(set_true.intersection(set_pred))/\
                    float( len(set_true.union(set_pred)) )
        #print('tmp_a: {0}'.format(tmp_a))
        acc_list.append(tmp_a)
    return np.mean(acc_list)

hamming_score(y_true,y_pred)
```

`y_true` and `y_pred` are arrays of arrays where each column is a label indicator.
```python
# example of y_true or y_pred
array([[0,0, ... ,0,1],
	   [0,1, ... ,1,0],
	   ...
	   ])
```","","Multi-label Text Classification","How well can you assign topics to the arXiv abstracts?","","11"
"501","539056","1008767","03/03/2020 21:22:17","Predicting the factors affecting the Payment for medical malpractice of a insurance company.","","Predicting the factors affecting the Payment .","","","0"
"555","552184","1008767","03/13/2020 21:28:14","Visualizing different cases and create findings.","","Inspecting different cases","","","0"
"5662","873998","7526686","08/10/2021 18:21:34","Hi sir...please upload innings by innings batting and bowling data of players if possible with the following fields - Player name,runs,balls,six,four,ones,twos,threes, dismissed bowler,type,start date","","Batting data","","","0"
"1094","701806","1014084","06/08/2020 18:39:35","## Task Details
Create a model to classify the sentiment of the given text.

## Expected Submission
Submit a notebook with the training, evaluating the binary classification.

## Evaluation
1. Clean and understandable code with comments.
2. Accuracy of the model.
3. Good methods to understand the data before running the training.","","Classify the tweet based on sentiment","Given a Tamil text say it's a ""happy"" or ""sad"" text.","","0"
"1257","754631","1032260","07/04/2020 12:07:26","used in malwere detection","","malware detection","","","0"
"1855","848023","1034242","08/28/2020 03:00:16","Here is your task

1. What are demographics? In your own words, provide in a simple explanation.
2. What demographic characteristics are provided in the dataset?
3. Which demographic group buys the most cookies?
4. What is the most popular type of cookie?

Provide a recommendation to company:

1. What types of cookie should Lana sell and why?
2. Which demographics should she target?","","Analyze businesss","","","0"
"1482","797861","1041505","07/25/2020 08:54:26","## Task Details
For all tweets with #Australia - what are the most commonly used words? I would like to know.

## Expected Submission
A Wordmap would be nice.

## Evaluation
If it looks dope.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Most Popular words to describe Australia","","08/28/2020 00:00:00","0"
"2242","893332","1042462","09/25/2020 23:07:28","## Task Details
Explore reviews and determine which products were successful or not. Why were they successful? Are some ingredient combinations more well-received?

## Expected Submission
Kaggle notebook

## Evaluation
Evaluation here is somewhat subjective. Good explorations & visualizations are key, along with well-thought out conclusions.","","What makes a good ice cream flavor?","","","1"
"1924","855604","1042462","09/02/2020 20:29:47","## Task Details
Predict the price of a bike from its image. You may also utilize the specification data, but improvements in model performance due to including image data is more valued.

## Expected Submission
Kaggle notebook or other.

## Evaluation
I'll leave this open-ended; although I think MAE and MAPE are ideal here for being easily interpretable (how many dollars are we off by?).

### Further help
If you need additional inspiration, check out these projects (not mine!!):
- [AI Blue Book: Vehicle Price Prediction using Visual Features](https://arxiv.org/abs/1803.11227)
- [Predicting Used Car Prices with Machine Learning Techniques](https://towardsdatascience.com/predicting-used-car-prices-with-machine-learning-techniques-8a9d8313952)","","Predict price from image","","","0"
"710","539693","1546625","04/08/2020 09:43:23","## Task Details
You like Data Visualization? You appreciate creating nice interactive graphs? This challenge is made for you! 

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1546625%2F4c6682c9c87a9d3d30462cb051f7f419%2Fgeoviews_ex.PNG?generation=1586352426054939&alt=media)
*Plot example with the python library **Geoviews***

## Expected Submission
Let's play with the MeteoNet dataset to perform wonderful data visualization notebooks!

## Evaluation
The most beautiful visualizations will be displayed to homepage of this Kaggle dataset and of our site at https://meteonet.umr-cnrm.fr/ Be creative and to your keyboards!","","[DataViz] Data Visualization Challenge","","","7"
"716","539693","1546625","04/08/2020 14:51:03","## Task Details
Observation data can contain some anomalies (for example, sensor breakdown, internet network loss...). The environment of the observation station can also induce errors of the records. In that case, they does not anymore represent well the meteorological reality at a given time (for example, the station is not very maintained and vegetation can cover partially the sensors). 
So it is why it is important to detect and correct observation data anomalies by crossing them with another sources like weather model forecasts, radar data or observation data from neighbouring stations. 

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1546625%2F770b80018ac493777022a04a39b81d07%2Fnetatmo70.png?generation=1586426825386303&alt=media)
*Example : correct observation data using several neighbouring stations*

## Expected Submission
Let's play with the MeteoNet dataset to perform notebooks of observation data corrections! Propose your own algorithms (like RNNs...)!
Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open‚Ä¶*). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). 

## Evaluation
For now, we do not impose specific baselines or scores. So, it is up to you!","","Observation Data Correction Challenge","","","4"
"719","539693","1546625","04/09/2020 08:20:45","## Task Details
METEO FRANCE produces several models to forecast the weather. These models are based on physical equations modelling the physical behaviours influencing the weather, such as atmospheric or oceanic phenomenons... They compute a lot of weather parameters (temperature, wind speed, humidity...) at different points on the globe in 4 dimensions (latitude, longitude, height and time). 

It could be interesting to compare the forecasts to another data (radar, ground station observations‚Ä¶) to compute the forecasts errors. They could be taken into account into the future forecasts to improve them. 

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1546625%2Fb0924480afc5e9567c7593dcb4673b0e%2Fimprove_forecast.png?generation=1586421570899245&alt=media)
*Data overview: forecasts, radar, ground stations observations*

## Expected Submission
Let's play with the MeteoNet dataset to perform notebooks to improve weather forecasts! Propose your own algorithms!
Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open‚Ä¶*). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). 

## Evaluation
For now, we do not impose specific baselines or scores. So, it is up to you!","","Forecast Improvement Challenge","","","3"
"720","539693","1546625","04/09/2020 09:51:26","## Task Details
In meteorology, precipitation forecast is a major issue. In case of short term forecast, with a time horizon lower than 2 hours, it is usually called nowcast. Rainfall nowcast is known to be of major interest for severe events anticipation and for agriculture. Different rainfall forecasting methods are already available, ranging from sophisticated numerical weather prediction (NWP) models to simple extrapolation of images acquired by satellites or radars, often based on optical flow. Recently, the Deep Learning models have been shown to outperform these traditional methods, suggesting that IA has a huge potential for solving the problem.

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1546625%2F4e70e56cf5f5b181a22742ae616db7d3%2Fnowcasting3.png?generation=1586426595584517&alt=media)

*Sample data: top, rainfall data at t=0, then at t=2 hours. Bottom, reflectivity data at t=0, then at t=2 hours.*

## Expected Submission
Let's play with the MeteoNet dataset to perform notebooks to nowcast rainfall and/or reflectivity! Propose your own algorithms (like CNN, ConvLSTM, U-Net‚Ä¶)!
Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open*‚Ä¶). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
For now, we do not impose specific baselines or scores. So, it is up to you!","","Rainfall Nowcasting Challenge","","","5"
"714","539693","1546625","04/08/2020 14:06:15","## Task Details
The objective of this challenge is to predict time series by crossing different data sources. For example, you could predict a humidity time series of an observation ground station, using different data sources:
- the humidity history of the parameter to predict
- the other parameters of the observation ground station (temperature, pressure, wind...)
- other data sources : weather model forecasts, radar data 

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1546625%2F2eef1b4ad0b79988833fcc71611bfd07%2Ftime_series_predict70.png?generation=1586426753711104&alt=media)
*Example of time series prediction*

## Expected Submission
Let's play with the MeteoNet dataset to perform time series prediction notebooks! Propose your own algorithms (like LSTMs, random forests...)!

Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open‚Ä¶*). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). You can also browse a first notebook about time series prediction (called *Univariate Time Series Prediction Tmax*).

## Evaluation
For now, we do not impose specific baselines or scores. So, it is up to you!","","[Time Series] Time Series Prediction Challenge","","","5"
"3438","539693","1546625","02/08/2021 15:52:53","## Task Details
Nowcasting is a field of meteorology which aims at forecasting weather on a short term of up to a few hours. Cloud Cover is field of interest for meteorological organisations, for observation satellite management, to optimize their image shots,  and for solar panels management, to forecast their production of electricity. Put simply, at METEO FRANCE, a nowcasting product is a fusion of two approaches: pure data extrapolation techniques, which perform better on the very short term (up to 1 hour), and a mix between extrapolation and classical physical models  which remain the best option for further forecasting.  Recently, the Deep Learning models have been shown to outperform these traditional methods about Nowcasting, suggesting that IA has a huge potential for solving the problem.


## Expected Submission
Let's play with the MeteoNet dataset to perform notebooks to nowcast cloud cover! Propose your own algorithms (like CNN, ConvLSTM, U-Net‚Ä¶)!
Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open*‚Ä¶). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
For now, we do not impose specific baselines or scores. So, it is up to you!","","Cloud Cover Nowcasting Challenge","","","0"
"4005","539693","1546625","04/08/2021 15:57:23","## Task Details
Thunderstorms and floods are huge issues about the safety of persons and property. The radar images are a relevant source of data about precipitation. However, their spatial coverages remain limited (only for areas which could be covered by ground radars). The satellite images have the advantage of covering worldwide. Recently, Deep Learning models have been shown good performances to generate rainfall from satellite images, by choosing the radar images as ground thruth. 

## Expected Submission
Let's play with the MeteoNet dataset to perform notebooks to estimate rainfall from satellite! Propose your own algorithms (like CNN, ConvLSTM, U-Net‚Ä¶)! Several notebooks are already available with that dataset to quickly visualize, explore the data (notebooks with names started with *Open*‚Ä¶). The notebook *Superimpose data* gives some ways to superimpose the data from different sources (observations, forecasts, point data, grid data‚Ä¶). 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
For now, we do not impose specific baselines or scores. So, it is up to you!","","Generate Rainfall from Satellite Challenge","","","0"
"1644","812388","1070674","08/09/2020 05:27:03","## Task Details
Chinese was written, for thousands of years without punctuation. Like, no comma, no stop, no question mark, no exclamation bar. In some cases stop characters are used, like: (ËÄÖ,‰πü) 

Punctuating text while reading is a part of literacy training Chinese stop educating since we found the convenience of punctuations.

But when facing text OCRed from images of books and stone tablets, it will be much easier if we can have a model doing such chore.

## Expected Submission
&gt; The tasks has 2 stages
* **Cleaning out he punctuated rows**: The csv in this dataset doesn't come with label, but it's pretty easy to program something rule based. The submission will return a csv with 2 column:
    * id: original id from wikisource
    * is_punctuated, int: 1 for True, 0 for False
* Use the punctuated data from stage one, and use such as auto-supervised nlp dataset, and train a tokenize level classifier

## Evaluation
Haven't think of evaluation at this stage for now, will update this section when stage 1 has reasonable outcome.","","Punctuating ancient Chinese text","Punctuating the text for easier research","","1"
"2293","672605","1104646","09/30/2020 20:03:01","## Task Details
Try to use the given data to train a model that can predict the characteristics that are present in an image. There are multiple domains to choose from, so you could either work on a singular domain (Eg: Animals) or build a model that will predict the characteristics across all the domains in the dataset.

## Expected Submission
The expected submission would be one that predicts a set of characteristics for each input image.

## Evaluation
As you are free to choose your own metric for evaluation, ensuring a high degree of similarity between the actual characteristics and the characteristics predicted by a model is of utmost importance.","","Characteristic Prediction","Predict characteristics of a particular domain","","0"
"2330","906450","1138256","10/05/2020 02:02:51","You must have GPU enabled","","Install l5kit with GPU support","A demo to show how to use it","","0"
"932","662620","1148841","05/19/2020 08:32:59","## Task Details
Perform further mapping in order to reduce size of main `csv` file","","Further Mapping","","","0"
"933","626690","1148841","05/19/2020 10:21:28","## Next steps

 * [x] Add games
 * [ ] TCI related data
 * [ ] Add reviews","","Enrich dataset","","","0"
"2306","902530","1149136","10/02/2020 07:01:35","Use transfer learning for label classification.","","Transfer Learning","","","1"
"2346","905635","1156596","10/06/2020 04:49:03","## Task Details
Recently age factor has been a major point which the enthusiasts and the commentators use to criticize the performance of specific players. Is there really a correlation between age and the performance of the players?

How has experience helped or affected the players p performance in the IPL tournament?","","Age Performance Correlation of Players","Does age play a factor in the performance of the players in IPL?","","0"
"599","560533","1158131","03/18/2020 06:31:03","## Task Details
Build an imputation and/or extrapolation model to fill the missing data gaps for select stores by analyzing the data and determine which factors/variables/features can help best predict the store sales.

## Expected Submission
Submission should be in Sample Submission.csv format.

## Evaluation
Evaluation will be done based on Root Mean Square Error (RMSE).","","Store Transaction Imputation","create a model which can help impute/extrapolate data to fill in the missing data gaps in the store level POS data currently received.","","1"
"1271","759621","1159011","07/06/2020 19:37:26","## Task Details
1. Combine different csv files into a single dataframe
2. Clean the city_name columns, which also contain the abreviated state names. 
3. Check which of the columns are redundant information (i.e. they can easily be computed from the other columns)
4. Find out the airports and the flight operators which correspond to maximum delay in general. 

## Submission
Submit your notebook containing all the tasks mentioned below. If you are interested in figuring out some more features from the data, you are free to do so. However please separate each of them with appropriate headings for ease of evaluation.","","ISec Learning Exercise","Collate and explore the data","07/12/2020 00:00:00","2"
"764","606119","1159113","04/16/2020 17:08:21","## Task Details
Plote um gr√°fico mostrando os 20 produtos de maior receita.","","Quais s√£o os 20 produtos de maior receita?","","","0"
"765","606119","1159113","04/16/2020 17:09:34","## Task Details
Mostre em um gr√°fico","","Quais s√£o os clientes mais frequentes?","","","0"
"766","606119","1159113","04/16/2020 17:10:26","## Task Details
Plote um gr√°fico","","Quais s√£o os clientes que compram mais?","","","0"
"767","606119","1159113","04/16/2020 17:11:36","## Task Details
Mais um gr√°fico","","Qual o hist√≥rico de vendas por m√™s?","","","0"
"768","606119","1159113","04/16/2020 17:12:34","## Task Details
Outro gr√°fico.","","Qual o hist√≥rico de vendas por ano?","","","0"
"769","606119","1159113","04/16/2020 17:14:10","## Task Details
T√° quase acabando. Mais um gr√°fico por favor.","","Qual o n√∫mero de compras por hora do dia?","","","0"
"770","606119","1159113","04/16/2020 17:16:28","## Task Details
Pen√∫ltimo gr√°fico.","","Qual a soma de vendas por hora do dia?","","","3"
"771","606119","1159113","04/16/2020 17:17:28","## Task Details
√öltimo gr√°fico","","Qual a soma de vendas por dia da semana?","","","0"
"820","623324","1165610","04/27/2020 08:14:58","## Task Details
ÔÉº	Apply exploratory data analysis
ÔÉº	Create Word cloud
ÔÉº	Mostly used words apart from name, nick name, happy, birthday
ÔÉº	Apply data preprocessing
ÔÉº	What is the general sentiment of tweets?
ÔÉº	Conclusion regarding tweet sentiments.","","Attractive WordCloud - Rohit Sharma Birthday Tweets","Sentiment Analysis","","0"
"2500","842582","1188276","10/21/2020 11:53:09","## Task Details
Create a notebook that asks your trained NN to explain what features of the photo decide of the result

## Expected Submission
Possibly, masked or tainted images that show the deciding features -- or any other way of explanation


## Evaluation
The shorter and more clear the explanation, the better.","","What makes the difference?","How does the NN know if this is a wasp or a bee? create an explainer","","1"
"1816","842582","1188276","08/24/2020 16:24:57","## Task Details
Most bee photos contain bees on top of a flower. Is the model recognizing a bee, or the flower?

## Expected Submission
?

## Evaluation
Some kind of demonstration

### Further help

Try adding photos of ""empty"" flowers","","Verify bias","is your model learning flowers?","","0"
"1817","842582","1188276","08/24/2020 16:26:48","## Task Details
Fast.ai claims that you do not need large datasets to do transfer training. 
Check how far can you push the fast.ai framework - what is the smallest count of training examples to still get 80%, 90%, 95% accuracy?

## Expected Submission
a plot explaining training time, observation count, and accuracy achieved","","What is the smallest count of training examples?","Perform a parameter sweep to find the accuracy as a function of training images","","0"
"1337","771105","1189765","07/12/2020 10:37:13","1. Perform Exploratory Data Analysis
2. Perform Feature Engineering
3. Perform Feature Selection
4. Develop a classifier model
5. Handle intrinsic class imbalance in the data
6. Hyper-tune the parameters
7. Discuss model retraining approaches","","Accurately Classify the Binary Dependent Variable(Class)","Explore classification algorithms to correctly classify the dependent variable","","0"
"898","652925","1195275","05/14/2020 05:31:05","## Task Details
Classify sentiment using only texts","","Classification","","","0"
"1103","705004","1195275","06/10/2020 08:06:20","https://www.kaggle.com/c/global-wheat-detection","","Global Wheat Detection","","","0"
"1158","500960","1196028","06/17/2020 08:27:43","## Task Details
Just for practice of detecting faces

## Expected Submission
You shouls submit a csv file with the name of the file and its corresponding label.

Give an upvote, if you like it.
Thanks in Advance","","Identify the khans","You need to identify the faces of top 3 khans od bollywood in testing data","","0"
"595","555276","328482","03/17/2020 20:07:31","https://www.kaggle.com/selfishgene/historical-hourly-weather-data","","Check whether it is possible to use weather data","https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133468","","0"
"780","566225","1212609","04/18/2020 06:25:50","## Task Details
Write a simple python web scraper to get population pyramid data for countries on [the website](https://www.populationpyramid.net/)

## Expected Submission
A python script which saves all CSVs as output.","","Web Scraper","Web scraper to automatically update data for all countries.","","1"
"1974","860658","1216095","09/06/2020 13:08:58","## Task Details
The task is to make a machine learning model to correctly identify the domain is real of homoglyph.","","Identify Real and Fake Domains","","","0"
"1139","711825","1220228","06/13/2020 22:39:35","## Task Details

This task was developed with the purpose of exploring the occurrence of accidents on the bridge. I crossed the bridge daily for years and it is notable that the number of accidents is high .. but how high?

## Expected Submission
The expected submission involves a report that shows relevant statistics on the behavior of that data

## Evaluation
The best solution will be the most creative! The intention is to encourage the use of statistics and data visualizations so the kernel will be better the more coherent, relevant and detailed

## Extra help

See this news, were we able to confirm this information with the available data?

![](https://gomesfellipe.github.io/img/2018-09-29-freq-acidente-ponte-rio-niteroi/noticia.png)

<br>

Source: [http://g1.globo.com/rio-de-janeiro/noticia/2015/11/monitoramento-com-cameras-ajuda-reduzir-acidentes-na-ponte-rio-niteroi.html](http://g1.globo.com/rio-de-janeiro/noticia/2015/11/monitoramento-com-cameras-ajuda-reduzir-acidentes-na-ponte-rio-niteroi.html)","","How often do accidents occur at the Rio - Niter√≥i bridge?","Create a report exploring the current accident on the bridge","","0"
"970","675029","1220228","05/26/2020 00:31:18","## Task details
The objective of this task is to present the flow of travel in Brazil in the most creative way

## Expected Submission
Submit a kernel with the most creative visualizations you can!","","Criative visualizations","","","0"
"1269","740355","843996","07/06/2020 15:43:18","## Task Details

Your client is a fast-growing mobile platform, for hosting coding challenges. They have a unique business model, where they crowdsource problems from various creators(authors). These authors create the problem and release it on the client's platform. The users then select the challenges they want to solve. The authors make money based on the level of difficulty of their problems and how many users take up their challenge.

The client, on the other hand makes money when the users can find challenges of their interest and continue to stay on the platform. Till date, the client has relied on its domain expertise, user interface and experience with user behaviour to suggest the problems a user might be interested in. You have now been appointed as the data scientist who needs to come up with the algorithm to keep the users engaged on the platform.

The client has provided you with history of last 10 challenges the user has solved, and you need to predict which might be the next 3 challenges the user might be interested to solve. Apply your data science skills to help the client make a big mark in their user engagements/revenue.

## Expected Submission

Please ensure that your final submission includes the following:
1. Solution file containing the predictions for next 3 challanges (format is given in sample submission csv)
2. Code file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission

## Evaluation

The evaluation metric is Mean Average Precision (MAP) at K (K = 3). MAP is a well-known metric used to evaluate ranked retrieval results.

### Further help

Original Competition Link: https://datahack.analyticsvidhya.com/contest/janatahack-recommendation-systems/True/#ProblemStatement","","Recommendation Systems","","","0"
"817","620034","1295382","04/25/2020 07:47:08","## Task Details
detect faces in image at any orientation

## Expected Submission
scripts with proper documented inference code and model details with subtle tricks if used. 

## Evaluation
mAP score and IoU (jaccard coefficient)

### Further help","","create a face detector from scratch using deep learning","Detect faces using limited data approach and extend generalization experiment","12/30/2020 00:00:00","0"
"1437","790103","1302116","07/21/2020 05:43:33","**Question 1**: Given a list of tuples, find the number of occurrences of each tuple in the list.

**Question 2**: Create a list of 10 random numbers between 1-100. Create a balanced BST (Binary search tree) from this array.

**Question 3**: Given the following dictionary:

{ ‚Äòblack :‚Äôr‚Äô, 
  ‚Äòhero‚Äô:‚Äòe‚Äô, 
  ‚Äògo‚Äô:‚Äôg‚Äô, 
  ‚Äòclue‚Äô:‚Äôi‚Äô,
  ‚Äômean‚Äô:‚Äôq‚Äô,
  ‚Äôgroan‚Äô:‚Äôo‚Äô,
  ‚Äôsin‚Äô:‚Äôp‚Äô,
  ‚Äôpint‚Äô:‚Äôu‚Äô,
  ‚Äôtone‚Äô:‚Äôn‚Äô,
  ‚Äôgraze‚Äô:‚Äôs‚Äô,
  ‚Äòsea‚Äô:‚Äôt‚Äô,
  ‚Äòplant‚Äô:‚Äôa‚Äô}

Create a list of all the keys of the dictionary, which can be formed from the values present in the dictionary.

**Question 4**: Given the following list of tuples, 

1. Sort the list by 1st item in the tuple in ascending order
1. Sort the list by 2nd item in the tuple in ascending order
1. Repeat (1,2) using descending order

listA = [(1,2), (4,3), (2,10), (12, 5), (6, 7), (9,11), (15, 4)]

**Question 5**: What do you mean by a mutable and immutable object. Are string, list, tuple, dictionary mutable type objects or immutable objects.

**Question 6**: Learn The usage of Pandas and Numpy:

1. Write the command to find unique values in education.
1. Command to find the number of customers subscribed and not subscribed.
1. Find the mean values of all the independent variables for every ‚Äòy‚Äô (outcome variable)
1. What is the mean age for every marital status
1. Check for null values
1. Find the descriptive statistics for each column
1. Use pd.query(), iloc, loc and range subsetting to extract insights from the data. You will be evaluated on the quality of your queries and the rationale behind selecting them. You can extract upto five (5) separate insights from the data.

Dataset : banking.csv","","Assignment 1 : Python Basics","","07/22/2020 00:00:00","2"
"1438","790103","1302116","07/21/2020 05:51:41","### Problem Statement : 

Using numpy, Implement Linear regression using gradient descent
(You can‚Äôt use scikit-learn) 

Dataset : assignment2_data.csv

Scoring Points - 
1. Explain what a cost function is.
1. Understanding of derivatives.
1. Should be able to explain the effects of having different learning rates.","","Assignment 2 : Linear Regression from Scratch","","07/22/2020 00:00:00","1"
"1439","790103","1302116","07/21/2020 05:53:42","### Problem Statement : 

The task is to train a classifier to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.

Apply :

1. Hyperparameter Tuning 
1. Detailed comparison of the performance

Models: Random Forest, Decision Trees, and SVC 


Dataset : sonar.all-data","","Assignment 3 : Classification","","07/22/2020 00:00:00","0"
"1379","732302","1302389","07/16/2020 07:11:40","## Task Details
Explore and visualize where the Nobel prize Laureates are from.
1. Which country has most no. of awardees
2. Which City has most no. of awardees
3. How many countries have an awardee
4. Are the no.s skewed towards any particular country or continent
5. Compare no. of awardees of the country with other socio-economic indexes","","Explore where the Nobel prize Laureates are from","","","0"
"1242","750842","1302389","07/02/2020 06:23:19","## Task Details
Use EDA, visualization tools to find and accurately show which are the countries that haven't had a Cholera case reported in the past 10 years?","","See which are the countries that haven't had a Cholera case reported in the past 10 years?","","","2"
"1241","750843","1302389","07/02/2020 06:13:16","## Task Details
Explore whether the no. of cases, deaths, CFR ... of malaria increases every year?","","Explore whether the no. of cases of malaria increases every year?","","","8"
"902","650365","1302389","05/14/2020 14:45:55","## Task Details
Use EDA and visualizations to see has the no. of satellite sent to orbit in each year increased ?","","See has the no. of satellite sent to orbit in each year increased","","","0"
"1117","708381","1302389","06/11/2020 15:21:28","## Task Details
Compare H1N1 pandemic with current COVID-19 pandemic

## Expected Submission
Data analysis, case studies, comparative study, data visualizations","","Compare H1N1 pandemic with current COVID-19 pandemic","","","1"
"1085","698998","1302389","06/07/2020 15:14:31","## Task Details
Explore, visualize and analayse the status HIV AIDS across the world in past two decades

## Expected Submission
Publication ready data visualizations
Exploratory analysis notebooks
Comparative studies

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/imdevskp/covid-19-analysis-visualization-comparisons
- https://www.kaggle.com/imdevskp/western-africa-ebola-outbreak-analysis
 - https://www.kaggle.com/imdevskp/a-brief-comparative-study-of-epidemics","","Explore the status HIV AIDS across the world in past two decades","","","5"
"600","494766","4691672","03/18/2020 08:17:08","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
hi, Can anyone help me with information regarding fakenews and Covid-19? 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

I am an academic and am conducting a linguistic study on how Covid 19 was amplified by fake news. Am interested in EU headlines (titles, journals, maybe participation, share or likes numbers)

Thank you very much.
Silvia","","fake news","","","31"
"2317","904712","1311672","10/03/2020 17:01:43","Rename it to something more readable. But how do I do that?","","Rename file","","","0"
"1554","789975","1311729","08/01/2020 19:14:07","## Details
Use text classification (or whatever methods you see fit) to predict which genre a book falls under. Genres listed are quite specific and can be consolidated (e.g. teen sci-fi can be regarded as just sci-fi). A single predicted genre is expected and if said genre is present in the book's genre list, it is regarded as a successfully prediction.

## Expected Submission
Model created through a notebook showing both training data and testing data accuracy marks.

## Evaluation
The best submission will have the highest testing accuracy, I would like to see at least 25% accuracy.

### Help
Check out the following articles for help getting started
- https://monkeylearn.com/text-classification/
- https://towardsdatascience.com/text-classification-in-python-dd95d264c802","","Predict Book Genre","Using Text Classification to Label a Book's Genre","","4"
"2149","843452","1313949","09/18/2020 11:08:49","## Task Details
Create the model for classifying the described word

## Expected Submission
You need to use the user column to split data. Train - users with id &lt;= 4. Test - users with id &gt;= 5. Create a model using train data and testing it on the test data.

## Evaluation
You need to get more than 80% accuracy for the test data

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Create the model for classifying the described word","","","0"
"1765","838778","1313949","08/20/2020 15:21:43","## Task Details
Create the model with the TensorFlow (Keras) library, that can achieve more than 90% accuracy on validation data. 

## Expected Submission
Your submission should contain the model, implemented in TensorFlow (you can use pre-trained models).
You need to split data using index.csv file (see the 'train-valid' column).
Train only with data that have the mark 'train' in that column.
Try to get more than 90% accuracy score with data with 'valid' mark.
Your code should be easy to read.

## Evaluation
Accuracy score, code quality.

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Predict With TensorFlow Models [90% acc]","","","4"
"1774","838778","1313949","08/20/2020 19:21:26","## Task Details
Create the model with the PyTorch library, that can achieve more than 90% accuracy on validation data.

## Expected Submission
Your submission should contain the model, implemented in PyTorch (you can use pre-trained models, but shouldn't use high-level frameworks such as Catalyst, Lightning).
You need to split data using index.csv file (see the 'train-valid' column).
Train only with data that have the mark 'train' in that column.
Try to get more than 90% accuracy score with data with 'valid' mark.
Your code should be easy to read.

## Evaluation
Accuracy score, code quality.

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Predict With PyTorch Models [90% acc]","","","5"
"2141","838778","1313949","09/17/2020 16:23:00","## Task Details
Create an algorithm that can help detect what pixels are the Minifigure for changing the background for this image. You need to create segmentation for the Minifigure

## Expected Submission
The result of the algorithm should be the mask for images. These masks should contain only Minifigures.

## Evaluation
Pretty masks, code quality.

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Minifigure Segmentation","","","2"
"1930","838778","1313949","09/03/2020 09:46:07","## Task Details
Create the Siamese Neural Network with the TensorFlow (Keras) library

## Expected Submission
Your submission should contain the model, implemented in TensorFlow.
Your code should be easy to read.

## Evaluation
F1 score, code quality.

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Siamese Neural Network in TensorFlow","","","3"
"1947","838778","1313949","09/04/2020 17:59:33","## Task Details
Create the Siamese Neural Network with the PyTorch library

## Expected Submission
Your submission should contain the model, implemented in PyTorch.
Your code should be easy to read.

## Evaluation
F1 score, code quality.

### Further help
You can find more datasets here:
- https://github.com/yisaienkov/tinysets","","Siamese Neural Network in PyTorch","","","1"
"1987","838778","1313949","09/06/2020 19:31:11","## Task Details
Try to use this dataset as unsupervised (don't use the class_id column from index.csv and don't use metadata.csv file).

## Expected Submission
The main task is to demonstrate the clustering analysis for this dataset. 
You can try to extract some features from images, and visualize pictures in the 2D space or 3D space (like MNIST example in https://projector.tensorflow.org/). 
Or you can try to get a high score using only clustering analysis (you can use the class_id column ONLY for verify results).

## Evaluation
Your work must be interesting for dataset authors and the community. And it's important to make clean and readable code.","","Cluster analysis","","","1"
"2122","876430","1313949","09/16/2020 12:34:20","## Task Details
Try to generate some interesting features for this Hello World dataset

## Expected Submission
Your kernel should contain a feature generation process and some explanations. 
Your code should be easy to read.

## Evaluation
Interesting approach. The number of new features.

### Further help
More interesting datasets:
- [LEGO Minifigures](https://www.kaggle.com/ihelon/lego-minifigures-classification)","","Feature Generation From Program Text","","","3"
"1224","745958","1314380","06/29/2020 21:27:11","## Task Details

Perform a consistent pattern of classifications / text extractions using the [Kaggle CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).  Create a notebook that automatically extracts the same information as the COVID-19 SOLES tables.   Specifically, we are trying to automate the manual classification / text extraction process that was used to create the `all_primary_data.csv` file within this dataset, except we want to use the Kaggle CORD-19 dataset as the data source because the Kaggle CORD-19 dataset is in a machine readable format (.JSON) and is updated on a frequent basis (there were more than 300 COVID-19 papers published per day during May 2020!)

For a ""getting started"" notebook, please refer to https://www.kaggle.com/davidmezzetti/cord-19-report-builder and https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/159849 and https://towardsdatascience.com/building-analysis-pipelines-with-kaggle-c745671d273e.  Note that this method will need to be adapted to work with the new table formats, described below.

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the file `all_primary_data.csv`.  


`[ 'objective', 'method', 'detail_method', 'subjects',
       'peer_reviewed', 'language', 'Url', 'DOI', 'Pdf', 'Link',
       'Author', 'Year', 'Title', 'Journal', 'Abstract', 'Volume',
       'Number', 'Pages', 'ISBN']`

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the [Kaggle CORD-19 dataset](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), but those datasets must also be publicly available in order for the submission to be valid.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the file `all_primary_data.csv` in this dataset for an example of what an article summary table might look like.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is NO DEADLINE
- Bonus points for early submissions!","","Perform a consistent pattern of classifications / text extractions using the Kaggle CORD-19 dataset","Create a notebook that automatically extracts the same information as what is contained within the COVID-19 SOLES tables.","","4"
"424","496090","1314380","02/02/2020 21:40:10","## Task Details

What are the top 10 highest and lowest paying universities?  What are the top 10 highest and lowest paying departments?  Use a Kaggle notebook to inspect the data and answer the question.

## Expected Submission

A public notebook that accomplishes the task.

## Evaluation

Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","What are the top 10 highest paying universities?","What are the top 10 highest paying departments?","","3"
"6290","496090","93035","10/09/2021 18:47:26","## Task Details
I think there are errors in the data simply sorting in ascending order by stipend reveals insane amounts of 1M$","","Are there errors on the data?","I don't think there exist stipends of 1M$","","0"
"498","534380","1314380","03/03/2020 02:27:41","## Task Details

Compare and contrast the viruses that cause COVID-19, SARS, and MERS. What are the key differences and similarities between the relevant viruses?

## Expected Submission

Submissions should be in the form of a public Kaggle notebook.  Notebooks should be attached to one or more public Kaggle datasets (including this one).

## Evaluation

Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Compare and contrast COVID-19, SARS, and MERS.","Compare and contrast the viruses that cause COVID-19, SARS, and MERS. What are the key differences and similarities between the relevant viruses?","","3"
"490","533438","1314380","02/29/2020 00:31:36","## Task Details

Compare and contrast the viruses that cause COVID-19, SARS, and MERS. What are the key differences and similarities between the relevant viruses?

## Expected Submission

Submissions should be in the form of a public Kaggle notebook.  Notebooks should be attached to one or more public Kaggle datasets (including this one).

## Evaluation

Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought provoking, and fresh all at the same time.

Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Compare and contrast COVID-19, SARS, and MERS.","Compare and contrast the viruses that cause COVID-19, SARS, and MERS. What are the key differences and similarities between the relevant viruses?","","10"
"1037","689894","1314380","06/02/2020 21:55:52","## Task Details

Compare the [LitCovid](https://www.kaggle.com/paultimothymooney/litcovid/) dataset to the [CORD-19](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) dataset.  Are there any papers or topics that are included in one dataset but not the other?

## Expected Submission

A public Kaggle notebook that addresses the task.

## Evaluation

Submissions will be scored using the following grading rubric:

Content (5 points)
 - Did the participant accomplish the task?
Documentation (5 points)
 - Is the code easy to read and reuse?
Presentation (5 points)
 - Did the participant communicate their findings in an effective manner?","","Compare the LitCovid dataset to the CORD-19 dataset.","Are there any papers or topics that are included in one dataset but not the other?","","1"
"1053","689979","1314380","06/04/2020 20:44:57","## Task Details

Which cities or states have the most inequality with regards to COVID-19 fatalities?  Do you notice any patterns in the data?

## Expected Submission

A public Kaggle notebook that addresses the task.

## Evaluation

Submissions will be scored using the following grading rubric:

Content (5 points)
 - Did the participant accomplish the task?

Documentation (5 points)
 - Is the code easy to read and reuse?

Presentation (5 points)
 - Did the participant communicate their findings in an effective manner?","","Which cities or states have the most inequality with regards to COVID-19 fatalities?","Do you notice any patterns in the data?","","2"
"434","491592","1315079","02/04/2020 21:39:49","## Task Details
Japan is one of the hardest starting sponsor.  Find the best location to start the colony for Japan.

## Expected Submission
The submission should give the coordinates of the map location and justification on why the location is great for Japan.  It should also include Commander recommendation.

## Evaluation
The submission with most detail explaining on why the location is great will win the task.","","Find best starting location for Japan","Best start for Japan","06/01/2020 00:00:00","1"
"1024","686004","1332385","06/01/2020 17:39:32","This data works best for recommendation problems.","","Recommendation for customers based on the purchase","","","0"
"1236","748772","1335842","07/01/2020 15:56:27","## Task Details
There are 531 total features in the dataset, need to carry out EDA and statistical analysis  to explore and find features that best decide the type of executable.

## Expected Submission
Your script with analysis.","","Exploratory Data Analysis","","","0"
"1692","808524","1341932","08/14/2020 06:03:57","do a few preprocessing and dataset cleaning to get it ready for the training step.","","Preprocessing","","08/31/2020 00:00:00","0"
"1661","608713","1352316","08/10/2020 22:45:27","## Task Details
I was trying to load the newest data (SnakeData - Copy.txt) into a pandas dataframe, but the format of the new column was not allowing it. 

## Expected Submission
I would like someone more versed in python to submit a notebook that reads in the data and parses out all of the snakes positions for each move. 

## Evaluation
I don't have a criteria for this. It is meant to be fun and challenge people to learn to clean data. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
- https://www.kaggle.com/ichabuddaeta/new-snake-data-feature-starter-code (contains my logic in R to do the task. It is not very efficient but I had fun doing it.)","","Parse full snake data in python","I was not having luck using python to load the file.","","0"
"6772","804181","8206139","11/06/2021 17:50:18","## Task Details
Make a CNN that generates a groundtruths or masks to it's structural defects.

## Expected Submission
Use a notebook to make a CNN using SDNET2018 dataset to generate it's masks.
The CNN must works well for all positive structural defects in this dataset.
Maybe you have to make multiples CNN's for every structural defects type.

## Evaluation
The CNN must only extract the positive structural defects.","","Mask or GroundTruth for SDNET2018","Generating masks for SDNET2018 with CNN","","0"
"529","548066","1396051","03/10/2020 14:24:33","## Task Details

Leveraging the longitude and latitude information of cases, visualize the cases with the distinction between old and new cases. Visualizations that also include the temporal information would give better insight into the spread of the virus in a much-magnified perspective.  

This could be very helpful for the locals to avoid going through those regions.","","Illustrate the spread of the virus w.r.t time","","","0"
"2068","865278","1396467","09/12/2020 07:42:16","Linear Regression is one of the first models that we come across while starting to do Data Science.

There are various assumptions on the data that need to be checked before applying linear regression. 

These include:
1. Normality
2. Autocorrelation
3. Linearity
4. Heteroskedasticity
5. Multicollinearity
+others


The Task is to create a notebook that checks all these assumptions on the dataset and then rectifies the errors, then applying regression models to the dataset.","","Check Linear Regression Assumptions","Predict Insurance Premium  Value Through Regression","","10"
"3290","575448","1406198","01/27/2021 22:09:28","Generate new shoes images","","Image generate","","","0"
"1948","858235","1409853","09/04/2020 18:01:35","## Task Details
This is an animal network and shows the interaction between birds monitored using RFID. 

## Expected Submission
 You need to profile the explore the graph and provide the topological information of graph. You also need to profile the graph meaning find out the min/max degree, graph density, average degree, associative property, number of communities in the graph etc.

## Evaluation
As always, you will be evaluated how thorough is your work. There are extra marks for creativity on this one - the more interesting and varied your graphs the more points you get.","","Graph Profiling","EDA of Graph","09/30/2020 23:59:00","0"
"2711","492394","1411532","11/16/2020 23:33:29",".","","Person detection through depth images","","","1"
"2453","848645","1411532","10/16/2020 23:57:05","Estimate the 2D registration(optical flow) between deformable surfaces images.","","Optical flow","","","0"
"500","538444","1429656","03/03/2020 13:19:15","## Task Details
Visualize or summarize the who, what, and where of the disengagement reports. Try to identify general causes of disengagements and thereby shortcoming of self-driving cars.","","Explore Disengagement Reports","","","0"
"512","538444","1429656","03/06/2020 02:16:40","## Task Details
Disengagement descriptions do not follow any standard format. Therefore, it would be interesting to apply NLP methodologies to summarize the root causes behind the disengagements.","","Analysis of Disengagement Descriptions","","","1"
"715","592486","1429925","04/08/2020 14:33:34","Next step is to identify best videos from each channel and list then as per topic.","","Identifying best videos","","","1"
"1228","748434","1429925","06/30/2020 14:39:51","I have over 100 videos on my channel. Can you suggest which type of videos should I create more of?","","Can you tell me which videos should I create more of?","","","1"
"1801","841888","1430847","08/23/2020 15:31:25","## Task Details
Classify the customer into four segments (A, B, C, and D). Obtain a high accuracy model using the test data column 'Segmentation' for evaluation of your model!","","Segment the customers","","","0"
"1802","841888","1430847","08/23/2020 16:18:47","## Task Details
Explore the data and provide different approaches to feature engineering!","","EDA and Feature engineering","","","1"
"1804","842650","1430847","08/23/2020 20:15:32","## Task Details
It's always a good step to start with knowing your data at first before falling in love with it!

Explore the data and posts your notebooks on your EDA approach!","","EDA Notebooks","","","1"
"1805","842650","1430847","08/23/2020 20:17:25","## Task Details
Help us understand how the tweets are globally distributed and how many well spread the tweets are!","","Geographical distribution of the tweets","","","0"
"1806","842650","1430847","08/23/2020 20:19:54","## Task Details
Hey there, Everyone is a fan of a team! So put your gloves on and cluster the tweets into one of the 9 categories, which includes 8 teams(CSK, RCB, Delhi etc.) and one Neutral category!","","Tweets Cluster based on the teams","","","1"
"1807","842650","1430847","08/23/2020 20:24:26","## Task Details
Identify the sentiments of the tweets","","Twitter sentiment analysis","","","2"
"1753","837624","1430847","08/19/2020 14:22:34","## Task Details
Submit a notebook to understand the geographic distribution of the tweets","","Geographical distribution of tweets","Who, from where is tweeting about Covid vaccine?","","0"
"1754","837624","1430847","08/19/2020 14:24:23","## Task Details
With the available data, analyze the trends of Covid vaccine subject in the collected tweets.","","Dynamic in time and space of the tweets","Analyze the dynamic of COVID tweets  (temporal, spatial)","","1"
"1755","837624","1430847","08/19/2020 14:25:31","## Task Details
Create Notebooks for sentiment analysis from Covid Vaccine tweets","","Tweets sentiment analysis","Sentiment analysis for tweets about Covid Vaccine","","1"
"1724","834759","1430847","08/17/2020 10:30:44","## Task Details
Identify the geographical distribution of tweets.","","Geographical distribution of tweets","Who, from where is tweeting about covid19?","","0"
"1725","834759","1430847","08/17/2020 10:32:53","## Task Details
 Publish notebooks on the sentiment analysis for tweets about MS dhoni's retirement","","Tweets sentiment analysis","Sentiment analysis for tweets about MS dhoni's retirement","","0"
"2130","877017","1430847","09/16/2020 21:38:48","## Task Details
With the available data, analyze the trends of the documentary in the collected tweets.","","Dynamic in time and space of the tweets","Analyze the dynamics of tweets (temporal, spatial)","","0"
"2131","877017","1430847","09/16/2020 21:39:53","## Task Details
Create Notebooks for sentiment analysis from the #TheSocialDilemma tweets","","Tweets sentiment analysis","Sentiment analysis for tweets about the documentary","","0"
"2174","885022","1430847","09/20/2020 21:06:49","## Task Details
Perform EDA on the match data and find interesting patterns and trends","","EDA on the IPL data","","","1"
"2197","885022","1430847","09/22/2020 15:05:57","## Task Details
Use the past IPL season data from 2008 to 2019 to predict the point table of IPL 2020 SEASON!","","Predict the IPL 2020 points table","","11/03/2020 23:59:00","0"
"2250","894059","1430847","09/26/2020 10:53:38","## Task Details
A kind request to kaggle users to create notebooks on different visualization charts as per their interest by choosing a dataset of their own as many beginners and other experts could find it useful!","","Notebooks on different visualization charts as per their interest by choosing a dataset of their own","","","0"
"2251","894059","1430847","09/26/2020 10:54:40","## Task Details
To create interactive EDA using animation with a combination of data visualization charts to give an idea about how to tackle data and extract the insights from the data","","Create interactive EDA using animation with a combination of data visualization charts","","","0"
"2112","875249","1433386","09/15/2020 18:06:05","Sooner I will add dataset for multiple languages then one can able to predict NER for multiple languages","","Predict Named Entity in all languages","","","0"
"1623","820589","1433386","08/07/2020 13:05:20","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
RMSE is the Evaluation Metric","","Predict the size of tumor","","","1"
"1484","797045","1433386","07/25/2020 14:31:46","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
Predict the amount of Energy produced

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
Evaluation for the dataset will be Root Mean squared Erro(RMSE)","","Energy Prediction","","","0"
"1140","714827","1468862","06/14/2020 15:04:53","## Task Details
A classic regression based problem which tries to predict either Likes/Views based on the video title

## Expected Submission
A notebook outlining your approach

## Evaluation
Since there is no precedent for this, we can look at tweet datasets which has engagement as it's prediction","","Predicting Engagement using Video Titles","","","1"
"874","617818","1468862","05/11/2020 12:13:49","## Task Details
Considering the proliferation of SoTA transformer models for NLP based classification, this task aims at predicting the flair (a flair is a category in a subreddit which is manually tagged either by the submitting user or the subreddit moderator into major ones such as 'Political', 'Non-Political', 'AskIndia', 'Coronavirus', 'Entertainment', 'Sports' etc


## Expected Submission
Users should submit notebooks/kernels neatly elucidating the approach as to why this particular transformer architecture etc

## Evaluation
An F1 score above 0.65 (for 15-19 flairs) can be considered good","","Multi-Class Flair Prediction","","","1"
"1999","862172","1473525","09/07/2020 12:16:36","## Task Details
Media platforms such as social networks, media advertisement, information retrieval and recommendation systems deal with exponentially growing data day after day. Enhancing the relevance of multimedia occurrences in our everyday life requires new ways to organize ‚Äì in particular, to retrieve ‚Äì digital content. Like other metrics of video importance, such as aesthetics or interestingness, memorability can be regarded as useful to help make a choice between competing videos. This is even truer when one considers the specific use cases of creating commercials or creating educational content. Because the impact of different multimedia content, images or videos, on human memory is unequal, the capability of predicting the memorability level of a given piece of content is obviously of high importance for professionals in the field of advertising. Beyond advertising, other applications, such as filmmaking, education, content retrieval, etc., may also be impacted by the proposed task.

The task requires participants to automatically predict memorability scores for videos, that reflect the probability for a video to be remembered. Participants will be provided with an extensive data set of videos with memorability annotations, related information, and pre-extracted state-of-the-art visual features.

## Expected Submission
*See the [MediaEval 2020 webpage](https://multimediaeval.github.io/editions/2020/) for information on how to register and participate.*

* 15 October: Runs due 
* 15 November: Results returned  
* 30 November: Working notes paper  
* Early December: MediaEval 2020 Workshop 

## Evaluation
The ground truth for memorability will be collected through recognition tests, and thus results from objective measures of memory performance.

The outputs of the prediction models ‚Äì i.e., the predicted memorability scores for the videos ‚Äì will be compared with ground truth memorability scores using classic evaluation metrics (e.g., Spearman‚Äôs rank correlation).


#### References and recommended reading
[1] Awad, G., Butt, A.A., Lee, Y., Fiscus, J., Godil, A., Delgado, A., Smeaton, A.F. and Graham, Y., [Trecvid 2019: An evaluation campaign to benchmark video activity detection, video captioning and matching, and video search & retrieval](https://www-nlpir.nist.gov/projects/tvpubs/tv19.papers/tv19overview.pdf). 2019.

[2] Romain Cohendet, Claire-H√©l√®ne Demarty, Ngoc Duong, and Martin Engilberge. [VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability](https://openaccess.thecvf.com/content_ICCV_2019/papers/Cohendet_VideoMem_Constructing_Analyzing_Predicting_Short-Term_and_Long-Term_Video_Memorability_ICCV_2019_paper.pdf). Proceedings of the IEEE International Conference on Computer Vision. 2019.

[3] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. 2015. [Understanding and predicting image memorability at a large scale](https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf). In Proc. IEEE Int. Conf. on Computer Vision (ICCV). 2390‚Äì2398.

[4] Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, and Aude Oliva. 2014. [What makes a photograph memorable?](http://web.mit.edu/phillipi/www/publications/memory_pami.pdf) IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 7 (2014), 1469‚Äì1482.

[5] Hammad Squalli-Houssaini, Ngoc Duong, Gwena√´lle Marquant, and Claire-H√©l√®ne Demarty. 2018. [Deep learning for predicting image memorability](https://hal.archives-ouvertes.fr/hal-01629297/file/main.pdf). In Proc. IEEE Int. Conf. on Audio, Speech and Language Processing (ICASSP).

[6] Junwei Han, Changyuan Chen, Ling Shao, Xintao Hu, Jungong Han, and Tianming Liu. 2015. [Learning computational models of video memorability from fMRI brain imaging](https://ieeexplore.ieee.org/abstract/document/6919270). IEEE transactions on cybernetics 45, 8 (2015), 1692‚Äì1703.

[7] Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, and Akhil Shetty. 2017. [Show and Recall: Learning What Makes Videos Memorable](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w40/Shekhar_Show_and_Recall_ICCV_2017_paper.pdf). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2730‚Äì2739.

[8] M.G. Constantin, M. Redi, G. Zen, B. Ionescu, [Computational Understanding of Visual Interestingness Beyond Semantics: Literature Survey and Analysis of Covariates](http://campus.pub.ro/lab7/bionescu/index_files/pub/2018_ACM_CSUR-draft.pdf), ACM Computing Surveys, 52(2), 2019.

#### Help for Annotations
We need more annotations for the dataset. We kindly ask for your help to get more annotations. Please visit the [link](https://annotator.uk/mediaeval/index.php) and participate in the funny game to contribute to the dataset. Thanks in advance for your contribution.","","Predicting Media Memorability 2020","","10/15/2020 23:59:00","0"
"1101","705033","1480586","06/10/2020 07:05:53","## Task Details
Classify different tasks given to AI speech bots based on wav files

## Expected Submission
Not applicable as this is for learning purpose.","","Speech classification","Try recognising speech from the given sets","","1"
"1612","811614","1480586","08/06/2020 15:56:17","## Task Details
We have to identify the Richest family names across Florida which has most real Estate amount in their banks

## Expected Submission
Top 10 Richest Families in Florida","","Richest Owners","Can you identify the Richest Owners","","0"
"1617","819786","1488294","08/07/2020 00:07:17","## Task Details

Re-stocking products with the best potential is crucial for shops and businesses around the world.

Through this dataset and task, you are requested to build a model that can help predict *how well a product is going to sell*.

Such a model has *many implications* and could be used in many different ways, the most straightforward being to adjust how much of a product should be kept in stock.

This would allow you to better understand e-commerce sales and get you a certain expertise for helping businesses optimize their stocks and sales.


## Inspiration

- How about trying to validate the established idea of human sensitiveness to price drops ?
- You may look for top categories of products so that you know what sells best
- Do bad products sell ? How about the relationship between the quality of a product (ratings) and its success ? Does the price factor into this ?
- Do seller's fame factor into top products ?
- Do the number of tags (making a product more discoverable) factor into the success of a product ?


## Expected Submission

*What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?*

You may solve this task using a notebook.


## Evaluation
*What makes a good solution?*

Your solution should **minimize the risks of**:

- *Buying way too much of a product*

- Ideally, identify products with a higher discount regarding the retail price. Practically, this means identifying products where the seller could increase his profit margin without losing too many buyers. See the concept of [Price Elasticity of the demande](https://en.wikipedia.org/wiki/Price_elasticity_of_demand)

- Lastly. There are always *a lot of product opportunities*, so *missing on a single opportunity is better than buying way too much* another product, since this might cancel out the profits gained elsewhere.


Your model will be evaluated on another similar dataset (same features&nbsp;and columns) to see how well it performs on new other data.


### Further help

If you have any question, feel free to reach out through the comments section.","","Develop a model for product success","Help businesses answer the crucial question ""How well is a product likely to sell ?""","","36"
"4571","819786","7486601","05/30/2021 18:15:20","Get Meaningful Insights.","","Get Information about Data","","","0"
"1234","750203","1488294","07/01/2020 13:06:37","## Task Details

### Motivation

Online searches have become an important aspect of our daily lives as well as a key aspect for many businesses. Everyday, tons of searches are made all over the world, be it to get information, education or entertainment.
On the other end, businesses want to increase their exposure and visibility within this chaos, and are willing to pay fortunes to rank higher and stay on top.

In that regard, understanding how search trends evolve over time has become of central importance, as the desire to create a buzz is becoming mainstream.

### Tasks

Your goal will be to study how trends evolve over time and predict how the search traffic should evolve.


### Ideas

- You may want to study by country.
- You may want to study the impact of having a trend start in several countries
- You may want to consider the density of population of each country (number of people in a country) (and maybe also % of access to internet) since the more people a country has, the higher the number of interactions among friends and thus more repost/sharing.

- You may want to consider the language or the alphabet of a trending result.
   For instance, since english is an international language and quite easy to learn (compared to several other mainstream language), we may suppose that a trend or an article posted in english can be understood by more people, and thus more people can like it and share it.
   Conversely, a post written in Cyrillic or using the Chinese alphabet would not be understood in western countries, and thus the trend would stay or ""die"" in the neighbouring countries where it originated.

- You may want to approach searched terms by domain (like business, entertainment, social, news, ...) and group the results in clusters.


## Expected Submission
*What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?*

You may contribute by submitting either one (or more) of those:
- using a notebook
- or a filtered version of the files in this dataset where you keep only the most interesting rows that denote your approach. This way, people will be able to build a model with the specific approach you took with your filtered data.
- or a dataset that can help complement this one and improve its quality


In all cases, an explanation of your approach will be much appreciated since it will allow others to better understand your contribution and give you proper credits !


## Evaluation
*What makes a good solution?*

Your submission should allow us to see how your model is validated against trends in the dataset.
Especially, the dataset contains trends that spanned over a few days (either because they grow over time or go down by losing the masses' interest).

Your submission should show examples of such trends and how they are handled (does your model help predict accurately these trends?)","","Build a model to predict the growth of search traffic","Study how top searches evolve over time","","0"
"821","625291","1488294","04/28/2020 12:45:14","## Task Details
Creating a great seller profile is the dream of every inspiring influencer. However, the road to success is often unexplored because the amount of success is inversely proportional to the number of people who got to that level.

The idea with this dataset is to explore how seller profile mood messages relate to their level of success and ‚Äìhopefully‚Äì discover what would make for a great seller profile status.

## Expected Submission
Ideally, it would be great to have contributions that show ‚Äìfor different levels of ""success""‚Äì what would be typical profiles look like.

This could be shown through a notebook, infographics, datasets or other forms.

## Note about a *Sucessful* profile
Note that the *success* of a seller is relative.
Since the dataset contains data about C2C sellers (customer-to-customer), you could choose to consider success:
- as the number of sold articles,
- or as the number of followers (since the more followers, the more sales potential a seller will have)","","Influencer-like profile ?","What paves the way to being an influencer ?","","0"
"1676","826739","1498852","08/11/2020 23:06:46","## Task Details
Can we accurately detect a trahs in different and diverse contexts ?

## Expected Submission
Providing the bounding boxes is advised. One can also predict trash category along with the bounding boxes.

## Evaluation
As usually, the evaluation metric is IoU.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Trash detection","","","0"
"1715","833222","1510254","08/16/2020 11:58:44","## Expected Submission
A working supervised learning model. Some EDA to go with would also be nice.

## Evaluation
Classification Score of 0.5 or higher.","","Classification Survival Analysis","Build a Classification model to predict if passengers were Lost/Saved","","0"
"1483","798182","1510254","07/25/2020 12:07:19","A starting point for analysis would be to conduct NLP Analysis for Etsy listings to see if there is any correlation between certain phrases and higher prices and better reviews.","","NLP Price Analysis","","","0"
"789","614034","274596","04/21/2020 13:09:08","## Task Details
The base plots are the outputs from the original R code. They are good for articles or on a website. You can improve them by e.g making them interactive.

## Expected Submission
Please submit proposals within a kernel that Kagglers can upvote. Feel free to use either R or Python since the data are in .csv format.

## Evaluation
A good proposal is obviously diifficult to assess but it certainly is beautiful (esthaetic criteria), remains informative. It should also inspire others to use it.

### Further help
The base plots are within the original report by Imperial College 
https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/covid-19/report-13-europe-npi-impact/","","Improve base plots","","","1"
"1585","802068","1526569","08/04/2020 12:57:04","## Task Details
The easiest way to start with this dataset is to try to train a classifier which is able to process an image and predicts if it has sign of sickness or no findings.
IN TFRecords you have both image and one-hot encoded the diagnosis.

## Expected Submission
A list with the image id and the probability that the image contains signs of one of the diagnosis

## Evaluation
Since the dataset is unbalanced, ROC AUC is the better score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classify in sick/no findings","","","2"
"1427","788872","1526569","07/20/2020 13:20:22","## Task Details
Try with this dataset how to do fast processing, reaching very high GPU utilization.","","Fast processing with TFRecords","","","0"
"1763","824697","1526569","08/20/2020 12:42:30","## Task Details
Diabetic Retinopathy is a severe disease that could affect diabetic people. If not cured it can cause blindness

You can use these files to train a multi-class classifier.

5 classes are expected:

0 - No DR
1 - Mild
2 - Moderate
3 - Severe
4 - Proliferative DR

## Expected Submission
You should prepare a submission.csv file according to the Diabetic Retinopathy competition (https://www.kaggle.com/c/diabetic-retinopathy-detection).

## Evaluation
What makes a good solution? 
You can use the above-mentioned competition, late submission, to evaluate the quality and performance of your classifier.

Accuracy should be higher than 80%.","","Diagnose Diabetic Retinopathy","","","0"
"1815","843108","1526569","08/24/2020 15:59:38","## Task Details
You can use this dataset to develop a Model for Diabetic Retinopathy Detection and **try a (late) submission to DR 2015 competition**.
You can check your results against the competition leaderboard and try to see if it is easier today to get in top 5%.

## Expected Submission
Follow the steps needed for https://www.kaggle.com/c/diabetic-retinopathy-detection competition. Check with late submission

## Evaluation
What makes a good solution? 
The easier thing is to check with accuracy. You should aim for at least 0.82.
Then, the best thing it to submit and see your score.

### Further help
If you need additional inspiration, you can have a look at my github repository:
https://github.com/luigisaetta/diabetic-retinopathy","","Verify how it has become easier with current technologies","Is it easier now?","","1"
"1957","859035","1535712","09/05/2020 10:56:36","BoBo Retail has launched a generic drug market 2 years back. They have been doing some promotion since then and now they have got the impact of that promotion analyzed. As a result, they have the response curves for the various personal channels. Now BoBo wants to know the optimal mix of their promotional channels. For this, they have approached Pallavi to analyze its response curves and to recommend the optimal spend for each promotional channel.","","Promotional Mix Modelling","","","0"
"4585","883517","1541942","06/01/2021 02:54:22","## Task Details
Dehaze every single image of this dataset using dark channel prior (DCP) method

## Expected Submission
Your code must contain minimally these functions:
1. getDarkChannel()
2. getAtmosphericLight()
3. getTransmissionMap()
4. restoreImage()

## Evaluation
- Every function worth 15 points.
- Code runs without error worth 20
- Code runs with the expected result worth 20

### Further help
- If you find any difficulties, open a Discussion
- See Rubrik Holistik in Google Classroom","","Single Image Dehazing","","","0"
"2134","877314","1541942","09/17/2020 04:13:02","## Task Details
Student needs to know whether they could graduate on-time or not.

## Expected Submission
Create a classification or prediction based on four features (ip1-ip4) which represent student GPAs from semester 1 to semester 4 respectively. You may use any classification methods for instance: K-NN, Neural Network, SVM, Random Forest, Naive Bayes

## Evaluation
Evaluate using k-Fold Cross Validation and using Confusion Matrix

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","On Time or Not?","On-time graduation prediction","09/23/2020 23:59:00","1"
"1810","842872","1541942","08/24/2020 06:40:00","## Task Details
This task is created as one of Digital Image Processing course. In order to finish this task, you have to satisfy every single task below:
1. Image Sharpening
2. Image Blurring
3. Image Edge Detection using Canny, Sobel X & Y, dan Laplacian Filter

## Expected Submission
You should submit your Jupyter Notebook only.

## Evaluation
We would evaluate your result using perceptive measurement.

### Further help
You may discuss in this task","","Image Filtering","Image Filtering for Cutlery","11/08/2020 00:00:00","1"
"4574","821885","1541942","05/31/2021 04:45:16","## Task Details
You need to apply image enhancement tasks for these three images:
1. For cameramen.png. Apply a noise using Gaussian Noise and use any filtering to improve its image quality.
2. For pollen_low_contrast.jpg. You need to apply a filter to improve its visibility.
3. For dog_dark.jpg. You need to apply a transformation filter to tackle the darker area of the dog image. 

## Expected Submission
You should submit your work using Kaggle Notebook.

## Evaluation
Every task is evaluated using Rubric Holistic (RH1). See RPKPS

### Further help
If you need any help, please start a Discussion","","Image Enhancement & Image Smoothing","","07/10/2021 00:00:00","1"
"4586","821885","1541942","06/01/2021 03:04:01","## Task Details
1. Do Line Detection for image wdg3.png
2. Do Edge Detection for cameraman.png
3. Do global thresholding on image1.png
4. Do local thresholding on image1.png

## Expected Submission
- Every task worth 25 points
- Your work should contain 4 functions, at least:
  a. DetectEdge()
  b. DetectLine()
  c. ThresholdGlobal()
  d. ThresholdLocal()

## Evaluation
Read Rubric Holistic in RPKPS

### Further help
If you find any difficulties, open a Discussion","","Image Segmentation","","","1"
"1493","801165","1541942","07/27/2020 08:00:41","## Task Details
Tulisan tangan pada manuskrip tua cenderung nampak kusam dan hampir sulit dibaca dikarenakan memudarnya warna dari kertas dan bercampur dengan tinta.

## Expected Submission
Tantangan ini bertujuan untuk memunculkan tulisan tangan saja tanpa adanya noda-noda dari kertas

## Evaluation
Semakin kurang noise, semakin bagus","","Handwritten challenge","Show me only the handwritten text","11/04/2020 00:00:00","8"
"1669","801165","1541942","08/11/2020 07:38:01","## Task Details
Tantangan operasi local adaptive terhadap handwritten text

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Local Adaptive Challenge","Tantangan operasi local adaptive terhadap handwritten text","11/04/2020 00:00:00","1"
"1518","804703","1541942","07/29/2020 06:10:32","Ada dua buah dataset, play_tennis_train.csv dan play_tennis_test.csv. 
Tugasnya:
1. Silakan menghitung probabilitas data play_tennis_train.csv
2. Silakan lakukan prediksi terhadap data play_tennis_test.csv
3. Hitung nilai akurasi prediksi kalian.

Silakan membuat klasifikasi dengan metode Naive Bayes.","","Model Probabilitas dengan Naive Bayes","Python-based task","11/04/2020 00:00:00","2"
"2957","804703","1541942","12/16/2020 00:08:08","## Task Details
Lakukan training pada dataset Main Tenis dengan algoritma Decision Tree

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Decision Tree","","12/17/2020 23:59:00","1"
"2958","804703","1541942","12/16/2020 02:40:06","## Task Details
Lakukan training pada dataset Main Tenis dengan algoritma Naive Bayes
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Support Vector Machine (SVM)","Lakukan training pada dataset Main Tenis dengan algoritma Support Vector Machine (SVM)","12/17/2020 23:59:00","1"
"2959","804703","1541942","12/16/2020 02:40:45","## Task Details

Lakukan training pada dataset Main Tenis dengan algoritma KNN

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","K-Nearest Neighbour (KNN)","Lakukan training pada dataset Main Tenis dengan algoritma KNN","12/17/2020 23:59:00","0"
"2201","870814","1546745","09/23/2020 00:34:07","## Task Details
The goal of this task is to compare and train a Linear Regressor and a MLP Regressor to forecast the traffic volume in the Nova Dutra BR-116 highway for the next two years (2020 and 2021).

## Task Notebook
Starter Notebook:
https://www.kaggle.com/marcosroriz/starter-notebook  

## Expected Submission
The submission should be a Notebook containing the code for training and forecasting the traffic volume for the next two years (2020-2021). 

It should also includes a comparison between a Linear Regressor and a MLP Regressor _w.r.t._ to accuracy in the training and test data.","","Forecast the Traffic Volume in Presidente Dutra's Toll Plazas","","","0"
"1307","693124","1559551","07/09/2020 16:00:31","This dataset set contains 30 images from 10 different human organs/tissues (3 image tiles for each organ/tissue). In order to have a fair comparison between segmentation models, please, divide the dataset to 10 folds (each fold should contain the images from just one organ/tissue). Then perform a 10-fold cross-validation to report the instance segmentation results based on the average dice score, average aggregate Jaccard index (AJI) and average PQ score as shown in the below table (upon new implementation the table will be updated).
A straightforward method to create the aforementioned 10 folds can be found in this public kernel.
https://www.kaggle.com/ipateam/u-net-with-binary-labels


|  method|  average Dice score (%)| average AJI (%)| average PQ(%)|
| --- |:---: | :---: |:---: |
|  standard U-Net without post-processing [(link)](https://www.kaggle.com/ipateam/u-net-with-binary-labels)| 79.4 |38.8| 36.7|
|  standard U-Net with watershed post-processing [(link)](https://www.kaggle.com/ipateam/u-net-with-binary-labels)| 79.3 | 47.8| 40.4|
|  Distance U-Net| 74.7 | 48.6| 37.5| 
|  two-stage U-Net [(link)](https://arxiv.org/abs/2101.00442)| 80.3 | 52.5| 47.7|","","Comparison of different segmentation models","based on average dice and AJI scores","","5"
"785","610140","1565395","04/19/2020 14:58:27","## Task Details
The complexity of StyleGan2 is not negligible whatsoever. I want to test whether a simpler GAN could achieve the same results by repetitively training over the generated fake data.

## Expected Submission
A simplistic and redundant GAN with results rivaling StyleGan2.

## Evaluation
The simpler the code, the better. Performance is the added bonus.","","Create a Exhaustive GAN to test the limits of Reproducibility","Train repeatedly on the generated fake data and create a simpler model than StyleGan2","","0"
"934","663605","1568725","05/19/2020 22:15:28","## Task Details
Create a LSTM capable of taking a pair and interval and predicting the current pair price

## Expected Submission
Make a kernel that predicts the next 24 hours of prices in whatever interval you want.

## Evaluation
Accuracy is based on the distance between the predicted price and the actual price.","","Train LSTM to predict market price.","","","0"
"2003","862527","1575265","09/07/2020 16:22:21","#Prediction of Christopher Nolan Script

## Task Details
Based on the past Movie Scripts of Christopher Nolans, Predict the Upcoming film's script based on his Directions Style and Theme

## Expected Submission
A Short Script of the Movie","","Predict the Scripts of Christopher Nolan Upcoming Movies","","","1"
"2108","873061","1575265","09/15/2020 13:42:46","## Task Details
This Task is to get you started in Training All the scripts to make the model generate its own** Dialogue and Screenplay** for a new **FRIENDS Episode**.

## Expected Submission
The Submission file should just be a Text File which contains the **Screenplay and Script** of the **New Episode**","","Directing a New FRIENDS Episode using Deep Learning","","","2"
"1114","631978","1576525","06/11/2020 14:29:45","## Task Details
Use owl-with-odds.csv to predict the matches in upcoming-with-odds.csv

## Expected Submission
A notebook with a trained model and a set of predictions.

## Evaluation
A good solution will predict the most winners.","","Predict Upcoming Matches","","","0"
"4905","731322","1576525","06/24/2021 12:03:49","## Overview
Before every UFC event there will be a bet prediction contest.  All submissions will be stored here.  Just make sure your submission has the date or name of the event in the title

## Current Event Contest
Name:  UFC 265: Lewis vs Gane
Date: 2021-08-07

## Task Details
The goal of this task is generate a list of fighter win probabilities that translates to profitable bets.

## Expected Submission
task-dummy.csv is the template of what is expected.  Simply replace`R_prob` and `B_prob` with the probabilities you think each fighter has of winning and upload that dataset.  The dummy dataset should already be uploaded as an example.

OR  

Feel free to submit a notebook that creates the properly formatted csv.  You can see how to do that here: https://www.kaggle.com/mdabbert/tutorial-prepare-a-prediction-task-submission


## Evaluation
Bets to be placed will be determined by the fighter's submitted probability and their closing betting odds.  Any bet that has a positive expected value will have a 100 unit bet placed on it.  The best submission is the submission who has the greatest return.  I created a kernel showing how expected value can be determined based off of odds and probabilities here: https://www.kaggle.com/mdabbert/tutorial-train-a-model-to-make-bet-predictions.

After the fights I will create a kernel evaluating every submission.  As an example,   here are the results for UFC 251:  https://www.kaggle.com/mdabbert/ufc-251-bet-prediction-contest-results

## Tutorial
Here is a step-by-step tutorial on how to create a submission by training a model: https://www.kaggle.com/mdabbert/tutorial-prepare-a-prediction-task-submission

## Tips
* If you want to just pick winners there is nothing stopping you from editing `task-dummy.csv` to have your choice of winner have a probability of `1` and the other fighter having a probability of `0`.  This will guarantee a bet on your chosen fighter for that match.

Submissions are due before the first fight starts.","","UFC Betting Contests - Ongoing - Put all Submissions Here","All future contest submissions from 2021-06-21 will be kept here","","2"
"1195","605991","1591620","06/23/2020 19:55:20","## Task Details
Based on the data available, is it feasible to predict which patients will need intensive care unit support?

## Persona
The aim is to provide tertiary and quarternary hospitals with the most accurate answer, so ICU resources can be arranged or patient transfer can be scheduled.

## Evaluation
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation.
Best submissions will be invited to lecture their approach to a group of experts and Data Intelligence Team at S√≠rio-Liban√™s.","","Predict admission to the ICU of confirmed COVID-19 cases","","","38"
"1196","605991","1591620","06/23/2020 19:56:40","## Task Details
Based on limited data (Vital Signs and Demographics), is it feasible to predict which patients will need intensive care unit support?

## Persona
The aim is to provide local and temporary hospitals a good enough answer, so frontline physicians can safely discharge and remotely follow up with these patients

## Evaluation
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation.
Best submissions will be invited to lecture their approach to a group of experts and Data Intelligence Team at S√≠rio-Liban√™s.","","Predict NOT admission to the ICU of confirmed COVID-19 cases - Vital Signs and Demographics","","","7"
"2925","722824","1600267","12/13/2020 08:27:56","## Task Details
Since this is all environmental monitoring data pertaining to various environmental monitoring stations, inferences can be drawn out regarding the behaviour of various parameters over a period of time.

## Expected Submission
Time-series plots for all parameters either station wise or parameter wise for all stations could help bring out inferences with regard to various aspects such as 
- Periodic trends of a particular parameter in a particular region
- Heat-maps generated with `geopandas` or similar geotagging packages could help analyse the trend/severity over different regions in the entire country (also called spatio-temporal analysis)
  - Up time and downtime analysis or faulty data in juxtaposition with natural events.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
- Plots that speak for themselves, i.e., self explanatory are important for a submission to standout.","","Perform Exploratory Analysis on CPCB Data","","","0"
"1315","760081","1604179","07/10/2020 12:55:19","Verify that the data has internal coherence.","","Analyse the data to ensure clean information","","","0"
"2916","760081","1604179","12/12/2020 14:44:48","## Task Details
The Dataset currently is optimized for database use in 3 separate table.

## Expected Submission
Create a de normalized versio. With all the required information in one table.

## Evaluation
A table with the same information in a de normalized versio.","","Denormalize dataset","","","0"
"465","519408","1630013","02/20/2020 04:56:40","## Task Details
data visualization","","data visualization","","","1"
"1362","766630","1636665","07/14/2020 12:43:19","BART data updates every month so new data become available for the most recent year. Would be great to automate this fetch instead of manually updating data every month","","Auto update","Automatically run associated notebook monthly to get updated month.","","0"
"1874","849487","1662335","08/29/2020 09:46:11","## Task Details
Find out the top 3 groups to join for data scientists
1. for taking help from others
2. for promoting their companies
3. for promoting their youtube channels
4. for promoting their digital content
5. for fun

## Expected Submission
1.Results
2. Visual results

## Evaluation
1.How can the group find a balance for people seeing help from others and people promoting their content.

### Further help","","Best groups to Join on Social Network for Data Scientists","","11/30/2020 23:59:00","0"
"1487","657922","1671177","07/26/2020 08:18:30","## Task Details
Corona has changed a lot and stock market also took a different direction. Use the available dataset to find out which Indian stocks have changed direction. Example, you can see that medical related stocks have suddenly gained and Banking stocks went down.","","Horizon shift","","","0"
"447","510835","3537960","02/13/2020 02:52:17","1.how to load data","","learn use the data","learn","02/14/2020 00:00:00","0"
"743","598679","1684288","04/13/2020 00:55:12","Âú®Â∑≤Ëé∑ÂèñÊâÄÊúâÂ∞èÁªÑidÁöÑÂü∫Á°Ä‰∏äÔºåÁà¨ÂèñÊâÄÊúâÂ∏ñÂ≠êÁöÑÈìæÊé•","","Get links of all topics","","","0"
"907","654714","1688884","05/15/2020 02:10:18","## Task Details
The Data Contains Attendance count in various tournaments . can you capture a trend ? 
## Expected Submission
 It is expected to submit notebook with visualizations and analysis results .","","Attendance Analysis","","","0"
"912","654714","1688884","05/15/2020 18:08:47","## Task Details
Try to predict who will win the next africa cup of nations .","","Africa Cup of Nations 2021 Winner Prediction","","","1"
"913","654714","1688884","05/15/2020 18:11:00","## Task Details
Based on the historical data given , can you predict the results of future matches between africa teams ?","","Match results prediction","","","0"
"2253","894296","1715390","09/26/2020 16:43:39","## Task Details
Build segmentation models that accurately and efficiently segment the various classes in the dataset.

## Evaluation
Models are evaluated based on their test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Semantic Segmentation for Scene Understanding","","12/30/2021 23:59:00","2"
"2262","895594","1715390","09/27/2020 11:20:45","## Task Details
Build segmentation models that accurately and efficiently parse various cloth categories in the dataset.

## Evaluation
Models are evaluated based on their test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Semantic Segmentation for Cloth Parsing","","12/30/2020 23:59:00","4"
"2244","893591","1715390","09/26/2020 04:26:57","## Task Details
Build segmentation models that accurately and efficiently segment roads from aerial imagery data. 

## Evaluation
Models are evaluated based on their test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Road Segmentation","Segment Roads from Aerial Imagery","12/31/2021 23:59:00","3"
"2231","892049","1715390","09/25/2020 09:59:19","## Task Details
Build segmentation models that accurately and efficiently segment buildings from Aerial Imagery Data. 

## Evaluation
Models are evaluated based on their Test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Building Segmentation","Segment Buildings from Aerial Imagery","12/31/2021 23:59:00","3"
"2148","879009","1747727","09/18/2020 10:01:48","Perform EDA, Statistics and NLP on IMDB Movies Analysis","","IMDB Movies Analysis","EDA, Statistics and NLP","","1"
"2151","879393","1747727","09/18/2020 13:13:49","Perform Exploratory Data Analysis","","International Astronaut Database","Analysis of International Astronaut Database","","0"
"2070","869636","1747727","09/12/2020 08:21:07","Perform Exploratory Data Analysis such as:

1. Which country has won the most gold medals?
2. Which athlete has won the most gold medals?
3. Which country had the most difference in their summer and winter gold medal counts?
4. Relationship between country and medals won
5. Relationship between events and won

and more such questions that you can think of!","","Exploratory Data Analysis","","","0"
"5908","805924","7409740","08/29/2021 09:54:22","Use the following dataset for the classification of sports images.
This dataset contains 22 categories from boxing, football to table tennis.","","Sports Image Classification","","01/29/2023 23:59:00","0"
"1858","848522","1761512","08/28/2020 12:35:24","Do basic EDA not much aware of NLP usage in this case so please come forward.","","No. of languages used by PM","","","0"
"1862","848522","1761512","08/28/2020 15:32:09","## Task Details
Being PM he wishes citizens,sometimes appeal people and other times launches a campaign

## Expected Submission
You may have to do some labelling on the kind of tweets PM does

## Evaluation
It would be good to have a scalable solution so that we just put in the data and know what the tweet is all about.","","Categorise the tweets in major categories","","","1"
"1654","824049","1761512","08/10/2020 09:53:53","## Task Details
Patreon usage is rising among people so that creative work can be continued despite low revenues from social media platforms so the people directly ask for support. It would be encouraging for people to have some monetary support.

PS: Few Kagglers like Krish Naik also use Patreon, was surprised to see just 5 patreon supporters.

## Expected Submission
The relation between no. of supporters and days of the campaign.","","Explore the data set","","","1"
"945","661950","1772071","05/21/2020 20:16:20","## Task Details
Villagers have favourite colours and styles. Can you come up with a sensible interior design recommendation for each villager's room? Items you can recommend include furniture, wallpaper, flooring, rugs, wall-mounted items.","","Recommend an interior design setup for each villager","","","72"
"946","661950","1772071","05/21/2020 21:20:46","## Task Details
Check out the recipes.csv list. Given a set of items as an input, what recipes can you craft? To take it a step further, what recipes are you close to crafting, and what items are left?","","Given a set of items, what DIY recipes can you make or are close to making?","","","33"
"940","665490","1775970","05/20/2020 18:40:18","Together we create a great dataset of dialogues of movies in Spanish.

**English**
Please collaborate and help create a great corpus of series and movies in Spanish. We will start by focusing the domain on police series and movies (to achieve higher quality in that domain). It is recommended to incorporate sagas or complete series to have great content on the same context and not a multitude of  films (independents) without a common context.

**Spanish**
Por favor, colabora y ayuda a crear un gran corpus de series y peliculas en espa√±ol. Comezaremos por centrar el dominio a series y peliculas policiaca (para conseguir mayor calidad en ese dominio). Se recomienda incorporara sagas o series completas para tener gran contenido sobre el mismo contexto y no multitud de peliculas (independientes) sin contexto comun.","","Collaborative Dataset","Together we create a great dataset of dialogues of series&movies in Spanish","","0"
"698","588154","1776506","04/05/2020 22:33:01","## Task Details
Which new insights can we extract to reinvent our lifestyles and optimize our individual wellbeing?
What do we learn from this dataset about the way we live our lives?
How do the attributes change by gender, age group and over time?

## Expected Submission
Publish your analysis and graphs in a notebook, preferably Python

## Evaluation
Truly new insights into our lifestyle and most votes from Kaggle users","","Can you extract new insights with EDA and visualization?","","","1"
"699","588154","1776506","04/05/2020 22:39:36","## Task Details
What are the strongest correlations between the various dimensions?
What are the best predictors of stress, personal productivity or personal achievements?
Which couple of attributes could best define a balanced life?

## Expected Submission
Notebook with graphs and result interpretation

## Evaluation
Brand new insights into the way we live our lives and most votes from Kaggle users","","Can you predict stress levels or personal achievements with ML?","","","1"
"1793","841289","1782098","08/22/2020 16:51:13","## Task Details
In the Getting started notebook , I tried very shallow network and a pretrained model. Not the deeper network. So try using the deeper network, dropout, Batch normalization

## Expected Submission
1. Add More layers and see how the validation accuracy improves.
2. Apply Batch normalization, dropout and evaluate the new results.
3. Use image augmentation or add external images and see how it helps to improve validation acuracy

## Evaluation
Higher the validation accuracy or lower validation error rate is a good place to start.

### Further help
If you need additional inspiration, check out these existing notebook for reference:

https://www.kaggle.com/kedarsai/getting-started-with-bird-species-classification","","Explore the results with Deeper networks","","","0"
"1310","766729","1788308","07/10/2020 01:10:17","## Task Details
Predict COVID-19

## Expected Submission
Use notebooks

## Evaluation
Try to find proper evaluation metrics

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","COVID-19 prediction for New York","","01/01/2021 00:00:00","1"
"1318","768471","1788308","07/10/2020 21:42:39","## Task Details
Regression and classification of quantities relevant for particle physics phenomenology.

## Evaluation
Look for metrics etc.","","Regression and Classificaiton with with Particle Physics phenomenological data","","01/01/2022 00:00:00","4"
"1327","769952","1788308","07/11/2020 17:34:41","## Task Details
Predict life expectancy based in historical data

## Expected Submission


## Evaluation
Chose metrics for forecasting the life expectancy","","Predicting life expectancy","","01/01/2023 00:00:00","1"
"510","536369","4543405","03/05/2020 12:33:07","DF","","Knife df","","","2"
"877","536369","3303517","05/12/2020 16:28:18","## Task Details
Create a GAN which generates new knife images.

## Expected Submission
A good solution should be a detailed notebook with some example images.

## Evaluation
The goal would be the create realistic images, with a secondary target being interesting or creative new knives.","","Knife GAN","","","1"
"1341","771947","1798416","07/12/2020 19:30:12","The main task: you are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.

Please see detail in the competition ""[Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/)""","","Real or Not? NLP with partially cleared Disaster Tweets","Predict which partially cleared Tweets are about real disasters and which ones are not","","1"
"1565","806968","1798416","08/02/2020 21:37:29","## Task Details
For each holiday in each country, it is important to select the optimal values ‚Äã‚Äãfor each of the parameters of Prophet models in order to most correctly take into account the effect of holidays on the overall statistics of COVID-19

## Expected Submission
Should solve the task primarily using and submitting Notebooks with Prophet models with holidays for COVID-19 forecasting from this dataset for different countries

## Evaluation
WAPE (Weighted Mean Average Percentage Error) of the coronavirus confirmed cases forecasting for the last 14 (or 21) days

### Further help
If you need additional inspiration, please see for my notebooks in this dataset.","","Optimization of holidays parameters in Prophet models for COVID-19","Optimization ""lower_window"", ""upper_window"", ""prior_scale""","","1"
"3153","806968","3412061","01/08/2021 06:31:56","## Task Details
Use an autoregressive integrated moving average (ARIMA), cubist
regression (CUBIST), random forest (RF), ridge regression (RIDGE), support vector regression (SVR), and stacking-ensemble learning are evaluated in the task of time series forecasting with one, three, and six-days ahead of the COVID-19 cumulative confirmed cases in ten Brazilian states with a high daily incidence. 
Also, plot the Box-plot for absolute error according to model and state for COVID-19 forecasting.

I need this work for research purposes. It can be done by either python or R","","Time series forecasting COVID-19 using ARIMA","Short-term forecasting COVID-19 cumulative confirmed cases in brazil","01/17/2021 23:59:00","2"
"1331","770389","1807042","07/12/2020 00:18:38","A car's price can be related to many features like its model, engine capacity and so forth. Which feature influences a cars price in Pakistan the most?","","What type of features influence a Car's price?","","","1"
"1332","770389","1807042","07/12/2020 00:19:34","Develope a Used Car Price Predictor based on the Pakistani market Data.","","Used Car price Prediction for Pakistani Cars","","","2"
"1863","848987","1815526","08/28/2020 18:47:37","Suppose you have been hired as Data Scientist of HealthMan ‚Äì a not for profit organization dedicated to manage the functioning of Hospitals in a professional and optimal manner.

## Task

The task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.","","Predict length of stay for the patient in Covid crisis","Metric: Accuracy","","1"
"2051","868764","1815526","09/11/2020 14:41:36","## Task Details
Fake news, defined by the New York Times as ‚Äúa made-up story with an intention to deceive‚Äù, often for a secondary gain, is arguably one of the most serious challenges facing the news industry today. In a December Pew Research poll, 64% of US adults said that ‚Äúmade-up news‚Äù has caused a ‚Äúgreat deal of confusion‚Äù about the facts of current events

In this hackathon, your goal as a data scientist is to create an NLP model, to combat fake content problems. We believe that these AI technologies hold promise for significantly automating parts of the procedure human fact-checkers use today to determine if a story is real or a hoax.

## Expected Submission
How to Generate a valid Submission File
Sklearn models support the predic_proba() method to generate the probabilities for every class.

You should submit a .csv/.xlsx file with exactly 1267 rows with 6 columns (one column per class). Your submission will return an Invalid Score if you have extra columns or rows.

The file should have exactly 6 (0-5) columns:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1815526%2F7557ddab5a29f9f52ec1c65c59fef3f3%2Fdownload.png?generation=1599835390235722&alt=media)


## Evaluation

What is the Metric In this competition? How is the Leaderboard Calculated?

The submission will be evaluated using the Log Loss metric. One can use sklearn.metric.log_loss to calculate the same
- This hackathon supports private and public leaderboards
- The public leaderboard is evaluated on 30% of Test data
- The private leaderboard will be made available at the end of the hackathon which will be evaluated on 100% Test data","","Fake News Content Detection","NLP, Sentiment Analysis using TF-IDF, CountVectorizer, Transformers, BERT","","3"
"2055","869050","1815526","09/11/2020 18:57:14","## Task Details

Your client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.

For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.

Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‚Äòsum assured‚Äô) to the customer.

Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue. 

Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.

## Evaluation Metric
The evaluation metric for this hackathon is ROC_AUC score.

## Public and Private split
The public leaderboard is based on 40% of test data, while final rank would be decided on remaining 60% of test data (which is private leaderboard)

## Guidelines for Final Submission
Please ensure that your final submission includes the following:

1. Solution file containing the predicted response of the customer (Probability of response 1)
2. Cross-sell PredictionCross-sell PredictionCode file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission","","Health Insurance Cross Sell Prediction","Cross-sell PredictionPredict Health Insurance Owners' who will be interested in Vehicle Insurance","","83"
"1944","858120","1815526","09/04/2020 15:48:15","## What is the Metric In this competition? How is the Leaderboard Calculated ??

- The submission will be evaluated using the Log Loss metric. One can use sklearn.metric.log_loss to calculate the same
- This hackathon supports private and public leaderboards
- The public leaderboard is evaluated on 30% of Test data
- The private leaderboard will be made available at the end of the hackathon which will be evaluated on 100% Test data
 
## How to Generate a valid Submission File
- Sklearn models support the predic_proba() method to generate the probabilities for every class.

# The file should have exactly 4 (0-3) columns:","","NLP - Product Sentiment Classification","Predict probabilities for product sentiments","","0"
"1904","852455","1815526","08/31/2020 13:58:45","Given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.","","Identify the Sentiments","Evaluation Metric: Weighted F1-Score","","0"
"2304","902117","1815526","10/01/2020 19:52:00","# House Price Prediction Challenge

## Overview

Welcome to the House Price Prediction Challenge, you will test your regression skills by designing an algorithm to accurately predict the house prices in India. Accurately predicting house prices can be a daunting task. The buyers are just not concerned about the size(square feet) of the house and there are various other factors that play a key role to decide the price of a house/property. It can be extremely difficult to figure out the right set of attributes that are contributing to understanding the buyer's behavior as such. This dataset has been collected across various property aggregators across India.  In this competition, provided the 12 influencing factors your role as a data scientist is to predict the prices as accurately as possible.

Also, in this competition, you will get a lot of room for feature engineering and mastering advanced regression techniques such as Random Forest, Deep Neural Nets, and various other ensembling techniques. 

## Data Description:
Train.csv - 29451 rows x 12 columns
Test.csv - 68720 rows x 11 columns
Sample Submission - Acceptable submission format. (.csv/.xlsx file with 68720 rows) 

## Attributes Description:
|Column  |  Description|
| --- | --- |
|POSTED_BY| Category marking who has listed the property|
|UNDER_CONSTRUCTION | Under Construction or Not|
|RERA | Rera approved or Not|
|BHK_NO | Number of Rooms|
|BHK_OR_RK | Type of property|
|SQUARE_FT | Total area of the house in square feet|
|READY_TO_MOVE|  Category marking Ready to move or Not|
|RESALE | Category marking Resale or not|
|ADDRESS | Address of the property|
|LONGITUDE | Longitude of the property|
|LATITUDE | Latitude of the property|
 
## Evaluation

What is the Metric In this competition? How is the Leaderboard Calculated ??
The submission will be evaluated using the RMSLE (Root Mean Squared Logarithmic Error) metric. One can use np.sqrt(mean_squared_log_error( actual, predicted))","","Predict the house prices in India","","","36"
"2797","902117","4781892","11/27/2020 12:17:16","Predict the house price based on the given two datasets as training and testing data using linear regression.","","House Price Prediction","","11/28/2020 23:59:00","1"
"2310","903283","3885917","10/02/2020 15:13:02","## Task Details
To create a question-answering chatbot for Mental Health.","","FAQ Chatbot","","","1"
"2284","899261","1817124","09/30/2020 03:04:30","## Task Details
Generate automated commentary for Cricket based on score (Four, Six or OUT). This dataset provides you the history of IPL highlights commentary from Cricbuzz.

### Further help
Basic starter notebook
- https://www.kaggle.com/narendrageek/starter-commentary-ipl-cricket-cricbuzz/","","Generate Cricket Commentary IPL","","","0"
"2199","888425","1817124","09/22/2020 17:16:52","## Task Details
Predict the Gold rate in India. The dataset contains from 2006 to Sep 2020.","","Forecast the Gold price in near Future in India (Chennai)","Predict the Gold rate","","1"
"2312","903531","1834449","10/02/2020 18:52:13","This is a supervised classification problem wherein you have to predict the value of is_bad. 0 value of is_bad indicates loan is good loan and 1 indicates loan is bad loan.

## Expected Submission
Perform EDA and also compare the result of the different models.
If model is performing poor then list the reason of the same.","","Predict default of the loan","","","0"
"917","652179","1834449","05/16/2020 15:34:46","## Task Details

One of the challenge for all Pharmaceutical companies is to understand the persistency of drug as per the physician prescription. 

With an objective to gather insights on the factors that are impacting the persistency, build a classification for the given dataset. 


Target Variable: Persistency_Flag
Variable description is attached along with the data.

Model Evaluation:
Following are the things we are expecting in the submission:
1.	R/Python code (executable) used for the analysis (with proper comments and readability). If it‚Äôs a Jupyter notebook with all the results in it, that will be best!
2.	Model diagnostics to be updated in the attached excel template (Excel File: Analysis Results)
3.	Final Analytical (processed) dataset used, which includes the additional derived variables and any other processing applied
4.	Attach a document along with brief description of following in the mail:
a.	Changes done in the analytical dataset provided
b.	Any other highlights about the process you followed to ensure a thorough evaluation","","Drug persistency Classification","Task","07/22/2020 00:00:00","0"
"1007","682306","1867059","05/29/2020 16:36:07","Read the data from the JSON file.","","Read_Data","","12/31/2021 00:00:00","0"
"463","519086","1895217","02/19/2020 11:57:59","## Task Details
Plot temperature and light with respect to the given timestamp. 

## Expected Submission
A plot that includes temperature and light with the rigth time 

## Evaluation
A good solution is to make this plot without doing to much ""manual work"" on the dataset","","Plot temperature and light with respect to time","Plot temperature and light with respect to timestamp given in the datasets","03/01/2020 00:00:00","1"
"466","519086","1895217","02/21/2020 10:01:48","## Task Details

Plot temperature and light with respect to the given timestamp in R.
Feel free to use [this notebook ](https://www.kaggle.com/bjoernjostein/time-stamp-problem)as your starting code

## Expected Submission

A plot that includes temperature and light with the right time

## Evaluation

A good solution is to make this plot without doing to much ""manual work"" on the dataset","","in R: Plot temp. and light with respect to time","Plot temperature and light with respect to timestamp given in the datasets","02/28/2020 00:00:00","0"
"4299","870546","1895217","05/07/2021 13:09:45","## Task Details
Add new images to the data set

## Expected Submission
20 new images in total. 
10 new images of mosquito bites and 10 new images of tick bites

## Evaluation
A good submission include 20 high quality images","","Adding 2 x 10 images","Add 10 new images of mosquito bites and 10 new images of tick bites","","0"
"723","594355","1904664","04/09/2020 17:57:11","## Task Details
###1.  Feature Transformation

- Transform categorical values into numerical values (discrete)

###2.  Exploratory data analysis of different factors of the dataset.

###3.  Additional Feature Engineering

- You will check the correlation between features and will drop those features which have a strong correlation

- This will help reduce the number of features and will leave you with the most relevant features

###4.  Modeling

- After applying EDA and feature engineering, you are now ready to build the predictive models


## Expected Submission
**Analysis to be done:** Perform data preprocessing and build a deep learning prediction model. 
## Evaluation
Evaluation metrics is Sensitivity","","Problem Statement","Create a model that predicts whether or not a loan will be default using the historical data.","","0"
"1313","613045","1904664","07/10/2020 05:43:19","# Following goals achieved from the Housing rent dataset:
  - **Merge two dataset i.e  history-rental-df.csv and address.xlsx and create a master dataset?**
  - **Data is valid only when r-model-check == 1?**
  - **Perfomed EDA to find meaningful information ?**  
  - **Find out the most important features for the dataset?**
  - **Predict the models to find the rent of the house?**

# Evaluation
**The Evalution metrics is Accuracy**","","Problem Statement","Following pointers will be helpful to structure your findings.","09/20/2020 00:00:00","0"
"2081","870850","1929307","09/13/2020 04:59:11","Use this data for entity recognition of geography location in text data","","Address Parsing","","","0"
"2192","887504","1929307","09/22/2020 06:40:24","We have 12 years of day wise weather attributes of Estespark area

Average temperature (¬∞F)
Average humidity (%)
Average dewpoint (¬∞F)
Average barometer (in)
Average windspeed (mph)
Average gustspeed (mph)
Average direction (¬∞deg)
Rainfall for month (in)
Rainfall for year (in)
Maximum rain per minute
Maximum temperature (¬∞F)
Minimum temperature (¬∞F)
Maximum humidity (%)
Minimum humidity (%)
Maximum pressure
Minimum pressure
Maximum windspeed (mph)
Maximum gust speed (mph)
Maximum heat index (¬∞F)","","weather analysis and forecasting","temperature trend analysis","","0"
"1251","635428","1956324","07/03/2020 18:20:43","## Task Details
Train a U-net using this dataset and evaluate the results.

## Expected Submission
Predictions for the validation and test set. Reporting the Mean IoU score for both is sufficient.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
@abhishek has a nice introduction to building a U-Net from scratch using PyTorch:
https://www.youtube.com/watch?v=u1loyDCoGbE","","Train a simple U-Net and evaluate results","","","0"
"615","560422","1962508","03/21/2020 18:10:19","## Task Details
We want to link Corona spread in Pakistan with rest of the world.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","To add a world wide dataset for tracking spread of Corona","","","0"
"2270","822806","1991843","09/28/2020 06:59:34","## Expected Submission
What words are used the most?
What is the mood of the whole statement?
Is it a positive, neutral or negative statement?","","Perform NLP analysis of various news in one csv","","","0"
"2116","821741","1991843","09/16/2020 04:29:37","## Task Details
- Perform Basic EDA
- Using this dataset can we predict the crime rates in 2019 and 2020?","","Basic Analysis","","","4"
"2172","884376","1991843","09/20/2020 14:02:00","## Task Details
- Explore names and gender data
- Perfrom quick Exploratory data analysis
- Use phonic information from names to predict the gender","","Use Phonic Information from Names to Predict Gender","","","0"
"2106","874443","1991843","09/15/2020 09:46:24","## Task Details
This is the first basic task to explore the data

## Expected Submission
- Let's check how many movies are streaming on a particular platform?
- What is the average rating?
- How many web series are there in every genre according to the streaming platform?","","Exploratory Data Analysis","","","1"
"2077","870043","1991843","09/12/2020 13:53:39","## Task Details
- Perform a basic EDA
- Price and Rating Analysis
- Is there any link between books of the same authors?","","Exploratory Data Analysis","","","0"
"1549","809930","1995670","08/01/2020 06:33:37","## Task Details
Analyze the data and perform data cleaning, EDA, modelling

## Expected Submission
EDA,ML,DP

## Evaluation
EDA and Model Building

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analyze the dataset","analyze the data and perform regression operation for salary","01/03/2021 00:00:00","0"
"2047","868255","1998401","09/11/2020 08:15:25","## Task Details
User can Find what kind of personality a person is and Is the person is sleepy, athlete.
What about this heart beat?

## Expected Submission
User Can submit the Chart based analysis

## Evaluation
There is no right answer for data analysis. üòè","","What Type of Person is that?","","","1"
"1035","689659","2002171","06/02/2020 18:05:22","**ML Coding Exercise: 
**
5 Robots, named quite unexpectedly **0,1,2,3,4 **are having a uniform conversation, where each of them spits out a series of 10 numbers at a time in a round-robin fashion. The task is to train a model which can predict the robot when given the 10 numbers spoken by it, with a good accuracy. A log of a long conversation between these 5 robots has been given, this is your datasets. 
**A snippet of their conversation: **

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2002171%2F8909cf6fcd17f7bee667e0302b004a72%2FCapture.PNG?generation=1591120849434732&alt=media)


**Help Notes:**
1. It is a classification problem.
2. The file has 500001 lines.
3. The first column is source. This column gives us the label for every row of entries. The label can have value - 0,1,2,3,4. So there are 5 possible labels (the five robots.)
4. The features here are the sequence of 10 numbers. For each row it is num1, num2, num3, num4, num5, num6, num7, num8, num9, num10. Thus 10 numbers. 
5. In this classification problem your input while testing/validating your model with be a sequence of 10 numbers - i.e. any row from the datasets (without the first column) and the output will be predicted source having potential values - 0,1,2,3,4 (which will mostly be one hot encoded making them like 10000,01000,00100,00010,00001)
6. You will be training and testing your model on not a single input sequence, but a train of inputs, traditionally the x-train. And your labels (the first column here) would sit in a y-train. 
7. Also, the dataset is big, don‚Äôt try to use all data, sample it out. Make training, test and validation trains out of it.","","Develop an ML model that predicts the robot from their sequence with a good accuracy","","","2"
"2454","876138","2012377","10/17/2020 07:17:07","## Task Details
From the dataset do some EDA and find the Monthly & Quarterly Sales and Visualize them.

## Expected Submission
Submit your notebooks and visualize your results.

## Evaluation
The task is quite a simple task and the notebook which makes good visualizations of the task is better than other.","","Find the Monthly and Quarterly Sales","","","4"
"2455","876138","2012377","10/17/2020 08:03:48","## Task Details
Which are the top 3 months in terms of overall sales

## Expected Submission
Sales Visualization and find the notebook with the months with the highest number of sales.

## Evaluation
Find the months with the highest sales.","","Top 3 Months with Highest Sales","","","4"
"2456","876138","2012377","10/17/2020 08:07:12","## Task Details
Which SKUs are outliers in terms of sales in any month?

## Expected Submission
Find the Outlier SKU.

## Evaluation
Simple task, find the Outlier SKUs and visualize them.","","Find the Outlier SKUs","","","4"
"1906","852450","2014141","08/31/2020 15:02:10","## Task Details
Task expect from you to clean and EDA.","","Exploratory Data Analysis(EDA)","","","0"
"6230","558169","8487946","10/01/2021 17:38:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. Predict

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","prediction","","","0"
"1025","652083","2030198","06/01/2020 21:03:54","## Task Details
Many have tried, but most have failed, to predict the stock market's ups and downs. Can you do any better? Time series prediction is a mysterious challenge, lets all become billionaires :)


## Expected Submission
From traditional algorithms like prophet to the most new approaches such as deep learning methods like LSTM, 1D CNNs and TCNs you are free to use to predict TSE (Tehran Securities Exchange Data) total index or the price of one of the available tickers. You may analyze data, use uni-variate or multivariate solutions, create an algorithmic trading strategy and test it using back trading on the present data.
Please kindly submit your precious notebooks to help students mostly and contribute to discussions.

## Evaluation
Please evaluate your model on the last 20 percent of data.
Final evaluation will take place on real live data (at the time of evaluation) not available in provided dataset.

### Further help
If you need additional inspiration, check out the existing kernels:
https://www.kaggle.com/mehradaria/tehran-stock-exchange/kernels

Nice Predicting ;)","","Tehran Stock Market Prediction","Predict TSE total index or one of tickers.","06/17/2021 00:00:00","0"
"1766","839008","2039603","08/20/2020 15:53:19","## Task Details
Since the ASL dataset is smaller, can you transfer knowledge from BSL to improve the classification?

## Expected Submission
Results for vanilla ASL classification compared to Transfer Learning from BSL to ASL

## Evaluation
Compare the two results, is there any change?

### Further help
We performed successful transfer learning from BSL to ASL with a neural network:
https://www.preprints.org/manuscript/202008.0209/v1","","Transfer Learn from ASL to BSL","","","0"
"2357","727820","2050099","10/07/2020 12:25:03","Build a sentiment analysis model specific for tourism reviews","","Sentiment Analysis","","","0"
"2358","727820","2050099","10/07/2020 12:27:34","Build a sentiment extraction model that extracts specific words or phrases which  actually lead to the sentiment description","","Sentiment Extraction","","","0"
"1197","735113","2050927","06/23/2020 20:15:25","##Research Question
At the time of writing, COVID-19 has spread to at least 114 countries. With viral flu, there are often geographic variations in how the disease will spread and if there are different variations of the virus in different areas. We‚Äôd like to explore what the literature and data say about this through this Task.

###Key Sub-Questions
Are there geographic variations in the rate of COVID-19 spread?
Are there geographic variations in the mortality rate of COVID-19?
Is there any evidence to suggest geographic based virus mutations?
.","","Predict whether patient have Covid or Not","Use Image segmentation for Detecting Covid","","0"
"1159","722014","2050966","06/17/2020 08:45:30","## Task Details
A marketplace data containing vast amount of behavioural user experience. Knowing the right method to decide whether the strategy of sales has already given the optimum result in deciding the price in the marketplace would be beneficial to merchant. This will give the right strategy also action to decide the best pricing of a product by seeing the marketplace behaviour.

## Expected Submission
Provide the notebook solution based on datasets containing best solution for the aim we seek.

## Evaluation
Giving a robust solution for the given aim","","Pricing Behaviour","How to decide product pricing following the marketplace data","12/31/2021 00:00:00","0"
"1141","715500","2054977","06/14/2020 15:41:24","https://www.drivendata.org/competitions/64/hateful-memes/data/
check out the link","","Meme Classification Challenge","","","0"
"1844","730170","2070357","08/27/2020 05:57:26","## Task Details
Since not everyone could participate in this competition, I thought may be I should create a task, like analyzing the effect of different augmentation of images and how well the model can differentiate the two classes.

## Expected Submission
Although I have created this task only for fun purpose and in order for people to understand the mechanism behind this image classification. I want people to submit their notebooks which will have results of different augmentations like effect of different augmentation on CV score etc. And also, I would love to get a `submission.csv` file, so that they can compare their work with the participants of that competition.

## Evaluation
The highest score on private dataset. You can make your notebook public though.
 
### Further help
I really find this competition a great place to fine-tune your image classification skills. This competition was really helpful for beginners like me.
The link to the competition is [this](https://www.kaggle.com/c/siim-isic-melanoma-classification).

If you want other data sizes, you can find them [here](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/161043#898505), low resolution images will make experimentation easier. 

Also, you are free to use any other dataset which was provided in this competition, for example, [Chris'](https://www.kaggle.com/cdeotte) data which was benchmark dataset in this competition. External data are allowed too.

Also, I am not setting any deadlines for this, I want it to continue as its only for learning and experimentation purpose.","","Analysing Melanoma and its Prediction","A task for those who may have missed this amazing competition.","","1"
"432","500992","2102373","02/04/2020 21:38:36","## Task Details
Each image in this dataset includes one or more fruit, however the number of fruits is not provided. This task is to create a table telling how many fruits are in each image. Could could be solved manually or via application of a machine learning method.

## Expected Submission
The submission should be a csv file with two columns: image path and number of fruits.

## Evaluation
A good solution would have 100% coverage of pictures and have accurate fruit count estimation.","","Annotate the number of fruits in each picture","","","13"
"594","500970","4238713","03/17/2020 07:48:24","## Task Details
This dataset contains 8732 labeled sound excerpts (&lt;=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to our paper. All excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.
*Why use CNN*
This is a very classical way of sound classification as it is observed that similar type of sounds have similar spectrogram. A spectrogram is a visual representation of the spectrum of frequencies of sound or another signal as they vary with time. And thus we can train a CNN network that takes these spectrogram images as input and using it tries to generalize patterns and hence classify them.

## Expected Submission
The Task should be solved using a self-implemented NN or a Keras Model. The Submission should consist of a single column with the classes of audio samples. You can split the complete data into Train and Test since they are not provided separately. 

## Evaluation
The model.score() on the test predictions determines whether the submission is better or not.","","Classification of audio samples.","Use CNN or a simple Neural Network to classify audio samples in their category based on features extracted using LIBROSA.","","2"
"1365","771911","2104265","07/14/2020 17:28:40","Use insights from Data_Train.csv to predict the prices of cars on Data_Test.csv","","Predict the prices of cars on the test dataset.","","","0"
"1443","790813","2114328","07/21/2020 13:35:14","## Task Details
Is it possible to predict the RUL of the machines with the existing telemetry?

## Expected Submission
Users must send the RUL forecast to the test file identifying the machine and the result.

## Evaluation
Validation will be done by the following KPIs
Accuracy
MSE
RMSE
R ^ 2 coefficient of determination

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the RUL of each machine","Is it possible to predict the RUL of the machines with the existing telemetry?","08/31/2020 00:00:00","1"
"717","592930","2122775","04/08/2020 18:18:15","Bike sharing systems are a means of renting bicycles where the process of
obtaining membership, rental, and bike return is automated via a network of
kiosk locations throughout a city. Using these systems, people are able to
rent a bike from one location and return it to a different place on an
as-needed basis. Currently, there are over 500 bike-sharing programs
around the world.
The data generated by these systems makes them attractive for
researchers because the duration of travel, departure location, arrival
location, and time elapsed is explicitly recorded. Bike sharing systems
therefore function as a sensor network, which can be used for studying
mobility in a city

**Problem Statement**
*In this project, you are asked to combine historical usage patterns with
weather data in order to forecast hourly bike rental demand.*","","Bike Sharing System","Regression","","0"
"607","545987","2124980","03/18/2020 20:33:34","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2124980%2F4de5faa22862485749d1488ca6605cd2%2F1920px-Linear_regression.svg.png?generation=1584565108445991&alt=media)

## Task Details
Train a regression model, based on the independent features in the train set.
Estimate the value of the dependent variable 'SalePrice', for houses in the test set.

## Expected Submission
Submit your predictions as a file ""submission.csv"", with the result of your regression model on the test set.
The file should have two columns: 'Order' and 'SalePrice'.

## Evaluation
Compare your results with the actual SalePrice.
The values are stored in the file ""target.csv"".","","Regression Model","Predict the Sale Price of Houses in Ames, Iowa","","1"
"1481","796860","2126622","07/25/2020 07:35:17","## Task Details
This is an image classification task. Create a model that classifies the satellite images of junctions into one of the 3 categories i.e. priority, roundabout or signal","","Create a model to classify traffic junctions images","","","0"
"1229","748494","2127141","06/30/2020 15:22:35","## Task Details
The task is all about to do image analysis and extract important information from that and building a Model with this small dataset.

## Expected Submission
The user has to submit the Notebook which contains Image Analysis and Model with Quite a Good accuracy and a detailed description of it.

## Evaluation
This is not a Competition, You can share your knowledge with this platform on this topic.

### Further help
https://www.hindawi.com/journals/trt/2012/970203/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822329/","","Do Image Analysis and Create a model....","","","1"
"1170","716741","2127141","06/18/2020 14:23:00","## Task Details
The task is all about to do Image Analysis And Model Building with this small dataset.

## Expected Submission
The user has to submit the Notebook which contains Image Analysis and Model with Quite a Good accuracy and a detailed description of it.

## Evaluation
This is not a Competition, You can share your knowledge with this platform on this topic.

### Further help
Further resources are in the dataset like a research paper, previous work etc...","","Image Analysis And Model Building...","","","1"
"1171","717006","2127141","06/18/2020 14:25:11","## Task Details
The task is all about to do Image Analysis And Model Building with this small dataset.

## Expected Submission
The user has to submit the Notebook which contains Image Analysis and Model with Quite a Good accuracy and a detailed description of it.

## Evaluation
This is not a Competition, You can share your knowledge with this platform on this topic.

### Further help
* Further resources are in the dataset like a research paper, previous work etc...
* You can take the help of my kernal on image analysis.[click here](https://www.kaggle.com/saife245/malaria-detail-analysis-on-malaria-species?scriptVersionId=36711541)","","Image Analysis And Model Building...","","","0"
"1172","698492","2127141","06/18/2020 14:27:53","## Task Details
The task is all about building a Model with this small dataset.

## Expected Submission
The user has to submit the Notebook which contains Model with Quite a Good accuracy and a detailed description of it.

## Evaluation
This is not a Competition, You can share your knowledge with this platform on this topic.

### Further help
* Further resources are in the dataset like a research paper, previous work, etc...
* You can take the help of my kernel on image analysis.[click here](https://www.kaggle.com/saife245/malaria-detail-analysis-on-malaria-species?scriptVersionId=36711541)","","Create A model....","","","0"
"1873","769740","2132407","08/29/2020 09:35:57","## Task Details
This task is first step in a long term vision to dynamically identify an object and help the user get image context based information

## Expected Submission
This is an open ended task, welcome to all the models that can identify the logos provided in the datasets.

## Evaluation
If the accuracy is above 90% then it can be considered as a successful model.","","1. Create a model to Identify the brand Logo","","","0"
"1945","858190","2171738","09/04/2020 17:30:32","## Task Details
Create a baseline of the speech recognition using the Free Spoken Digit Dataset (FSDD)

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

The notebook should be well documented and contain:

Any steps you're taking to prepare the data, including references to external data sources
Training of your model
Select Model
An evaluation of your model. Let's keep it simple and measure the accuracy in this classification task.

## Evaluation
This is not a formal competition, so won't be measure the results strictly against a given validation set using a strict metric. Rather, what everyone like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

1. **Accuracy** - How well does the model perform on the dataset?
 Can it be generalized over time? 
Can it be applied to other scenarios? 
Was it overfit?
2. **Data Preparation** - How well was the data analyzed prior to feeding it into the model? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.
3. **Documentation** - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.

The spirit of this Task is looking for approaches to building models on a spoken audio data that can serve as learning examples for our community","","Speech recognition of digits (English)","A free audio dataset of spoken digits. Think MNIST for audio.","","7"
"1891","850697","2182134","08/30/2020 07:39:48","Given the data, how well can you predict the number of Brownlow votes a player will average per game?

Submit a note book showing your method. Excited to learn the techniques required.","","Predict Brownlow Votes","What does it take to win?","","0"
"1457","782808","2198790","07/22/2020 17:41:37","## Task Details
Which players are expected to have a big change in fantasy points in 2020 compared to their 2019 performance? 
Which players are expected to score much more points in 2020 than they scored in 2019? 
Which players are expected to much less points in 2020 than they scored in 2019?","","Boom or Bust","Which players will dramatically improve or worsen their fantasy performance this year?","","0"
"1647","823221","2198790","08/09/2020 16:09:54","## Task Details
What will the scores be for all the week 24 games?

## Expected Submission
Submit score predictions.
Also please explain how you made these predictions

## Evaluation
We will see the scores after week 24 is complete.","","Predict the scores of the week 24 NBA games","","","0"
"1819","843808","2219307","08/24/2020 19:13:26","Does an easy send make a route seem better? Or are the classics sandbagged?","","Are high-quality routes given harder difficulty ratings?","","","0"
"423","497997","2226962","02/02/2020 18:28:48","## Task Details
Try segment vacancies into big groups","","Segmentation task","","","0"
"2039","566027","2236338","09/10/2020 08:44:15","Because the ministry of health of morocco announce a daily update about the Covid-19 situation in Morocco. I have to run the notebook https://www.kaggle.com/jmourad100/scrape-morocco-covid19-daily-cases each and every day after 18:30 PM GMT+1. So sometimes I forget to do this, and in this case, I have to enter the numbers late manually; So if someone interested in the Covid-19 situation can provide help by running the Scrapping notebook, That will be much helpful.","","Scrape Covid-19 Daily records","","","5"
"1587","816080","2248378","08/04/2020 14:45:46","Data Visualization","","Data Visualization","","","0"
"1333","770789","2252519","07/12/2020 06:20:53","## Basically use this data to discover how different columns define the basic economic structure of countries
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Anaylsis and Trends","","","0"
"778","607627","2252987","04/17/2020 17:49:08","## Task Details
Everyone loves a fair challenge right ?

In the leaderboard of each segment, there are people who cheated and used a motorbike or a car to participate. If you could find them, this would make a huge impact on keeping the community healthy by ensuring the fairness of the challenge.","","Suspicious Leaderboard entries","","","0"
"748","601661","2266058","04/14/2020 12:43:30","## Task Details
To detect Pneumonia using Chest X-Ray with the lowest latency for inference.

## Expected Submission
Submit the CSV file with classes for a set of 50 test images 

## Evaluation
The best accuracy with the lowest inference time","","Early Detection of Pneumonia","","","0"
"749","601661","2266058","04/14/2020 12:43:36","## Task Details
To detect Pneumonia using Chest X-Ray with the lowest latency for inference.

## Expected Submission
Submit the CSV file with classes for a set of 50 test images 

## Evaluation
The best accuracy with the lowest inference time","","Early Detection of Pneumonia","","","0"
"643","553308","2266058","03/27/2020 13:31:51","## Task Details
Diagnosis of Brain Tumor is very time consuming for medical practitioners, by this task we aim to aid them and provide them ML-based solution...

## Evaluation
Accuracy above 94% is expected as it should aid doctors and not mislead or give incorrect information.","","Prediction on Diagnosis of Brain Tumor","","","0"
"1894","844804","2273576","08/30/2020 13:55:12","For the top 10 users (with more checkins) build:-A basket of recommendation : venues(places)

Correct order of my notebooks to look at:
1) Data exploration: https://www.kaggle.com/teesoong/data-exploration
2)Recommendation Model training: https://www.kaggle.com/teesoong/recommendation-model-training
3) Prediction preparation: https://www.kaggle.com/teesoong/prediction-preparation
4) All the userxx venue shortlist ( I had to use multiple notebooks to parallelize the computation for distance calculation)
5) Final recommendations: https://www.kaggle.com/teesoong/recommendations","","Recommendation of venues","","","0"
"628","569234","2278978","03/24/2020 17:16:00","## Summary
In financial data analysis, prediction is widely studied topic to enhance the accuracy of forecasting individual stock index or indexes correlating to other stock market value future trends. For this, we will analyze and forecast time series over the specific span of days aiming to the Pakistan stock market. Then in next phase, we will imply this method to other respective indexes and find correlation between different index values.

## Task Details
1.) What was the change in price of the stock over time?
2.) What was the daily return of the stock on average?
3.) What was the moving average of the various stocks?
4.) What was the correlation between different stocks'?
5.) How much value do we put at risk by investing in a particular stock?
6.) How can we attempt to predict future stock behavior? (Predicting the closing price stock price of any company using LSTM)
7) How to efficiently predict stock share price using single variable value?
8) How multiple variable value can be used to efficiently predict stock share price?
9)How to find a correlation and forecast time-series financial data?

## Objective: 
‚Ä¢	Predict stock share price single variable value.
‚Ä¢	Predict stock share price multiple variable value.
‚Ä¢	To find a correlation or forecast time-series data.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?","","KSE Data Set","Karachi Stock Market , Pakistan","","1"
"584","555298","2301650","03/15/2020 09:00:24","Solution should contain the conclusion

Submission should be in XLS or XLSX","","Deadline for Submission","Submit in XLSX , XLS","04/30/2020 00:00:00","0"
"2767","816525","6234957","11/24/2020 17:20:44","# # *****For* win to wingo

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Wingo prediction","Success","11/25/2020 23:59:00","11"
"2706","816525","6168616","11/16/2020 08:26:13","## Task Details
task is about to predict next color in wingo","","win go","wingo","11/21/2020 23:59:00","8"
"3332","816525","6623799","01/30/2021 09:27:22","Tell me the right color","","Go win","Prediction","01/31/2021 23:59:00","13"
"5325","816525","7979250","07/25/2021 11:22:31","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Jsjxkxid","","Mantrimall","Prediction data","","4"
"826","624550","2319274","04/29/2020 19:25:15","## Task Details
As presented in the paper ""Automatic Detection of Sexist Statements
Commonly Used at the Workplace,"" an F1 score of 0.88 was attainable using GloVe embeddings fed into 2 BiLSTM layers with Dropout on top of which was a simple attention layer. 
Avenues for possible improvement involve preprocessing the dataset beyond removing stop words, introducing more complex attention mechanisms, using Google BERT word/phrase embeddings, among others.

## Expected Submission
Users should submit a notebook outlining any preprocessing and modeling steps that led to their performance. Additional analysis on the explainability of model to results are encouraged: Why did the model mark a statement as sexist? What specific words tipped off the model to marking a statement as sexist?

## Evaluation
We are looking for F1 performances that beat 0.88. All submissions will be judged based on their F1 performance. Precision and recall should be detailed as well since previous iterations of this model tend to exhibit better recall than precision, meaning that the model tends to over-label statements as sexist. A more balanced precision and recall will make for a better use case in the workplace.","","Improve Classification Performance","Beat an F1 score of 0.88","","0"
"836","633089","2322783","05/03/2020 09:25:47",".","","Who is the coolest superhero?","Given only the two text columns, can you find a formula to find the coolest superhero?","","0"
"837","633089","2322783","05/03/2020 09:26:54",".","","Who is the stronger superhero of all time?","By combining text features with the power stats features, can you try to say who is the most strong superhero of all time?","","0"
"838","633089","2322783","05/03/2020 09:28:35",".","","Text classification: can you predict who is the Superhero creator using only the text columns?","It's the superhero created by DC Comics or Marvel Comics?","","1"
"839","633089","2322783","05/03/2020 09:28:54",".","","Who is the top 10 Woman Superheroes?","23% of the Superheroes are woman, can you spot who is the top 10?","","0"
"3209","564449","4796888","01/16/2021 02:22:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
*data can be uploaded to data brick with spark and apply ML/regression and get R^2. R^2 is used for model predictor in relation to the noise/error term.*

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
*Application of ML/regression of the data set with spark/data brick containing R^2 can be submitted.
*

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
use spark/data brick with Python API; not excel/scala/sql or R

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","data analysis on spark/databrics","ML: getting R^2","01/27/2021 23:59:00","0"
"430","500923","2326382","02/04/2020 21:37:26","## Task Details
I am wondering if there are any trends among Super Bowl ads over the years. Have certain types of companies been using more ads or less over time? Do certain categories of ads increase in frequency?

## Expected Submission
A kernel answering this fuzzy question. This is a ambiguous task, so there's not one correct answer.","","Super Bowl Ads Over Time","Have the Super Bowl Ads changed in content over time?","","0"
"604","560731","2348066","03/18/2020 16:26:30","## Task Details
Find the nearest drug possible to treat coronavirus cases.

## Expected Submission
Details of the medicine will be uploaded shortly.","","Find the nearest drug to fight against COVID-19","","","2"
"827","628496","2348066","04/30/2020 10:04:53","## Task Detail
Due to coronavirus pandemic, we all are obeying our social distancing measure. So, I came up with this video dataset.

## Expected Submission
Detect the person with a red flag, find the midpoint between persons.
The detected person need to be cropped and the distance between the 2 people and the violater need to cropped and data need to be saved in CSV or generated with pyplot.","","Detect and crop the person, not obeying the social distancing","","12/31/2020 00:00:00","1"
"773","600878","2352583","04/16/2020 18:26:25","## Task Details
We want you to evaluate your question-answering models using the `news.csv` dataset. Feel free to get started with the baseline models!

## Expected Submission
Please make sure that all the code is publicly available, preferably through Kaggle. Your notebook should clearly display your metrics, and should be easily copied and edited by anyone.

## Evaluation

We recommend using AP Score, and evaluate your model on each news source. For an example, check out [this notebook](https://www.kaggle.com/xhlulu/evaluate-models-on-news-data)","","News-QA Evaluation","How well does your model perform on the News-QA task?","","0"
"774","600878","2352583","04/16/2020 18:27:10","## Task Details
We want you to evaluate your question-answering models using the `community.csv` dataset. Feel free to get started with the baseline models!

## Expected Submission
Please make sure that all the code is publicly available, preferably through Kaggle. Your notebook should clearly display your metrics, and should be easily copied and edited by anyone.

## Evaluation

We recommend using AP Score, and evaluate your model on each community source (biomedical, expert, general). For an example, check out [this notebook](https://www.kaggle.com/xhlulu/evaluate-models-on-community-data)","","Community-QA Evaluation","How well does your model perform on the Community-QA task?","","0"
"775","600878","2352583","04/16/2020 18:29:07","## Task Details
We want you to evaluate your question-answering models using the `multilingual.csv` dataset. Feel free to get started with the baseline models!

## Expected Submission
Please make sure that all the code is publicly available, preferably through Kaggle. Your notebook should clearly display your metrics, and should be easily copied and edited by anyone.

## Evaluation

We recommend using AP Score, and evaluate your model on each community source (biomedical, expert, general). For an example, check out [this notebook](https://www.kaggle.com/xhlulu/evaluate-models-on-multilingual-data)","","Multilingual-QA Evaluation","How well does your model perform on the Multilingual-QA task?","","1"
"530","501529","2352583","03/11/2020 05:34:15","## Getting Started
Start by making a kernel, and run predictions on the test data! Since the test results are already given, we encourage you to publicly share your results as soon as you can!

Please use the [Discussions](https://www.kaggle.com/xhlulu/140k-real-and-fake-faces/discussion) to talk about interesting results you get.

## Expected Submission

To know what your submission should look like, check out the [starter kernel](https://www.kaggle.com/xhlulu/real-vs-fake-starter-code/data)

## Evaluation

The metric for this task is the f1 score:

$$
F_1 = \frac{precision \cdot recall}{precision + recall}
$$","","[Beginner] Predict if an image was GAN-generated","Build a ML model that can predict, with a high F1 score, whether an image is real (from Flickr) or fake (gan-generated)","","3"
"531","501529","2352583","03/11/2020 05:53:22","## Task Details
Start by making a kernel, and run predictions on the test data! Since the test results are already given, we encourage you to publicly share your results as soon as you can!

Please use the Discussions to talk about interesting results you get.

## Expected Submission

Same as the [this task](https://www.kaggle.com/xhlulu/140k-real-and-fake-faces/tasks?taskId=530).

## Evaluation

The metric for this task is the f1 score:

$$
F_1 = \frac{precision \cdot recall}{precision + recall}
$$","","[Intermediate] Unsupervised Clustering of Faces","Come up with a method that can accurately predict if an image is real or fake without using the labels","","0"
"1829","844929","2353983","08/25/2020 15:55:35","Analyze and forecast cases and deaths growth could be useful in many ways, governments could estimate medical equipment and take appropriate policy responses, and experts could approximate the peak and the end of the disease.","","COVID-19 Data analysis and Forecasting","","","0"
"1830","844929","2353983","08/25/2020 16:03:00","Find correlations and relationships between the increase/decrease in the number of cases and deaths and external factors that may contribute to accelerate/slow the spread of the virus, such as geographic, climatic, health, economic, and demographic factors.","","Impact of external factors on the number of cases and deaths","","","0"
"469","523948","2373989","02/21/2020 21:32:59","## Task Details
Take the variable that you think that is good to predict to future survey and what variable you need to add to another work

## Expected Submission
Visualization and predict the time of each travel ¬ø What is the best way to predict that time ? 

## Evaluation
Just practice","","Data visualization and machine learning algorithm","","","0"
"401","488693","2382518","01/25/2020 07:23:04","Find out which city or town has more so2 and visualize them","","Find which city/town has more no of so2","","","0"
"402","488693","2382518","01/25/2020 07:24:42","Find which city/town has more No2","","Find which state has more No2","","","0"
"1588","488693","2616667","08/04/2020 16:01:30","Find out the multicollinearity among the various pollutants such as NO2,SO2,PM10,PM2.5 .","","Find the multicollinearity among the various pollutants","","","0"
"1582","815085","2386634","08/04/2020 05:42:14","## Task Details
A leading pet adoption agency is planning to create a virtual tour experience for its customers showcasing all animals that are available in their shelter. To enable this tour experience, you are required to build a Machine Learning model that determines type and breed of the animal-based on its physical attributes and other factors.

## Expected Submission
You just want  to submit your CSV file which is your final Submission

## Evaluation
Your Score Evaluate using f1_score (pet category + breed category)
score = [100 * (f1_score(pet category) + f1_score(breed category))] / 2","","Hackerearth-machine-learning-challenge-pet-adoption","You all need to getting better accuracy after all that our main purpose.","","3"
"494","533339","2387372","02/29/2020 18:40:05","## Auto update","","Update","","","0"
"519","545505","2387372","03/08/2020 15:58:24","## Tasks
+ add more pictures for each class
+ add more classes","","TODO v1.0","","","0"
"1691","828921","2398395","08/13/2020 20:59:23","I am a big fan of Data Visualization, and it would be awesome to see different Notebooks, with different Style of Charts to rapresent the Space Race :)","","Data Visualization","Space Missions","","19"
"1876","828921","5653153","08/29/2020 14:20:45","web scraping","","web scraping","","","1"
"1752","837364","2417154","08/19/2020 09:04:38","## Task Details
**Dataset contains pseudo Facebook data.**

**Attribute Information:**
- Userid : ID of user
- Age: User‚Äôs age(years)
- dob_day : Day of date of birth(1-31) 
- dob_year : Year of date of birth 
- dob_month : - Month of date of birth
- gender : M/F
- tenure : How long have facebook users been on site
- friend_count : Total number of friends 
- friendships_initiated : Friend requests sent 
- likes : Total number of likes by user
- likes_received : Total number of likes received by user
- mobile_likes : Number of likes by user(through mobile) 
- mobile_likes_received : Numberoflikes received byuser(through mobile) - - 
- www_likes : Number of likes by user(through desktop website) - 
- www_likes_received : Numberoflikes received byuser(through desktop)


**Exploration ideas:**
- Date of birth analysis
- Friend count analysis 
- Tenure analysis 
- Data transformations
- Frequency polygons, Boxplots.","","Social Network","Facebook data","","0"
"1046","689444","2425416","06/04/2020 05:52:26","## Task Details
Asteroid Dataset contains different Physical Parameters and measurements. First Task is to predict whether an Asteroid is potential hazards or not.

## Expected Submission
Submit a notebook that implements the full life-cycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other datasets you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model. 
With this model, you should produce a table in the following format

## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. We will check the following points at the submitted Notebooks
- Accuracy
- Data Preparation
- Proper Documentation
* We‚Äôre looking for genuine approaches to building models on a real problem that can serve as learning examples for our Astronomy Community.","","Predict Asteroids Potential Hazards or Not","Find out potential hazards asteroid and create a machine learning model","","3"
"1614","819644","2426643","08/06/2020 20:07:12","Select XI players who could provide the best outcome","","Choose Best Liverpool XI of the Season 19/20","","","1"
"2223","591243","4425497","09/24/2020 13:16:33","you need to get know what ads is the most attractive for users and what ad makes them order something","","define what ads have the highest conversion","","","0"
"1478","788816","2475674","07/24/2020 20:40:02","## Task Details
If possible, based on the sensor telemetry data, use Machine Learning to determine when a person is near one of the IoT devices, using more than just the motion detected reading.

## Expected Submission
A Notebook.

## Evaluation
Accuracy of the prediction, based on a subset of test data.","","Use ML to Determine when a Person is near IoT Device","","","9"
"2031","865838","2486145","09/09/2020 15:57:58","## Task Details
Detect the objects present in each of the images in the dataset. Use pre-trained object detection model or train your own model on some annotated dataset.

## Expected Submission
Submit a list of all unique objects present throughout dataset and also a csv file mapping image name with list of objects in that image","","Object Detection","","","0"
"1881","850024","2493750","08/29/2020 16:28:32","## Task Details
Try to predict the category of an article through Machine Learning.

## Expected Submission
A funtional model which takes the tokenized content/author as input and predicts the category of the news article.

## Evaluation
Evaluations should be based on the F1 Score of the model.","","Category Prediction","Try to predict the category of an article through Machine Learning","","1"
"1822","843852","2504634","08/25/2020 00:57:56","This is a playground dataset, so feel free to explore and train your models!","","Car Object Prediction","Predict Cars Using YOLO","12/31/2020 23:59:00","8"
"1708","832494","2504634","08/15/2020 21:07:45","## Task Details
given certain parameters, figure out if deaths were due to covid-19","","Predict Disease Contraction","","","0"
"1709","832691","2504634","08/16/2020 01:31:13","## Task Details
Feel free to explore with EDA, and have fun creating models!","","Identify if an Image contains a mask or not","","","1"
"1943","855494","2504634","09/04/2020 14:31:08","Have fun and explore!","","Covid Prediction","EDA and Modeling","","0"
"1942","857145","2504634","09/04/2020 13:53:50","Create a machine learning model, that detects glaucoma in the eyes","","Glaucoma Detection","Data Augmentation and Modeling","","8"
"1920","852234","2540724","09/02/2020 07:50:46","Based on the given features, classify the website as legitimate website or phishing website link.","","Based on the given features, classify the website as legitimate website or phishing website link.","","","1"
"2146","878881","2540724","09/18/2020 06:02:19","Guidelines
1. Prepare the data
2. Create training and testing data for the model
3. Train the model in Keras using NLP models covered in Deep Learning track .
4. Test the model","","Perform irony classification of the given text.","","","0"
"1566","813049","2567351","08/03/2020 02:30:09","Try to get an insight on seasonality of bank transaction data.","","Time series analysis","Seasonality on Bank Transaction Data","","0"
"2190","886983","2573291","09/21/2020 23:21:04","## Task Details
The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. I came across this useful data from [DOT's database](https://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time) at working and figured this would be a really helpful dataset: Summary information on the number of on-time, delayed, canceled, and diverted flight. However, the DOT's monthly Air Travel Consumer Report only entails plots of the percentage of weather delay, security delay and others in a pie chart, rather than providing the real figure. So here, the dataset is provided after my work is finished; aside from that, you could also gather some weather information to complete your study. 



## Expected Submission
The expected submission could be a simple jupyter notebook, consisting of time predictions or time delay prediction on each airline. Additionally, you can also pair the flight information to each airport, each main airline company or other meaningful groups to rank what you recommend choose under what specific situation. 


## Evaluation
Feel free to interpret it on your own!","","Airline Delay Predictive Analysis","","","1"
"1081","686237","2588949","06/07/2020 02:26:45","## Task Details
A few weeks ago, I received a request from a client on one of the freelance exchanges to create a tool that could recognize the gender of the owner by the name of the email address.
Looking for a few ready-made solutions, I was surprised that there is still no decent open source solution. After which I suggested Em to train his own neural network, which would do an excellent job. Since he was interested in recognizing email addresses exclusively for owners from the countries of the former CIS, I completed the task very quickly, it was simple. But I was interested in the question whether there is a clear line between men and women in the name of the email address, regardless of region, country, language or any other factors. In this task, you are provided with over 100,000,000 addresses belonging to people from different countries of the regions and points of the planet marked distinguished by gender.

## Expected Submission
To achieve the goal, you can use any material that you find on the Internet. The main goal is to predict gender by email regardless of other conditions, country, religion, age, mother tongue, etc.

## Evaluation
Testing the model on a test sample with the largest number of correctly predicted results. 

The test sample contains more than 10,000,000 unique email addresses.","","Gender predict from email","111 000 000 examples","","0"
"6110","573975","6828272","09/16/2021 18:38:48","## Task Details
The author of the paper suggested to try and classify the `G3` grade by the following table:

![](https://i.imgur.com/LULMfa5.png)

But I went and added a little twist, I instead went on and classified the average of `G1`, `G2` & `G3`.

## Expected Submission
Any type of classification that follows the rules of the paper is accepted.

## Evaluation
Accuracy, recall & precision, but you evaluate by other metrics too.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classifying the annual grades averages according to the 5-level classification","","","3"
"1679","810902","2600198","08/12/2020 10:40:17","## Task Details
Extract conclusions about the most important factors in becoming the top Dataset Kagglers.

## Expected Submission
A notebook containing all the conclusions extracted from the dataset.

## Evaluation
Every new insight is welcomed. This task is not a race or a competition, is about gaining knowledge.","","What needs someone to be good at Datasets?","","","0"
"1429","789289","2600198","07/20/2020 17:08:20","### Task Details
Which factors are the most important to evaluate the popularity of a manga?

### Expected Submission
This task has to be made primarily using Notebooks, as it is a good way of showing how you came to a solution and help people learn new things.

### Evaluation
A good solution is one that shows a well thought out solution. One that had solidified ideas and good foundations.","","What makes a manga popular?","","","0"
"1414","785391","2600198","07/18/2020 17:22:34","## Task Details
Which factors are the most important to evaluate the popularity of a manga?

## Expected Submission
This task has to be made primarily using Notebooks, as it is a good way of showing how you came to a solution and help people learn new things.

## Evaluation
A good solution is one that shows a well thought out solution. One that had solidified ideas and good foundations.","","What makes a manga popular?","","","0"
"412","492138","2610418","01/28/2020 09:51:20","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
# Make a Neural Machine Translater English to Hindi Using Tensorflow with good Accuracy.","","NMT English to Hindi Parallel Corpus   Datasets","Neural Machine Translater","04/03/2020 00:00:00","0"
"3268","621251","2613318","01/24/2021 22:31:02","## Task Details
Thorax usg image and lesioned image

## Expected Submission
 ## Task Details
Thorax usg image and lesioned image

## Expected Submission
 Medical image processing 
## Evaluation
Bening and malignant tumors detect model 
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

M","","For mask_rcnn PNG","Mask_rcnn thorax usg dataset","01/25/2021 23:59:00","0"
"617","522541","2614045","03/22/2020 08:08:20","## Task Details
Some of the images in the data set are present as noise, for example, there are are a few images of the pause screen which shouldn't be there. These need to be removed.

## Expected Submission
A cleaner dataset with noises remove.","","Data Cleaning","","","0"
"1610","816870","5260966","08/06/2020 12:40:38","create two data frame one for top 5 youngest nobel prize winner and another for top 5 oldest Nobel prize winner.","","Top 5 youngest and Oldest Nobel Prize winner.","","","1"
"1127","699454","2663222","06/12/2020 21:05:43","Try to build a model to detect if the image contains flowers or not.","","Classification model detect flowers","","","0"
"1128","699454","2663222","06/12/2020 21:06:55","Draw a box on the flowers if the Image contains flowers by using YOLO or any preference algorithm.","","Draw a box on the flowers if the Image contains flowers","","","0"
"2323","905538","2672748","10/04/2020 09:19:17","## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Simple Data Visualization","Just see what's in the dataset how we can interpret it.","","1"
"1786","840176","2674182","08/21/2020 16:56:49","## Task Details
The submission should use a suitable model to predict the population in the future.

## Expected Submission
.csv files should be submitted which could be added to this dataset. The file should contain two columns, ""year"" and ""predicted population"", containing a year in the future and the predicted population for the corresponding year.","","Population Trend Prediction","","","0"
"451","515242","4435864","02/16/2020 19:38:31","## Task Details
Create a model making Riga real estate price predictions based on a set of input parameters.","","Riga real estate price prediction model","","","0"
"1937","844952","2680939","09/03/2020 19:33:27","Thank you for completing a graph!","","Make a chart","Make a chart to see if random is biased","","0"
"1936","857076","2680939","09/03/2020 19:05:13","Please graph this dataset","","Make a graph","Thanks!","","0"
"1933","856992","2680939","09/03/2020 18:04:14","## Task Details
Visualize the data!

## Expected Submission
A seaborn or matplotlib generated graph 

## Evaluation
great job!

### Further help
None","","Graph the Data","Graph the data in the csv","","0"
"1935","857011","2680939","09/03/2020 18:26:20","## Task Details
Graph this data

## Expected Submission
Seaborn or matplotlib graph

## Evaluation
great job!

### Further help
None","","Graph this data","Thank you if you complete this task","","0"
"1952","858417","2680939","09/04/2020 21:59:34","Graph it","","Graph it","Graph it","","0"
"1951","858189","2681031","09/04/2020 19:11:57","Use the Getting started with Dictionary and Pandas csv file to create your own panda file. Add some items or remove stuff. Play around with the csv and see what great notebooks you can create. https://www.kaggle.com/brendan45774/getting-started-with-dictionary-and-pandas","","Getting started with Dictionary and Pandas","","","1"
"2029","865755","2681031","09/09/2020 14:44:15","## Task Details
Keep track which state numbers keep going up. Use charts and graphs or the data.","","Tracking covid-19","","","3"
"1847","847452","2681031","08/27/2020 14:58:26","## Task Details
Help someone identify buying an ad on which keyword will help them make the most money.

## Expected Submission
Having graphs and data to show which is the best choice.","","Finding the best option.","","","1"
"1039","691304","2686945","06/03/2020 16:18:21","Do an analysis on the dataset","","Analyze the DDoS  attacks.","","","1"
"1040","691304","2686945","06/03/2020 16:18:51","Implement machine learning on to predict malicious packets","","Implement machine learning on to predict malicious packets","","","0"
"965","673276","2686945","05/25/2020 07:22:29","Visualize the dataset by complete analysis on it.","","Data Visualization","","","8"
"995","638974","2688393","05/28/2020 12:39:05","## Task Details
Add more posters of different genres as the dataset is not distributed uniformly

## Expected Submission
Updated dataset consisting of updated images folder and csv folder with respected genre's.","","New Posters","","","0"
"822","600600","4943238","04/28/2020 18:38:38","****## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Global hospital capacity","Bed capacity","05/02/2020 00:00:00","1"
"2783","892212","4808718","11/26/2020 14:04:51","The task for Bread_Basket using Apriori Algorithm","","Bread_Basket_Analysis","","11/27/2020 23:59:00","3"
"2228","892212","2693690","09/25/2020 06:28:20","## Task Details
The task here is to find the association rules between the items using the Apriori Algorithm. Or in other words, you have to find out those items/itemset that customers bought together which helps the owner for store layout/marketing.

## Expected Submission
This is a self-evaluation task and learning the new algorithm on python. There is no submission. You can match the final answer and share your answer, insights a and suggestions.

## Evaluation
There are many measures to check which rule is best. But one commonly used is ""confidence"".","","Market Basket Analysis","","","14"
"2093","872748","2697552","09/14/2020 10:00:04","This study presents a research approach using data mining for predicting the performance metrics of posts published in brands' Facebook pages. Twelve posts' performance metrics extracted from a cosmetic company's page including 790 publications were modeled, with the two best results achieving a mean absolute percentage error of around 27%. One of them, the ‚ÄúLifetime Post Consumers‚Äù model, was assessed using sensitivity analysis to understand how each of the seven input features influenced it (category, page total likes, type, month, hour,
weekday, paid). The type of content was considered the most relevant feature for the model, with a relevance of 36%. A status post captures around twice the attention of the remaining three types (link, photo, video).We have drawn a decision process flow from the ‚ÄúLifetime Post Consumers‚Äù model, which by complementing the sensitivity analysis information may be used to support manager's decisions on whether to publish a post.","","Predicting social media performance metrics","","","5"
"688","579867","2707292","04/03/2020 16:19:18","What are stable root causes of R0?","","Understand COVID-19 Factors","","04/15/2020 00:00:00","0"
"2054","869003","2708391","09/11/2020 18:12:11","A renowned hypermarket, that has multiple stores, is all set to grow its business. As a part of its expansion, the owner plans to introduce an exclusive section for newborns and toddlers. To help him expand its business, teams across all stores have collated numerous images that consist of products and goods purchased for children. 

You are a Machine Learning expert. Your task is to:

    Identify the product type [toys,consumer_products] 

    Extract and tag brand names of these products from each image. In case if no brand names are mentioned, tag it as ‚ÄòUnnamed‚Äô. 

The extracted brand names must be represented in the format provided in sample submission.

sample_submission.csv:

Image     Class_of_image     Brand_name

1.jpg     toys     Unnamed

2.jpg     toys     Unnamed

3.jpg     consumer_products     Happy Cappy/Mustela

4.jpg     consumer_products     Johnsons

5.jpg     toys     Hot Wheels","","Keep babies safe","","","0"
"2532","612927","2723137","10/24/2020 13:56:50","## Task Details
With corona virus making the mask an essential part of all our lives, this dataset aims to bring deep learning to detect if people are wearing a mask or not.

## Expected Submission
A notebook/script with a classifier using any technology/library that classifies between mask or no mask.","","Classify masked/non-masked individuals","","","1"
"2325","905688","2737089","10/04/2020 11:54:22","Use this dataset to make accurate land use predictions across 10 classes.","","Land cover and land use","","","0"
"2377","910077","2737089","10/08/2020 07:43:18","Predict Covid cases using X-ray images","","Predict Covid cases using X-ray images","","","1"
"1648","823359","2739498","08/09/2020 18:28:43","## Task Details
In this dataset, there are images with both single and multiple hands. Detecting hand keypoints is a tricky task especially when there are multiple hands in the picture. 
## Expected Submission
There is no deadline for this task.","","Hand Keypoints Detection","","","0"
"1657","824374","2739498","08/10/2020 12:46:57","## Task Details
There are three types of images in this dataset: negative, positive and non-informative. So the task is to detect whether a patient is COVID-19 positive or not from a CT scan.","","COVID-19 Classification","","","3"
"2041","866581","2769315","09/10/2020 12:27:21","## Task Details
As an active reader, an active user of Goodreads, and also a data scientist I wanted to make a content-based recommendation system for those who don't have any active accounts but are interested in books. so with a content-based recommender, you can choose a book or even more(depending on the model) and get good book recommendations.

nothing is better than a good book recommendation.

## Expected Submission
In this dataset, you can use descriptions or tags or both together to generate your metrics to find similarities between books. you can also add authors or languages to your soup.","","Content Based Recommender","a Content Based Recommendation system based on book descriptions + tags or shelves","","0"
"5505","652566","4656934","08/02/2021 18:03:05","Build a Clustering Model for Iris data.","","Clustering","","","0"
"2393","735529","2781854","10/10/2020 06:36:54","Almost 3 million (2,971,554) Sea turtle images from drones
Identify which part of the image has turtles","","Identify which part of the image has turtles","","","0"
"978","676302","2789944","05/26/2020 14:48:09","Predict whether a customer is interested in a caravan insurance policy from other data about the customer. Information about customers consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was supplied by the Dutch data mining company Sentient Machine Research and is based on a real-world business problem. The training set contains over 5000 descriptions of customers, including the information on whether or not they have a caravan insurance policy. A test set contains 4000 customers of whom only the organizers know if they have a caravan insurance policy.

Original Task Link: http://liacs.leidenuniv.nl/~puttenpwhvander/library/cc2000/problem.html","","Predict which customers are potentially interested in a caravan insurance policy.","","","3"
"1149","715658","2794448","06/16/2020 09:44:37","## Task Details

The Mubi SVOD platform database contains about 750,000 movie critics. 
Machine learning algorithms can be used to better understand movie lovers. 



## Expected Submission
Submit a notebook that implements the data preparation, EDA, model creation and evaluation. You can chose to focus on either EDA, modelling or both. 

## Evaluation
- Aesthetics: does the notebook include impactful visualisations?
- Composition: is the notebook easy and pleasant to read?
- Accuracy: how well does the model perform on the real data?","","Sentiment Analyses on Movie Critics with Mubi Data","","","0"
"1150","715658","2794448","06/16/2020 09:55:34","## Task Details


## Expected Submission


## Evaluation


### Further help","","Build a Recommendation System for Mubi","","","0"
"4310","504383","7379142","05/09/2021 14:36:33","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

asdf","","Tarea kaggle","afd","","0"
"1783","839703","2809550","08/21/2020 12:04:57","## Task Details

The data must first go through a pre-processing. To apply NLP techniques and contribute to this field.

## Expected Submission

Building models and making comparisons","","Topic Modeling","Topic modeling of the collected English data","12/31/2030 23:59:00","0"
"468","522275","2809756","02/21/2020 20:46:30","## Task Details
One of the most vital parts of a machine learning project is understanding the key features that correlate to the variable which is being predicted. The goal of this task is to determine what features best correlated to customer satisfaction. You can use clustering techniques, new feature engineering, visualizations, anything to get the point across.

## Expected Submission
Please submit a notebook containing your findings, preferably with good visualizations to help tell the story.","","What factors contribute to customer satisfaction?","","","33"
"535","527325","795355","03/12/2020 22:24:42","Get familiar with UI.","","Read through dataset page","","","36"
"2063","869564","2821158","09/12/2020 06:09:57","## Task Details
n order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc

## Evaluation
The evaluation metric is the ROC_AUC score","","Task 1: We want to predict whether a customer would be interested in Vehicle Insurance","","","0"
"523","547272","2821953","03/09/2020 20:31:08","Qual a idade m√©dia das esta√ß√µes da CPTM?","","Qual a idade m√©dia das esta√ß√µes da CPTM?","An√°lise Explorat√≥ria dos Dados","","0"
"524","547272","2821953","03/09/2020 20:31:48","Qual a idade m√©dia das esta√ß√µes do Metr√¥?","","Qual a idade m√©dia das esta√ß√µes do Metr√¥?","An√°lise Explorat√≥ria dos Dados","","0"
"525","547272","2821953","03/09/2020 20:33:08","Crie um gr√°fico que permita a r√°pida visualiza√ß√£o das datas de inaugura√ß√£o das esta√ß√µes por linha e por ano.","","Visualiza√ß√£o gr√°fica dos dados do dataset","An√°lise Explorat√≥ria dos Dados","","0"
"483","525913","2828407","02/25/2020 15:15:37","## Task Details
Present Data Effectively!!","","EDA, Visualization","","","1"
"1247","753072","2828488","07/02/2020 23:53:06","## Task Details
The objective is to identify the position of the letters within the image using a non-machine learning method, just using image processing techniques.

## Expected Submission
A notebook containing the original image and the image that contains bounding boxes around the letters of the captcha.

## Evaluation
Creativity - The programmer manages to create a notebook that accomplishes the task using a different technique than the ones already used in other notebooks.","","Detect the letters within the image","","","0"
"1316","767912","2830398","07/10/2020 14:25:41","## Expected Submission

A notebook highlighting sentiments of characters over the course of the show, per season and highlighting changes in the shows tone over time.","","Sentiment Analysis","","","1"
"1733","836083","2831265","08/18/2020 07:05:24","## Task Details
The questions in the survey can be divided into two parts:

‚Ä¢ One is about people‚Äôs attitude or opinion about Start War movies.
‚Ä¢ The other is about people‚Äôs demographics

Build a classiÔ¨Åer (or some classiÔ¨Åers, for example one classiÔ¨Åer per demographic feature), which can classify people‚Äôs demographics (gender, age, household income, education, location (census region)) based on their attitude or opinion about Star War movies.","","Data Exploration","","","0"
"847","619289","2837261","05/04/2020 08:40:43","## Task Details
Make a ML model with good F1 score

## Expected Submission
need f1 score above   0.6","","Find best F1 Score","","","0"
"2009","863648","2846557","09/08/2020 08:22:11","## Task Details
Use this dataset that I collected during my college days to explore more on the data visualization aspect!

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis","","","1"
"5448","587187","8010537","07/31/2021 17:44:55","## Task Details
In the below dataset we are trying to know the complete details of the people new to machine learning.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Dataset of begginers in machine learning","Analysis of beginners in machine learning data set","","0"
"911","655861","2851794","05/15/2020 14:25:52","The Berlin Wall, like the much longer inner German border between East and West Germany, was designed with two purposes in mind: to obstruct would-be border-crossers and to enable border guards to detect and stop illegal border crossings. In its final form, the 156 km (97 mi) wall consisted of inner and outer concrete walls separated by a ""death strip""[12] some 15 m (49 ft) to 150 m (490 ft) wide. It was guarded by around 11,500 Grenztruppen, the Border Troops of the German Democratic Republic who were authorised to use any means necessary, including firearms, to prevent border breaches. The shooting orders, or Schie√übefehl, issued to the border guards instructed that people attempting to cross the Wall were criminals, and that the use of deadly force was required to deal with them: ""Do not hesitate to use your firearm, not even when the border is breached in the company of women and children, which is a tactic the traitors have often used"".","","Try to use it for data visualisation","","","0"
"924","660604","2851794","05/18/2020 09:05:10","Try and make a model which uses ATK/DEF stats to judge the Level of the card.
Should be easy as all you guys are PRO's.","","Relation Between level and ATK/DEF Points","","","1"
"1848","837430","2858601","08/27/2020 15:29:59","A leading pet adoption agency is planning to create a virtual tour experience for their customers showcasing all animals that are available in their shelter. To enable this tour experience, you are required to build a Machine Learning model that determines type and breed of the animal based on its physical attributes and other factors.","","Predict the breed and pet category of the pet","","","2"
"2017","864317","2860254","09/08/2020 16:10:10","Teams are more than just the sum of their Players, sometimes players mesh well and play well beyond their capabilities while at times they fall flat in the new organization.","","Player Maketh the team","Players have a huge impact on the teams wins, but How much of that is the player itself?","","0"
"1637","822082","2865243","08/08/2020 17:30:47","**COVID19 - Bar Chart Race**
Create a bar chart race with the number of confirmed cases per states


**Task Details**
Let us create a bar chart race to see how the confirmed cases developed in each country across the dates.

Use the WHO-COVID-19-global-data.csv file.

**Expected Submission**
Submit a video/gif file with the cases rising since the beginning of March.

**Evaluation**
The bar chart race should have the date, country name, cases per country, total cases across the world.","","COVID19  - Bar Chart Race","","","0"
"931","612351","2866353","05/19/2020 06:51:02","## Task Details
This dataset contains news articles with 4 different genres namely ""world news"" , ""sports news"" , ""business news"" and ""science and technology"". These categories are represented by 1,2,3 and 4 in the class id respectively. The task requires to classify the articles accurately in the different categories (genres). 
Also, preprocess the data and do EDA for the data set.

## Expected Submission
The solution should be in the form of notebooks displaying the accuracy of the model for the classification. Also a little bit of EDA must be done on the data. Machne learning algorithms or Neural networks can be used for classification.","","Classification of the news articles in various categories","Use different classification algorithms for finding the genre of articles","","3"
"497","537163","2881218","03/02/2020 14:21:52","## Task Details
The Task is to generate the everyday todo list.

## Expected Submission
The submission should consist of a CSV file, first column is the data and the second column is the todo list in string format","","Smart TodoList","","","1"
"2050","868643","410158","09/11/2020 13:41:03","## Task Details
Find the given dataset, you should predict the top 10 tech keys in 2021. For the sample, comparing Java and Python, Python will lead in 2021.

Like this, you will have to compare all keys by analyzing the historical data and predict them.

## Expected Submission
Submit the notebook saying the top 10 technologies of 2021","","Predict the top 10 tech keys in 2021","","","5"
"2105","874070","2883450","09/15/2020 09:07:24","## Task Details
From the given dataset analyze the production of the crops in various districts and seasons and visualize it.

## Expected Submission
Submit the notebook visualizing the crop production in various districts and seasons.","","Analyze various crops production district wise","","","1"
"2075","869852","2884552","09/12/2020 11:03:36","## Task Details
Analyze the data to find out what job roles have the widest gender pay gap.","","Gender Pay Gap","","","1"
"1998","861913","2884552","09/07/2020 09:41:59","## Task Details
Perform EDA and generate relevant insights from the data.","","Exploratory Data Analysis","","","1"
"1263","755910","2884552","07/05/2020 18:35:48","## Task Details
Generate useful insights from the data. Visualize. Analyze how the countries are handling the pandemic, which countries have been able to completely eliminate the virus and recovered or countries which have no new cases.

## Expected Submission
Submit the notebook. Submission should include visualizations, insights from data, your analysis.

## Evaluation
The evaluation will be based on how well the notebook is written, storytelling through data, how understandable is the code, and are visualizations accompanied by a description or not, and the insights generated.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/raenish/cheatsheet-50-plotly-charts
-https://www.kaggle.com/vanshjatana/a-simple-tutorial-to-data-visualization","","Determine how different countries are handling the pandemic","","","0"
"4898","903046","7668850","06/23/2021 12:31:44","EDA-on-Automobile-Data Set

Performed EDA on a Automobile Dataset. Used Pandas and numpy for Data Manipulation followed by matplotlib and Seaborn for Data visualisation. A starter project just to get acquainted with the libraries mentioned above.","","EDA on Automobile DataSet","","","4"
"4899","903046","7653855","06/23/2021 12:33:18","Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.

As we now move into building machine learning models to automate our analysis, feeding the model with variables that meaningfully affect our target variable will improve our model's prediction performance.","","EDA on automoble dataset","","06/25/2021 23:59:00","2"
"2700","506691","2889836","11/14/2020 19:59:38","Create visualizations and/or models showing what factors influenced the percentage of the vote Joe Biden and Donald Trump received in 2020.","","What Factors Influenced Voter Preferences in the 2020 Election?","","","2"
"2701","506691","2889836","11/14/2020 20:00:22","Create visualizations and/or models showing which factors influenced voter shift between the 2016 and 2020 elections. Explore the magnitude of these shifts throughout the country.","","How Did Voter Preferences Shift Between 2016 and 2020? What Factors Contributed to This?","","","0"
"2702","506691","2889836","11/14/2020 20:00:56","Explore the differences between the polls and actual results in both 2016 and 2020. Did the accuracy improve? Should we trust pollsters?","","How Accurate Were the Polls?","","","0"
"1157","721951","2895916","06/17/2020 08:14:47","# EDA
data cleaning, univariate analysis, bivariate analysis etc.

# Outlier Analysis 
You must perform the Outlier Analysis on the dataset.
However, you do have the flexibility of not removing the outliers if it suits the
business needs or a lot of countries are getting removed. 
Hence, all you need to do is find the outliers in the dataset, and then choose whether to keep them or remove them depending on the results you get.

# Clustering
K-means and Hierarchical clustering(both single and complete
linkage) on this dataset to create the clusters. [Note that both the methods
may not produce identical results and you might have to choose one of them
for the final list of countries.]
Analyse the clusters and identify the ones which are in dire need of aid. You
can analyse the clusters by comparing how these three variables - [gdpp,
child_mort and income] vary for each cluster of countries to recognise and
differentiate the clusters of developed countries from the clusters of
under-developed countries.

# Visualisation
Also, you need to perform visualisations on the clusters that have been
formed. You can do this by choosing any two of the three variables
mentioned above on the X-Y axes and plotting a scatter plot of all the
countries and differentiating the clusters. Make sure you create visualisations
for all the three pairs. You can also choose other types of plots like boxplots,
etc.

# Results Expected
Both K-means and Hierarchical may give different results because of previous
analysis (whether you chose to keep or remove the outliers, how many
clusters you chose, etc.) Hence, there might be some subjectivity in the final
number of countries that you think should be reported back to the CEO since
they depend upon the preceding analysis as well. 
### Here, make sure that you report back at least 5 countries which are in direst need of aid from the analysis work that you perform.","","K-Means & Hierarchical Clustering","","","6"
"790","614385","2896970","04/21/2020 14:57:27","Load the dataset from the Kaggle and then segment the character, and predict the character from segmented image","","Character-Recognition-from-Number-Plate","Convert the Number plate image into Text using the Dataset","12/31/2025 00:00:00","0"
"1180","728412","2907842","06/20/2020 11:49:01","End result by a system that Recommends posts for the given user that user may like most by using Collaborative Filtering or Content Based Filtering","","Recommend posts for the given user","Collaborative Filtering or Content Based Filtering","","3"
"1181","728412","2907842","06/20/2020 11:53:43","If you have given a post id you have to generate similar post id that users may like to watch using Collaborative Filtering or Content Based Filtering","","similar posts Recommendation for the given post","Collaborative Filtering or Content Based Filtering","","1"
"1246","753018","2910492","07/02/2020 22:20:49","There are many classification algorithms that give accurate classifications of the test dataset, however, what we need in reality is help with decision making. Use any classification algorithm and employ the recognized pattern from the data to make the best decision about each customer in the test set. Use the calculated FNs and FPs in the dataset as decision-making mistakes.","","Optimum Churn Decision Making","","","0"
"2027","865314","2911491","09/09/2020 09:18:57","## Task Details
You are provided with a dataset of ~5k 512x512 images, your program should accept an 512x512 input image and return N images from the provided dataset similar to the input image.
To solve this problem, building an AutoEncoder model is recommended‚Äã .

## Expected Submission
Your code submission will be evaluated based code quality and on how accurate it is able to find similar images

simple score of C/N
C = no. of correct similar images returned
N = no. requested images

Plus points, for finding similar images with respect to unique feature
simple score of F/N
F = no. of images returned with the unique feature specific to the input image
N = no. requested images

Bonus points, if the provided dataset was clustered into K groups
Quality of Code based on Modularity, Reusability, Maintainability, Readability","","Find the similar","Cluster the given dataset into 5 different species.","","0"
"1712","559967","2792740","08/16/2020 09:59:32","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? If using ARIMA model, then evaluation will be based on less aic score.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Check accuracy of your time series model by taking some of the data as test data","","","1"
"1890","850702","2920422","08/30/2020 07:39:37","Classify whether the URLs are bening or malicious","","Mytask","","","0"
"2267","896267","2920422","09/27/2020 17:47:33","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
The reviews of this dataset contains lots of missing values so cleaning is to be done properly and then perform text classification on the dataset.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Text classification","","","0"
"435","500979","2931338","02/04/2020 21:40:49","## Task Details
This dataset contains the voting record of all 435 congressmen in the House of Representatives for 16 key issues. In this task, we're interested in answering the question: can we confidently predict what party a congressperson belongs to given only their voting record on 16 issues?

If it is possible to identify congresspeople with high confidence given the 16 issues, I believe another interesting task to look at will be: what is the minimal spanning set of voting record issues you would need to see before having a high confidence in a congressperson's party affiliation. 

## Expected Submission
The submission for this competition should be as a Notebook, which will build a predictive model or clustering that can take in a list of voting records on the 16 issues and then output a prediction on party affiliation. Then, you should draw a conclusion as to whether or not you can make a high confidence prediction on party affiliation just based on this voting record. 

## Evaluation
In order to evaluate success, please split your training and validation data randomly with a 65%-35% split. So, you will use observations from 65% of the data to assess if you can make high confidence predictions in the balance 35%. Please use any objective function (log loss may work well) to assess the  error rates of your classification.","","Predict party affiliation based on voting record?","Can you tell what party a congressperson belongs to based on how they vote on 16 issues?","","4"
"558","551982","1314380","03/14/2020 00:29:21","## Task Details

**What do we know about COVID-19 risk factors?  What have we learned from epidemiological studies?**

Specifically, we want to know what the literature reports about: 
 - Data on potential risks factors
  - Smoking, pre-existing pulmonary disease
  - Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities
  - Neonates and pregnant women
  - Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.
 - Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors
 - Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups
 - Susceptibility of populations
 - Public health mitigation measures that could be effective for control

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What do we know about COVID-19 risk factors?","COVID-19 Open Research Dataset Challenge (CORD-19)","","428"
"561","551982","1314380","03/14/2020 01:30:57","## Task Details

**What do we know about vaccines and therapeutics?  What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?**

Specifically, we want to know what the literature reports about:
 - Effectiveness of drugs being developed and tried to treat COVID-19 patients.
  - Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.
 - Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.
 - Exploration of use of best animal models and their predictive value for a human vaccine.
 - Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.
 - Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.
 - Efforts targeted at a universal coronavirus vaccine.
 - Efforts to develop animal models and standardize challenge studies
 - Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers
 - Approaches to evaluate risk for enhanced disease after vaccination
 - Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What do we know about vaccines and therapeutics?","COVID-19 Open Research Dataset Challenge (CORD-19)","","185"
"563","551982","1314380","03/14/2020 01:36:47","## Task Details

**What has been published concerning ethical considerations for research?  What has been published concerning social sciences at the outbreak response?**

Specifically, we want to know what the literature reports about:
 - Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019
 - Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight
 - Efforts to support sustained education, access, and capacity building in the area of ethics
 - Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.
 - Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)
 - Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.
 - Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What has been published about ethical and social science considerations?","COVID-19 Open Research Dataset Challenge (CORD-19)","","112"
"587","551982","1314380","03/15/2020 15:20:53","## Task Details

**What do we know about the effectiveness of non-pharmaceutical interventions? What is known about equity and barriers to compliance for non-pharmaceutical interventions?**

Specifically, we want to know what the literature reports about:
- Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.
- Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.
- Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.
- Methods to control the spread in communities, barriers to compliance and how these vary among different populations..
- Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.
- Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.
- Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).
- Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What do we know about non-pharmaceutical interventions?","COVID-19 Open Research Dataset Challenge (CORD-19)","","127"
"567","551982","1314380","03/14/2020 21:08:04","## Task Details

**What do we know about virus genetics, origin, and evolution?  What do we know about the virus origin and management measures at the human-animal interface?**

Specifically, we want to know what the literature reports about:
 - Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.
 - Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.
 - Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.
  - Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.
  - Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.
  - Experimental infections to test host range for this pathogen.
 - Animal host(s) and any evidence of continued spill-over to humans
 - Socioeconomic and behavioral risk factors for this spill-over
 - Sustainable risk reduction strategies

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What do we know about virus genetics, origin, and evolution?","COVID-19 Open Research Dataset Challenge (CORD-19)","","199"
"568","551982","1314380","03/14/2020 21:09:55","## Task Details

**What is known about transmission, incubation, and environmental stability?  What do we know about natural history, transmission, and diagnostics for the virus?  What have we learned about infection prevention and control?**

Specifically, we want to know what the literature reports about:
 - Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.
 - Prevalence of asymptomatic shedding and transmission (e.g., particularly children).
 - Seasonality of transmission.
 - Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).
 - Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).
 - Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).
 - Natural history of the virus and shedding of it from an infected person
 - Implementation of diagnostics and products to improve clinical processes
 - Disease models, including animal models for infection, disease and transmission
 - Tools and studies to monitor phenotypic change and potential adaptation of the virus
 - Immune response and immunity
 - Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings
 - Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings
 - Role of the environment in transmission

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What is known about transmission, incubation, and environmental stability?","COVID-19 Open Research Dataset Challenge (CORD-19)","","1268"
"570","551982","1314380","03/14/2020 21:14:51","## Task Details

**What do we know about diagnostics and surveillance?  What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?**

Specifically, we want to know what the literature reports about:
 - How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).
 - Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.
 - Recruitment, support, and coordination of local expertise and capacity (public, private‚Äîcommercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.
 - National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).
 - Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.
 - Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).
 - Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.
 - Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes.
 - Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.
 - Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.
 - Policies and protocols for screening and testing.
 - Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.
 - Technology roadmap for diagnostics.
 - Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.
 - New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.
 - Coupling genomics and diagnostic testing on a large scale.
 - Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.
 - Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.
 - One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What do we know about diagnostics and surveillance?","COVID-19 Open Research Dataset Challenge (CORD-19)","","123"
"572","551982","1314380","03/14/2020 21:17:57","## Task Details

**What has been published about medical care?  What has been published concerning surge capacity and nursing homes?  What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment?  What has been published concerning alternative methods to advise on disease management?  What has been published concerning processes of care?  What do we know about the clinical characterization and management of the virus?**

Specifically, we want to know what the literature reports about:
 - Resources to support skilled nursing facilities and long term care facilities.
 - Mobilization of surge medical staff to address shortages in overwhelmed communities
 - Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure ‚Äì particularly for viral etiologies
 - Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients
 - Outcomes data for COVID-19 after mechanical ventilation adjusted for age.
 - Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.
 - Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.
 - Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.
 - Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.
 - Guidance on the simple things people can do at home to take care of sick people and manage disease.
 - Oral medications that might potentially work.
 - Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.
 - Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.
 - Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials
 - Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials
 - Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)


## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What has been published about medical care?","COVID-19 Open Research Dataset Challenge (CORD-19)","","134"
"583","551982","1314380","03/14/2020 21:33:41","## Task Details

**What has been published about information sharing and inter-sectoral collaboration?  What has been published about data standards and nomenclature?  What has been published about governmental public health?  What do we know about risk communication?  What has been published about communicating with high-risk populations?  What has been published to clarify community measures?  What has been published about equity considerations and problems of inequity?**

Specifically, we want to know what the literature reports about:
 - Methods for coordinating data-gathering with standardized nomenclature.
 - Sharing response information among planners, providers, and others.
 - Understanding and mitigating barriers to information-sharing.
 - How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).
 - Integration of federal/state/local public health surveillance systems.
 - Value of investments in baseline public health response infrastructure preparedness
 - Modes of communicating with target high-risk populations (elderly, health care workers).
 - Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations‚Äô families too).
 - Communication that indicates potential risk of disease to all population groups.
 - Misunderstanding around containment and mitigation.
 - Action plan to mitigate gaps and problems of inequity in the Nation‚Äôs public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.
 - Measures to reach marginalized and disadvantaged populations.
Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.
 - Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.
 - Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care

## Expected Submission

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid.  Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).

## Evaluation

**Submissions will be scored using the following grading rubric:**
 - Accuracy (5 points)
  - Did the participant accomplish the task?
  - Did the participant discuss the pros and cons of their approach?
 - Documentation (5 points)
  - Is the methodology well documented? 
  - Is the code easy to read and reuse?
 - Presentation (5 points)
  - Did the participant communicate their findings in an effective manner?
  - Did the participant make effective use of data visualizations?

## Timeline

- Submissions will be evaluated in 2 rounds:
 - Round 1: Submission deadline is April 16, 2020 at 11:59pm UTC. Task Submissions which the evaluation committee deems to meet the threshold criteria will be awarded prizes in this initial round.
 - Round 2: Submission deadline is June 16, 2020 at 11:59pm UTC. The hosts may add prize-eligible tasks in round 2. We may also re-award prizes for existing tasks whose submissions have advanced from the prior award, as judged by the evaluation committee.**

**By each of these submission deadlines, you must have accepted the [rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","What has been published about information sharing and inter-sectoral collaboration?","COVID-19 Open Research Dataset Challenge (CORD-19)","","119"
"881","551982","1314380","05/13/2020 15:48:31","## Task Details

**Create summary tables that address population studies related to COVID-19**

Specifically, we want to know what the literature reports about:
 - Modes of communicating with target high-risk populations (elderly, health care workers).
 - Management of patients who are underhoused or otherwise lower socioeconomic status.
 - What are ways to create hospital infrastructure to prevent nosocomial outbreaks and protect 
uninfected patients?
 - Methods to control the spread in communities, barriers to compliance
 - What are recommendations for combating/overcoming resource failures

 
And we also want to know what the literature reports about the following questions that 
we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 1 - Population`.**  


`Date	Study	Study Link	Journal	Study Type	Addressed Population	Challenge	Solution	Measure of Evidence	Added on	DOI	CORD_UID`


There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address population studies related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","15"
"882","551982","1314380","05/13/2020 15:51:17","## Task Details

**Create summary tables that address relevant factors related to COVID-19**

Specifically, we want to know what the literature reports about:
 - Effectiveness of case isolation/isolation of exposed individuals (i.e. quarantine)
 - Effectiveness of community contact reduction
 - Effectiveness of inter/inner travel restriction
 - Effectiveness of school distancing
 - Effectiveness of workplace distancing
 - Effectiveness of a multifactorial strategy prevent secondary transmission
 - Seasonality of transmission
 - How does temperature and humidity affect the transmission of 2019-nCoV?
 - Significant changes in transmissibility in changing seasons?
 - Effectiveness of personal protective equipment (PPE)

 
And we also want to know what the literature reports about the following questions that we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 2 - Relevant Factors`.**  


`Date	Study	Study Link	Journal	Study Type	Factors	Influential	Excerpt	Measure of Evidence	Added on	DOI	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address relevant factors related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","116"
"883","551982","1314380","05/13/2020 15:52:45","## Task Details

**Create summary tables that address patient characteristics related to COVID-19**

Specifically, we want to know what the literature reports about:
 - Length of viral shedding after illness onset
 - Incubation period across different age groups
 - What is the Incubation Period of the Virus?
 - Proportion of patients who were asymptomatic
 - Pediatric patients who were asymptomatic
 - Asymptomatic transmission during incubation
 - Natural history of the virus from an infected person
 - What is the median viral shedding duration?
 - What is the longest duration of viral shedding?
 - Manifestations of COVID-19 including but not limited to possible cardiomyopathy and cardiac arrest
 - How does viral load relate to disease presentation which includes likelihood of a positive diagnostic test?
 - What do we know about disease models?


 
And we also want to know what the literature reports about the following questions that we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 3 - Patient Descriptions`.**  


```
Date	Study	Study Link	Journal	Study Type	Sample Size	Age	Sample obtained	Aymptomatic	Characteristic Related to Question 2	Excerpt	Added on	DOI	CORD_UID

```


There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition 
rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook 
submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address patient descriptions related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","23"
"884","551982","1314380","05/13/2020 15:53:47","## Task Details

**Create summary tables that address models and open questions related to COVID-19**

Specifically, we want to know what the literature reports about:
 - Human immune response to COVID-19
 - What is known about mutations of the virus?
 - Studies to monitor potential adaptations
 - Are there studies about phenotypic change?
 - Changes in COVID-19 as the virus evolves
 - What regional genetic variations (mutations) exist
 - What do models for transmission predict?
 - Serial Interval (for infector-infectee pair)
 - Efforts to develop qualitative assessment frameworks to systematically collect


 
And we also want to know what the literature reports about the following questions that we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 4 - Models, Open Questions`.**  


`Date	Study	Study Link	Journal	Study Type	Method	Result	Measure of Evidence	Added on	DOI	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition 
rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook 
submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address models and open questions related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","14"
"885","551982","1314380","05/13/2020 15:54:52","## Task Details

**Create summary tables that address material studies related to COVID-19**

Specifically, we want to know what the literature reports about:
 - What do we know about viral shedding in stool?
 - What do we know about viral shedding nasopharynx?
 - What do we know about viral shedding in urine?
 - What do we know about viral shedding in blood?
 - How does the virus persist on inanimate surfaces?
 - How long can the virus remain viable on surfaces?
 - Persistence on different materials
 - Adhesion to hydrophilic/phobic surfaces
 - Susceptibility to environmental cleaning agents
 - Decontamination based on physical science
 


And we also want to know what the literature reports about the following questions that we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 5 - Materials`.**  


`Publication date	Study	Study Link	Journal	Study Type	Material	Method	Days After Onset/Admission (+) Covid-19 Presence (maximum unless otherwise stated)	Property 2	Conclusion	Measure of Evidence	Added on	DOI	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address material studies related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","12"
"886","551982","1314380","05/13/2020 15:55:48","## Task Details

**Create summary tables that address diagnostics studies related to COVID-19**

Specifically, we want to know what the literature reports about:
 - What do we know about diagnostics and coronavirus?
 - New advances in diagnosing SARS-COV-2

 
And we also want to know what the literature reports about the following questions that we added only very recently:
 - Development of a point-of-care test and rapid bed-side tests
 - Diagnosing SARS-COV-2 with Nucleic-acid based tech
 - Diagnosing SARS-COV-2 with antibodies
 - How does viral load relate to disease presentations and likelihood of a positive diagnostic test?

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 6 - Diagnostics`.**  


`Publication date	Study	Study Link	Journal	Study Type	Detection Method	Sample	Obtained Sample	Measure of Evidence  Speed of assay	FDA approval	Added on	DOI	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition 
rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address diagnostics for COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","17"
"887","551982","1314380","05/13/2020 15:56:44","## Task Details

**Create summary tables that address therapeutics, interventions, and clinical studies related to COVID-19**

Specifically, we want to know what the literature reports about:
 - What is the best method to combat the hypercoagulable state seen in COVID-19?
 - What is the efficacy of novel therapeutics being tested currently?


 
And we also want to know what the literature reports about the following questions that we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 7 - Therapeutics, Interventions, and Clinical Studies`.**  


`Date	Study	Study Link	Journal	Study Type	Therapeutic method(s) utilized/assessed	Sample	Severity of Symptoms	General Outcome/Conclusion Excerpt	Primary Endpoint(s) of Study	Clinical Improvement (Y/N)	Added on	DOI	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition 
rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address therapeutics, interventions, and clinical studies","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","22"
"888","551982","1314380","05/13/2020 15:58:25","## Task Details

**Create summary tables that address risk factor studies related to COVID-19**

Specifically, we want to know what the literature reports about:
 - Hypertension
 - Diabetes
 - Male gender
 - Heart Disease
 - COPD
 - Smoking Status
 - Age
 - Cerebrovascular disease
 - Cardio- and cerebrovascular disease
 - Cancer
 - Respiratory system diseases
 - Chronic kidney disease
 - Chronic respiratory diseases
 - Drinking
 - Overweight or obese
 - Chronic liver disease



 
And we also want to know what the literature reports about the following questions that 
we added only very recently:
 - This is where you will find new questions, if applicable

## Expected Submission

**Article summary tables.  Specifically, the expected submission will be one or more .CSV files saved to the output folder of a Kaggle Notebook.  The .CSV files should contain summary tables that follow the table formatting that is both described and demonstrated in the folder that is titled `target_tables`.  For this specific task, article summary tables should follow the table formatting: `Group 8 - Risk Factors`.**  


`Date,	Study,	Study Link,	Journal,	Study Type,	Severity of Disease,	Severity lower bound,	Severity upper bound,	Severity p-value,	Severe significance,	Severe adjusted,	Hand-calculated Severe,	Fatality,	Fatality lower bound,	Fatality upper bound,	Fatality p-value,	Fatality significance,	Fatality adjusted,	Hand-calculated Fatality,	Multivariate adjustment,	Sample size,	Study population,	Critical Only, Discharged vs. Death, Added on,	DOI,	CORD_UID`



There should be one .CSV file per target table (and one or more .CSV file per notebook), and the title of the .CSV files should be the same as the titles of the target tables.  It may be advantageous to extract larger excerpts instead of specific values and, likewise, it may be advantageous for values to be prefixed by a human-readable indication of the location of that same value within the full-text document (in square brackets [] to facilitate error-checking). 

To be valid, a [submission](https://www.kaggle.com/product-feedback/121068) must be contained in one or more Kaggle notebooks made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available in order for the submission to be valid.  Previous versions of the article summary tables can be found in the folder that is titled `target_tables`.  

An ideal submission will be able to: (1) recreate the target tables; (2) create new summary tables; and/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles. Notebook authors should attempt to minimize the number of missing values within each new row while also attempting to avoid any errors or inaccuracies.  See the [Kaggle COVID-19 Contributions](https://www.kaggle.com/covid-19-contributions) page for an example of what an article summary table might look like.

Participants must also accept the [competition 
rules](https://www.kaggle.com/cord-19-rules-accept-form).  Please note that you will have to fill out a new rules acceptance form for Round #2 even if you have already filled out a rules acceptance form for Round #1.

## Evaluation

**Submissions will be scored using the following grading rubric:**

Completeness (5 points)
 - Did the participant accomplish the task?
 - Are there missing values?
 - Bonus points for early submissions!

Accuracy (5 points)
 - Are the results relevant?  
 - Are there inaccurate values?

Documentation (5 points)
 - Is the methodology well documented?
 - Is the code easy to read and reuse?

## Timeline

- Submission deadline is June 16, 2020 at 11:59pm UTC.**
- Bonus points for early submissions!

**By this deadline you must have accepted the 
[rules](https://www.kaggle.com/cord-19-rules-accept-form) and shared your notebook submission publicly to be considered for prizes in each respective round

## Prizes
- Kaggle is sponsoring a **$1,000 per task** award to the winner who is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. Some details:
 - Prize-eligible tasks are defined and specified as such on the task by the sponsor and hosts.
 - A team or user is eligible to receive awards for multiple tasks. 
 - If the team consists of multiple users, they may elect to divide the monetary prize equally or donate their entire award.
 - Prizes will be distributed in 2 rounds, per details contained in the timeline.
 - Evaluation will be judged by a committee consisting of subject matter experts. 
 - All judgments are considered final.
 - Kaggle reserves the right to adjust the prize distribution schedule if needed.","","Create summary tables that address risk factors related to COVID-19","COVID-19 Open Research Dataset Challenge (CORD-19) Round #2","06/17/2020 00:00:00","40"
"2194","888068","2941945","09/22/2020 12:40:01","## Task Details
The task is to predict whether patient have 10 year risk of coronary heart disease CHD or not. Additionally, participants also asked to create some data visualization about the data to gained actionable insight about the topic.","","HME Workshop 2020 : Predict  Heart Disease of Patients","Can you predict whether patient have 10 year risk of coronary heart disease CHD or not from given data?","","1"
"1630","730110","2944641","08/07/2020 21:40:11","## Task Details
Our teacher gave us the task to use the data in dataset to test the model only, and they did not tell how to train the model. First, What i done is that, i used to predict the order of one genome using the trainng model of ther genome. Example: I train the model using plasmid data and then using this model i try to predict the order of other genome assembler ( hamburgensis) but the order prediction is not between (1-10). The predicted orders got are in thousands and negative hundreds. So what i do is that i used the same data for training as well as testing by train the model and serialize it and then de-serialize the same model for prediction which is not a good practice .Give me the solution or suggestion how to deal with it. 

## Expected Submission
1) Model Training and Prediction technique according to the scenario discuss above using best practices in Data Science.
2) Predict the orders

## Evaluation
Model Evaluation metrics must be Good..","","Model Training and Testing","","","0"
"1857","848405","2948167","08/28/2020 11:10:42","## Task Details
First part for a decent usage is the data cleaning.

## Expected Submission
Data cleaning can be performed with the well-known libraries e.g. of the python or R language.

## Evaluation
A good solution reflects the fact that the steps and results are explained and the code is well readable.","","data cleaning","","","0"
"1864","848196","2979319","08/29/2020 00:05:57","## Task Details
What are the most common skill and education requirements for Machine Learning jobs?","","Machine Learning Jobs","","","0"
"1865","848918","2979319","08/29/2020 00:07:12","## Task Details
What are the most important factors in employees happiness in a company?","","Company Reviews","","","0"
"2202","888463","2980755","09/23/2020 03:59:17","Predict whether a patient has heart disease or not. Make sure you randomize the dataset before using it in any learning model. Reserve 10% of the dataset for testing.","","Predict whether a patient has heart disease.","To predict based on the given attributes of a patient that whether that particular person has a heart disease","","9"
"2005","862935","2991387","09/07/2020 21:38:36","## Task Details
Single Nucleotide Polymorphisms (SNPs) are among the most important types of genetic variations influencing common diseases and phenotypes. Recently, some corpora and methods have been developed with the purpose of extracting mutations and diseases from texts. However, there is no available corpus, for extracting associations from texts, that is annotated with linguistic-based negation, modality markers, neutral candidates, and confidence level of associations.


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","SNP-Phenotype association extraction from text","","","0"
"1262","617345","2996189","07/05/2020 15:14:27","Which Team Will Win the League if matches lasted for 45 Minutes only.","","Which Team will Win the League if matches lasted for 45 Minutes only.","Find out winner for each season.","","1"
"2887","617345","5856381","12/08/2020 06:34:24","## Task Details
I am interesting in knowing how the position on the leaderboard of a team affects the probability of the team winning or losing the match.
So for example, Betis is in position 10 and Sevilla is in position 12 in LaLiga.
What is the probability based on the data available of every team to win?
What is the probability of team in position 10 to win a team in position 12?

## Expected Submission
Analyzing the historical data and come with a table when one can see what is the historical probability of a team in a x position to win the match and to draw the match against any other team in position y.

## Evaluation
We will  analyze every answer to this question in deep","","What is the probability of a team winning the match based on their position on the leaderboard?","Probability based on the leaderboard","01/31/2021 23:59:00","1"
"1494","797181","2837140","07/27/2020 09:31:08","## Task Details
The Dataset gives the features of the crops. In the training set, you can see the Crop_Damage  feature which shows the outcome of the harvest of that particular crop. The outcome depends on the pesticides used, it's doses, the season, and many more. Use this data to predict the outcome of the crops in the test data set. The possible values of the target column are 0, 1, and 2. 0 implies alive, 1 implies Damage due to other causes and 2 represents Damage due to Pesticides.

## Expected Submission
Submit the outcome column along with the ID. Refer to the sample submission dataset for clarification.

## Evaluation
The evaluation criteria are Accuracy.

### Happy Coding üòé","","How's the Crop?","Harvest Outcome Prediction","","2"
"1054","693770","3012786","06/04/2020 23:37:04","## Task Details
A Notebook with PyViz graphs with the names of the Philosophers as the nodes.

## Expected Submission
What should users submit? They should solve the task creating a Notebook. 
 A Notebook with at least one single PyViz graph.  

The more PyViz graphs the better it would be. 

### Further help
Inspiration: https://www.kaggle.com/saipradeepvg/cord-19-network-analysis-for-drugs-and-vaccines","","Philosophers PyViz","","","0"
"1959","856740","3014824","09/05/2020 14:41:39","You can make models and compare!


Observation values ‚Äã‚Äãare available, you can apply different transformations and get more information!","","Comparison","","","0"
"1629","737822","3015400","08/07/2020 19:46:51","## Task Details
Power Generation and Consumption never stop but its reports might. The task requires you to impute missing values for the days when reports were not generated (from 18th March 2020 to 31st May 2020) due to National Lockdown in India.
## Expected Submission
A .csv file containing tentative production values sorted date-wise.

## Evaluation
The evaluation will be based on tentative power consumption dataset available for the missing dates [here](https://www.kaggle.com/twinkle0705/state-wise-power-consumption-in-india).","","Predict Electricity Generation in India for lockdown period","Electricity Generation Prediction using 3 years time series data","","9"
"1529","805905","3023930","07/29/2020 19:08:27","## Task Details
What is the best approach to predict COVID-19 Cases & Deaths?

## Expected Submission
Submit notebooks focused on forecasting techniques for this specific task.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)","","Forecasting COVID-19 Cases & Deaths","","","0"
"1967","836326","3034187","09/06/2020 08:37:43","The task here is simple, you are provided with some images of a night sky patch that i took from my pretty much standard smart phone, you are to apply different stacking techniques to find the stars that are hidden in these images, i also have a notebook where i implemented this using max stacking included with this data set.

Looking forwards for your submissions !!","","Stack Images : Find whats hidden!","","","0"
"1201","737379","3048417","06/25/2020 07:31:24","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Predict by using supervised algorithms. Evaluate and compare the model performamce.","","Predict by using supervised algorithms. Evaluate and compare the model performamce.","","","1"
"1278","761218","3048417","07/07/2020 09:31:32","## Task Details
The task is to detect the individual colors that are presented in jpg file.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Detect individual colors using jpg file","","","0"
"3979","575453","7112496","04/06/2021 03:10:25","## Task Details
A dream of DeepLearning","","module","","04/12/2021 23:59:00","0"
"915","656533","3071439","05/16/2020 01:43:13","Use Machine Learning algorithms to find out the lowest misclassification error rate. and also note the best accuracy rate.","","Find Lowest Misclassification Error rate","Machine Learning Techniques","","0"
"1266","757458","3074555","07/06/2020 10:57:59","## Details
You need to develop a set of models to predict respondent satisfaction (the `satisfied` field on the dataset) when working in a particular profession (the `jobtitle` field).

You can make a prediction by:
1. answers to questionnaire questions;
2. scale values;
3. the calculated psychotype.
It is possible to use one or several attributes simultaneously.","","Prediction of job satisfaction","","","0"
"1267","757458","3074555","07/06/2020 11:07:51","Suggest a way to reduce the number of questions in the questionnaire with the lowest decrease in the quality of the job satisfaction forecast for any profession from the dataset (`jobtitle` field).

To solve this problem, you should first solve the problem of [predictive model creation](https://www.kaggle.com/pmenshih/kpmi-mbti-mod-test/tasks?taskId=1266). After that, you can try different approaches to reducing the number of questions by comparing the model performance on the full and reduced sets of questions.","","Reducing the number of questions in the questionnaire","","","0"
"1293","764416","3077375","07/08/2020 22:18:00","Perform exploratory data analysis to collect more insights and better visualizations to understand the data.","","Exploratory Data Analysis (EDA)","","","0"
"1295","764421","3077375","07/08/2020 22:46:07","# Exploratory Data Analysis (EDA)

Perform exploratory data analysis to collect more insights and better visualizations to understand the data.","","Exploratory Data Analysis (EDA)","","","0"
"1296","764422","3077375","07/08/2020 22:46:18","# Exploratory Data Analysis (EDA)

Perform exploratory data analysis to collect more insights and better visualizations to understand the data.","","Exploratory Data Analysis (EDA)","","","0"
"1297","764424","3077375","07/08/2020 22:46:28","# Exploratory Data Analysis (EDA)

Perform exploratory data analysis to collect more insights and better visualizations to understand the data.","","Exploratory Data Analysis (EDA)","","","0"
"1294","764425","3077375","07/08/2020 22:45:54","# Exploratory Data Analysis (EDA)

Perform exploratory data analysis to collect more insights and better visualizations to understand the data.","","Exploratory Data Analysis (EDA)","","","0"
"1305","764425","3077375","07/09/2020 10:49:33","There are various types of comments. Sentiment analysis can be done from those comments.","","Sentiment Analysis","","","1"
"2332","900904","5687361","10/05/2020 03:40:31","classification algorithm","","ML exercise","","","1"
"1840","845992","3090840","08/26/2020 12:13:22","Yearly baseball batting averages:  A good example of simple regression is the exercise of predicting a numerical measure of a professional athlete's performance in a given year by a linear function of his or her performance on the same measure in the previous year.  Baseball batting averages are particularly good raw material for this kind of analysis because they are averages of almost-independent and almost-identically distributed random variables with large sample sizes, and they measure a skill that needs to be exhibited within acceptable limits by all players, not merely specialists at a position.","","Simple Regression Model to Predict the batting average","","","1"
"1831","845025","3090840","08/25/2020 16:59:28","An insurance company need to come up with a good marketing strategy.  They want to run an e-mail campaign.  Before sending a mail to all the available e-mail addresses, they want to first build a predictive model and identify the customers who are most likely to respond.

Analyze the historical data and build a predictive model that helps us in maximizing the response rate

Please write small bullet point descriptions of the steps. 
‚Ä¢	Business
o	Business Background
o	Objective of the problem 
‚Ä¢	Data 
o	Training , testing and holdout data
o	Data Explanation 
o	Cleaning, if required(Many variables have missing values)
o	Reporting the important insights
o	Include screen shots, if required
‚Ä¢	Model Building
o	Model building code and explanation
o	Model recalibration, if required
o	All steps in model building
o	Include screen shots, if required
o	Include variable transformations if required. 
‚Ä¢	Validation
o	Validation measures 
o	Sensitivity & Specificity
o	Accuracy
o	ROC & AUC
o	Include screen shots, if required
‚Ä¢	Implementation
o	Prediction examples
o	How the model can be used","","Direct Mail Marketing","","","1"
"1832","845045","3090840","08/25/2020 17:18:17","Predict which insurance policy a customer will buy based on his shopping history

As a customer shops an insurance policy, he/she will receive a number of quotes with different coverage options before purchasing a plan. This is represented in this challenge as a series of rows that include a customer ID, information about the customer, information about the quoted policy, and the cost. Your task is to predict the purchased coverage options using a limited subset of the total interaction history. If the eventual purchase can be predicted sooner in the shopping window, the quoting process is shortened and the issuer is less likely to lose the customer's business.

Using a customer‚Äôs shopping history, can you predict what policy they will end up choosing?","","Analysis on purchase pattern based on shopping history","","","0"
"4568","595677","7486601","05/30/2021 18:08:34","Get Information from the Data.","","Get Meaningful Insights","","","0"
"6811","902131","4561355","11/10/2021 13:21:16","## Task Details

Perform forecasting using ARIMA or SARIMAX on any dataset from the list and use graphs for visualization.

## Expected Submission
1.  Check if data is stationary? (If not, convert to stationary)
2. Plot ACF and PACF
3. Use graphs to display forecasted data.","","Forecasting using SARIMAX or ARIMA","Perform forecasting on any dataset","","1"
"2842","787284","6092736","12/02/2020 20:35:18","How can I use this dataset ""photo2cartoon"" instead of ""kr.datasets.fashion_mnist.load_data()"" in this script:
(Xtrain, _),(_, _) = kr.datasets.fashion_mnist.load_data()","","mnist Dataset Change","","12/31/2020 23:59:00","0"
"2569","889404","3149056","10/28/2020 22:33:19","## Task Details

Please read the Description of this dataset first.

Let's start from the most basic prediction task - use one or multiple sensor readings to predict the flow meter reading. 

Why flow meter reading, because it is the most simple feature that could be understood and explained easily. It directly links to the manufacturing process condition or process quality. If the flow rate is too low, then the core process will not be supplied with sufficient material and may cause product defect. 

The manufacturing team would normally decide the maintenance activity according to the flow meter reading: if the flow is below a defined threshold, then replace the filter. However, the degradation of the filter is not the only source of the risks. Everything connected within the pipeline may reduce or stop the flow, such as the failure of the pump, leakage of the valves. To narrow down the scope, in this case, we only focus on the equipment and sensors introduced in the dataset.

To make the use case more vivid, image when Easter or Christmas is coming, the maintenance manager wants to know how likely the production line would have equipment failure (continue process facility would not be shut down during holidays, even no part will be produced at holidays). So now it is your task, to predict the flow meter reading for the coming holiday (say at least 3 days).  Based on your prediction result, the maintenance manager would decide whether to add one maintenance action before the holiday.

Before you offered the power of prediction (via machine learning), the maintenance manager normally asks some staff to visit the shop floor during the holiday. The night's watch may not fully enjoy the holiday, and they may not visit the shop floor every day. If something got wrong, and the night's watch found it too late or react too late, you may find that the leaked fluid has flooded the entire shop floor.  

So, you'd better make the prediction work, and accurately.
 

## Expected Submission

You should submit a notebook that illustrates your approach to which able to predict flow sensor readings (FM1) in the next 3 days, based on the past 14 days records.

You may able to carry out the time-series forecasting with 

1. univariate input (past sensor readings), single-step output (predict the sensor reading at 72 hours ahead)
2. univariate input (past flow sensor readings), multiple-step output (predict the sensor reading from current to next 72 hours, every 30 min)
3. multivariate input (the record from multiple sensors), single-step output (predict the sensor reading at 72 hours ahead)
4. multivariate input (the record from multiple sensors), multiple-step output (predict the sensor reading from current to next 72 hours, every 30 min)


## Evaluation

The result can be judged by Mean absolute percentage error (MAPE). The smaller MAPE, the better.

## Hint

- CNN, RNN, LSTM and their combination stacking can be used here for time-series forecasting. 
- if you are using RNN kind Deep Learning methods, you probably would use the sliding window technique.  
- If you have no idea of which features to be included as the multivariate input, try all features at first.
- Try with or without feature engineering.


## Advanced tasks:

1. You can verify the size of the sliding window and prediction window. Could you find the minimum sliding window size and the maximum prediction window size while keeping an acceptable accuracy?
2. Normally we take the first 70% samples for training and validation, and the rest 30% samples for test. Is this the best way to distribute training and testing sets for a time-series dataset?
3. Anomaly detection and classification can also be applied to this dataset. We may create new tasks with additional information for anomaly detection and classification soon.

Have fun!","","Process condition prediction","Predict the flow meter readings","","1"
"613","564100","3171608","03/21/2020 11:06:23","With all these data, one can simply build a chatbot for the people, to let them know some common symptoms leads to major problems.","","Chatbot","Symptoms Checker Chatbot","","20"
"1798","841625","3177784","08/22/2020 23:37:20","Predict the next draw to see whether Lotto results are completely random or not.","","Predict Future Draws","","","2"
"2249","894067","3198184","09/26/2020 10:10:45","Predict whether the loan will be available or not.","","Loan Status","","","0"
"1686","600276","4222412","08/13/2020 09:11:37","Does killing dragons make you more likely to win?","","Dragons Effect on Winning","","","9"
"2381","600276","5880715","10/08/2020 13:00:10","1
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict game outcome","","","4"
"800","616334","3210935","04/22/2020 21:00:39","Classify the game outcome at the different frames of the game.","","Predict the game outcome","Will the blue team gonna win?","","0"
"801","616334","3210935","04/22/2020 21:01:38","Predict the game duration at the different frames of the game.","","Predict the game duration","Can you predict the game duration?","","0"
"802","616334","3210935","04/22/2020 21:03:26","Find features that are key to win a game.","","Identify winning most important features","Which features are key to win the game?","","2"
"943","666227","3220660","05/21/2020 07:44:07","Implement a Regression model that can be used for predict one parameter from the other and vice-versa.","","Build a predictive model for determining height or weight of a person.","","","1"
"2042","867355","3232592","09/10/2020 16:38:14","## Task Details
Predicting the winner only based on champion selection is often a coin flip, so your goal is to try to predict the winner side with the highest possible accuracy .

## Expected Submission
You have to solve the task primarily using Notebooks

## Evaluation
Use EDA and various Classification Algorithms to predict the target variable with higher accuracy score.","","Predict the winner","","","0"
"2348","908328","3247269","10/06/2020 06:56:12","## Task Details
Apply all the regularization techniques

## Expected Submission
- Notebook consisting of the different regularizer
- Save your notebook in your name and register number
- make your submissions private and add me as a collaborator, so that I can check the progress

## Evaluation
-Depends on the number of regularization technique
-Uniqueness on the methodologies
-on-time submission","","Regularization","Apply different regularization techniques","10/07/2020 23:59:00","6"
"2334","906511","3252339","10/05/2020 05:28:45","Task Details
Try and find the highly demanded in the transfers

Expected Submission
You're expected to submit a visualization of your findings

Evaluation
If your visualization says something‚Ä¶ submit it
And also check out my notebook on the IPL auction

Further help
If you need additional inspiration, check out these existing high-quality tasks:

https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysis of Highly Demanded players(age,position, etc.)","","","0"
"2333","906586","3252339","10/05/2020 05:25:07","## Task Details
Try and find the highly demanded in the transfers

## Expected Submission
You're expected to submit a visualization of your findings

## Evaluation
If your visualization says something... submit it
And also check out my notebook on the EPL transfers

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysis of Highly Demanded players(age,position, etc.)","","","1"
"2355","909816","3252339","10/07/2020 06:54:40","## Task Details
Toos is really important in cricket. It can make or break a game

## Expected Submission
Your submission should be made of each team's choices and predict what will be their most used choice this season

## Evaluation
Your evaluation will be based on the accuracy of your model

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","What is the preferred toss decision for each team?","","","0"
"1739","836531","3257097","08/18/2020 14:05:34","## Task Details
Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.

Given the abstract and title for a set of research articles, predict the topics for each article included in the test set. 

Note that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: 

1. Computer Science

2. Physics

3. Mathematics

4. Statistics

5. Quantitative Biology

6. Quantitative Finance

------------------------------------------------------------------------------------

**Data Dictionary **
train.csv

 ID       : Unique ID for each article

TITLE  : Title of the research article

ABSTRACT  :  Abstract of the research article

Computer Science  :  Whether article belongs to topic computer science (1/0)

Physics	:  Whether article belongs to topic physics (1/0)

Mathematics	:  Whether article belongs to topic Mathematics (1/0)

Statistics  :  Whether article belongs to topic Statistics (1/0)

Quantitative Biology  :  Whether article belongs to topic Quantitative Biology (1/0)

Quantitative Finance  :  Whether article belongs to topic Quantitative Finance (1/0)

------------------------------------------------------------------------------------

**test.csv**

ID    :  Unique ID for each article

TITLE   :  Title of the research article

ABSTRACT  :  Abstract of the research article

------------------------------------------------------------------------------------

**sample_submission.csv**

ID  :  Unique ID for each article

TITLE  :  Title of the research article

ABSTRACT  :  Abstract of the research article

Computer Science  :  Whether article belongs to topic computer science (1/0)

Physics	 :  Whether article belongs to topic physics (1/0)

Mathematics	 :  Whether article belongs to topic Mathematics (1/0)

Statistics  :  Whether article belongs to topic Statistics (1/0)

Quantitative Biology  :  Whether article belongs to topic Quantitative Biology (1/0)

Quantitative Finance   : Whether article belongs to topic Quantitative Finance (1/0)

------------------------------------------------------------------------------------

## Expected Submission
Please ensure that your final submission includes the following:

Solution file containing the predicted 1/0 for each of the 6 topics for every research article in the test set
Code file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission

## Evaluation

Submissions are evaluated on micro F1 Score between the predicted and observed topics for each article in the test set

### Further help
-","","Topic Modeling for Research Articles","Independence Day 2020 ML Hackathon","10/31/2020 00:00:00","0"
"1696","830660","3257097","08/14/2020 13:45:16","## Task Details
Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.
Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. Brands can use this data to measure the success of their products in an objective manner. In this challenge, you are provided with tweet data to predict sentiment on electronic products of netizens.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
The metric used for evaluating the performance of classification model would be weighted F1-Score.

### Further help

Data
train.csv - For training the models, we provide a labelled dataset of 7920 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.

test.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.

sample_submission.csv - The exact format for a valid submission

Most profane and vulgar terms in the tweets have been replaced with ‚Äú$&@*#‚Äù. However, please note that the dataset still might contain text that may be considered profane, vulgar, or offensive.

Data Science Resources

Get started with NLP and text classification with our latest offering ‚ÄòNatural Language Processing (NLP) using Python‚Äô course
Refer this comprehensive guide that exhaustively covers text classification techniques using different libraries and its implementation in python.
You can also refer this guide that covers multiple techniques including TF-IDF, Word2Vec etc. to tackle problems related to Sentiment Analysis.","","Identify the Sentiments","Sentiment analysis","","0"
"1744","836766","3259680","08/18/2020 17:33:51","## Task Details
High Frequency trading in stock market is always challenging, here we want to build a autonomous trading pipeline which can be used to do high frequency trading with high portfolio


## Expected Submission
Please submit your notebook which includes your model and your profit


## Evaluation
There is some constraints on your model:
1. First it should be very fast since time is money for high frequency trading
2. Second, there is a limit on max Volume you can have. Eg: your cash can at most buy 10 at the same time.
3. There is a transaction fee for each buy and sell. For google stock, on average it's 0.005 per Volume.
4. Your model should be robust. Eg: You should train and test that on training and testing data that are independent of each other.
5. Remember: Traders don't do trading unless they are pretty sure about that. Thus, you model should buy and sell as few as possible.

### Further help
Feel free to contact me on Kaggle","","Build a autonomous pipeline in dealing with high frequency stock trading","Let's go high frequency trading!","","0"
"4842","806447","4960963","06/19/2021 07:52:31","Explore the Data","","Exploratory Data Analysis","","03/19/2024 23:59:00","1"
"815","618797","3263330","04/24/2020 10:42:00","## Task Details
Your task is to predict the performance on such tests given the demographic information and training program/test details. This will enable your client to strengthen its training problem by figuring out the most important factors that lead to better engagement and performance for a trainee.


## Evaluation
The evaluation metric is the AUC ROC score.","","Target Variable  0 - test failed, 1 -  test passed;","","","0"
"786","611222","3281202","04/19/2020 18:08:00","## Task Details
Their is a lot of misinformation about China reported cases of corona virus and due to this want to know if their is any correlation between China cases since 100th case and other parts of the world against days.

## Expected Submission
Use the data to determine if China blindfolded the world on how infectious corona virus was by comparing different part of the world cases against those reported in China over time.

## Evaluation
This is a knowledge sharing task hence no good or bad submission as we try to understand this global pandemic using public available data.   

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","correlation between number of cases in China against the rest of the world.","novel corona virus 2019 confirmed cases since 100th case dataset","","0"
"706","589930","3281202","04/06/2020 21:30:11","## Task Details
Aim of this data set is to find out if their is any correlation between number of covid19 infections cases and climatic factors Temperature and Air Density. 

## Expected Submission
Help understand how those climatic factors influence spread of covid19 virus across different geographical areas as while as predicting future cases in regard to changes in temperature.","","Effects of various weather aspect on number of Covid19 cases","Correlation Between Temperature,Air Density and Covid19 cases","","0"
"2299","901009","3290492","10/01/2020 03:09:39","## Task Details
Predict least- RMSLE (Root Mean Squared Logarithmic Error) metric

## Expected Submission

One can use np.sqrt(mean_squared_log_error( actual, predicted))

## Evaluation
Link to submit solution-https://www.machinehack.com/hackathons/house_price_prediction_beat_the_benchmark/submission","","Predict Least RMSLE (Root Mean Squared Logarithmic Error)","","10/25/2020 23:59:00","0"
"2354","848697","3292276","10/07/2020 05:01:01","I have posted a notebook of my submission, listing the approach I used for this challenge..!

Can you list out your approach / solution which can further improve the accuracy of this dataset..!","","Can you suggest me some more better techniques to solve this problem statement","Machine Learning Hackathon","02/28/2021 00:00:00","0"
"2169","881848","3303517","09/19/2020 16:50:17","## Task Details
The dataset contains the boiling point and melting point of only the naturally occurring elements. The goal would be to use machine learning to attempt to figure out the values for the non-naturally occurring chemicals.

## Expected Submission
A valid submission should include and will be evaluated on:
* The r**2 and MAE of the model on the training set
* The predicted values
* some visualizations to show how these new values might look
* a reasonable explanation of why the chosen model was used","","Use machine learning to estimate missing values","","","0"
"2153","879551","3305603","09/18/2020 16:13:48","## Task Details
In this task more and more information to be extracted from this dataset.  The task seems vague, but more information in there under the hood.

## Expected Submission
Users are expected to submit at least three more columns to this dataset.

## Evaluation
Submissions with most features will be great to have. No external information to be added.","","Extract Data from Description of this dataset","Extract more and more data about the flower from the description given.","","0"
"1879","846956","3309826","08/29/2020 15:34:53","## Task Details
With 40 explanatory variables describing various aspects of new and resale houses in the metropolitan areas of India, one can predict the final price of houses in these regions.","","Price of properties prediction","","","5"
"1887","826586","3309826","08/29/2020 20:43:14","## Task Details
Use AIML files to train a chatbot","","Train a chatbot","","","0"
"1885","825851","3309826","08/29/2020 20:40:32","## Task Details
Recommend relevant books to a particular user","","Book recommendations","","","4"
"1888","825981","3309826","08/29/2020 20:44:43","## Task Details
To find words that essentially mean the same","","Sentiment Analysis","","","1"
"1886","826031","3309826","08/29/2020 20:42:16","## Task Details
Social network analysis of Star Wars characters","","Star Wars EDA","","","0"
"1884","822182","3309826","08/29/2020 20:38:19","## Task Details
Periocular Recognition for partially obscured faces","","Periocular Recognition","","","0"
"1126","669193","5289158","06/12/2020 20:09:27","Can you please get data - voting count for both imdb and rotten tomatoes?","","Voting count please","","","3"
"1038","669193","4945413","06/03/2020 00:32:22","Would it be possible to have the unique id's changed to the ID's that are used by their streaming platform? For example Inceptions ID on netflix is not ""1"" as listed in the dataset.","","Movie ID's","","","4"
"1028","669193","5153753","06/02/2020 05:03:51","Thumbnail and URL of the movies, please.","","Thumbnail and URL of the movies, please.","","09/30/2020 00:00:00","19"
"2628","669193","4533747","11/04/2020 15:12:45","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. I created this task to do Data Analysis of Movies on Netflix, Prime Video, Hulu and Disney+

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain? User will analyze the data & will tell details.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? More data representation is better solution.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Analysis of Movies on Netflix, Prime Video, Hulu and Disney+","Data Analysis of Movies on Netflix, Prime Video, Hulu and Disney+","","6"
"1784","680165","5260966","08/21/2020 12:14:13","List out the drugs , which are reasons for more deaths.","","Most dangerous drugs","","","3"
"2321","905414","3310580","10/04/2020 07:26:39","## Task Details
Predict number of chocochips in a cookie.

## Expected Submission
Submit your notebooks","","predict number of chocochips in a cookie","","","1"
"2322","905414","3310580","10/04/2020 07:31:30","## Task Details
What other things do you look for in a cookie?

## Expected Submission
You can post your work if you think it helps in regulating the cookie quality control.","","A task of your choice.","","","1"
"685","583749","3315550","04/02/2020 11:43:39","this task is all about localization of object","","localization of mask","","","0"
"532","540717","3315822","03/11/2020 10:39:53","this bank has records of complaints made by users of telecommunications services to the regulatory agency. 

The main challenge of this database is to identify the main reasons for complaints and find solutions to improve customer service.","","Creating solutions to improve service.","","","1"
"1070","696491","3319726","06/06/2020 04:52:28","The task is to visualize the data and show useful insights with respect to the overall performance, map wise, week wise, group-wise, etc.

The aim is to play with data and have fun.","","Data Visualization","","","0"
"1559","811872","3321361","08/02/2020 15:24:29","Think this data could be given a better plot? Check the original paper for what the authors did and then let's see what you can do!","","Fit a curve!","","","0"
"1509","803933","3324417","07/28/2020 16:50:48","## Task Details
Every task has a story   Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Do stock data analysis on NFLX stock data","","","0"
"1204","738102","3335785","06/25/2020 13:19:03","## Task Details
This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","","Churn Modeling","","","0"
"1859","846899","3335785","08/28/2020 13:16:14","Check how all factors depend on the performance of the university","","Check how all factors depend performance of the university","","","0"
"2320","905262","3335785","10/04/2020 04:30:12","The aim is to build a predictive model and predict the sales of each product at a particular outlet.","","Sales Prediction","","","0"
"2138","877858","3335785","09/17/2020 11:58:04","Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.","","cross_cell_prediction","","","0"
"2086","871803","3335785","09/13/2020 17:55:09","## Task Details
You need to predict the chances (probability) of having a favorable outcome","","You need to predict the chances (probability) of having a favorable outcome","","","17"
"2338","907519","3345538","10/05/2020 19:33:46","## Task Details
Carry out preprocessing of data (search for anomalies, removal / cut of outliers, etc.)","","Preprocessing","Carry out preprocessing of data (search for anomalies, removal / cut of outliers, etc.)","","1"
"2339","907519","3345538","10/05/2020 19:35:36","Conduct time series analysis (graphical presentation, descriptive statistics).","","Time series analysis","Conduct time series analysis (graphical presentation, descriptive statistics).","","1"
"2340","907519","3345538","10/05/2020 19:36:23","Summarize the results of the cleaning, data preprocessing. Draw conclusions based on your analysis.","","Summarize the results of the preprocessing","Summarize the results of the cleaning, data preprocessing. Draw conclusions based on your analysis.","","0"
"2341","907519","3345538","10/05/2020 19:36:56","Extract features from raw data via data mining techniques.","","Feature engineering","Feature engineering","","0"
"2342","907519","3345538","10/05/2020 19:37:54","## Task Details
Which of the algorithms is better suited. What data do you think you need to enrich the current dataset for more accurate, effective sales forecasting.","","Algorithms analysis","Which of the algorithms is better suited. What data do you think you need to enrich the current dataset for more accurate, effective sales forecasting.","","0"
"2343","907519","3345538","10/05/2020 19:38:47","## Task Details
Construction of a forecast and its assessment.","","Making a sales forecast","Construction of a forecast and its assessment.","","1"
"2344","907519","3345538","10/05/2020 19:40:28","## Task Details
Please comment on your chosen metric to evaluate the forecast. MAE, RMSE, etc? and why. Is it necessary to use CV in this example? Will it give the best result.","","Metrics","","","0"
"507","522124","4607913","03/04/2020 18:07:53","Can you explain how we can preprocess the data into kdd like file for intrusion detection in IoT devices ?
Please give us a hint on how you preprocessed the raw data?","","Preprocessing data","","","1"
"1688","829066","3375965","08/13/2020 11:28:15","Based on the 1st half of the season, predict standings and points for any or all teams for any week of the 2nd half","","Predict 2nd half points and standings","","","0"
"1689","829066","3375965","08/13/2020 11:31:39","In football, any game can become the turning point for the team by increasing or decreasing the confidence of the team. The team can go on a winning a losing streak after that game. Analyze the data to find such a game and return the week of that game","","Find out turning week in the season for any team","","","0"
"426","496608","4429543","02/03/2020 19:12:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? 
Make a submission file.","","Predict how many people survived ?","","02/04/2020 00:00:00","0"
"2636","609697","5991644","11/06/2020 06:37:53","Create a recommendation engine based on audience type, trending and watch history","","What to watch on Disney+ ?","","","0"
"1009","683067","3387210","05/30/2020 05:33:48","## Visualisation

Do analysis  and visualisation of data.","","Analysis of Life Expectancy trends over time.","","12/31/2020 00:00:00","0"
"1095","701499","3389234","06/08/2020 18:52:11","## Task Details
Analyzing the sentiments of every reviews for each restaurant

## Expected Submission
Submission needs to be in the form of visualizing the key factors of good vs bad ratings

## Evaluation
Evaluation will be based methodology used and intuitiveness of the solution","","Sentiment analysis of reviews","","","9"
"1193","734045","3390077","06/23/2020 08:32:07","The data is for company X which is trying to control attrition. There are two sets of data: ‚ÄúExisting employees‚Äù and ‚ÄúEmployees who have left‚Äù. Following attributes are available for every employee.
Satisfaction Level
Last evaluation
Number of projects
Average monthly hours
Time spent at the company
Whether they have had a work accident
Whether they have had a promotion in the last 5 years
Departments (column sales)
Salary
Whether the employee has left
 
Objective
What type of employees are leaving? Determine which employees are prone to leave next.","","Employee Attrition","Predicting Employee Churn","","1"
"1346","772761","3390641","07/13/2020 06:42:34","The text file contains all the episodes' script. We can train a model to generate new scripts for our beloved TV show! For reference work, see this Github repo by Apoorv Kansal [here](https://github.com/uragirii/Friends-Generator)","","Generate Scripts from the given scripts","","","0"
"1491","498795","3390641","07/27/2020 06:20:26","The physical and chemical properties of different Li-ion silicate cathodes are given. Your task is to classify them into one of the 3 classes monoclinic, orthorhombic and triclinic. Could you beat the score of 74(+/-)5? For a reference work see the work [here](https://www.kaggle.com/divyansh22/neural-network-for-li-ion-classification)","","Classify the Li-ion batteries","","","1"
"2198","885537","3390641","09/22/2020 16:27:30","Classify the sheep into one of the four classes namely Suffolk, White Suffolk, Poll Dorset, and Marino. Can you beat the score of 95%? 

I would be happy to review any solutions submitted anyway. Go around and play with this data!! Have fun classifying!!!","","Classify the sheep into one of the 4 classes.","","","2"
"1336","576340","3391014","07/12/2020 09:02:46","## Task Details
This dataset is created so that users can train their models to predict future price of ripple coin.
## Expected Submission
Users can submit analysis on any type of patterns that price shows while rising or droping suddenly.

## Evaluation
if the future price they predicted matched with real price then model is good. or just try to create bot and give some money in variable and run the algorithm, predict whether bot can make profit.","","Future price prediction","","","0"
"4861","783258","7731687","06/20/2021 15:50:43","https://csuiteparadigm.org


## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","C-Suite Paradigm","Parity in the C-Suite 2030","08/31/2021 23:59:00","0"
"1405","783309","3391014","07/17/2020 17:35:07","## Task Details
Users can practice data cleaning skill on this dataset.","","Data cleaning","Users can practice data cleaning skill on this dataset.","","0"
"4146","841826","4872645","04/21/2021 23:37:12","## Task Details
Creating a classificaiont model to predict the gender of any given student of an MBA class","","Predicting the gender of MBA Class","","06/30/2021 23:59:00","1"
"5644","841826","5146187","08/09/2021 19:04:26","## Task Details
How accurately can I predict the price of a house, given the values of all variables?

## Expected Submission
Use the dataset ""House_Price.csv"" to predict the prices of the houses based on the values of the given features.

## Evaluation
What makes a good solution? 
Well, the model with high performance should be considered to be the best one.","","House Price prediction","","","0"
"2220","891124","3392685","09/24/2020 12:32:15","## Task Details
Develop additional features from the already available ones.

## Expected Submission
Csv file with initial columns and added columns along with the notebook documenting the steps followed in designing new features and code.","","Feature Engineering","","","1"
"1404","782636","3398915","07/17/2020 13:55:26","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3398915%2F0a8b2cf207ef51073c420eab10474bfb%2FScreenshot%20(77).png?generation=1594995724974160&alt=media)

1. Identify the average population growth over the years 
2. Identify the largest age group in Singapore.
 Their average population growth over the years.
3. Identify the group (by age, ethnicity and gender) that: 
a. Has shown the highest growth rate 
b. Has shown the lowest growth rate 
c. Has remained the same
4. Plot a graph for population trends","","EDA on dataset","plot visualisation that shows growth over the years by age group, ethnicity and gender","","1"
"1312","766904","3398915","07/10/2020 04:29:54","### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Insights","Create a data insight report for this dataset and extract relevant information.","","1"
"1132","712911","3406741","06/13/2020 13:15:17","As we all know what is going on in America, the George Floyd issue. So I thought of extracting twitter data for the hashtag #GeorgeFloydFuneral.  My intention is to do sentiment analysis for the tweets for this hashtag. 
I will upload my solution by today, and I want to see what others do, that I didn't so that I can learn from you guys.
Note:- I have been able to extract 200 tweets only. Sorry for that.","","Sentiment Analysis","","","1"
"1154","721058","3406741","06/16/2020 21:23:59","I extracted this data for doing sentiment analysis. As I am a Lucifer TV series fan, so I wanted to see to see how others are feeling for this new upcoming season. I will very soon upload my own analysis, and you guys are free to do analysis of your own.","","Sentiment Analysis","","","0"
"1964","860116","3408653","09/06/2020 06:10:52","## Task Details
This task is for the learning purpose.
By leveraging Foursquare API, try to cluster the Stations based on the count of nearby venues and make insights from each cluster.

## Expected Submission
Expected submission is the Notebook(s) that contains all the valuable insights from the data. Separate Notebooks with Detailed Data exploration and Modelling are also appreciated.

## Evaluation
This task is basically to learn different aspects and insights that one can bring.","","Cluster Metro Stations","Create clusters of the metro stations and Classify them.","","1"
"6434","506737","5309641","10/22/2021 10:11:06","The dataset contains the data referenced in Rieth et al. (2017). Issues and Advances in Anomaly Detection Evaluation for Joint Human-Automated Systems. Each .RData file is an external representation of an R dataframe that can be read into an R environment with the ‚Äòload‚Äô function. The variables loaded are named ‚Äòfault_free_training‚Äô, ‚Äòfault_free_testing‚Äô, ‚Äòfaulty_testing‚Äô, and ‚Äòfaulty_training‚Äô, corresponding to the RData files.
Each dataframe contains 55 columns:
Column 1 (‚ÄòfaultNumber‚Äô) ranges from 1 to 20 in the ‚ÄúFaulty‚Äù datasets and represents the fault type in the TEP. The ‚ÄúFaultFree‚Äù datasets only contain fault 0 (i.e. normal operating conditions).
Column 2 (‚ÄòsimulationRun‚Äô) ranges from 1 to 500 and represents a different random number generator state from which a full TEP dataset was generated (Note: the actual seeds used to generate training and testing datasets were non-overlapping).
Column 3 (‚Äòsample‚Äô) ranges either from 1 to 500 (‚ÄúTraining‚Äù datasets) or 1 to 960 (‚ÄúTesting‚Äù datasets). The TEP variables (columns 4 to 55) were sampled every 3 minutes for a total duration of 25 hours and 48 hours respectively. Note that the faults were introduced 1 and 8 hours into the Faulty Training and Faulty Testing datasets, respectively.
Columns 4 to 55 contain the process variables; the column names retain the original variable names.","","Fault detection","","12/31/2021 23:59:00","0"
"1008","682532","3414979","05/29/2020 19:42:50","## Task Details
This Task is about finding some insights in the data and assest the data quality,
see there is duplicated columns or rows in the data, answer the question why is there nans in some columns and do some plots to gain insights.

## Expected Submission
The solution should be in a form a notebook.

## Evaluation
You can compare your analysis with other people's analysis","","Data Analysis","","12/31/2021 00:00:00","0"
"2028","862378","3416291","09/09/2020 12:14:25","Try analysis on this dataset and try to find some patterns on how Presidents were elected in America","","EDA on The American Presidency Project","","","1"
"1222","744747","3421460","06/29/2020 15:35:48","1. Prepare a model and Submit your Kernel and analytic reports.
2. Report should include all the information detailed in the CriteriaforData_Challenge.docx","","Prepare Model","","","0"
"833","620378","3425231","05/01/2020 21:33:28","## Task Details
this task is basically for temperature prediction of future dates 

## Expected Submission
they have to submit future predicted data with lat, lon , tem, date
## Evaluat
F1  accuracy score 

### Further help","","predict temperature of one month june 2020 to july 2020","predict temperature of future values from provided dataset","05/15/2020 00:00:00","1"
"1124","710428","3427212","06/12/2020 13:59:39","## Task Details

Start by doing some basic checks ‚Äì are there any data issues? Does the data need to be cleaned?

Gather some interesting overall insights about the data. For example -- what is the average transaction amount? How many transactions do customers make each month, on average?

Segment the dataset by transaction date and time. Visualise transaction volume and spending over the course of an average day or week. Consider the effect of any outliers that may distort your analysis.

What insights can you draw from the location information provided in the dataset?","","Exploratory Data Analysis","Segment the dataset and draw unique insights, including visualisation of the transaction volume and assessing the effect of any outliers.","","5"
"1125","710428","3427212","06/12/2020 16:03:14","## Task Details

Using the transaction dataset, identify the annual salary for each customer

Explore correlations between annual salary and various customer attributes (e.g. age). These attributes could be those that are readily available in the data (e.g. age) or those that you construct or derive yourself (e.g. those relating to purchasing behaviour). Visualise any interesting correlations using a scatter plot.

Build a simple regression model to predict the annual salary for each customer using the attributes you identified above

Build a decision-tree based model to predict salary. Does it perform better? How would you accurately test the performance of this model?","","Predictive Analytics","Explore correlations between customer attributes, build a regression and a decision-tree prediction model based on your findings.","","2"
"1625","820702","3427490","08/07/2020 15:52:18","### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","To preprocess data for further use","if you download/use the dataset pls upvote!!","","0"
"694","586885","3444071","04/04/2020 18:09:48","## Task Details
There is a Summer Analytics Kernel in the Kernels section. Click on Copy and Edit and download the notebook and complete all of the given subtasks to score maximum points. Note you must complete at least 8 out of 10 subtasks to Pass the week","","Summer_Analytics","MIni Project Week 2","04/23/2020 00:00:00","0"
"1842","845612","3449691","08/27/2020 04:08:18","## Task Details
One has to create different types of visualizations using external multilingual dataset and the `font files` that are being provided in this dataset. This task would be helpful when we are required to perform EDA on different languages present in the dataset.

## What if font file for the required language is not present?
In this situation you could simply download the corresponding font file over the internet and try visualizations with that and if no issue arises in testing of that font file, you may also contribute that font file to this dataset if you wish.

## Expected Submission
The users are required to submit their approach for EDA on different languages for any external multilingual dataset with the best use of this dataset.

## Evaluation
The best notebook would be based on the number of votes it recieved based on its content. The more the number of different and intuitive visualizations, the more it would be better. However the purpose of this task is to make users comfortable 
when working with a multilingual dataset.

### Further help
If you need additional inspiration, check out my notebook in which I used the multilingual dataset from ""Contradiction, My Dear Watson"" comeptition and created Word Cloud visualizations for different languages using this dataset.
- https://www.kaggle.com/forwet/data-upsampling-data-visualization-robert-model","","Language(s) visualization with (Multilingual + Font) dataset.","Create visualizations for multiple languages using external data and the provided font files.","","2"
"2154","855240","4422823","09/18/2020 18:30:50","## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
Submission should be a notebook clearly showing the performance on unseen data.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
Evaluation metric - Overall Accuracy","","Predict Age Gender and Ethnicity from Images","","","12"
"751","602288","3458103","04/14/2020 17:28:06","## Task Details
Users can perform Eda technique
visualize data in different plots
Build statistical models
parameters that can be predicted ??

## Evaluation
Good for a fresher interested and passionate about datascience
can learn a lot
way of coding shows your effort","","DATA ANALYSIS/VISUALIZATION","","","11"
"2404","890079","3461174","10/10/2020 22:48:39","No particular details yet.

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict violence from a video","Frame-level or near frame-level prediction","","1"
"991","672377","3469060","05/28/2020 06:15:43","## Task (50 points)
Build ML models that can classify MRI into one of the following class:
- No Tumor
- Benign Tumor
- Malignant Tumor
- Pituitary Tumor 

## Description
The primary goal of the data-set is to be used for classification. Building ANN/CNN/TL models that can classify these MRI into four classes. 

## Evaluation
How is my kernel better than others?
- Your model works! It can *correctly* classify the MRI.
- If the model you have built has generalized well. Your accuracy is &gt;90%. Your F1-Score is &gt;90, etc. 
- Your model is light, it has minimum possible parameters. The size of the model on disk is as small as possible.
- Try it with some random images from google, can your model correctly classify those images? If yes, then congratulations! You rock!


### Bonus ( + 50 points)
If your model can flag invalid inputs, it's a bonus score. (Eg. if your model receives images like a cat/ a dog/ etc, your model should be able to label it as invalid.) Your model should only accept MR images.

### Help ?
We have already completed work on this dataset. Do check the link if you want to read more about the topic.
[Medium.com](https://medium.com/@sartajbhuvaji/brain-tumor-classification-546a72d4103b)","","MRI Classification","Classify MRI into four respective classes.","10/05/2022 00:00:00","9"
"992","672377","3469060","05/28/2020 06:23:47","## Task  (50 points)
Segment the tumor in MRI

## Description 
Segment the tumor from MRI. Plot a box around the tumor area and also provide a mask to the region.

## Evaluation
- Correct segmentation of tumors.
- Can segment more than one tumor.
- Can plot box around the region.

### Bonus (+50 points)
- If you can find the are of tumor region segmented you are officially amazing! You get bonus points for your skill.

### Help ?
We have already completed work on this dataset. Do check the link if you want to read more about the topic.
[Medium.com](https://medium.com/@prajbhumkar/brain-tumor-segmentation-in-mri-abc268faa304)
[Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)","","MRI Segmentation","Segment the tumor from MRI","","7"
"993","672377","3469060","05/28/2020 06:32:46","## Task (50 points)
Build Flask/Django applications and connect the ML models with GUI for ease of access.

## Description
- Build a front end where users can upload their MRI to the system, link the best ML model at the back-end. Display the result of classification [Task 1](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri/tasks?taskId=991) to the user on the web-page.
-  Build a front end where users can upload their MRI to the system, link the best ML model at the back-end.  Display the result of segmentation [Task 2](https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri/tasks?taskId=992) to the user on the web-page.

## Evaluation
- Your system can display the result correctly
- All image formats are accepted
- All resolution of images are accepted
- Your system can display the segmented image

### Help ?
We have already completed work on this dataset. Do check the link if you want to read more about the topic. [Medium.com](https://medium.com/@ankitakadam/deploying-a-flask-web-application-on-aws-771909373a4)","","Flask/Django Application","Link models with GUI","","7"
"1519","804753","3475408","07/29/2020 07:04:22","## Task Details
Train a model to detect accidents from CCTV footage.
Train multiple models (Inception, Xception, Mobilenet, ResNet etc) and compare the performance among them

## Evaluation
A good solution will consist precision and accuracy values close to 1 for the test set.","","Accident Detection from CCTV Camera","","","0"
"1520","804792","3475408","07/29/2020 07:09:25","## Task Details
Detect number plates from images

## Evaluation
The score will be based on the model's performance on the test set","","Indian Number Plate Detection","","","6"
"1187","729845","3477619","06/21/2020 05:40:48","## Problem Statement
StockTwits is used by people who regularly trade stocks. People on this platform tweet about stocks using the special character ‚Äò$‚Äô to indicate the name of the stock. These microblogs similar to tweets might contain important information about the alpha signal in a stock.

## Task of this project
1. Review the Sentiment
2. Perform the Visualizations & EDA on the data gathered.
3. Perform Sentiment Classification using supervised learning.
4. Clustering the Reviews ‚Äì Comparing ‚ÄòCluster Label‚Äô with Train data Target ‚Äòsentiment‚Äô
5. Merge with stock data
7. Build models using the 7 anonymized stock factors (SF1 - SF7) to predict alpha.","","Predicting the alpha signal using microblogging data","","06/01/2021 00:00:00","0"
"1389","781959","3479899","07/17/2020 07:19:55","Time-series;","","Region wise spread","","","0"
"1383","582271","3479899","07/16/2020 13:49:22","For faster testing of CNN and DNN, pre-trained models, but not limited to.","","Detecting multi-class images","","","1"
"1779","824787","3501914","08/21/2020 04:45:15","## Task Details
The goal of this task is to show the top Integrated Development Environments since 2004.

## Expected Submission
Submit a Notebook which contains the visualization of top IDEs

## Evaluation
Visualize with good explanation","","Visualize the top 10 IDEs","Create a Notebook which visualize the top 10 IDEs","","1"
"1716","831948","3501914","08/16/2020 14:42:14","## Task Details
The goal of this task is to show the top programming languages since 2004.

## Expected Submission
Submit a Notebook  which contains the visualization of top programming languages

## Evaluation
Visualize with good explanation","","Visualize the top 10 programming languages","Create a Notebook which visualize the top 10 programming languages","","3"
"1835","845423","3507600","08/26/2020 03:31:49","## Task Details
Visualization for Episode-wise IMDb ratings, votes and US-views. Or comparative analysis with other TV shows.","","Data Visualization","","","0"
"1413","784501","3511431","07/18/2020 07:49:48","The objective of this competition is to build a recommendation engine to predict what restaurants customers are most likely to order from given the customer location, restaurant information, and the customer order history.

The error metric for this competition is the F1 score, which ranges from 0 (total failure) to 1 (perfect score). Hence, the closer your score is to 1, the better your model.","","Recommendation Task","","","10"
"1291","763768","5437411","07/08/2020 20:17:23","## Task Details
6 million votes are missing. Today 100% of votes counted and we can download later results for this dataset.
It would be great if you can update it.

[Here](http://www.vybory.izbirkom.ru/region/region/izbirkom?action=show&root=1&tvd=100100163596969&vrn=100100163596966¬Æion=0&global=1&sub_region=0&prver=0&pronetvd=null&vibid=100100163596969&type=465) are official results.","","Update dataset after 100% results counting","","","0"
"1286","723010","3511431","07/08/2020 08:54:17","**Questions that you can analyze, for example**:

- What about rate of shootings
- What is the rate of killings relative to race and age
- Which states have the most kills","","Analyze the police shootings","","","17"
"897","653007","3512471","05/14/2020 05:19:23","## Task Details
Build Amazon User based Recommendation model

### Further help
For any reference please refer these: 
1. https://www.kaggle.com/eswarchandt/amazon-user-based-recommendation-system
2. https://github.com/eswarchandt/Amazon-User-Based-Recommendation-Model","","Amazon User Based Recommendation Model","Build Amazon User Based Recommendation Model with the help of this data","","0"
"922","660021","3545810","05/18/2020 00:12:40","## Task Details
Can you come up with an image segmentation method for this dataset? If you do, please submit a kernel with your model for consideration

## Expected Submission
I expect you to submit a kernel that will be ran against unseen data. The kernel should use the mean IoU metric.

## Evaluation
I'll evaluate the submissions by running your kernel against unseen data. The best mean IoU score will be considered the winner.","","What is the best method for segmentating these images?","","","1"
"1502","799933","3555000","07/28/2020 07:20:31","This data is provided to visualize the top 250 movies on IMDB.

Maybe you can answer a few questions given below?

1. Which year produced the most successful movies?
2. Which rated movies are the most popular?
3. Which is the best month to release the movie?
4. Which is the best month to release DVD?
5. Show the origin of the movie.
6. Show the top 10 movies with corresponding genre and country.
7. Show the peak time.
8. Which genre attracts the audience most?
9. Movies with the longest and lowest runtime.
10. Which rating got the highest votes?
11. Show the rating curve against the year?","","IMDB Top 250 Movies","","","0"
"2162","809358","3559882","09/19/2020 07:12:27","## Submit solution with Maximum Training as well as Validation Accuracy but Do Not Use Transfer Learning. 

You can use any framework such as TensorFlow, PyTorch, Keras,....
You can use any activation function in order to see the practical performance of them. 
Sigmoid, Softmax in Output layer for classification.
Relu, LeakyRelu in Hidden layer.

This task is for learning Hyper Parameter Optimization in Deep Learning to understand most of the technical aspects which helps to develop strong skills.
 
#Expectate Submission:
Expecting model with maximum training and validation accuracy which is build from scratch, Without any transfer learning i.e. without using any Pre-Trained model. 

#Evaluation:
Model build from scratch with Maximum Training and Validation accuracy will be considered as an Optimal Solution.

#Notebok for Reference 
[Face Mask Detection Tr_Acc~93.20, Val_Acc~96.56](https://www.kaggle.com/omkargurav/face-mask-detection)

Dataset Credit.
I have taken 1776 images including both With and Without Face Mask images from Prajna Bhandary's Github account

https://github.com/prajnasb/observations

The remaining 5777 images are collected and filtered from The Google search engine.

3725 Images of Face with Mask

3828 Images of Face without Mask.","","Max Accuracy without Transfer Learning Face Mask Detection","Maximum accuracy without transfer learning","","8"
"1445","791192","3564381","07/21/2020 15:55:42","Predict using LSTM/RNN/Time series analysis or any other Machine learning algorithms.","","Predict the future prices of cotton based on the past data.","","","0"
"1446","791192","3564381","07/21/2020 15:57:14","Visualization can help in understanding the prices better.","","Visualization of prices state wise","","","0"
"2072","869703","3564986","09/12/2020 10:24:00","## Task Details

There is huge list of speciality. Go through the dataset and find unique specialities.

## Expected Submission

Just submit a most famous speciality. 


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find most famous doctor's speciality.","","","0"
"2059","869251","3566127","09/11/2020 22:48:53","Please refer to https://datahack.analyticsvidhya.com/contest/janatahack-cross-sell-prediction/True/#ProblemStatement 

Your client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.

An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.

For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.

Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‚Äòsum assured‚Äô) to the customer.

Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue. 

Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.","","Janatahack: Cross-sell Prediction by analyticsvidhya.com","Janatahack: Cross-sell Prediction by analyticsvidhya.com","","0"
"1515","804435","3566127","07/29/2020 02:04:39","FHFA House Price Indexes (HPIs)
Metadata Updated: May 29, 2014

Referenced from https://catalog.data.gov/dataset/fhfa-house-price-indexes-hpis

The FHFA House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. This information is obtained by reviewing repeat mortgage transactions on single-family properties whose mortgages have been purchased or securitized by Fannie Mae or Freddie Mac since January 1975.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","FHFA Housing Price","","","0"
"1517","804576","3566127","07/29/2020 04:14:05","This is referenced from https://github.com/vishwajeetsinghrana8/Tensorflow/tree/master/TF_Ex10","","Salary Slab Classification","","","0"
"1432","789699","3566127","07/20/2020 22:52:50","Refer to link for details.

https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/#ProblemStatement

We have total 70,000 images (28 x 28), out of which 60,000 are part of train images with the label of the type of apparel (total classes: 10) and rest 10,000 images are unlabelled (known as test images).The task is to identify the type of apparel for all test images. Given below is the code description for each of the apparel class/label.

 
Label	Description
0	T-shirt/top
1	Trouser
2	Pullover
3	Dress
4	Coat
5	Sandal
6	Shirt
7	Sneaker
8	Bag
9	Ankle boot","","Classification Problem","","","0"
"1900","851890","3566275","08/31/2020 06:02:51","## Task Details
What makes the top performers different from the rest? Explore the data and tell a story about the patterns found as you traverse the leaderboard.

## Expected Submission
A detailed exploration of data with analysis and visualizations

## Evaluation
The reader should get a clear idea about the data and its features.","","Exploratory Data Analysis and Visualization","","","0"
"2243","549773","4130755","09/26/2020 04:00:33","Label Encode column Class & Plot","","Label Encode column Class & Plot","","","0"
"1390","781986","3573270","07/17/2020 07:34:43","Submit the model for the given task","","To Make a Language converter from English to Hindi and Vice Versa.","","","0"
"1547","806894","3579737","07/31/2020 22:52:13","## Task Details
Its very confusing when talk about which food belongs to which country. I thought lets deep learning do this job.

## Expected Submission
Submit your notebooks with different methodologies of Deep Learning.

## Evaluation
Based on the validation accuracy we will evaluate results.

### Further help
Identify confusing food dishes with learning.","","Classify Cuisines based on the Region","","","0"
"1030","688730","3588702","06/02/2020 10:08:55","## Task Details
Air Pollution continues to be the reason for the killing of Innocent lives in the world. This data will open your eyes and give you a great insight.

## Expected Submission
The solution must contain a good visualization of the data and a geopolitical map.","","Which type of air pollution is more dangerous?","","","1"
"1476","796750","3591254","07/24/2020 14:29:57","## Task Details
The aim is to use multistep time series forecasting to predict power that can be generated from the windmill for the next 15 days. 

## Expected Submission
Feel free to use the data to get a feel of multivariate time series analysis. Note that wind speed is a very unpredictable variable so be prepared to handle a very noisy time series!

## Evaluation
Clear and concise code with a model that results in a low mean absolute error 

##Interesting Observation
A hybrid ARIMA-ANN model has been tested and given good results for modelling a single variable. Check out the paper below

### Further help
https://www.sciencedirect.com/science/article/abs/pii/S0925231201007020","","Wind Power Forecasting","To forecast wind power that can be generated for the next 15 days","","5"
"708","585684","3592260","04/07/2020 22:00:22","## Task Details
Make a memory based recommendation engine using explicit algorithm.

## Evaluation
Must be a well explained notebook","","Make an Explicit recommendation engine (Explicit Collaborative Filtering)","","","1"
"1088","685985","3595027","06/07/2020 16:11:57","## Task Details
A user has to use the training data and make a model which can be deployed in the real world.

## Expected Submission
No necessary submission. Store the best model in a model.h5 file or whichever format you feel comfortable with.

## Evaluation
Better the accuracy, better the solution.","","Recognising the facial expression of a person","","","1"
"1049","693346","3595027","06/04/2020 16:17:09","## Task Details
This was created for the Machine Learning Summer Term course's assignment

## Expected Submission
A user should submit an excel/csv file containing the silhouette score and DB index for clusters ranging from 2 to 10","","Assignment 1","","","0"
"1852","847735","3595027","08/27/2020 18:51:59","## Task Details
Create notebooks for exploring the dataset and classifying the images.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
- https://www.pyimagesearch.com/","","Creating Models","Create any model(user-defined or transfer learning)","","0"
"1734","806042","4857935","08/18/2020 07:56:03","## Task Details
Use your  various nlp and text cleaning techniques to clean the text.

## Expected Submission
solve the task primarily using Notebooks","","Text Cleaning And Preprocessing","","","2"
"491","532714","3607081","02/29/2020 08:10:02","## Task Details
Which city is most polluted and when was it most polluted?

## Expected Submission
Submit a notebook with the analysis

## Evaluation
It will be based on what metrics you chose, your thinking process, accuracy, and efficiency of the answer.","","Air quality index prediction and analysis","","","1"
"492","532714","3607081","02/29/2020 08:13:12","## Task Details
Analyze the data using your methods and gain insights

## Expected Submission
Submit a notebook containing all the analysis

## Evaluation
What insights have you obtained and how useful are they?","","Analysis of the air data with visualizations","","","1"
"997","677088","3612572","05/28/2020 15:11:35","Other data augmentation methods exist, that I didn't implement : words insertion/deletion, back translation, contextualized embedding, text generation...","","Advanced data augmentation methods","","","0"
"2020","864595","3617520","09/08/2020 19:12:44","## Task Details

Task of the dataset is to perform binary classification on dependent feature column that is the output variable (desired target):
whether the client subscribed a term deposit? (binary: 'yes','no')","","Task for the dataset !","","","0"
"1710","832919","3617856","08/16/2020 06:41:08","**Following are the details of the data:**
Interested Users ‚Äì Users (child) who have shown interest in joining the demo classes.
Booked Users ‚Äì Users who booked a trail class on our portal. There are cases where duplicate trails are possible, count both.
Trainers ‚Äì Trainers assigned to Trail Batch.
Subscription Bought ‚Äì Details of Sales that are made.


**Using these data, create following metrics:**
Sales conversion with respect to Trainers
Sales conversion with respect to Time slot
Sales conversion with respect to Topic
Sales conversion with respect to Source (Field available in Interested Users)
Sales conversion wrt grade
Time taken for Sales conversion
Time slot ‚Äì attendance %
Sales conversion refers to the number of users who have bought the session after showing interest and attending the trial session.","","Online Demo classes","","","0"
"794","615098","3621270","04/22/2020 03:16:53","## Task Details
Every task has a story. So, here you have to analysis why customers are
leaving bank? what is the reason behind it? Exploring the data.

## Expected Submission
There is not any specific date for submission

## Evaluation
What makes a good solution?
Here, you to use all basics and beginner's code
This is mainly focusing on beginners 
to deal with this dataset
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

o","","Bank_churn_modelling","EDA,DATA EXTRACTION,FEATURE ENGINEERING,","","1"
"1168","723905","3633236","06/18/2020 07:15:57","predict using ML model and get which Algorithm is giving good Accuracy","","Get Accuracy More Than 85%","good accuracy with visualization","08/18/2020 00:00:00","0"
"1023","687008","3635877","06/01/2020 10:18:23","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
Find out the most rating movies in Netflix","","Top Rated Movies","","08/11/2020 00:00:00","0"
"1833","845253","3635877","08/25/2020 22:23:14","Good Luck !!","","Pridect True Classes","Prediction","12/31/2021 23:59:00","1"
"1504","791543","3635877","07/28/2020 11:18:53","## Task Details
I created this dataset to get a model which can perform good and bad URL.

## Expected Submission
Create Kernels to write code and submission use some NLTK stuff. 


### Further help
If you need additional inspiration, check out my notebook of this dataset.","","Prediction Good Or Bad URL's","","07/28/2021 00:00:00","1"
"1916","854249","3635877","09/01/2020 19:48:12","### Task Details
The task covers 7 Indic Languages (Bengali, Hindi, Malayalam, Tamil, Telugu, Gujarati, and Marathi) and English. There are a total of 14 translation directions we will evaluate. Individually, Indic languages are resource-poor which hampers translation quality but by leveraging multilingualism and abundant monolingual corpora, the translation quality can be substantially boosted. The purpose of this task is to validate the utility of MT techniques that focus on multilingualism and/or monolingual data.","","TASK DESCRIPTION","","12/31/2022 23:59:00","0"
"1368","695144","3649586","07/15/2020 06:23:59","Answering the following questions:-
1. Which region consumes maximum & minimum power yearly?
2. State in each region which consume maximum & minimum power yearly?
3. Top 10 states that consume maximum power yearly.
4. Top 10 states that consume least power yearly.
5. In which months do you see a rise in consumption?
6. In which months do you see a decline in consumption?
7. Contrast between consumption during lockdown and without lockdown. (Compare the values in march-may 2019 and march-may 2020)
8. Percentage change in consumption during the lockdown(as compared to previous year). 
9. What is the highest value reached by each state at any time of the year in 2019?
10. What is the lowest value reached by each state at any time of the year in 2019?","","Insights From The Data By Answering Questions","Create insightful visuals and draw inferences","","6"
"1867","844934","3650646","08/29/2020 06:07:37","## Task Details
This data is highly periodic but never repeat itself exactly so its an best practice to predict the future trends with this highly imbalanced dataset.
for Training and Evaluation split data into training and testing as (80%, 20%).
and use a MAE as an Evaluation metrics 

## Tip
1. Use ARIMA or LSTMS so that good results can be achieved.
2. If someone done an hybrid ANN+Arima model through which evaluation will be great


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
just submit notebooks

## Evaluation
whichever notebook have minimum Mean Absolute error score will be considered as best notebook","","Time series forecasting Using LSTMs/ ARIMA models","","01/01/2024 23:59:00","0"
"2245","893737","3650646","09/26/2020 06:39:12","## Task Details
perform data analytics so that we can obtain inner insides of dataset

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","perform data analytics","","01/02/2025 23:59:00","0"
"1917","854417","3650701","09/01/2020 23:36:43","There are several ways to analyze the datasets, each data engineer can look into various aspects of the power usage and look for any interesting patterns worth exploring in detail.
For beginners, here are some suggestions.
‚Ä¢	Yearly on Year power usage
‚Ä¢	Month on Month power usage
‚Ä¢	Week day, weekend, vacations, COVID lockdown data categories","","DATA Visualization","","","1"
"1918","854417","3650701","09/01/2020 23:38:24","Power usage during day time is different from night time. The electrical devices that are inside the house are security DVR and POI cameras, 2 x refrigerators, 2 x 50gallon water heater that are on during day time. At night several electrical bulbs, TV's, washing machine, dryer and AC run from evening 6pm to morning 8am.
Thanks to wonderful weather at Houston, Texas we are blessed with almost 9 months of summer. Starting from Feb month to Nov month and winter is only for two months December and January for most of the years.

Vacation setting:- '""AC is turned off""
COVID- Lockdown:- 'AC is turned on during day time, laptops, monitors etc are on""
Weekday:- 'Morning 7am to 5pm ""AC is 84 F temperature during summer and heating set at 60 F during winter"".
Weekends:- 'Room Temperature is set at 78 F during summer and 68 F for heating during winter""

**For beginners:**-
1.	It is essential to understand critical weather features that could contribute to power usage.
2.	Power usage estimator :-
2.1. Critical features of weather data that are co-related to power usage.
2.2. On a given day & temperature (Max, Min and Average) power usage estimate.
3.	Classification of power usage based on ""weekday, weekend, vacation and COVID-lockdown""","","Data analytics","Regression & multiclass classification analysis","","4"
"1608","818300","3650837","08/05/2020 21:58:48","## Task Details
Every data has lot of hidden information. These hidden information was required to be investigated to find out the hidden patterns . These patterns can be helpful in making decisions on the procedure , removal of any ambiguity and also in getting key business insights. To solve all this questions, exploratory data analysis was introduced.

## Expected Submission
Highst AC score need it! and please explain with comments!","","Outcome high accuracy score prediction","","","10"
"1479","797699","3650837","07/25/2020 06:35:15","## Task Details
Find which customer will be churn.","","Find Churn Customer","","","5"
"1747","758094","3650837","08/18/2020 20:56:32","## Task Details
It is a rule-based machine learning technique used to find patterns (relationships, structures) in the data.

## Expected Submission
These applications may have come up in the following ways, such as ""bought this product that bought that product"" or ""those who viewed that ad also looked at these ads"" or ""we created a playlist for you"" or ""recommended video for the next video"".","","What is the Association Rules?","","","2"
"1330","770231","3650837","07/11/2020 22:52:11","## Task Details
Focus 'Salary' feature.

## Expected Submission
Full in all NaN values with logical technic. And give minimum predict error.","","Salary","","","3"
"1223","745526","3650837","06/29/2020 16:14:09","## Task Details
Here you can find instance questions about data but you can add in your kernel.

1. What is the number of unique products?
2. Which product do you have?
3. Which product is the most ordered?
4. How do we rank this output?
5. How many invoices have been issued?
6. How much money has been earned per invoice?
7. Which are the most expensive products?
8. How many orders came from which country?
9. Which country gained how much?
10. Which product is the most returned?
11. Show the last shopping dates of each customer.
12. What should we do for customer segmentation with RFM?
13. Scoring for RFM.
14. Finally, create an excel file named New Customer.

Note: All questions about 2009-2010 years. You can also do for 2010-2011 years.","","Inspiration and some questions about data set","","","2"
"1480","622524","3650837","07/25/2020 07:11:45","## Task Details
See whats going on every country.","","COVID-19 View","","","0"
"1748","645058","3650837","08/18/2020 21:08:20","## Task Details and Expected Submission
Use K_Means and Hierarchical Clustering Model","","Clustering Violent Crime Rates by US State","","","2"
"5877","851062","6796772","08/25/2021 20:57:38","With supervised learning, you need to detect math score as a y variable","","Evaluating Math Score","","","0"
"892","652326","3658270","05/13/2020 19:19:52","There are several datasets with earthquake location data, including:
- [Significant Earthquakes, 1965-2016](https://www.kaggle.com/usgs/earthquake-database)
- [Italy's Earthquakes](https://www.kaggle.com/blackecho/italy-earthquakes)
- [Earthquakes - Indian Subcontinent](https://www.kaggle.com/nksingh673/earthquake-indian-subcontinent)

Can we plot these on a map and see which tectonic plates are causing (or related to) more earthquakes?","","Plotting Earthquakes and Plate Boundaries","","","1"
"1194","726859","3658270","06/23/2020 15:35:03","In a Kaggle kernel, implement the experiments in the original paper:
https://arxiv.org/pdf/1904.05862.pdf","","Implement Paper Experiments","","","0"
"1277","761160","3658270","07/07/2020 09:15:21","The weights are for the following model:
https://www.kaggle.com/cwthompson/predictive-text-model
Modify/train the weights so that the model is improved.","","Improve The Network","LSTM predictive text model","","1"
"864","643280","3672289","05/08/2020 23:05:27","## Task Details
Every task has a story. 
You should build a model that classifies heroes.

## Expected Submission
What should users submit? 
No Submissions, all models are unique in their own way. 
But your model should predict heroes in the test folder correctly.

## Evaluation
What makes a good solution? 
Accuracy.

### Further help
If you need additional inspiration or any help regarding this,
Please ping me. Let's help others together. :)","","Build a deep learning model to classify heroes.","First and easy datasets to build your Deep learning models.","","0"
"1344","762927","3684121","07/13/2020 04:33:38","# Why we need this?
Empty Container is one of most important resource for sea transport business.

If port doesn't have a enough stock of empty container, they can not send their stuff even they have a ship to departure.

So Predicting future stock of empty container may useful for transport company!","","Time-Series Forecast model for predict empty/full container stock.","Let's predict the future stock of empty/full container!","","1"
"1190","732147","3687601","06/22/2020 09:57:12","The parlour would like to see how many customers would they witness in the next few months. Can you forecast and help them out?","","Forecast the number of customers the parlour would witness for the next 3 months","","","6"
"1211","741555","3687601","06/27/2020 13:10:08","Play around with the employee attributes and find interesting insights on what makes employees satisfied with their jobs?","","Can you analyze what makes an employee to be satisfied with his job?","Unravel the insights behind employee job satisfaction","","0"
"2203","889076","3700049","09/23/2020 06:43:02","Perform EDA and visualize the most impacted status  of educational institutions","","Perform EDA on the impact of schools due to the COVID-19","","","0"
"2022","772636","3702167","09/09/2020 01:18:14","## Task Details
Reason behind this task is, Data that Lending club provides has a few formatting inconsistency which has to be handled and Explored before starting a Model building process

## Expected Submission
Proper Cleaning of the Columns, checking proper data type, handling missing values.Prepare notebook which generates basic insights (visualization) from data. Save the final table in any format suitable.?

## Evaluation
Method used to handle missing values
Understandability of Visualization 

### Further help","","Data Cleaning and Exploartion","","","1"
"1814","843531","3709049","08/24/2020 14:00:17","## Task Details
I guess some cool metrics over Covid-19 and the quality of the quarantine could be made.","","Measuring the Effectiveness of Quarantine with Bus Passangers Data","","","1"
"2214","890064","3710604","09/23/2020 18:50:29","Building an ML model that can predict the popularity of an application based on certain factors.","","Popularity of an Application","predict the popularity of an application based on certain factors.","","1"
"1846","739728","3720924","08/27/2020 12:54:28","## Task Details
This task is to find the potential players who can create immediate impact and will be with the club for long term.

## Expected Submission
players age should be below 28 and value should be below 30 million Euros .any graph or table will do good .
players should be segregated according to their position in a separate dataframes. 

## Evaluation
Best visual and top ten players in each position will do good.","","Best players under 20M","","","0"
"431","500927","3722901","02/04/2020 21:38:21","## Task Details
How much influence does an endorsement have. This dataset tries to measure the magnitude of the endorsement based on the position of the person who endorsed the candidate. How much impact do we see reflected on polls?  

## Expected Submission
Upload a Dataset with polls results for the 2020 Democratic Primary.

## Evaluation
Dataset should include date of polls, results of most polls done since the beginning of the primary campaign. Use polls from trusted sources.","","Upload Democratic Primary Polls Results","Compare endorsement impact reflected on polls","","2"
"1926","618214","3724244","09/03/2020 03:29:27","Task is to get the list of stars with Indian Movies.","","Star with Indian Movie","","","1"
"1005","681964","3725706","05/29/2020 13:36:30","## Task Details
The objective of this data is to create neural network that can identify locust swarm","","locust swarm detection","","","0"
"1682","828209","3739937","08/12/2020 19:50:33","## Task Details
Apply pre-trained model of Sentiment Analysis over tweets","","Sentiment Analysis","","","0"
"1946","858229","3739937","09/04/2020 17:58:06","## Task Details
You would have found that data need some cleaning and a suitable column names to be given for easy work through. So, that's the task.

## Expected Submission
- Single Row for Column Name
- Column names should have not more than 5-8 characters

## Evaluation
best cleaning and appropriate names will be selected","","Suitable Column Names","","","0"
"1949","858229","3739937","09/04/2020 18:01:49","## Task Details
Well you can see that data contains dates and we need to have great plots to find what inside it. Use plots for data having **Years** as your main column.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508https://www.kaggle.com/iabhishekmaurya/case-study-cholera (Case Study Cholera)","","Best Plots by year","","","0"
"1950","858229","3739937","09/04/2020 18:11:54","Create a fresh notebook with visualizations that help understand the data and underlying patterns.

Start with graphs that explain the patterns for attributes independent of other variables.

###Evaluation

As always, you will be evaluated how thorough is your work. There are extra marks for creativity on this one - the more interesting and varied your graphs the more points you get.","","Visualization and further exploration","","","1"
"1828","844247","3741633","08/25/2020 13:14:55","## Task Details
Check which object detection algorithm works best and detect with small data.

## Expected Submission
Solutions should be notebooks which use this dataset to detect and localize cat, dog, monkey in different images.","","Detect and localize monkeys, cats and dogs","apply different object detection techniques to check which is best","","1"
"1403","782505","3746286","07/17/2020 12:22:40","The City of Boston inspects every restaurant to monitor and improve food safety and public health. Health inspections are usually random, which can increase time spent at clean restaurants that have been following the rules carefully ‚Äî and missed opportunities to improve health and hygiene at places with food safety issues.
The features in this dataset are the data provided by Yelp. This data includes business descriptions, restaurant reviews, restaurant tips, user review history, and check-ins. It's up to you to process these reviews into features that are predictive of hygeine inspection violations.

All of the Yelp data is structured as one JSON object per line in the file. You can find some examples of working with Yelp data on their Academic Dataset GitHub repository.","","Can we predict better view of active risks to public health?","","","1"
"1010","683247","3746884","05/30/2020 07:36:42","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysis of piano performance: Towards a common performance standard?","","","0"
"950","667923","3746884","05/22/2020 07:47:01","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Work on finding the percentage the people who have different types of emotion at different times and the places where these problems is a problems

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Health Conditions Identification","","","1"
"2983","641820","3442205","12/19/2020 02:06:31","about food demanding forecasting analysis","","food demanding forecasting","","","1"
"2460","601523","3769217","10/17/2020 11:13:40","We want a solution for the data set","","asl and same words","","10/20/2020 23:59:00","0"
"1615","791513","3773993","08/06/2020 20:10:43","I have achieved multiple tasks on this data but I have been blocked to build unsupervised /semi-supervised model to predict the labels / classess of every comment/post of this 10000 text.","","Create semi-supervised / unsupervised sentiment","Semi-supervised / unsupervised sentiment analysis with Tunisian dialect corpus","08/06/2021 00:00:00","0"
"2034","866079","3774958","09/09/2020 22:32:45","### Task Details
The outbreak of Covid-19 is developing into a major international crisis, and it's starting to influence important aspects of daily life. For example:

- Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
- Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether.
- Grocery stores: In highly affected areas, people are starting to stock up on essential goods.
A strong model that predicts how the virus could spread across different countries and regions may be able to help mitigation efforts. The goal of this task is to build a model that predicts the progression of the virus in 2020.

### Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

### Further help
If you need additional inspiration, check out this high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)","","Visualize COVID-19 spreading","","11/09/2020 23:59:00","0"
"853","638636","3778112","05/06/2020 10:56:56","On the given data, fit the polynomial curve and determine mean squared error and R2 score.","","Curve Fitting","","","0"
"878","650521","3778112","05/12/2020 19:26:31","Compare Global and Local Scenario from given data using Pie Charts","","Create Pie Charts","Compare Global and Local Scenario from given data using Pie Charts","","0"
"982","677380","3780248","05/27/2020 04:10:35","Using this dataset, predict meat prices for the next 30 years.","","Predict price for the next 30 years","","","0"
"983","677380","3780248","05/27/2020 04:11:19","Using appropriate correlation models, identify correlation among each variable.","","Find correlation","","","1"
"985","677484","3780248","05/27/2020 06:19:54","Perform an EDA and decide which ML model for your prediction","","Predict future prices","","","3"
"479","527799","3792689","02/24/2020 23:25:45","## Task Details

The dataset contains a 2 classes of images , class0 for non socket type E images and class1 for socket type E.

## Submission

It's expected that you develop a model able to classify the type E socket from other types with the highest accuracy scored. 

## Evaluation

It's expected that the model having the highest accuracy be top rated.","","Image Classification task "" Classify socket type-E ""","","","1"
"1960","858446","3795794","09/05/2020 15:44:51","## Task Details
This dataset contains a lot of factors, such as the different types of food served on each day (different kinds of protein, vegetarian for example), the occupancy rate for the different halls close to the restaurant

## Expected Submission
You should be able to predict the final week of the dataset with decent accuracy, given the data that is provided.","","Regression Analysis on messy data","Predict sales for a restaurant using Regression models","","2"
"1973","860531","3795794","09/06/2020 11:24:19","## Visualise it in a suitable way

* The data seem to indicate that some line charts would be decent for this dataset","","Visualise Swedish Emissions","What does the greenhouse emission history look like for Sweden?","","2"
"1660","824722","3800337","08/10/2020 16:44:16","## Task Details
The dataset I chose is the affairs dataset that comes with Statsmodels. It 
was derived from a survey of women in 1974 by Redbook magazine, in 
which married women were asked about their participation in extramarital 
affairs. More information about the study is available in a 1978 paper from 
the Journal of Political Economy.


## Expected Submission

I decided to treat this as a  classification problem  by creating a new binary 
variable affair (did the woman have at least one affair?) and trying to 
predict the classification for each woman.



#description of dataset


The dataset is [affairs dataset](http://statsmodels.sourceforge.net/stable/datasets/generated/fair.html ""Affairs data"") and comes with [Statsmodels](http://statsmodels.sourceforge.net/ ""Statsmodels""). It was derived from a survey of women in 1974 by Redbook magazine, in which married women were asked about their participation in extramarital affairs. More information about the study is available in a [1978 paper](https://fairmodel.econ.yale.edu/rayfair/pdf/1978a200.pdf ""Affairs paper"") from the Journal of Political Economy.

### Description of Variables
The dataset contains 6366 observations of 9 variables:

   + `rate_marriage`: woman's rating of her marriage (1 = very poor, 5 = very good)
   + `age`: woman's age
   + `yrs_married`: number of years married
   + `children`: number of children
   + `religious`: woman's rating of how religious she is (1 = not religious, 4 = strongly religious)
   + `educ`: level of education (9 = grade school, 12 = high school, 14 = some college, 16 = college graduate, 17 = some graduate school, 20 = advanced degree)
   + `occupation`: woman's occupation (1 = student, 2 = farming/semi-skilled/unskilled, 3 = ""white collar"", 4 = teacher/nurse/writer/technician/skilled, 5 = managerial/business, 6 = professional with advanced degree)
   + `occupation_husb`: husband's occupation (same coding as above)
   + `affairs`: time spent in extra-marital affairs","","Extramarital Affairs Dataset","","","0"
"730","595769","3809469","04/10/2020 15:52:24","Try to create some insights out of this dataset. 
Also, tell me what other columns would make this dataset more interesting and potentially insightful.","","Exploratory Data Analysis","","","1"
"470","524022","3814038","02/22/2020 00:46:27","Five different QCM gas sensors are used, and five different gas measurements (1-octanol, 1-propanol, 2-butanol, 2-propanol and 1-isobutanol) are conducted in each of these sensors.","","Alcohol QCM Sensor Dataset Data Set","","","0"
"481","514794","3817020","02/25/2020 11:49:51","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Submission columns : ""Id"" and ""Survived"". Using Test data.     Primarily using Notebooks or Datasets.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Hyperparameter tuning using LGBM, XGBM,SVM","Hyperparameter tuning using LGBM & XGBM (TITANIC ML)","","0"
"4792","771889","6156932","06/16/2021 14:21:13","``## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Use text mining analysise data ingredant of milk","Use lnformation extraction","","0"
"1449","791863","3828601","07/22/2020 02:09:53","## Task Details
Neural Machine Translation (English to Hindi)

## Expected Submission
na

## Evaluation
na

### Further help
na","","Neural Machine Translation (English to Hindi)","","","0"
"1898","851551","3828601","08/30/2020 21:42:24","## Task Details
Data Analysis: Impact of Covid19 on NZ Commodity Trading with other countries - 

## Expected Submission
-na-

## Evaluation
-na-

### Further help
-na-","","EDA: How has Covid19 affected NZ Commodity Trading with other Countries","Data Analysis","","0"
"2297","900942","3828601","10/01/2020 00:40:06","EDA - Fine Dust Readings @ Malin's Head","","EDA - Fine Dust Readings @ Malin's Head","","","0"
"2283","899512","3828601","09/29/2020 20:43:48","Exploratory Data Analysis on the TDI / Concussion claims registered with ACC between 1 January 2015 and 31 December 2019","","Exploratory Data Analysis","","","0"
"1827","844380","3830210","08/25/2020 08:39:09","## Task Details
I am huge fan of cricket so I collected data to explore cricket prediction.

## Expected Submission
You have to predict best 11 from the data using EDA or ML. 

## Evaluation
Predicted team may have 1 Wicket-Keeper,4 Batsmen,2 Allrounders and 4 bowlers but keep in mind that this year's IPL to be held in UAE.Good Luck.","","Predict Best Playing 11","ipl","12/31/2020 23:59:00","1"
"1014","684458","3830562","05/30/2020 23:26:16","A basic data exploration is important.
Explore the data and find which users are most active, and are they liking the movie like most others, are they ""bashing"" the movie and always giving negative reviews?","","Sentiment analysis and Data exploration","","","0"
"1452","664081","3831612","07/22/2020 12:22:52","Hey!!
You want to practice Machine Learning? I have a perfect opportunity for you to showcase your skills.
 
Practice on this Ecommerce Fraud Dataset to enhance you ML skills.
This problem requires you to perform extensive data analysis, feature engg. and modelling to find the classifier for detection.

It simulates the real world job of a ML Engg. 
Take your time and do your best. Practice on this Dataset to hone your skills to the very best.","","Can you detect and stop Ecommerce Fraud?","","","0"
"1697","830489","3833504","08/14/2020 15:35:34","While the dataset tracks the roles and champions being played, it doesn't track which player is playing. So if a team has a substitute Support player, this isn't taken into account.

It could prove useful to source this information and add it to the database","","Add which player is playing","","","0"
"1523","804778","3837716","07/29/2020 09:15:47","Perform EDA and give valuable insights.","","ED analysis","Perform analysis","","0"
"2976","733140","3837716","12/18/2020 09:19:47","## Task Details
Predicting, by using specific features and using blending ensembling. 

## Expected Submission
The submission file is already given

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict: if the employee leaves or stays?","Regression algorithms","","0"
"876","590234","3843377","05/12/2020 11:27:22","## Task Details.
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Creating the Pharmacy Drug Effectiveness","","Drug Effectiveness","Patient","","1"
"2944","545913","3858244","12/15/2020 03:54:01","Predict ratings using review text","","Predict ratings using review text","","","1"
"2945","545913","3858244","12/15/2020 03:56:40","Sentiment classification (positive, negative and neutral) based on review text.
Hint: Train the data using following criteria 
1. &gt;=7 rating as positive
2. &gt;4 and &lt;7 as neutral
3. &lt;=4 as negative","","Sentiment classification using review text","","","1"
"2946","545913","3858244","12/15/2020 03:58:02","Predict condition of a user using review text","","Predict condition of a user using review text","","","5"
"2947","545913","3858244","12/15/2020 04:00:33","Top drugs, top conditions, prominent age group per drug/condition, word cloud for side effects, dominant gender for a drug/condition","","Exploratory data analysis","","","1"
"452","516930","3863136","02/17/2020 19:47:34","Predict Sales for the company for next 2 weeks (day-wise)


# Use neural nets and compare with traditional models","","Predict Sales","","02/28/2020 00:00:00","0"
"2011","863934","3863136","09/08/2020 11:52:52","Perform text Classification on multi-label data","","Text Classification (NLP)","Text Classification (NLP)","04/23/2021 23:59:00","29"
"5183","863934","6682587","07/18/2021 08:50:13","covid-19 tweets - text classification.","","Text Classification","","07/19/2021 23:59:00","10"
"6945","863934","9013486","11/27/2021 21:53:44","Clean and pre-process the data, develop a naive Bayes classifier using TFIDF vectorization of the data.","","Clean and pre-process the data","","","1"
"2001","862444","3863136","09/07/2020 15:10:37","This data consists of the incidents involving guns. Perform EDA to find out the hidden patterns.","","EDA for gun usage patterns","This data consists of the incidents involving guns. Perform EDA to find out the hidden patterns.","01/01/2021 23:59:00","1"
"1980","860998","3863136","09/06/2020 16:31:43","Create MMM and Brand Equity Models","","Create MMM models","","07/14/2021 23:59:00","0"
"2057","869090","3863136","09/11/2020 19:17:22","Perform Sentiment Analysis","","Perform Sentiment Analysis","","01/01/2021 23:59:00","0"
"2196","888171","3863136","09/22/2020 13:44:22","Do not forget to leave an upvote :)

This can be your entry ticket to NLP. The first project you do for text classification.
The data consists of email body and labels for text classification.

1) Message body
2) Labels


Check other NLP data on the profile.","","Classification","Classify as spam","","0"
"1249","753772","3873980","07/03/2020 09:47:39","## Task Details
Let's take a closer look on pandemic data and find some important dates and facts:
- Can you identify from data what are important dates (increase, decrease?)
- Is there any relation between health care accessories and cured/infected/death cases?
- Can you predict next 7 days of pandemic in Czech Republic?

## Expected Submission
Kernel / Notebook

## Evaluation
There will be no evaluation, purpose of this task is just to improve your skills üññ","","Czech Republic Covid-19 Study","Find out interesting facts about pandemic in Czech Republic","","0"
"1770","839100","3875243","08/20/2020 17:59:35","## Task Details
build a machine learning model to classify if given the data of a patient should tell if the patient is at risk of heart attack","","Machine Learning model","","","0"
"1771","839100","3875243","08/20/2020 18:02:07","## Task Details
Analyze the data using visualization packages","","Analyze the data using visualization","","","0"
"1680","827699","5074082","08/12/2020 14:16:57","## Expected Submission
Make a neural network which predicts the winner of a specific award from a list of names or a list of arrays of names.

## Evaluation
A good solution would be one that scores well on the latest award ceremonies (2016-2020). If your task submission predicts the winners of the next Oscars, thats an added bonus!","","Predict the winners","","","2"
"1866","849299","3885917","08/29/2020 04:05:44","**Task Details**
The task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.

**Evaluation Metrics**
The evaluation metric for this hackathon is 100*Accuracy Score.","","Task 1 : To predict the length of stay for each patient","","","17"
"6765","849299","2812082","11/06/2021 00:06:44","## Task Details
For this task is easy and simple. Predict whether the severity of patient's illness is High, low or medium. You can also replace the values with 0,1,2.

## Evaluation
Solution with better accuracy and presentation shall be preferable.","","Predict the Severity of illness","","","0"
"1927","752130","3885917","09/03/2020 06:36:57","**Task Details**
The task is to recommend top 5 episodes similar to a particular episode based on the description, ratings, etc. 

Example : Consider episode name 'Booze Cruise'  then recommend top 5 episodes similar to this episode.","","Task 1 : Episode Recommendation","","","1"
"2207","889169","3885917","09/23/2020 09:13:07","## Task Details
Data analysis can be done to answer questions like - 

1) Which State/UT has highest crime against Child and Women ?
2) Which age group of Women has most number of crimes against them ?
3) Which State/UT has highest number of complaints registered against police personnels ?","","Crime Analysis","","","4"
"2161","880223","3885917","09/19/2020 05:41:39","## Task Details
To recognize the text/caption from the meme and use it for analysis.","","Task 1 : OCR","","","0"
"2180","865197","3885917","09/21/2020 10:22:01","## Task Details

**Exploratory Data Analysis:**
To analyze Indian dishes based on factors such as cooking time, preparation time, ingredients, etc.

**Similarity:**
To find out similar dishes region-wise or state-wise based on list of ingredients.

## Submission Format 
The submission should be a notebook based on any one of the above mentioned points - either EDA or similarity.","","Cuisine Analysis & Similarity","","","51"
"1274","758199","3888147","07/07/2020 01:00:51","Calcule a taxa de homic√≠dios por 100 mil habitantes para cada estado.","","Calcule a taxa por 100 mil habitantes","","","0"
"2140","877973","3890590","09/17/2020 15:47:50","I have uploaded Datasets from all the different categories available on Udemy. With each category having around 10 thousand courses. This data can be used to get various insights regarding all the available courses on Udemy, price insights, popularity and lots of other details.","","Exploratory Data Analysis for Beginners","","","11"
"2103","874363","3890590","09/15/2020 08:13:25","This data can be used to answer lots of open ended questions like what factors contribute to a course's popularity/ what should you ideally price a course on Udemy, if price has any influence on the ratings and much more!","","Exploratory Data Analysis for Beginners","","","0"
"1616","819567","3891240","08/06/2020 20:13:36","## Task Details
The dataset is only 300 images for training and testing combined. This task aims to achieve a model which performs well even with less data

## Expected Submission
The users should submit an efficient model accurate even with less data

## Evaluation
The models will be looked for least overfitting and the maximum accuracy","","Building an accurate model with less data","The dataset is only 300 images for training and testing combined. This task aims to achieve a model which performs well even with less data","","0"
"2208","889355","3906112","09/23/2020 09:57:53","## Task Details
Because the names were just grepped out of the text by ```grep -wo '[:upper:][:lower:][:lower:]*' septuagint.csv |sort|uniq -c|sort -rgk1 &gt;septuagint-names-greek.txt'```
there might be words that are not proper names or other named entities. The capitalization rules of Septuagint are not entirely clear to me.  

I have already removed frequent word known to me
  &gt;  1401 ŒöŒ±·Ω∂
   1027 ŒöŒµœÜŒ¨ŒªŒ±ŒπŒø
    375 Œ§·Ω∑
    349 ·º∏Œ¥Œø·Ω∫
    224 Œú·Ω¥
    204 Œü·Ωê
    203 Œ§·Ω±Œ¥Œµ
    202 ·ºòŒΩ
    179 Œ®Œ±ŒªŒº·ΩπœÇ
    179 Œü·ΩêŒ∫
    159 Œï·º∞
    118 Œ§·Ω∑œÇ
    111 ·ºòŒ≥·Ωº
     83 Œï·º∞œÇ
     55 ·ºàœÄ·Ω∏
     55 Œü·Ωêœá·Ω∑
     76 Œö·ΩªœÅŒπŒµ
     43 Œõ·Ω±ŒªŒ∑œÉŒøŒΩ
     43 Œö·ΩªœÅŒπŒøœÇ
     32 Œ•·º±Œø·Ω∂

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Filter out non NE words","determine the words that are not proper names or other named entities","","0"
"2195","887616","3906112","09/22/2020 13:13:58","## Task Details
The Bible is split by books, chapters and verses. As my source didn't have the verses outlined in any way and I did not need that anywa y I have uploaded the dataset as is. Yet the verses are marked by their number in the chapters themselves. It would be great to have the split so the Greek Septuagint/Old Testament could be correlated to other languages/versions. 

## Expected Submission
Have the verses separated by the new line inside the chapter. Remove all other new lines. Keep the verse number (maybe prepended with a ""#""?)","","Split the chapters into verses","Introduce a markup by verse to corellate to other Bible sources.","","0"
"2204","887616","3906112","09/23/2020 07:57:52","## Task Details
The stopwords from nltk Greek might not cover ancient Greek. 

## Expected Submission
Check the coverage of the existing nltk Greek Stopwords list and create a list in the format of the nltk to work with the Koine Greek.

## Evaluation
Actually I do not have a formal criteria. How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","List of Stopwords for NLP","Complie an NLP stopword list specific to ancient Koine Greek.","","0"
"2205","887616","3906112","09/23/2020 08:06:22","## Task Details
When going after NLP tasks it is often beneficial to exclude or give a special treatment to the Named Entities (NE). 

## Expected Submission
It would be great to have another csv with the type of the entity and its value. Ideally it could be included into Spacy library for Greek.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","List of Named Entities","Names of People, Places, Insitutions, Currencies etc.","","0"
"4017","619412","3925987","04/09/2021 06:48:24","## Task Details

This task is similar to the theme followed by the task that I released on 2021-03-26 [""Identify correlations between KPIs on UN SDG for EU 27""](https://www.kaggle.com/robertolofaro/sdgeu-datamart-sample/tasks?taskId=3867), focused on a subset of the UN KPI derived from the UN SDGs, but just for EU 27.

In this case, the task is to do something else- not just correlations, but (possibly visual) clustering of countries using the 33 KPIs that are included in this dataset extract from the World Bank, covering (for now) 2000-2019.

As an example, I shared within a Kaggle dataset ([Italy - shp files in CSV- from EEA and ISTAT](https://www.kaggle.com/robertolofaro/italy-shp-files-in-csv-from-eea-and-istat)) the SHP files (e.g. to be used with geopandas and others) that I had converted using GDAL for Italy: will share an example of its use within the next task (publication due on 2021-04-23)
 
The key element in this case is that there is plenty of data transformation to do, as the 33 indicators (actually 34, as the dataset contains also the GINI estimate by the World Bank) are quite heterogenous in content (both in terms of formats, presence of values, range).

For more details, you can visit the source website (see the [documentation of this dataset](https://www.kaggle.com/robertolofaro/selected-indicators-from-world-bank-20002019)).


TO CONTINUE FROM THIS POINT
The aim is to 
1. identify clusters between KPIs (e.g. look at the discussion about the KPI IT_USE_ii99 within the 2019-12-30 article listed above)
2. find supporting datasets that are not within those that I already posted online
3. identify further tasks (or subtasks)
4. considering that the KPIs should be updated yearly, define and implement a pipeline that automates as much as possible activities (e.g. from scraping to cleaning/transformation to aggregation and presentation).

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask03 : Identify clustering of countries by World Bank indicators","use the subset of 33 indicators (+GINI estimate) to ""map"" clusters","","0"
"2171","884359","3928176","09/20/2020 13:48:57","## Task Details
Predict which protein sequence (length 15) is glycated (positive sites) or non-glycated (negative sites)

## Expected Submission
Divide the dataset with train test split. then predict for the test part. 

## Evaluation
Accuracy, f1-score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Prediction of Glycated Lysine","","","0"
"436","503128","3928966","02/06/2020 09:37:28","## Task Details
To create a number classification model based on these numbers.

## Expected Submission
A notebook showing the model for classification of different numbers.

## Evaluation
The best score should approach 100%","","Number Classification","","","0"
"598","560492","3935632","03/18/2020 04:45:13","The goal of our project is to predict the outcome of Super Smash Bros Melee based on the current state of the game. Though there is no official dataset of games, there is a website (http://slippi.gg) called ‚ÄúProject Slippi‚Äù that contains links to game files in a custom ‚Äú.slp‚Äù format. To this date, Project Slippi has games from 10 tournaments, each with varying total game counts from 51-8567. Our project will first need to scrape these ‚Äú.slp‚Äù files, then process them into a more readable format such as a ‚Äú.csv‚Äù file.  Professional matches often have a ‚Äúhand warmer‚Äù stage before formal games, which allows competitors to practice for a while. This stage may heavily interfere with our model training, and is not labeled in any of the data, so we have to develop our own methods to detect and remove this type of data.  All recordings from Project Slippi are 60 frames per second (FPS). Due to the game‚Äôs mechanics, the distance of movement in 1 frame is limited, so using every frame has little benefit. Due to this, it is necessary to subsample the data to between 1-10 frames per second, as using every frame would be too noisy.","","Predict who wins","","05/01/2020 00:00:00","0"
"458","516984","3944866","02/18/2020 16:08:40","## Task Details
Add visuals for geographical location and spread prediction, with RNA and protein mutation hits by timeline. Build models that predict spread by region,  by strength or emphatic protein expression and RNA nucleotide host adaptation and / or mutation.

## Expected Submission

## Evaluation","","Build model that predicts the spread by mutations","Predict spread and strength of mutations","","17"
"854","638404","3952925","05/06/2020 14:24:39","## Task Details
Use Train dataset and apply your favorite ML algorithms to predict the expected crime rate in the test dataset.

## Expected Submission
Create a notebook using different visualization and find correlation between different features such as region, senior population etc.

## Evaluation
Use mean_squared error as a accuracy matrix and try against different folds of train data because test data doesn't have label so accuracy can't be checked.","","Find the expected crime rate in the test dataset .","","","0"
"475","526945","3974712","02/24/2020 09:50:11","## Task Details
1. Linking data with map and Visualization
- Longest route
- Frequency.
- Reachability.
- Direction.
- Coverage.
- Reduncy.
- Node strength.

2. Wanna survey without paperwork headache.

## Expected Submission
Discover networks and possibilities.

## Evaluation
Epic space Visualization and EDA of the network","","Analysing Pune Bus Network !","The data set had bus stops, routes and trips. We inspected frequency, coverage, redundancy and reachability.","","0"
"486","531914","3974712","02/27/2020 19:41:27","## Task Details
Predicting Sentiment Score for a Post's Title and Headline.

## Expected Submission
Predict the sentiment score for Title and Headline üíØ  

## Evaluation
On your test-data üëç","","Predicting Sentiment Score.","Predicting Sentimnet Score for a Post's Title and Headline.","","0"
"1797","841581","3985758","08/22/2020 22:39:34","[https://github.com/rvanasa/deep-antibody](https://github.com/rvanasa/deep-antibody)","","Experiment with example notebooks on Google Colab","","","1"
"1541","732443","3991068","07/31/2020 07:32:10","## Task Details
Extraction of most relevant words out of text.","","Extract most popular words out of each text","","","1"
"1548","732443","3991068","08/01/2020 05:51:55","## Task Details
Predict Sentiments for each text.","","Predict sentiments","","","1"
"1671","825754","3995454","08/11/2020 11:10:51","## Task Details
The Task is to Update this dataset with monthly Top 50 Trending TV Shows on Netflix.

## Expected Submission
The community will get the top 50 Trending Shows Using this Task on Monthly Basis.

## Evaluation
Submission with a new feature column is allowed.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Updating Dataset with monthly stats of Top Trending Shows","Dataset will be Updated Monthly.","","2"
"1134","713809","4010560","06/13/2020 20:50:36","Este projeto tem como objetivo identificar correla√ß√µes entre a estrutura educacional dos estados / munic√≠pios e o desempenho dos estudantes na prova do ENEM.
Os desempenhos ser√£o comparados pela mediana das notas dos estudates de cada estado / munic√≠pio.

Os dados utilizados neste estudo foram coletados entre 2014 e 2019 em pesquisas realizadas pelo IBGE e pelo INEP que est√£o dispon√≠veis para consulta p√∫blica nos seguintes links:
    
IBGE. Diretoria de Pesquisas - DPE -  Coordena√ß√£o de Popula√ß√£o e Indicadores Sociais - COPIS.	
Dispon√≠vel em:","","Prever medianas","Tente prever o desempenho de cada estado (Mediana das notas de cada mat√©ria)","","0"
"1135","713809","4010560","06/13/2020 21:01:07","A partir dos dados hist√≥ricos de 2010 a 2018, tente prever a quantidade de inscri√ß√µes no ENEM de cada UF.","","Prever n¬∫ de inscritos no ano de 2019","Os dados de 2019 ainda n√£o foram disponibilizados, para predi√ß√£o das notas talvez seja necess√°rio fazer as predi√ß√µes.","","0"
"1136","713809","4010560","06/13/2020 21:10:17","Considere 3 classes: Insatisfat√≥rio, Satisfat√≥rio e Ideal
Especifique os valores da m√©dia simples das medianas das mat√©rias, sujest√£o: 450, 600 e 700 respectivamente","","Classificar desempenho de cada Estado e prever desempenho de 2019","Com o desempenho hist√≥rico de 2010 a 2018 classifique 2019","","0"
"1137","713809","4010560","06/13/2020 21:15:14","Ser√° que consegue visualizar padr√µes ou correla√ß√µes entre a estrutura educacional, quantidade de escolas e n√≠vel, quantidade de docentes, quantidade de matr√≠culas por escola e popula√ß√£o, e o desempenho do estado/DF na prova do ENEM?","","Identificar padr√µes entre o desempenho e a estrutura educacional","Correla√ß√£o entre estrutura e desempenho Estado/DF","","1"
"1138","713809","4010560","06/13/2020 21:33:35","O Inep estipulou o crit√©rio de avalia√ß√£o de cada prova a partir da quantidade de acertos de cada quest√£o, ou seja, quanto mais acertos menor o peso na avalia√ß√£o.","","Calcular notas pela quantidade de acertos","Cada prova e cada quest√£o tem um peso de acordo com a dificuldade","","0"
"1270","748442","4010585","07/06/2020 15:51:24","Predict the success of a blog post based on title, subtitle, and featured image would be a nice thing to do. As Medium doesn't show the number of views, the only variable that I think is a good indicator of a story's success is the number of claps.","","Predict number of claps based on title, subtitle, and featured image","","","5"
"2239","836844","4010658","09/25/2020 19:55:18","## Task Details
Develop a CNN model for Clean Dirty water classification

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classification","","","0"
"2235","892701","4010658","09/25/2020 12:06:35","## Task Details
Predict the next price in time series analysis

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict","","","0"
"2236","892731","4010658","09/25/2020 12:17:26","## Task Details
Predict crypto prices for the 2021 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict","","","0"
"2256","867855","4010658","09/26/2020 19:24:58","## Task Details
Exploratory data analysis on IMDB values for movies

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis","","","0"
"2058","869149","4010658","09/11/2020 20:12:11","## Task Details
Grouping all the matches of a single favorite team, creating a new CSV in order for further predicting the winning chance of the Champions League next year. 

# Can be as a classification task to define whether it is True or False
# Can be a regression or DL task to check out the chance of winning for each team

## Expected Submission
Submission is not necessarily needed, however, kernels show the power of that dataset

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Predicting the Winner","Predict the Winner team, and runner-up for the next year of UEFA Champions League","09/12/2021 23:59:00","0"
"687","565348","4381828","04/03/2020 16:09:44","Arrange the data set according to date 
remove the column country

This is to see how many people throughout the world viewed the following tags on google on the respective date 

the data set will come in handy for Query log analysis, thus epidemic surveillance","","Data Wrangling","","","1"
"3673","879542","878378","03/06/2021 08:33:16","## Task Details
Can you write a good solution for violence detection in videos?

## Evaluation metrics

Evaluation metrics that have to be included here:
- ROC AUC curve 
- True Positive, False Positive
- True Negative, False Negative
- Validation/Test Binary Cross Entropy Loss","","Violence Detection in Videos","","","0"
"659","577574","4036693","03/29/2020 22:26:05","1. How many deaths were related to pneumonia. 
2. How distant are the deaths - were the deaths contained in a small geographical area e.g. one suburb of Chicago or were they distributed throughout the city?","","Analyze_Deaths_By_Location","Analyze Deaths by Age and Gender","04/02/2020 00:00:00","2"
"1869","849218","3358739","08/29/2020 07:29:18","Visualization for States and Districts giving best recovery rate over time.","","Districts with Best Recovery Rate","","","0"
"953","669276","4074355","05/23/2020 00:12:50","## EDA
Watch how the datas are availabloe","","EDA analysis","","","0"
"3148","891340","4652562","01/07/2021 13:53:08","Classifying the cotton leaves having a disease or not.","","Cotton leaves disease classification","","","1"
"1086","699241","4078756","06/07/2020 15:42:21","## Task Details
In order to enrich the dataset first we need to list all the works by category . This will give us an idea about the completion of the dataset. 

## Expected Submission
Create a CSV file with below information:
Title of the Work
Type of the Work (Poem/Song/fiction/non-fiction/drama/other keywords )
Is it available in the current data set .
location of the work in the current dataset.

## Evaluation
The submission should consider all categories of work by the poet. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create a list of works by Tagore","Create a consolidated list of works by Tagore","","0"
"1087","699241","4078756","06/07/2020 16:01:12","## Task Details
Making each work item available seperately will help increase the usability of the dataset .
This will also help us identifying what information is not available in the dataset.

## Expected Submission
Submit a dataset with seperated files for each work classified properly .

## Evaluation
the submission text font must be in Bengali.
The works must be classified correctly. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create Separate file for each work item","Create Separate file for each work item","","0"
"3915","623275","4095338","03/31/2021 03:39:22","Write scripts for cleaning dataset","","Clean dataset","","","1"
"3916","609612","4095338","03/31/2021 03:40:25","setup workflow to update dataset","","Update dataet","setup workflow to update dataset","","0"
"1146","717011","4096354","06/15/2020 12:11:41","Overall analysis around the resolution time
- An analysis on the effect of Resolution time on the user rating
- How many doubts are resolved by each TA?
- Company's performance in doubt resolution in different courses.","","Doubt & Doubt Activity Analysis:","","","0"
"1084","698953","4118399","06/07/2020 12:46:04","**Source:**

Creators:
Flavia Huerta,Gaurav Gawade
Ramon Huerta, University of California San Diego, USA

Donors:
Flavia Huerta,Gaurav Gawade
Ramon Huerta, University of California San Diego, USA (rhuerta √¢‚Ç¨Àú@√¢‚Ç¨‚Ñ¢ ucsd.edu)
Thiago Mosqueiro, University of California San Diego, USA (thmosqueiro √¢‚Ç¨Àú@√¢‚Ç¨‚Ñ¢ ucsd.edu)
Jordi Fonollosa, Institute for Bioengineering of Catalunya, Spain (jfonollosa √¢‚Ç¨Àú@√¢‚Ç¨‚Ñ¢ ibecbarcelona.eu)
Nikolai Rulkov, University of California San Diego, USA ( nrulkov √¢‚Ç¨Àú@√¢‚Ç¨‚Ñ¢ ucsd.edu )
Irene Rodriguez-Lujan, Universidad Autonoma de Madrid, Spain ( Irene.rodriguez √¢‚Ç¨Àú@√¢‚Ç¨‚Ñ¢ uam.es )

**Data Set Information:**

This dataset has recordings of a gas sensor array composed of 8 MOX gas sensors, and a temperature and humidity sensor. This sensor array was exposed to background home activity while subject to two different stimuli: wine and banana. The responses to banana and wine stimuli were recorded by placing the stimulus close to the sensors. The duration of each stimulation varied from 7min to 2h, with an average duration of 42min. This dataset contains a set of time series from three different conditions: wine, banana and background activity. There are 36 inductions with wine, 33 with banana and 31 recordings of background activity. One possible application is to discriminate among background, wine and banana.

This dataset is composed of two files: HT_sensor_dataset.dat (zipped), where the actual time series are stored, and the HT_Sensor_metadata.dat, where metadata for each induction is stored. Each induction is uniquely identified by an id in both files. Thus, metadata for a particular induction can be easily found by matching columns id from each file.

We also made available python scripts to exemplify how to import, organize and plot our data. The scripts are available on GitHub:
[Web Link]

For each induction, we include one hour of background activity prior to and after the stimulus presentation. Time series were recorded at one sample per second, with minor variations at some data points due to issues in the wireless communication. For details on which sensors were used and how the time series is organized, see Attribute Information below.

The metadata stored in file HT_Sensor_metadata.dat is divided in the following columns:

* id: identification of the induction, to be matched with id in file HT_Sensor_dataset.dat;
* date: day, month and year when this induction was recorded;
* class: what was used to generate this induction (wine, banana or background);
* t0: time in hours in which the induction started (represents the time zero in file HT_Sensor_dataset.dat);
* dt: interval that this induction lasted.


**Attribute Information:
**
The dataset is composed of 100 snippets of time series, each being a single induction or background activity. On total, there are 919438 points. For each induction, the time when the stimulus was presented is set to zero. For the actual time, see column t0 of the metadata file. In file HT_Sensor_dataset.dat, each column has a title according to the following

* id: identification of the induction, to be matched with id in file HT_Sensor_metadata.dat;
* time: time in hours, where zero is the start of the induction;
* R1 √¢‚Ç¨‚Äú R8: value of each of the 8 MOX sensors resistance at that time;
* Temp.: measurement of temperature in Celsius at that time;
* Humidity: measurement of humidity in percent at that time.


Temperature and humidity were measured using the Sensirion SHT75. The 8 MOX sensors are commercially available from Figaro, and are detailed below:
R1: TGS2611
R2: TGS2612
R3: TGS2610
R4: TGS2600
R5: TGS2602
R6: TGS2602
R7: TGS2620
R8: TGS2620



Relevant Papers:

Ramon Huerta, Thiago Mosqueiro, Jordi Fonollosa, Nikolai Rulkov, Irene Rodriguez-Lujan. Online Decorrelation of Humidity and Temperature in Chemical Sensors for Continuous Monitoring. Chemometrics and Intelligent Laboratory Systems 2016.



Citation Request:

Ramon Huerta, Thiago Mosqueiro, Jordi Fonollosa, Nikolai Rulkov, Irene Rodriguez-Lujan. Online Decorrelation of Humidity and Temperature in Chemical Sensors for Continuous Monitoring. Chemometrics and Intelligent Laboratory Systems 2016.","","Gas sensors for home activity monitoring Data Set","","02/02/2021 00:00:00","1"
"2287","728949","4126309","09/30/2020 14:05:00","## Task Details
Develop the recommendation system which recommends the anime.  

## Expected Submission
When the user enters the name of anime top 10 similar anime should get recommended.","","Anime recommendation system","content based anime recommendation","","1"
"3159","886101","4126309","01/09/2021 10:36:16","Use this dataset to create the system which takes an image as an input and recognize the denomination of currency","","Indian Currency Recognition system","","","7"
"1254","754707","4177045","07/03/2020 20:42:04","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
Q1. What are the key skills reuirement for data science domain?

Q2. What are the location one should look for?

Q3. Size of companies which are actively hiring?

Q4. Types of Companies hiring Public or Private ?

Q5. Types of Industry which are on hiring spree ?

Q6. Types of Sectors which are on hiring spree ?

Q7. Companies one should look for?

Q8. What makes a good solution? How do you evaluate which submission is better than another?


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","What is expected from this data set","","","1"
"1207","592563","5369388","06/25/2020 23:11:32","Its only for me","","New Title","","06/27/2020 00:00:00","1"
"1372","777248","4182566","07/15/2020 09:41:15","## Task Details
This Data set contains values of the stock of Reliance Industries Limited. It contains all the columns and values required to predict future stock prices. Moreover the task is to visualize the behaviour of the stock before and after Investment by Other Companies such as Facebook in Reliance.

## Expected Submission
The submission should contain Detailed visualization of the various data provided in the Data set.  This Data set if for you to explore your Data analysis and visualization skills. You can also apply your choice of ML algorithm to apply and predict the future stock prices and other outcomes.

## Evaluation
A submission is considered to be good if it has a good analysis as well as visualization of the data. Also, a notebook that can predict a price that is close to the current price will be considered as  a good notebook.","","Reliance Industries Stock Price Prediction","","07/31/2020 00:00:00","1"
"1760","837885","4182566","08/20/2020 06:16:52","## Task Details
All you have to do is Analyse this data and conclude something unique.

## Expected Submission
Any sort of EDA works

## Evaluation
A good EDA is always appreciated","","EDA on Twitch data","","","4"
"722","565898","4190447","04/09/2020 15:46:37","## Task Details
Russia is at the stage of expectation of a large-scale outbreak of morbidity. However, Russia has an advantage. Now we can already look at the experience of other countries in the fight against the virus. We can combine our data with data where this stage of the epidemic has already passed. So other projects can use detailed information about the situation in Russia for their research.

## Expected Submission
Let's create Notebooks using COVID-19 data of Russia and various other data around the world. Multiple COVID-19 datasets are already available on Kaggle. You can also attach data from other sources.

## Evaluation
Any notebooks of this kind are very welcome.","","Combining COVID-19 Russian data with other datasets","","","0"
"1260","756962","4195600","07/05/2020 06:56:34","## Task Details
A company has a dataset that has long descriptions (Introduction) of it‚Äôs products, Now as a DS Help the company to make a text summariser that takes these descriptions as input and summarises them into shorter versions without loosing the context. The length of the summary is also adjustable by the user. Also Document each stage of the code by adding proper comments. Mention the key technologies used and why?

## Expected Submission
The submission should contain the summaries of the long description","","Text Summarisation","","","2"
"1367","775339","4196226","07/15/2020 05:13:31","## Task Details
classify crops to their types.

## Expected Submission
submit as image label name as id and crop as output feature.","","Classification_task","","","3"
"1424","775339","4196226","07/20/2020 05:54:13","## Task Details
**Upload valid dataset of given class images and become a collaborator of the dataset**","","Become a collaborator","For collaborator","","1"
"1472","775339","4196226","07/24/2020 04:34:02","# Task Details
##Test_data found at https://www.kaggle.com/aman2000jaiswal/testssss
## Submit Prediction on testdata.csv found in the above link

## Expected Submission
Submission file should be submission.csv file features will be pathname and label.
Sample submission found in above link.

## Evaluation
Evaluation on accuracy.","","Challenging task","Crop_image_classification_task","","1"
"1590","816914","4196226","08/05/2020 02:59:05","## Provide  EDA on the dataset","","Analytics Task","Analytics Task","","3"
"2278","896794","4196226","09/29/2020 04:26:35","## Task Details
Implement yolo algorithm to detect circle.
## Expected Submission
Sample Submission is result.csv

## Evaluation
Evaluation is based on IOU metrics.","","Object Detection","","","0"
"1113","707873","4200382","06/11/2020 11:04:37","### Context

Data from Rouen University services. Those data is about campus france application at Rouen for 2019. But identifications features are all anonymize (please check original version to see it)

This dataset ([v1.capuse_france_rouen_dataset](https://www.kaggle.com/williamscanisius/campus-france-rouen-2019-dataset)) base on the original dataset(which is also upload here) collect informations about international students that have applied for campus france admission in 2019. Those data is only about 2019 and the aim is to collect the maximum informations about candidate learning levels and the university commission decision about their application.

### Content

What's inside is more than just rows and columns. I've remove ID row because it doesn't have any specific informations. 
* **Libell√© formation de la candidature** : the field where the candidate apply
* **Ann√©e d'entr√©e de la candidature** : the degree candidate have applied for. 1 for master 1, 2 for Licence 2 and 3 for Licence 3
* **Avis SCAC**: favorable or not 
* **Niveau du dernier dipl√¥me obtenu** : last graduate level
* **Moyenne du dernier dipl√¥me obtenu** : note of last graduate
* **Appr√©ciation SCAC sur cursus** : SCAS appreciation about candidate school curriculum
* **Capacit√© √† se faire comprendre** : capacity of candidate to make himself understand
* **Capacit√© √† comprendre** : capacity of candidate to understand
* **Etat de la candidature** : application result

Check this link to better understand **SCAS** : *http://etudier-en-france.fr/avis-scac/*
### Acknowledgements

Self learning data science i do believe that  ""the best way to learn is to give"". 


### Inspiration

How far can we how what make Rouen University commision to select or not a future lectures? What are the similarity and the dissimilarity between those selected and those rejected? How well can we predict the future candidate result base on theirs informations? 

Feel free to contact me if you have any questions.
my linkedIn : williams AVOCE
mail : canisius.avoce@outlook.com
Or kindly ask [Rouen university](https://www.univ-rouen.fr/) services 
Thanks and good job all.","","Rouen Campus France Dataset Analysis","Data from Rouen University services. Those data is about campus france application at Rouen for 2019. But identifications features are all anonymize (please check original version to see it)","","0"
"1794","841352","4208638","08/22/2020 16:58:18","## Task Details
This task about Exploring the dataset and drawing insights on farmers and agricultural labourers suicides.

## Expected Submission
A basic EDA with useful insights which can be consumed readily to take actions. 

## Evaluation
An interactive visualization and easily understandable tables or charts will stand out among the other notebooks.","","EDA on Farmer and agricultural labourers suicides","","","0"
"1795","841352","4208638","08/22/2020 17:01:02","## Task Details
This task is about show-casing your EDA skills and drawing insights on Economic, Educational, and Social Status wise suicides in India during 2015.

## Expected Submission
A basic EDA with useful insights which can be consumed readily to take actions.

## Evaluation
An interactive visualization and easily understandable tables or charts or infographics will stand out among the other notebooks.","","EDA on Economic, Educational, and Social Status Wise Suicides in India during 2015","","","0"
"1369","775353","4208638","07/15/2020 08:35:18","## Task Details
This task is about extracting the sentiment out of Elon Musk's tweets.

## Expected Submission
The solution is about how creative you can get in using the available libraries or models to give us sentiment analysis on an unsupervised text data.

## Evaluation
- The models used.
- The validation on why your submission/solution is better than others.","","Sentiment Analysis on Elon Musk's Tweets","","","0"
"1370","775353","4208638","07/15/2020 08:36:44","## Task Details
Let's get creative with EDA on unstructured text data!

## Expected Submission
EDA with visualizations and interactive plots!","","Exploratory Data Analysis on Text Data","","","0"
"1371","775353","4208638","07/15/2020 08:38:14","## Task Details
What are the topics which Elon Musk mostly tweets about?

## Expected Submission
Topics segregated into different categories and with a name to those categories.

## Evaluation
At least 3 distinct topics.","","Topic Modeling on Elon Musk's tweets","","","0"
"1469","794109","4208638","07/23/2020 18:01:43","## Task Details
This task is about doing Exploratory Data Analysis on the given dataset. Create an interactive and draw some interesting insights from them using your skills!

## Expected Submission
The solution should contain EDA with beautiful reporting skills and storytelling skills.

## Evaluation
Any kernel which is engaging is considered to for evaluation.","","Exploratory Data Analysis","","","16"
"2035","866294","4208779","09/10/2020 01:28:22","## Task Details
Merge all the relevante field and the Descriptions to use the K-means in the to find the cluster of events

## Expected Submission
The best number of clusters.

## Evaluation
I found 25 clusters.","","Use K-means in the Description to find the cluster of events","","","0"
"1043","588546","4215092","06/03/2020 19:37:40","One potential use of this data is to analyze players' rookie year statistics, and based on them, determine which variables, types of players, and future players will be most successful in their NBA career.","","Rookie Data Analysis","","","1"
"2150","879323","4224735","09/18/2020 12:32:29","You receive the basic data for demographics and economy for last 2000 years. Show it properly.","","Visualize GDP/ Population development through ages","","","1"
"1965","860208","4225844","09/06/2020 07:47:37","Healthcare fraud is considered a challenge for many societies. Health care funding that could be spent on medicine, care for the elderly, or emergency room visits is instead lost to fraudulent activities by materialistic practitioners or patients. With rising healthcare costs, healthcare fraud is a major contributor to these increasing healthcare costs.

Try out various unsupervised techniques to find the anomalies in the data.

**Detailed Data File:**

The following variables are included in the detailed Physician and Other Supplier data file (see Appendix A for a condensed version of variables included)).

**npi** ‚Äì National Provider Identifier (NPI) for the performing provider on the claim. The provider NPI is the numeric identifier registered in NPPES.

**nppes_provider_last_org_name** ‚Äì When the provider is registered in NPPES as an individual (entity type code=‚ÄôI‚Äô), this is the provider‚Äôs last name. When the provider is registered as an organization (entity type code = ‚ÄòO‚Äô), this is the organization's name.

**nppes_provider_first_name** ‚Äì When the provider is registered in NPPES as an individual (entity type code=‚ÄôI‚Äô), this is the provider‚Äôs first name. When the provider is registered as an organization (entity type code = ‚ÄòO‚Äô), this will be blank.

**nppes_provider_mi** ‚Äì When the provider is registered in NPPES as an individual (entity type code=‚ÄôI‚Äô), this is the provider‚Äôs middle initial. When the provider is registered as an organization (entity type code= ‚ÄòO‚Äô), this will be blank.

**nppes_credentials** ‚Äì When the provider is registered in NPPES as an individual (entity type code=‚ÄôI‚Äô), these are the provider‚Äôs credentials. When the provider is registered as an organization (entity type code = ‚ÄòO‚Äô), this will be blank.

**nppes_provider_gender** ‚Äì When the provider is registered in NPPES as an individual (entity type code=‚ÄôI‚Äô), this is the provider‚Äôs gender. When the provider is registered as an organization (entity type code = ‚ÄòO‚Äô), this will be blank.

**nppes_entity_code** ‚Äì Type of entity reported in NPPES. An entity code of ‚ÄòI‚Äô identifies providers registered as individuals and an entity type code of ‚ÄòO‚Äô identifies providers registered as organizations.

**nppes_provider_street1** ‚Äì The first line of the provider‚Äôs street address, as reported in NPPES.

**nppes_provider_street** ‚Äì The second line of the provider‚Äôs street address, as reported in NPPES.

**nppes_provider_city** ‚Äì The city where the provider is located, as reported in NPPES.

**nppes_provider_zip** ‚Äì The provider‚Äôs zip code, as reported in NPPES.

**nppes_provider_state** ‚Äì The state where the provider is located, as reported in NPPES. The fifty U.S. states and the District of Columbia are reported by the state postal abbreviation. The following values are used for all other areas:

'XX' = 'Unknown'
'AA' = 'Armed Forces Central/South America'
'AE' = 'Armed Forces Europe'
'AP' = 'Armed Forces Pacific'
'AS' = 'American Samoa'
'GU' = 'Guam'
'MP' = 'North Mariana Islands'
'PR' = 'Puerto Rico'
'VI' = 'Virgin Islands'
'ZZ' = 'Foreign Country'

**nppes_provider_country** ‚Äì The country where the provider is located, as reported in NPPES. The country code will be ‚ÄòUS‚Äô for any state or U.S. possession. For foreign countries (i.e., state values of ‚ÄòZZ‚Äô), the provider country values include the following:
AE=United Arab Emirates IT=Italy
AG=Antigua JO= Jordan
AR=Argentina JP=Japan
AU=Australia KR=Korea
BO=Bolivia KW=Kuwait
BR=Brazil KY=Cayman Islands
CA=Canada LB=Lebanon
CH=Switzerland MX=Mexico
CN=China NL=Netherlands
CO=Colombia NO=Norway
DE= Germany NZ=New Zealand
ES= Spain PA=Panama
FR=France PK=Pakistan
GB=Great Britain RW=Rwanda
GR=Greece SA=Saudi Arabia
HU= Hungary SY=Syria
IL= Israel TH=Thailand
IN=India TR=Turkey
IS= Iceland VE=Venezuela

**provider_type**  ‚Äì Derived from the provider specialty code reported on the claim. 

**medicare_participation_indicator** ‚Äì Identifies whether the provider participates in Medicare and/or accepts the assigned assignment of Medicare allowed amounts.

**place_of_service** ‚Äì Identifies whether the place of service submitted on the claims is a facility (value of ‚ÄòF‚Äô) or non-facility (value of ‚ÄòO‚Äô). Non-facility is generally an office setting; however other entities are included in non-facility.

**hcpcs_code** ‚Äì HCPCS code used to identify the specific medical service furnished by the provider.

**hcpcs_description** ‚Äì Description of the HCPCS code for the specific medical service furnished by the provider.

**hcpcs_drug_indicator** ‚ÄìIdentifies whether the HCPCS code for the specific service furnished by the provider is an HCPCS listed on the Medicare Part B Drug Average Sales Price (ASP) File. 

**line_srvc_cnt** ‚Äì Number of services provided; note that the metrics used to count the number provided can vary from service to service.

**bene_unique_cnt** ‚Äì Number of distinct Medicare beneficiaries receiving the service.

**bene_day_srvc_cnt** ‚Äì Number of distinct Medicare beneficiary/per day services.

**average_Medicare_allowed_amt** ‚Äì Average of the Medicare allowed amount for the service.

**stdev_Medicare_allowed_amt** ‚Äì Standard deviation of the Medicare allowed amounts.

**average_submitted_chrg_amt** ‚Äì Average of the charges that the provider submitted for the service.

**stdev_submitted_chrg_amt** ‚Äì Standard deviation of the charge amounts submitted by the provider. 

**average_Medicare_payment_amt** ‚Äì Average amount that Medicare paid after deductible and coinsurance amounts have been deducted for the line item service.","","Healthcare Providers Data For Anomaly Detection","Healthcare Provider Fraud Detection Using Unsupervised Learning","","1"
"1384","752890","4227753","07/16/2020 14:02:23","## Task

## Preprocessing
1.Check for null values in the data. Get the number of null values for each column.
Drop records with nulls in any of the columns. 

Variables seem to have incorrect type and inconsistent formatting. You need to fix them: 

Size column has sizes in Kb as well as Mb. To analyze, you‚Äôll need to convert these to numeric.
Extract the numeric value from the column
Multiply the value by 1,000, if size is mentioned in Mb

Reviews is a numeric field that is loaded as a string field. Convert it to numeric (int/float).

Installs field is currently stored as string and has values like 1,000,000+. 
Treat 1,000,000+ as 1,000,000
remove ‚Äò+‚Äô, ‚Äò,‚Äô from the field, convert it to integer

Price field is a string and has $ symbol. Remove ‚Äò$‚Äô sign, and convert it to numeric.

 ## Sanity checks:

Average rating should be between 1 and 5 as only these values are allowed on the play store. Drop the rows that have a value outside this range.

Reviews should not be more than installs as only those who installed can review the app. If there are any such records, drop them.

For free apps (type = ‚ÄúFree‚Äù), the price should not be &gt;0. Drop any such rows.

##  Performing univariate analysis: 
Boxplot for Price
Are there any outliers? Think about the price of usual apps on Play Store.

Boxplot for Reviews
Are there any apps with very high number of reviews? Do the values seem right?

Histogram for Rating
How are the ratings distributed? Is it more toward higher ratings?

Histogram for Size

Note down your observations for the plots made above. Which of these seem to have outliers?

## Outlier treatment: 

Price: From the box plot, it seems like there are some apps with very high price. A price of $200 for an application on the Play Store is very high and suspicious!

Check out the records with very high price
Is 200 indeed a high price?
Drop these as most seem to be junk apps

Reviews: Very few apps have very high number of reviews. These are all star apps that don‚Äôt help with the analysis and, in fact, will skew it. Drop records having more than 2 million reviews.

Installs:  There seems to be some outliers in this field too. Apps having very high number of installs should be dropped from the analysis.

Find out the different percentiles ‚Äì 10, 25, 50, 70, 90, 95, 99

Decide a threshold as cutoff for outlier and drop records having values more than that

## Bivariate analysis:
 Let‚Äôs look at how the available predictors relate to the variable of interest, i.e., our target variable rating. Make scatter plots (for numeric features) and box plots (for character features) to assess the relations between rating and the other features.

Make scatter plot/joinplot for Rating vs. Price
What pattern do you observe? Does rating increase with price?
Make scatter plot/joinplot for Rating vs. Size
Are heavier apps rated better?
Make scatter plot/joinplot for Rating vs. Reviews
Does more review mean a better rating always?
Make boxplot for Rating vs. Content Rating
Is there any difference in the ratings? Are some types liked better?
Make boxplot for Ratings vs. Category
Which genre has the best ratings?

For each of the plots above, note down your observation.

## Model building
Use Regression Models To predict the ratings, if interested can also use other models like KNN,boosting algorithms to predict the ratings","","Predict Ratings Using Regression Models","","","11"
"2069","869676","4238048","09/12/2020 07:53:34","These data set contains 700 records for car_resell price,collected from online source.the data is unprocessed. It can be used for EDA and predicting car price","","predict the car resell price","","","0"
"6879","709448","4561355","11/19/2021 13:55:39","## Task Details
Dataset owner has specified list of task for this dataset.","","Complete task submitted by dataset owner","","","0"
"1216","743171","4239460","06/28/2020 12:03:21","## Task Details

Show patterns that occur most in the data ...
For example:
Does 5 heads in a row occur more than 3 heads and 2 tails?
Does the sum of the dice values worth 6 have less probability than failing 20 or 10?","","View the probabilities","Some patterns occur more","","1"
"1292","764305","4245493","07/08/2020 20:24:08","Try building a recommendation system based on my data set.","","Recommendation system","","","0"
"1351","764584","4245493","07/13/2020 22:07:54","Compare the life expectancy of men and women in different countries!","","Compare the life expectancy","","","1"
"3245","674792","5095813","01/22/2021 08:45:22","## Task Details
As a new to EDA. I am looking to work around ML. without ML there is simple sorting and selecting technique in Pandas. Use it to build your own team. It is possible with EDA.

## Expected Submission
Cost Effective (Low Release Clause) 2 Players on each position. Who is young and has Solid Potential. This are basic requirement. You can also add your own Condition as you suit. 

## Evaluation
This is for self learning. One version of my code is given in notebook.","","EDA Make a efficient team","Make a List of two players on each position, who is young and has good potential.","","1"
"1503","575374","2418406","07/28/2020 10:58:11","Data visualization of covid-19 US counties","","Data Visualization","","","5"
"1448","575374","5491491","07/22/2020 00:11:20","identify top states and counties for the dataset","","COVID 19","US County Data","08/29/2020 00:00:00","9"
"1836","575374","3513208","08/26/2020 10:22:19","User must submit complete visualization of cases and deaths in the state and county and also predict cases and deaths using time series method.","","Time Series models and their visualizations.","","","1"
"2337","737503","5361263","10/05/2020 18:42:11","Predict possibility of heart attack","","Predict Possibility of Heart Attack","","","15"
"2170","883755","4259735","09/20/2020 06:13:45","## Task Details
Apply NLP techniques and clean the text. Use this to find similar movies. End by building a recommendation system.

## Expected Submission
Submit Notebooks to share your process of solving the task.

## References
For getting started, you may refer to this:
https://dataphrase.github.io/movierec2/","","Find similar movies based on summary and genre.","","","0"
"693","586796","4261509","04/04/2020 16:48:44","## Task Details
The dataset contains information of corona paients.","","Corona cases in India","","","0"
"476","521690","4261509","02/24/2020 10:04:41","## Task Details
I was learning pandas. So for row and column manipulations a small dataset is very handy.","","Zika_brazil","For practise for beginners","","0"
"760","604713","4264318","04/15/2020 23:22:04","## Task Details
Try to create a numeric recognition on the device display.

I have been trying to create a dataset for a long time to teach a model that could be used to read the thermometer. Consequently, an application built above tensorflow.js would read the value and work with it. Unfortunately, my skills in creating datasets and models are not great. I tried using IBM annotatiton to create a model from image annotations, but it didn't work correctly. Then I tried to go through the dataset as MNIST.

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4264318%2F68f47b332e2d4a06eb74642ea68bd78c%2Ftest2.jpg?generation=1586992604125300&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4264318%2Fc8dfbbe686bf28f19e8f332253f6ed6f%2FSnimek%20obrazovky%202020-04-15%20v20.57.11.png?generation=1586992653056790&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4264318%2F232e18ce1c9e5710f67c19ecfb8a4f62%2FSnimek%20obrazovky%202020-04-15%20v22.52.38.png?generation=1586992668360337&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4264318%2F94d55101d1ecaa594795cee6e4fc1b83%2FSnimek%20obrazovky%202020-04-08%20v10.37.46.png?generation=1586992688309378&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4264318%2F52c64087e4d499b9446376aecd659f51%2FIMG_9087.JPG?generation=1586992904869703&alt=media)","","Value recognition from thermometer","","","0"
"1325","769612","4266027","07/11/2020 13:26:26","the purpose of this project is to predict whether a woman can give birth to a premature baby or not","","prediction of premature deliveries","","08/13/2020 00:00:00","1"
"439","505125","4268444","02/07/2020 23:49:23","The goal is to build a RNN or so ever model that has a good accuracy on detecting playing cards","","Object Detection","Create a model that allows identifying each of the 52 cards","","0"
"2012","863921","4269304","09/08/2020 12:01:33","# Consider a subset ( 30 rows)  of @bar. How do 'bid_price', 'ask_price', 'average_price' of any stock ( any symbol) are moving in that subset period? Maybe you can try for ‚Äò2020-08-05‚Äô. Graphical and data analysis. Dataset @quote @bar dataset
# make a table [symbol,ratingBuy,ratingScaleMark,mean of average_price,consensusStartDate,consensusEndDate] from @rating and @bar dataset
# 'average_price' column @bar dataset. How average price changes over 'consensusStartDate' an 'consensusEndDate' from @rating dataset.Graph
# How the 'average_price' @bar dataset changes a few days before and after the 'reportDate' for any stock. Graphical. 
# pull 'priceTargetAverage' column from @target dataset. Changes of 'priceTargetAverage' around 'reportDate'  from @event dataset on or before. Show by table. 
# need to add the 'average_price' at updated date from @bar dataset
# need to find the trend in price prior and post updated date 
# [present trend vs.prior trend]. ranking.up/down.
# 'datetime' upto minute ['2020-08-12 11:31'] and @average_price at that time from @bar dataset. [datetime,stock,summary,average_price] from @bar and @news dataset","","Beat the street with Analytics","Analytics using python, scikit learn, tensorflow and keras","09/15/2020 00:00:00","12"
"1354","774355","4271950","07/14/2020 00:45:18","## Task Details
Analis diminta untuk mencari korelasi antar variabel, sehingga didapatlah informasi yang berguna untuk ilmu pengetahuan.

## Expected Submission
Submisi berupa notebook yang di dalamnya terdapat langkah-langkah analisis data.","","Korelasi","","","1"
"2053","835308","4277074","09/11/2020 15:41:32","Perform EDA and Predict the sales of the next 7 days from the last date of the Training dataset!","","EDA and sales Forecasting","","","14"
"1693","829547","4283622","08/14/2020 06:21:59","## Task Details
 The aim behind making this dataset is to improve the data science community's innovative ideas..
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","What is the value in this data?","Try to find out some interesting facts out of this small set of data and show your data science skills for real world usecase","10/31/2020 00:00:00","0"
"609","562755","4285564","03/20/2020 01:53:39","## Task Details
crude oil and naphtha data set is about last 5 year extract seasonal variation between them and analyse the variation between them.
## Expected Submission
this is the open data set you can summit this data upto may 2020.

## Evaluation
extract the hidden pattern in this data set","","Relation between brent crude oil and naphtha price","extract the open brent crude oil price and naphtha price","06/09/2020 00:00:00","0"
"1248","669547","4285827","07/03/2020 09:19:25","Predict the sentiment of the word said by Mr. Trump","","Sentiment Analysis","","","0"
"1675","826110","4285883","08/11/2020 18:49:51","## Task Details
You have been hired to deliver actionable insight to support your client who is a national charitable organization. The client seeks to use the results of a previous postcard mail solicitation for donations to improve outcome in the next campaign. You want to determine which of the individuals in their mailing database have characteristics similar to those of your most profitable donors. By soliciting only these people, your client can spend less money on the solicitation effort and more money on charitable concerns 

## Expected Submission
You are required to submit a notebook showing the stages in your machine learning pipeline and the number of predicted donors.

## Evaluation
A good solution should have a high AUC score (&gt;0.5) and  &gt;10 predicted donors","","Predict the donors","check out https://www.kaggle.com/momohmustapha/donorsprediction for the datasets","","1"
"1339","771260","4287167","07/12/2020 11:35:21","1. Find if the Movies are released with the same name.If there are,find how many and their release years.
2. Find the number of movies released per year.
    Make a intuitive Visualization.
3.Find the movie which is rated highest and most rated by audience?
4. Find the number of movies made in each genre. Generate a pie chart.","","Exploratory data Analysis","","","1"
"1289","764088","4287407","07/08/2020 16:36:35","## Task Details
Can you create creative visualization that shows impact of COVID-19 on Education ?","","Impact of Covid-19 on Education","","","3"
"1486","798735","4287407","07/25/2020 18:51:52","## Task Details
A set of messages related to disaster response, covering multiple languages, suitable for text categorization and related natural language processing tasks.","","Natural Language Processing","","","1"
"1605","818027","4287407","08/05/2020 18:50:15","## Task Details
Classifying handwritten text and to convert handwritten text into the digital format using various approaches out there","","Classify & Convert handwritten text","","","6"
"1290","710046","4287407","07/08/2020 18:21:29","## Task Details
Can you predict Future sales Estimates for various categories

### Further help
Can check out these kernels:
1. https://www.kaggle.com/landlord/us-retail-all-cat-covid-19-facebook-prophet (Prediction using Facebook Prophet)
2. https://www.kaggle.com/landlord/us-retail-all-categories-covid-19 (Cleaned Data and ready for Time-series Analysis for Major categories)
3. https://www.kaggle.com/landlord/forecasting-multiple-time-series-using-prophet (For forecasting multiple Time-series using Multiprocessing)","","Predict Future Sales Estimate","","","1"
"1897","851440","4287407","08/30/2020 20:00:46","## Task Details
Let's go through the history of the world's biggest codebases starting in 1970 all the way to now. Can YOU guess the largest?

## Expected Submission
Creative visualizations, Analysis","","Codebase History","","","0"
"686","563799","4298987","04/02/2020 19:19:28","## Task Details
Looking to help out my friend's small business which has cut back their employees by 80% due to the corona virus.  The data being used is actually from their online sales, which I have permission to display.   See below for what projections I'm seeking.


## Expected Submission
I'm specifically looking for one analysis and your opinion on something else. 

1. Projections for Q4 of 2020 (oct-dec).  What should we expect based on data from previous years?  A list of sales per item type would be ideal. I'm selecting this date range with the expectation the corona virus won't still be a global pandemic.

2. Would you expect any pent-up demand?  Would you recommend any discounts based on previous data?

3.  Any other analysis would be so incredibly helpful.  

Thank you!!!! 

Tyler","","Future Sales Predictions","With the data available can you make sales predictions for 2020?","04/30/2020 00:00:00","8"
"1542","802506","4306089","07/31/2020 07:42:36","## Task Details
Every record contains hidden knowledge. Try to discover it by connecting the individual data sets. This can be done through statistical explanatory models such as regressions, through predictions or through sophisticated visualizations.

## Expected Submission
A good submission should include at least one of the three aspects mentioned. Visualization, explanation or prediction.

## Evaluation
When something interesting is discovered, then the goal is achieved.","","Find insights.","Visualize, explain, predict.","","1"
"1500","802506","4306089","07/27/2020 23:08:12","## Task Details
Visualize the gender pay gap and try to find explanations in the economic indicators. You can also use external data.

## Expected Submission
The aim is to create a rounded picture that illustrates the gender paygap. Both time and geographical aspects should play a role.

## Evaluation
A good submission contains all relevant parts of the data set and gives a clear picture of the situation. A good analysis should also be open about its own weaknesses. Therefore, the results should also be critically examined.

### Further help
Maybe the gender pay gap has something to do with the income situation in the countries? 

Maybe it changes over time?

A country's income consists roughly of wages and profits.","","Gender Pay Gap","explain and present","","0"
"1475","796580","4306089","07/24/2020 12:51:58","## Task Details

Try to visualise the situation of European non-financial enterprises in as much detail as possible, taking into account changes over time.

## Expected Submission
Graphs per size, sector, country for each individual indicator. Gladly also geospatial.

## Evaluation
A complete picture results from a complete analysis of all aspects of the data set.

### Further help
https://www.bach.banque-france.fr/?lang=en","","Visualizations","Timeseries","","0"
"682","583052","4319916","04/02/2020 08:44:11","## Task Details
Recently there has been quite a lot of talk around the relationship between the countryside communities and those in large cities, and Stockholm in particular [(https://www.expressen.se/nyheter/hatet-mot-stockholmare-vaxer-genuin-frustration/)]((https://www.expressen.se/nyheter/hatet-mot-stockholmare-vaxer-genuin-frustration/)).

An important question to consider therefore is how much of the spread in other communities is caused by travelling city folk and could this lead to similar outbreaks in other parts of the country?

This task thus looks to model the outbreak in regions outside the three largest cities (Stockholm, V√§stra G√∂taland and Sk√•ne) from 1 April 2020 to 1 May 2020. 

## Expected Submission
Submit a suitable notebook, and not simply a submission file. 

The notebook should be well documented and contain:

- Data preparation and cleaning, including external references
- Training methodology of your model
- Your model results (see below)
- Evaluate your model against new data. Use Mean Absolute Error (or another metric which can be suitably justified).

With your model, you should produce an output table in the following format for the period specified.

**Date**: Observation date in dd/mm/yyyy
**Region**: Region name
**Confirmed**: Cumulative number of confirmed cases
**Deaths**: Cumulative number of deaths cases


## Evaluation
A good solution is well-documented and explains any/all assumptions surrounding the data and the models used. 

Good luck üòÑ","","Predicting the spread from big cities to the countryside","","","3"
"684","583052","4319916","04/02/2020 11:07:36","## Task Details
Recently there has been quite a lot of talk around whether Sweden's Coronavirus strategy is based on the idea of herd immunity, i.e. that a large portion of the population will get infected and then become immune to the virus, shielding others. This has been controversial to say the least https://www.theguardian.com/world/2020/mar/30/catastrophe-sweden-coronavirus-stoicism-lockdown-europe, and firmly denied by the Public Health Agency in Sweden. 

However, an important question to consider therefore is with the current rate of spread and the death rate, how many deaths are we expecting in the coming months if this does indeed become the official strategy?

This task thus involves modelling the number of expected deaths over time. 

## Expected Submission
Submit a suitable notebook, and not simply a submission file. 

The notebook should be well documented and contain:

- Data preparation and cleaning, including external references
- Training methodology of your model
- Your model results (see below)
- Evaluate your model against new data. Use Mean Absolute Error (or another metric which can be suitably justified).

With your model, you should produce an output table in the following format for the period specified.

**Date**: Observation date in dd/mm/yyyy
**Region**: Region name
**Deaths**: Cumulative number of deaths cases

## Evaluation
A good solution is well-documented and explains any/all assumptions surrounding the data and the models used. 

Good luck üòÑ","","Herd Immunity: An experiment in review","","","2"
"1849","841033","4322675","08/27/2020 15:31:14","Clean the data, remove redundant and invaluable columns. drop bad observations,
make the data ready for EDA and modelling. 

Submit a final dataset which is cleaned and ready for EDA and modelling","","Cleaning Data","","","0"
"1525","805358","4323772","07/29/2020 12:51:19","## Task Details
MahaBharat is an Indian epic with lot's of plot twist and dialouges let's make a language model which can generate somewhat some meaningful stories preserving the style of Mahabharat



## Evaluation
Evaluation will be based on funny, meaningful or sensible text generation.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Language Model on Mahabharat","Let's make a language model which can generate some story based on text of MahaBharat.","","0"
"1562","789867","4330677","08/02/2020 20:54:09","## Task Details
Clustering or classification systems, both supervised and unsupervised,  will allow the meeting with the necessary links to understand this historical phenomenon in its true proportion.  The expectation is to travel through countries, languages, and cultures to find the transmission rates of the virus and the triggering of affective states. The challenge is that the history of emotions discusses with qualitative methods of unstructured data analysis to understand complex social phenomena. Mathematical models, which enable Natural Language Processing or allow the Affective Analysis of Texts, are called to be a vitally important tool for implementing a historical study of certain social phenomena in all their complexity. 

## Expected Submission
Notebooks for the basic interpretation of the dataset.

## Evaluation
The most complete submission will be adequate.

## A paper for inspiration
[Sublime - A mathematical fold for the history of these emotions](https://www.researchgate.net/publication/342872616_Sublime_-_Un_pliegue_matematico_para_la_historia_de_estas_emociones_Sublime_-_A_mathematical_fold_for_the_history_of_these_emotions)","","General analysis for dataset","","","1"
"1838","837434","4334005","08/26/2020 11:13:52","## Task Details
During the last years, the exponential growth of spreading misinformation throughout the social media platforms such as Twitter, Facebook or Instagram have fostered applied research in order to detect and consequently prevent the intrusion of these kind of content in such networks. Thus, it is clear that Artificial Intelligence (AI) has a crucial role in this topic in order to support these platforms in order to automatically notify or restrict the access to those accounts that look suspicious according to a certain set of descriptors or features.

As an example of this, in 2019, Twitter had to remove more than 26 thousand of accounts since they were suspicious of spreading misinformation and/or non-appropriate content. So then, the question is, how AI may support end-users in the task of detecting suspicious accounts?

The objective of the task is to classify whether a user account is suspicious to be a bot considering the available set of features provided in the dataset (Additional feautures can also be collected to promote the performance of the systems). 

To do so, the datasets provides a response variable denoted as account_type which contains a binary label to distinguish between human and bot accounts.","","Detecting Bots in Twitter via AI","","","0"
"2359","910242","4346459","10/07/2020 12:54:49","## Task Details
Find 5 interesting insights from the data 

## Expected Submission
 You can write the insights on jupyter notebook and summit that notebook on kaggle

## Evaluation
will be based the quality of output.","","Find 5 interesting insights from the data","Find 5 interesting insights from the data","","1"
"2040","864519","4348447","09/10/2020 12:14:25","## Task Details
Classify graphs and divide them into 7 classes. One class for irrelevant images.

0 - just image
1 - bar chart
2 - diagram
3 - flow chart
4 - graph
5 - growth chart
6 - pie chart
7 - table

*Image names start with the **class names**.*

## Expected Submission
Notebooks preferred. 

## Evaluation
Primary goal is to get the best **accuracy** score.","","Classify graphs","Classify 7 graph types and irrelevant images","","0"
"2166","881503","4357049","09/19/2020 10:15:19","## Task Details
Guys, Unemployment is big problem we are facing today. So this dataset is basically all about unemployment tweets on Twitter. 
**Task** : Visualize this dataset with efficient and different techniques.

## Expected Submission
This expected solution is Great Visualization Output of this datasets.

## Evaluation
Can use **Matplotlib** , **Plotly** etc.

### Further help
https://matplotlib.org/3.2.1/tutorials/index.html
https://plotly.com/python/","","Visualization of Dataset","","","2"
"2206","889142","4359440","09/23/2020 08:11:48","## Task Details
Each commands in common or os specific csv files have human curated TLDR summary and respective man page entry.

## Expected Submission
Train deep learning model to generate TLDR summary from these man page entry.
You can publish your notebooks and results for others to compare.

## Evaluation
How good the model summary compares with Human curated summary.
We can use ROUGE-1, ROUGE-2, ROUGE-L, and METEOR for evaluation of quality of the summary","","Abstractive Summarization","Measure quality of machine generated summary","","1"
"2143","878361","4364895","09/17/2020 18:05:35","## Task Details
- This is totally for learning, No deadline, No evaluation.

**What you have to do with the dataset?**

Design a Image based attendence system using any techniques like ML, OpenCV, Computer Vision etc.
Dataset already contains few images of various person, task is to build the model such that for any input image/selfie, the system should be able to check whether it's a match with the given dataset.
So, For any new image you should print PRESENT if the images matches and ABSENT if the image does not matches with the given dataset or not.","","Build some model to check if new image matches with any image in the given dataset","","","0"
"851","638400","4368973","05/06/2020 10:16:40","## Task Details
Many users update random names like 'adfadf', 'Tera Baap' etc. Need to find such names and give a prediction about which among the new names is a spam name.

## Expected Submission
Python Model using the dataset we provided in a Notebook or code in python. No need to use any other additional data. Categorize spam or non spam from the data given.

## Evaluation
There are around 200+ daily signups on our app, we will scale it to a 1,000+ per day within few months. Good solution must tell with 70% + accuracy about which names among these names are spam.","","Designing a model to identify spam names","","05/30/2020 00:00:00","0"
"1778","839450","4371011","08/21/2020 03:42:39","## Task Details
Use great Visuals and Analysis Techniques

## Expected Submission
You are free to submit at anytime.

## Evaluation
Let people's upvote decide evaluation on your work.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Explore your tools and Skills","Try to use best of your","","3"
"1718","832343","4371011","08/17/2020 03:26:19","## Task Details
Guys go through csv file attach to this and create some phenomenal visual to understand more that present in this data. 
## Expected Submission
You have this full year..!!
## Evaluation
Let people upvote you for the evaluation‚úåÔ∏è

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict Profit ??","Data Analysis","12/31/2020 00:00:00","1"
"1093","700843","4372478","06/08/2020 10:45:05","## Task Details
Apply various topic modeling techniques on the dataset.","","Topic Modeling","","","1"
"1652","823893","4374082","08/10/2020 06:33:27","## Task Details
So I have a problem statement which is to extract names from uncontextual text data.Being uncontextual I am facing problems extracting names from bank documents and other related docs.

## Expected Submission
I would advise to use bank docs text data and build a model to extract names present in the text.For more clarity you can refer amazon textract working and see the type of output i am expecting.","","Name Extraction from uncontextual text data","","","0"
"2117","874951","4374082","09/16/2020 05:16:18","## Task Details
Detect and train a model to detect and classify the images into respective class labels.","","Multiclass Image Classifiaction","","","0"
"2164","880426","4388314","09/19/2020 09:46:31","There's 3000+ freaking columns in this thing that are documented in a PDF, going to build a script or something to automatically populate descriptions","","Label Columns","","","0"
"565","553973","4421435","03/14/2020 11:26:17","## Task Details
The outbreak of Covid-19 is developing into a major international crisis, and it's starting to influence important aspects of daily life. For example:

- Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
- Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether.
- Grocery stores: In highly affected areas, people are starting to stock up on essential goods.

A strong model that predicts how the virus could spread across different countries and regions may be able to help mitigation efforts. The goal of this task is to build a model that predicts the progression of the virus throughout March 2020.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

With this model, you should produce a table in the following format for all future days of March (similar to **covid_19_data.csv**)

- ObservationDate: Observation date in mm/dd/yyyy
- Province/State: Province or State
- Country/Region: Country or region
- Confirmed: Cumulative number of confirmed cases
- Deaths: Cumulative number of deaths cases
- Recovered: Cumulative number of recovered cases

The notebook should be well documented and contain:

1. Any steps you're taking to prepare the data, including references to external data sources
2. Training of your model
3. The table mentioned above
4. An evaluation of your table against the real data. Let's keep it simple and measure Mean Absolute Error.


## Evaluation

This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

However, for declaring winners(if any), we'll be looking at -

- Accuracy - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?
- Data Preparation - How well was the data analyzed prior to feeding it into the model? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.
- Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Predict the Spreading of Coronavirus","Can we help mitigate the secondary effects of covid-19 by predicting its spread","","1"
"566","553973","4421435","03/14/2020 11:34:12","## Task Details
**Visualize the dataset according to your convenience and submit your notebooks to us.**

If you want a formal task , go to task 1 for a detailed problem statement.

## Expected Submission
Submit your results in form of kaggle notebooks(ipynb) to us.
There is no criterion or constraints on submission.


## Evaluation
There is no strict evaluation parameters since we want this event to be a fun and educational one.

Happy Kaggling!","","Visualize data any way you like!!","","","1"
"2229","670889","4422823","09/25/2020 07:49:30","### Object detection model utilising this dataset on Tensorflow","","Create an Object Detection Model on Tensorflow","","","2"
"1650","823434","4426538","08/10/2020 05:19:00","## Task Details
Gurgaon Road, is displayed out of New Delhi locality.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Location error","","","0"
"660","578324","4426849","03/30/2020 09:20:41","## Task Details
Hi,
This is dataset of Indian Cities of Air Quality Index of New Delhi, Kolkata, Chennai, Hyderabad and Mumbai from Dec 2019 to March 2020. Due to lockdown amid COVID-19 huge effect on environment has been monitored. So do analysis on this dataset and find out some new color of the sky.

## Expected Submission
Work with your own pace.

## Evaluation
Better Insights and Visualization.","","EDA and Data Visualization","EDA and Data Visualization of AQI amid coronavirus lockdown.","04/30/2020 00:00:00","0"
"1983","861116","4439150","09/06/2020 18:31:12","Can you train an object detection model to identify kangaroos in the images?","","Object Detection","","","0"
"1320","767742","4443026","07/11/2020 03:02:17","## Task Details
Using this dataset can you come up with the best performing deep learning model for image classification and Object Detection? 

## Expected Submission
Presented comparison of Accuracy and Loss. Compiled model file.

## Evaluation
Detailed steps to developing the deep learning model.  Dynamic code setup (providing options to modify specific parameters)

### Further Works
Use of different frameworks (TensorFlow, PyTorch etc) to also carryout some benchmark comparison.","","Which model yields better accuracy in Classification of these plant images?","","","0"
"1321","767742","4443026","07/11/2020 03:08:56","##Task Details
Using this dataset can you come up with the best performing deep learning model for image classification and Object Detection that can be deployed on Android mobile device?

##Expected Submission
Presented comparison of Accuracy and Loss. Compiled and Converted model file.

##Evaluation
1. Detailed steps to developing the deep learning model. 
2. Dynamic code setup (providing options to modify specific parameters)
3. Lightweight model
4. Accuracy","","Create a deep learning model deployable on Mobile (Android)","Image Classification / Object Detection","","0"
"1322","767742","4443026","07/11/2020 03:10:15","##Task Details
Using this dataset can you come up with the best performing deep learning model for image classification and Object Detection that can be deployed on iOS mobile device?

##Expected Submission
Presented comparison of Accuracy and Loss. Compiled and Converted model file.

##Evaluation
1. Detailed steps to developing the deep learning model. 
2. Dynamic code setup (providing options to modify specific parameters)
3. Lightweight model
4. Accuracy","","Create a deep learning model deployable on Mobile (iOS)","Image Classification / Object Detection","","0"
"3376","508669","5702112","02/02/2021 07:33:50","yesTask Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","android","","","0"
"450","514871","4453870","02/16/2020 05:06:28","## Task Details
Images Detection & Insights Output

## Expected Submission
Looking for help to run an algorithm to generate insights for these images & find relationship between these paintings.

## Evaluation
3+ outputs from the code","","Chinese Fine Art","Famous Chinese Paintings (images)","02/17/2020 00:00:00","1"
"1516","803991","4454045","07/29/2020 03:32:50","## Task Details
I'm the author of this dataset. I want people to explore this dataset and come up with unique ideas providing insights on how one can use the game's weapons effectively to win a match. (Winner Winner Chicken Dinner!)

## Expected Submission
The user is expected to submit a kernel consisting EDA, but anyone can come up with their own ideas!","","EDA on PUBG","Find out which weapons are suitable for specific situations.","","0"
"1436","789928","4455233","07/21/2020 05:28:36","# Task

`Food-101-tiny` contains 10 easily classified types of foods. Train a classifier that achieves `90% accuracy` or higher on the validation set.","","Image Classification","Train a classifier to 90% accuracy on food-101-tiny.","","0"
"1100","605358","3013545","06/10/2020 03:00:06","Could you convert the candy data to:

Response   MEH    JOY	    DESPAIR
type_1	 count	count	count
type_2	 count	count	count
type_3      count      count       count

type _* are the type of candies in the questionnaire. 

For reference you could check part 2 of this notebook: (its incomplete)
https://www.kaggle.com/kalyanikaware/data-cleaning-for-candy-data","","Create a frequency table","","","0"
"2099","508739","5773599","09/14/2020 17:37:49","I do research on Image classification on Meat images of different animal meat images, Please provide if available the meat images of above subjected animals.","","Cow, Bufelo, Dog, Cat, Camel and Donkey meat images required","I do research on Image classification on Meat images of different animal meat images, Please provide if available the meat images of above subjected animals.","12/31/2020 23:59:00","0"
"772","595119","4473443","04/16/2020 17:57:33","This dataset is a mall customers dataset. In which the clustring algorithums will used to maje the clusters.","","Mall_Customer","","","0"
"462","511638","2889731","02/19/2020 10:48:35","## Task Details
Can we predict the possibility of a booking for a hotel based on the previous_cancellation as the target?

## Expected Submission
Two columns --&gt; Hotel Name and Booking_Possibility (0 or 1)","","Can we predict the possibility of a booking?","","","193"
"3440","846179","4484062","02/08/2021 17:44:18","Any feedback on this dateset would be appreciated.","","Have fun","","","0"
"2056","869081","4496228","09/11/2020 19:05:30","## Task Details
Do EDA on already done some excel EDA. But you have to show your coding knowledge.

## Expected Submission
You are expected to submit the codes for all the visualization techniques used.","","Time Series EDS","","","1"
"503","539687","4500271","03/04/2020 09:46:44","Hello, the goal of this database should not be on getting 100% accuracy but instead processing each frame/instance as fast as possible. I think that Image segmentation may help improve speeds because of the large difference between the background and object. Have fun with my shit database.","","Image Segmentation","Use Image Segmentation to Improve Accuracy and Speeds","","1"
"656","577170","4507744","03/29/2020 18:12:48","## Task Details
take a look on it","","Data visualization","","","10"
"657","577170","4507744","03/29/2020 18:13:42","## Task Details
Use this data to predict corona virus preading in Russia","","Predict corona spread","","","4"
"652","575866","4507744","03/28/2020 21:59:06","Take a look on this data","","Data visulization","EDA","","1"
"644","573617","4507744","03/27/2020 13:57:20","Take a look on it","","Data visualiztion","","","1"
"989","678816","4507744","05/27/2020 23:00:14","An active discussion about the mortality data in Moscow has erupted in the days.
The Moscow Times newspaper drew attention to a significant increase in official mortality rates in April 2020:
""Moscow recorded 20% more fatalities in April 2020 compared to its average April mortality total over the past decade, according to newly published preliminary data from Moscow‚Äôs civil registry office.
The data comes as Russia sees the fastest growth in coronavirus infections in Europe, while its mortality rate remains much lower than in many countries. Moscow, the epicenter of Russia‚Äôs coronavirus outbreak, has continued to see daily spikes in new cases despite being under lockdown since March 30.
According to the official data, 11,846 people died in Russia‚Äôs capital in April of this year, roughly a 20% increase from the 10-year average for April deaths, which is 9,866. The numbers suggest that the city‚Äôs statistics of coronavirus deaths may be higher in reality than official numbers indicate.
Russia boasts a relatively low coronavirus mortality rate of 0.9%, which experts believe is linked to the way coronavirus-related deaths are counted.""

After this publication have been realesed The Moscow Department of Health has denied the statement of the inaccuracy of counting.:

First, Moscow is a region that openly publishes mortality data on its websites. Moscow on an initiative basis published data for April before the federal structures did it.
Secondly, the comparison of mortality rates in the monthly dynamics is incorrect and is not a clear evidence of any trends. In April 2020, indeed, according to the Civil Registry Office in Moscow, 11,846 death certificates were issued. So, the increase compared to April 2019 amounted to 1841 people, and compared to the same month of 2018 - 985 people, i.e. 2 times less.
Thirdly, the diagnosis of coronavirus-infected deaths in Moscow is established after a mandatory autopsy is performed in strict accordance with the Provisional Guidelines of the Russian Ministry of Health.Of the total number of deaths in April 2020, 639 are people whose cause of death is coronavirus infection and its complications, most often pneumonia.It should be emphasized that the pathological autopsy of the dead with suspected CoV-19 in Russia and Moscow is carried out in 100% of cases, unlike most other countries.It is impossible to name the cause of death of COVID-19 in other cases. For example, over 60% of deaths occurred from obvious alternative causes, such as vascular accidents (myocardial infarction and stroke), stage 4 malignant diseases (essentially palliative patients), leukemia, systemic diseases with the development of organ failure (e.g. amyloidosis and terminal renal insufficiency) and other non-curable deadly diseases.
Fourth, any seasonal increase in the incidence of SARS, not to mention the pandemic caused by the spread of the new coronavirus, is always accompanied by an increase in mortality. This is due to the appearance of the dead directly from an infectious disease, but to an even greater extent from other diseases, the exacerbation of which and the decompensation of the condition of patients suffering from these diseases also leads to death. In these cases, the infectious onset is a catalyst for the rapid progression of chronic diseases and the manifestation of new diseases.
Fifthly, a similar situation with statistics is observed in other countries - mortality from COVID-19 is lower than the overall increase in mortality.
According to the official sites of cities:In New York, mortality from coronavirus in April amounted to 11,861 people. At the same time, the total increase in mortality compared to the same period in 2019 is 15709.In London, in April, 3,589 people died with a diagnosis of coronavirus, while the total increase was 5531
Sixth, even if all the additional mortality for April in Moscow is attributed to coronavirus, the mortality from COVID will be slightly more than 3%, which is lower than the official mortality in New York and London (10% and 23%, respectively). Moreover, if you make such a recount in these cities, the mortality rate in them will be 13% and 32%, respectively.
Seventh, Moscow is open for discussion and is ready to share experience with both Russian and foreign experts.","","Statisitc analys","Can you confirm or refute the theory of falsification of statistics on mortality from covid in Moscow?","","2"
"3817","819798","4525433","03/20/2021 00:36:12","## Task Details
The notebook ""Feature engineering for posterior regressions"" cleans the data from ar_properties.csv and gets multiple dataframes for immediate model training.
The dataframes' columns are 5 subsets of the [""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""surface_covered"", ""surface_total"", ""price_period"", ""property_type""] columns. 

Here's a description of each:

'A' dataframes store entries from the last 3 months
'B' dataframes store entries from the last 14 months

0. ""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""surface_covered"", ""surface_total"", ""price_period"", ""property_type"" (drop missing surfaces, impute for the mean in everything else) 
1. ""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""surface_covered"", ""surface_total"", ""price_period"", ""property_type""  
2. ""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""surface_covered"", ""price_period"", ""property_type""(drop lat/lon missing, imputation on the mean for rooms, bathrooms and surface)
3. ""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""surface_covered"", ""price_period"", ""property_type""(drop lat/lon &rooms missing, imputation on the mean for bathrooms)
4. ""created_on"",""lat"", ""lon"", ""rooms"", ""bathrooms"", ""price_period"", ""property_type"" (drop lat/lon missing)  

For greater detail feel free to read ""Feature engineering for posterior regressions"" notebook, suggestions and corrections are highly appreciated.



## Expected Submission
A model that receives entries with the corresponding columns and outputs rent price estimations.


What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
Mean Average Error (minimize it)

What makes a good solution? How do you evaluate which submission is better than another?

### Further help
https://www.kaggle.com/msorondo/feature-engineering-for-posterior-regressions","","Predict rent with clean data","Use the output from ""Feature engineering for posterior regressions""","","0"
"928","661605","4530800","05/18/2020 17:37:58","Predict Student final marks using the most efficent  ML algorithm.
Dataset is in a proper format there is no NaN values no missing values.","","predict Student Final Marks","mission marks","","2"
"2629","703335","4533747","11/04/2020 16:06:06","## Task Details
Every task has a story. Tell users what this task is all about and why you created it. to analyze Pendulum Dataset. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain? to explore Pendulum Dataset

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another? different visualization for Pendulum Dataset

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Pendulum Dataset","Pendulum Dataset","","0"
"496","535591","4538297","03/01/2020 16:41:32","## Task Details
pridiction from china fund data daily
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","pridiction from china fund data daily","pridiction from china fund data daily","","0"
"1843","846741","4542957","08/27/2020 04:14:05","Help government to take necessary steps to tackle climate changes and disasters like flood, drought, etc. by timely predictions.","","Develop a model to predict rainfall of a city in particular month","","","0"
"1910","853374","4542957","09/01/2020 07:39:54","### Task Details
Predict the way market will behave in next few month by taking into consideration of covid19 slowdown and year to year monthly reports.

### Evaluation
One can use linear regression for the prediction.","","Prediction of impact of Covid 19 on Indian Economy","Predictions and regression","","3"
"1913","853852","4542957","09/01/2020 14:04:24","## Task Details
The linguistic style in Sanskrit and the respective translation to English can be studied in detail using NLP methodology.","","Sanskrit English Translation","","","0"
"2133","877277","4542957","09/17/2020 03:59:10","Can we perform some EDA to find the reason for soaring onion prices and resultant export ban?
Can we find any ML model to predict how long it will last, what will be prices in the end, etc?","","EDA to find This years Onion Crisis","","12/31/2020 23:59:00","1"
"750","592974","4521496","04/14/2020 13:24:27","## Task Details
From the data, it's pretty clear that the main interest is *mission status*. So the task is basically visualization of the different relations between the *variables* and the *mission status*. The visualization shouldn't be limited to relation to the mission status but among the variables too. 
As for the model fitting, fit a model that best predicts the *mission status* and highlight the most influential variables.

## Expected Submission
Notebooks .

## Evaluation
I'll live that to discussions.","","VISUALIZATION and MODEL FITTING","Models that best predicts the mission status and visualisation to show r/ships among the variables.","","0"
"4465","535863","6394673","05/23/2021 10:43:09","## Task Details
1 ) Converting time series to separate month-wise / year-wise  columns 
2 ) Using line plots and Heatmaps to interpret the data

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Visualisation and Time Series","","","1"
"1174","725295","4550392","06/18/2020 20:15:05","The most important objective of the Swachh Bharat Mission or Clean India Mission is to end open defecation forever in all villages by 2 October 2019. The catalog contains the status of Open Defecation Free Villages, Blocks and number of Swachhagrahi's working for the mission.

Perform an EDA to develop meaningful insights on the data that can help to more understand the siutation.","","Block-wise no.of ODF non-ODF Villages in India","Block-wise number of Villages and Open Defecation Free (ODF) Villages as on date","10/01/2020 00:00:00","0"
"1501","802772","4551724","07/28/2020 04:20:05","How does a certain champion scale? Could I go through a different path for their build?","","Analysis","","","0"
"848","636017","4551724","05/05/2020 19:43:23","The whole point of this data set is building some model that, given the 10 champions involved, the positions, and the mastery of the players using them, can somehow predict the chances of each team to win.
The outcome of League of Legends games is unpredictable, as it relies a lot on luck. However, unbalanced teams (for example, 5 tanks or 5 AD champions) clearly has a disadvantage, so their chances of winning should be lower.
Any idea on how to do that? I have tried R, but the resulting model is useless.","","How to predict outcomes","","","0"
"1080","697828","4552206","06/06/2020 19:10:12","## Task Details
Based on the plot and the summary of the movie you have to find the similarity between the movies

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find similarity between different movies","","","0"
"893","652529","4555895","05/13/2020 20:19:02","Analyze the dataset to infer the weekday effect on the continuously compounded returns while using each weekday as base/reference once and infer the interpretations from all the models.","","Analyse the Weekday effect on Continuously Compounded Returns.","","05/30/2020 00:00:00","0"
"1323","766646","4565482","07/11/2020 04:40:42","## Task Details
interactive visualization of arrests by race by year and total

## Expected Submission
Mid-July

## Evaluation
Interactive, filterable

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Arrests by Race","","","0"
"485","530723","4565606","02/26/2020 21:14:11","whatever","","Get it done","","02/28/2020 00:00:00","0"
"2142","878286","4570297","09/17/2020 16:59:27","To find which feature to select and visualize the data, which feature correlates to the price.","","To predict the price.","","","0"
"2189","886627","4570297","09/21/2020 18:40:53","Analyzing the data.","","Performing EDA and extracting valuable information","","","1"
"1178","727860","4574389","06/20/2020 05:35:55","The content which can help in categorizing the news to different categories.","","Identification of keywords","","","0"
"1179","727860","4574389","06/20/2020 05:37:35","The categories will be provided to the participants and they will have to figure out the belonging categories for a news and form the category tree. The sample submission file has category trees for some news","","Formation of category tree on the basis of the identified keywords with ML","","","0"
"1465","794642","4575748","07/23/2020 11:27:09","Deaths from weather conditions or locations or others.","","Accident cause patterns","","","0"
"2351","909243","4575748","10/06/2020 18:38:35","Municipal budget analysis by type of expenditure and others.","","Municipal budget analysis","","","0"
"2335","907115","4575748","10/05/2020 12:50:42","WiFi access points, or hotspots, located in various municipal amenities and public access points in the city of Barcelona.","","Wifi hotspots of the city of Barcelona","","","0"
"1536","802817","4579735","07/30/2020 17:28:10","Try to find out the keywords used in video title which has more views compared to another video in the same time period.

Time period means number of days or hours between upload date and timestamp.","","which keywords are more catchy?","","","0"
"2026","865077","4586025","09/09/2020 05:58:30","## Task Details
You should see the Convolutional Neural Network basic.

## Expected Submission
Just Notebook, enough.

## Evaluation
Confusion Matrix and for more detailed Classification Report

### Further help
https://en.wikipedia.org/wiki/Object_detection#:~:text=Object%20detection%20is%20a%20computer,in%20digital%20images%20and%20videos.","","Build a deep learning model for image classification","","","0"
"1584","812110","4593647","08/04/2020 10:41:52","## Task Details
Based on the past World Cup results and other climbing factors from the dataset such as age and team dynamics, it would be valuable to predict the ranking of all nations for the overall Lead World Cup Scores. The same analysis had been conducted for World Championships in the attached paper, but predictions would be even more accurate if done on World Cups as results would show more consistency. 

## Expected Submission
The submission should be based on the dataset and the attached paper. The same analysis from the paper can be conducted, but simply with different data. 

## Evaluation
A good solution would be to show all steps of the process, replicate as much as possible the paper's analysis, and even write a conclusion based on those results.


### Further help
All the help you can get can be found in the attached paper. Else you can always email me on the email address in the appendix of the paper.","","2021 Lead Climbing World Cup Predicted Overall Results by Nation","","","1"
"866","618353","4568636","05/09/2020 04:53:07","1.  You Analysis should cover discriptive statistics and any two ML algorithm (linear or logistics Regression, Decission tree or Random forest).
2.import the data set to WAMP server and write any 10 SQL enquiries to extract data about covid19 cases like State wise ,district wise ,city wise,age wise ,gender wise.","","Data analysis","","05/12/2020 00:00:00","0"
"1809","839457","5640504","08/24/2020 04:34:52","The purpose of this experiment is to predict patient‚Äôs age from the person‚Äôs normal EEG signal. The size of the dataset is big enough compared to other studies on this topic (e.g. Dimitriadis 2017 and Alzoubi 2018). We would like the community to try out different methods and algorithms (e.g. FFT/wavelet + SVM/LSTM etc.).","","Predict age based on EEG signal","","","3"
"1454","792971","4606536","07/22/2020 14:51:13","## Task Details
It is to find the customer response on campaign done by credit card company.

## Expected Submission
User should come up with the pattern using different regression techniques.","","Analysis on the basis of campaign","credit card campaign","09/30/2020 00:00:00","0"
"623","567329","4607640","03/23/2020 14:01:26","## Details
All of the images in this dataset contain GPS metadata, allowing you to identify where they were taken. Can you predict the location without using this metadata though, based purely on the image content?

For example, you might look at the skyline and match distinctive features to an elevation model... Or perhaps you might use OCR to read road signs and work out where you might be from those... Or perhaps you'll spot a bus number, and identify a route that the image must have been taken on... Or perhaps you'll spot some flora or fauna that only grows in a certain part of Scotland...

There are many different ways this task can be tackled, and different approaches will be best suited to different situations. We're interested in all automatic approaches, even if they only work on a subset of the images or just narrow down the search area.

## Example Use Case
An example defence use case for this capability is locating hostages based on videos or photos sent by captors.

In this use case, analysts are likely to only have one or two images to work from, and may not accurately know the time the image was taken.","","Geolocation of Images","Identify image location based on content","","0"
"1169","724099","4614139","06/18/2020 08:58:33","I'm expecting a treemap","","Charting the GDP","","","1"
"1162","722929","4614139","06/17/2020 17:34:06","I'd love to see this data set charted.","","Charting the GDP","","","0"
"1550","810142","4614224","08/01/2020 09:19:35","## Background
At Shopee, we always strive to ensure the correct listing and categorization of products. For example, due to the recent pandemic situation, face masks become extremely popular for both buyers and sellers, every day we need to categorize and update a huge number of masks items. A robust product detection system will significantly improve the listing and categorization efficiency. But in the industrial field, the data is always much more complicated and there exist mislabeled images, complex background images, and low-resolution images, etc. The noisy and imbalanced data and multiple categories make this problem still challenging in the modern computer vision field.

| ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F8983b696df39ecb3bbd2fb5dcf7303be%2F5d04dd453bbe5b71c00236f9383751e5.jpg?generation=1591669667767485&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F56456e8a500366e15d468ccd8d5f13c2%2F3b1519ee1932c8ed960649be4ff35dec.jpg?generation=1591670501765344&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F24c4289a420487e7ea3117492a03b29a%2F3d52392446452e4eb7e26e4d762467b3.jpg?generation=1591670956360484&alt=media) |
|---|---|---|
| ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2Ff4d283fc00b5f2055893fbca2e62a3b3%2F0dbeed1585ee580bf3d9670e8b23ecc8.jpg?generation=1591671179189986&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F31f1700efb459542f6d5ad5964f61c39%2F2b346cae0305ece2a7e819b833dfd635.jpg?generation=1591671472191358&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F4ca0ca158d7b79f20d3bc2d16abf97e3%2F1cf0391bad6debae1a9cff35153e8962.jpg?generation=1591671513570675&alt=media) |
| ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F74c7ee5182be4902d42f845f3889a433%2F0ad3826c91d647b659794160adb70f92.jpg?generation=1591672539773817&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2Fec184aadda0d10cb1b09fa58deb98fc6%2F00ba143008cf454fa24c5f1e1c60c8e5.jpg?generation=1591672611376483&alt=media) | ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5198646%2F6cd8f24cdca4b4dc835fcdbe5a9e3cba%2F0d5fae3df34696d2cdc6dc19dfd02057.jpg?generation=1591672648172617&alt=media) |


## Task
In this competition, a multiple image classification model needs to be built. There are ~100k images within 42 different categories, including essential medical tools like masks, protective suits, and thermometers, home & living products like air-conditioner and fashion products like T-shirts, rings, etc. For the data security purpose, the category names will be desensitized. The evaluation metrics is t top-1 accuracy.


## Expected Submission
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2Fe4bea24da492db0f888a077acc41ef7c%2FCapture.JPG?generation=1596179133894547&alt=media)


## Evaluation
Submission file format should be `csv` file only. And for each `filename` in the test dataset, you must predict only one proper category name. The `csv` file should contain a header and have the following format:
&gt; filename, category
&gt; 2f096e5e8e8955d43632be16e35993b5.jpg, 0
&gt; 3d63a44c82c9d1299b5791bdd2c7a4e8.jpg, 1


## Partners
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4182692%2F23cbd1271fca3aebf629f0bc5788c564%2F4.jpg?generation=1593835947231939&alt=media)","","Detect Real Product On E-commercial Platform","Product Detection - Data Science Challenge","","5"
"1569","810142","4614224","08/03/2020 04:17:42","## Background Story
Due to the recent COVID-19 pandemic across the globe, many individuals are increasingly turning to online platforms like Shopee to purchase their daily necessities. This surge in online orders has placed a strain onto Shopee and our logistics providers but customer expectations on the timely delivery of their goods remain high. On-time delivery is arguably one of the most important factors of success in the eCommerce industry and now more than ever, we need to ensure the orders reach our buyers on time in order to build our users‚Äô confidence in us.

In order to handle the millions of parcels that need to be delivered everyday, we have engaged multiple logistics providers across the region. Only the best logistics providers that are able to meet Shopee‚Äôs delivery standards are partnered with us.

The performance of these providers is monitored regularly and each provider is held accountable based on the Service Level Agreements (SLA). Late deliveries are flagged out and penalties are imposed on the providers to ensure they perform their utmost.

The consistent monitoring and process of holding our logistics providers accountable allow us to maintain our promise of timely deliveries to our buyers.

## Tasks
Identify all the orders that are considered late depending on the Service Level Agreements (SLA) with our Logistics Provider.

For the purpose of this question, assume that all deliveries are considered successful by the second attempt.


## Basic Concepts

-   Each orderid represents a distinct transaction on Shopee.

-   SLA can vary across each route (A route is defined as Seller‚Äôs Location to Buyer‚Äôs Location) - Refer to SLA_matrix.xlsx

-   Pick Up Time is defined as the time when the 3PL picks up the parcel and begins to process for delivery. It marks the start of the SLA calculation.

-   Delivery Attempt is defined as an attempt made by the 3PL to deliver the parcel to the customer. It may or may not be delivered successfully. In the case when it is unsuccessful, a 2nd attempt will be made. A parcel that has no 2nd attempt is deemed to have been successfully delivered on the 1st attempt.

-   All time formats are stored in epoch time based on Local Time (GMT+8).

-   Only consider the date when determining if the order is late; ignore the time.

-   Working Days are defined as Mon - Sat, Excluding Public Holidays.

-   SLA calculation begins from the next day after pickup (Day 0 = Day of Pickup; Day 1 = Next Day after Pickup)

**2nd Attempt must be no later than 3 working days after the 1st Attempt, regardless of origin to destination route**  (Day 0 = Day of 1st Attempt; Day 1 = Next Day after 1st Attempt).

Only consider the date when determining if the order is late; ignore the time.

Assume the following Public Holidays:

1.  2020-03-08 (Sunday);
2.  2020-03-25 (Wednesday);
3.  2020-03-30 (Monday);
4.  2020-03-31 (Tuesday)

## Submission Format

Check each delivery order and determine whether it is late.

Two columns required:

-   orderid.
-   is_late: assign value 1 if the order is late, otherwise 0.

| orderid | is_late |
| ------- | --- |
| 1955512445 | 0 |
| 1955598428 | 1 |

#### Your submission should have 3,176,313 rows (excluding headers), each with 2 columns.

## Examples
### Example 1
| Orderid          | 1955598428                                                                                                                                                                       |
|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Seller‚Äôs Address | ‚ÄúBlock 2, Lots 2,3,10 & 11, Honest St cor. Determined Street, Calamba Premiere International Park (CPIP), Batino, Calamba, Laguna, Philippines Calamba City Batino Laguna Luzon‚Äù |
| Buyer‚Äôs Address  | ‚Äúunit 2 seaviews castles, Tambo, Paranaque City, Metro Manila, Metro Manila‚Äù                                                                                                     |
| Pick Up Time     | 1583137548 (Converted to 2020-03-02 4:25:48 PM Local Time)                                                                                                                       |
| 1st Attempt Time | 1583733540 (Converted to 2020-03-09 1:59:00 PM Local Time)                                                                                                                       |
| 2nd Attempt Time | NaN                                                                                                                                                                              |

Based on the SLA matrix:

- Luzon -&gt; Metro Manila: 1st Attempt must be within 5 days from Pick Up Time.
This means the Logistics Provider has up to 2020-03-07 to attempt a delivery. This order is deemed as late.

Calculating the Days

Day 0 = 2020-03-02 (Pick Up Time)
Day 1 = 2020-03-03 
Day 5 = 2020-03-07 
Day 6 = 2020-03-09 (we exclude 2020-03-08 from the calculation because it is a Sunday)

### Example 2
| Orderid          | 1955598428                                                                                                                                                                       |
|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Seller‚Äôs Address | ‚ÄúBlock 2, Lots 2,3,10 & 11, Honest St cor. Determined Street, Calamba Premiere International Park (CPIP), Batino, Calamba, Laguna, Philippines Calamba City Batino Laguna Luzon‚Äù |
| Buyer‚Äôs Address  | ‚Äúunit 2 seaviews castles, Tambo, Paranaque City, Metro Manila, Metro Manila‚Äù                                                                                                     |
| Pick Up Time     | 1583137548 (Converted to 2020-03-02 4:25:48 PM Local Time)                                                                                                                       |
| 1st Attempt Time | 1583412300 (Converted to 2020-03-05 8:45:00 PM Local Time)                                                                                                                       |
| 2nd Attempt Time | 1583850180 (Converted to 2020-03-10 10:23:00 PM Local Time)                                                                                                                      |

Based on the SLA matrix:

- Luzon -&gt; Metro Manila: 1st Attempt must be within 5 days from Pick Up Time
- 2nd Attempt must be within 3 days from the 1st Attempt

This order has a 1st Attempt within 5 days. However, the 2nd Attempt is 4 days from the 1st Attempt (we exclude Sundays from the calculation). This order is deemed late.

## Scoring Metrics
Matthews Correlation Coefficient

### Evaluation Description
Matthews Correlation Coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure that can be used even if the classes are of very different sizes.[2] The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between ‚àí1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and ‚àí1 indicates total disagreement between prediction and observation.

The MCC can be calculated directly from the confusion matrix using the formula:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2F20360df230604529478983468697fdb1%2FCapture.JPG?generation=1596428531142443&alt=media)

In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.","","Logistics Performance","E-Commerce Logistics: Data Analytics Challenge","","4"
"1571","810142","4614224","08/03/2020 04:33:30","## Background

At Shopee, we always strive to ensure the customer‚Äôs highest satisfaction. Whatever product is sold on Shopee, we ensure the best user experience starting from product searching to product delivery, including product packaging, and product quality. Once a product is delivered, we always encourage our customers to rate the product and write their overall experience on the product landing page.

The rating and comments provided for a product by our buyers are most important to us. These product reviews help us to understand our customer's needs and quickly adapt our services to provide a much better experience for our customers for the next order. The user's comments for a product ranges from aspects including delivery services, product packaging, product quality, product specifications, payment method, etc. Therefore it is important for us to build an accurate system to understand these reviews which have a great impact on overall Shopee‚Äôs user experience. This system is termed: ""Shopee Product Review Sentiment Analyser"".

**Note: This page is for participants from the student group!**

## Task

In this competition, a multiple product review sentiment classification model needs to be built. There are ~150k product reviews from different categories, including electronics, furniture, home & living products like air-conditioner and fashion products like T-shirts, rings, etc. For data security purposes, the review ids will be desensitized. The evaluation metrics are top-1 accuracy.

### Sample Dataset
| review_id | review                                                                                                          | rating |
|-----------|-----------------------------------------------------------------------------------------------------------------|--------|
| 11576     | It's working properly. Very quick heating capability. Good product with this price thanks                       | 5      |
| 10293     | Excellent service by the staff, helpful and polite. Great experience overall.                                   | 5      |
| 01820     | The delivery was fast but the packaging was not that good, the price is reasonable, overall the product is ok., | 4      |
| 32090     | Package not that well                                                                                           | 2      |
| .....     | .....                                                                                                           | .....  |

## Evaluation
Submissions are scored on top-1 precision:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2F92048a320b41937d0ef2341eade6ab41%2FCapture.JPG?generation=1596429178653302&alt=media)

### Submission File
Submission file format should be `csv` file only. And for each `review_id` in the test dataset, you must predict the rating (from 1 -5). The `csv` file should contain a header and have the following format:

```
review_id, rating
0, 2
1, 4
2, 1

```
Your submission should have 60,427 rows, each with 2 columns.","","Given product reviews, predict user ratings (1 to 5) for the review","E-Commerce Product Review Sentiment Analysis - Data Science Challenge","","3"
"1572","810142","4614224","08/03/2020 04:39:10","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2F64bf49b367c3468664c0b43674272690%2FOverview.png?generation=1596429547258595&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2Fa4c50fb9697b23961f133e33f255c141%2FExamples.png?generation=1596429638860886&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2F889e956544dfaf531b4318afbd45a495%2FData.png?generation=1596429741140502&alt=media)

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2F5007bf5521ce278beeda9da468164b25%2FEvaluation.png?generation=1596429755857347&alt=media)","","Order Brushing","Some E-Commerce Sellers Create Some Fake Orders - Data Analytics Challenge","","3"
"1573","810142","4614224","08/03/2020 04:51:02","## Background

At Shopee, we have customers and sellers across Southeast Asia and Taiwan. In order to provide a better shopping experience, we set up the cross border business to improve the variety of products for our customers. In order to help the local sellers to migrate the SKUs to different foreign markets, we hire a group of professional human translators with e-commerce background to take care of the product translation. The product information translated includes product title, variation, and description. However, as Shopee is growing tremendously in recent years, the amount of translation per day is beyond the human translators‚Äô capacity. At the same time, with the development of AI technology, machine translation is now deployed in many industrial areas to assist the human translators and they can achieve a near-human level translation quality.

In Shopee, we have an in-house machine translation pipeline which can translate millions of SKUs per week in different languages. The languages include Traditional Chinese, Bahasa, English, Vietnamese, Thai, and Portuguese. The challenges in machine translation are usually the lack of labeled data. However, different ways of unsupervised machine translation have been explored and proven to be effective, with little or even no label data. Some techniques are cross-lingual word alignments or pre-trained cross-lingual language models.

## Task

Given a product title in Traditional Chinese, the candidate is expected to translate the title into English.

Dataset: Candidates are provided with two monolingual product title data (in Traditional Chinese and English). The use of public data is encouraged.

Metrics: Bleu score of the whole test set is used to assess the translation quality.

## Evaluation

The file should contain a header and have the following format:

    translation_output # header
    translated title string one
    translated title string two
    ...","","Shopee Product Title translation","Chinese to English Product Title Translation Model - Data Science Challenge","","1"
"1574","810142","4614224","08/03/2020 04:55:16","## Background Information

The aim of this project is to build a model that can predict whether a user opens the emails sent by Shopee.

Sending emails is one of the marketing channels Shopee uses to reach out to our users. Being able to predict whether a user opens an email allows Shopee to forecast and evaluate the performance of future marketing campaigns before launch. This is because when a user opens an email, the probability of the user knowing the campaign increases and this in turn increases the probability of the user making a checkout during the campaign period. Therefore, with the predicted open rates, Shopee can better develop, strategize and implement future marketing campaigns.

## Task

We provide you with data related to marketing emails (Electronic Direct Mail) that were sent to Shopee users over a certain period. It contains information about

-   User-specific information

-   Email nature

-   Users‚Äô engagement on the platform

-   User‚Äôs reaction to the email, including whether users opened the email

Based on the data provided, you must predict whether each user will open an email sent to him/her.

## Evaluation
Submissions are evaluated on the [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC) between the predicted and the observed response. The MCC is given by:

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4614224%2Ff7884e4451762639ffa19f6e3b384d67%2FCapture.JPG?generation=1596430478337586&alt=media)

where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.

### Submission File

For each row in the test set, you must make a binary prediction for the target variable. The file should contain a header and have the following format:

    row_id,open_flag  
    0,0  
    1,1  
    2,0  
    etc.

Your submission should have 55,970 rows, each with 2 columns.","","Email Campaign Analysis","Shopee Marketing Analytics: Data Analytics Challenge","","2"
"2084","871321","4615465","09/13/2020 15:31:59","## Task Details
Let us work on an exploratory Data Analysis Project. Kindly find the questions which we need to find an answer to.

## Expected Submission
I am listing the following 15 Questions- for which we need to analyze and find the answers to. Please feel free to make further analysis and post your findings. We all are really excited to learn and grow together:

1. Top 10 most Popular movie of all times.
2. Top 3 movies by Genre (Subject)
3. The Top Actors/Actresses of All time
4. The Top actors by Genre 
5. Actors/Actresses who have been the most experimental- i.e., have worked on multiple Genre movies.
6. The Best Directors of all Time
7.  Are the directors only restricted to a single Genre, or they have experimented into multiple Genres.
8. The most popular movies of all time.
9. The most popular movies by Genre
10. The most award winning movie by Genre.
11. Is award a metrics to identify the popularity? Let us find.
12. Which Genres have been more in demand with time or rather the Most popular  genre with Time.
13. The Most Popular Actors/Actresses/Directors in the different Decades.
14.  The most successful Director-Actor/ Director-Actress pairs that have been the most successful in terms of popularity.
15. The most successful Director-Actor/ Director-Actress pairs that have been the most successful in terms of winning awards.


## Evaluation

We expect the presentation in the form of visuals more- as they are easier to read and comprehend. But the presentation will be as per the Kagglers interest. We hope to find answers to questions that we have missed, and have a great learning session together.","","Exploratory Data Analysis Tasks","I have posted some Questions as tasks for this data set. Hope you all like the tasks and we all learn and grow together using this set.","10/15/2020 23:59:00","0"
"1240","751433","4626491","07/02/2020 03:58:50","As I am a relative beginner, I would like to see others' approach in making sense of this data.","","Sentiment Analysis","","","0"
"1044","691832","4626491","06/03/2020 20:40:09","As the title suggests.","","Possible relation between past flu seasons and COVID-19?","","","0"
"533","550464","4639881","03/11/2020 21:30:37","...then exist!","","just think...","","","0"
"526","547690","4641073","03/10/2020 05:33:04","# corona virus **##** Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","corona virus","corona virus patient","","1"
"1467","792627","4642984","07/23/2020 14:45:36","Find out the salary differences of data scientists based on working experience and level of education","","Find out salary difference based on  experience","","","0"
"1399","782408","4644225","07/17/2020 11:30:06","You can use the data to predict the stock prices .","","TRY TO PREDICT STOCK PRICE !","STOCK PRICE PREDICTION","","1"
"960","672162","4659665","05/24/2020 15:16:15","## Task Details
You can create another file or can edit into the existing file for inserting a column for age. where it will show the weight of effect  on how a disease will act on a person according to the age.

## Expected Submission
Create a CSV file as it will be easy for every on to process in future.

## Evaluation
Try to give clean values.


### Further help
For any query:
pratikrathod947@gmail.com
uchihaitachi9604@gmail.com","","Age column","","","3"
"2811","672162","771372","11/29/2020 11:59:24","Feature Selection of Symptoms","","Identify the top 5 common symptoms for a disease","","","3"
"2812","672162","771372","11/29/2020 12:00:42","Make it Realtime","","Create a model and Flask App using the top 5 symptoms to identfy the disease","","","1"
"2813","672162","771372","11/29/2020 12:01:33","Return the remedy as well as the class label for disease identified","","Return the precuations for the identified disease","","","3"
"2261","895747","4666562","09/27/2020 11:07:39","## Task Details
This task is about analyzing the performance o the two players in various formats and later come to a conclusion based on who is better than the other.
The solution of this task should contain various graphs that clearly tell like what the graph is about and the notebook owner's conclusion based on the analysis he made.
The submission with attractive graphs and the graphs that makes sense would be the best submission.","","Who is better?","Make a analysis based on their performance.","","1"
"1892","850822","4667203","08/30/2020 10:19:39","## Task Details
Build a deep learning model to classify 20 motifs of Indonesian Batik.","","Batik Motifs Classification","","","0"
"1955","858948","4667203","09/05/2020 09:14:14","## Task Details
Predict movie rating based on available features.","","Predict Movie Rating","","","0"
"1956","858948","4667203","09/05/2020 09:14:44","## Task Details
Do some Exploratory Data Analysis (EDA) with the dataset","","Exploratory Data Analysis","","","1"
"1856","848391","4667203","08/28/2020 10:31:16","## Task Details
Predict the gender of a person using his/her name.

## Evaluation
Accuracy metrics","","Gender Prediction Based on Name","","","1"
"1434","789985","4682657","07/21/2020 03:32:26","extract 2 -3 features which are most influential toward the consumption","","most influential feature","","","0"
"4934","615631","6332659","06/28/2021 13:12:36","## Task Details
1. Create a pie chart that has Total Quantity Sold per Category
2. Create another pie chart showing the Total Quantity Sold per Sub-Category

## Expected Submission
Use Group by functions to group together the categories and find their total quantity sold. Then use any visualization tool to show the pie chart.

## Evaluation
Put understandable title and color combinations.

### Further help
For Data visualization:
https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners

Group-by Tutorial:
https://www.kaggle.com/crawford/python-groupby-tutorial","","Sales per Category and Sub-Categories","Find the Percentage of Sales per Category and Sub-Categories","","0"
"6390","596958","7899406","10/16/2021 13:03:41","## Task Details
Some attributes may contribute more than other attributes, find out which attributes have the most weightage and affect the placement of a student

## Expected Submission
notebooks containing all the relevant metrics (each column vs all etc) 

## Evaluation
Accuracy score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","SVM classifier","to check if SVM can help predict if a student gets placed or not","","2"
"734","596958","4683527","04/11/2020 11:57:25","The Director of the B-School would like to understand the impact of type of Degree (degree_t) on the average MBA percentage (mba_p). Use the one-way ANOVA to check the impact at alpha = 0.05. Show the summary of the model. Perform a diagnostic check using Bartlett test to check for assumption of homogeneity of variance.","","One Way ANOVA","The Director Demands !","","10"
"735","596958","4683527","04/11/2020 11:58:22","1) Develop an estimated multiple linear regression equation with mba_p as response variable and ssc_p & hsc_p as the two predictor variables. Interpret the regression coefficients and check whether they are significant based on the summary output
2) Estimate a multiple regression equation for each of the below scenarios and based on the model‚Äôs R-square comment which model is better. 
(i)	Use mba_p as outcome variable and ssc_p & degree_p as the two predictor variables. 
(ii)	Use mba_p as outcome variable and hsc_p & degree_p as the two predictor variables.
3) Show the functional form of a multiple regression model. Build a regression model with mba_p as dependent variable and ssc_p, hsc_p and degree_p as three independent variables. Divide the dataset in the ratio of 80:20 for train and test set (set seed as 1001) and use the train set to build the model. Show the model summary and interpret the p-values of the regression coefficients. Remove any insignificant variables and rebuild the model. Use this model for prediction on the test set and show the first few observations‚Äô actual value of the test set in comparison to the predicted value.","","Multiple Linear Regression","","","43"
"736","596958","4683527","04/11/2020 11:58:52","Select a random sample of size 100 (set seed at 25). Conduct a one-sample t-test to prove that the average degree_p differs from 60%. Use alpha = 0.05 and mention the null and alternative for the test.","","T-test","","","9"
"737","596958","4683527","04/11/2020 12:00:03","The B-School wants to establish the existence of an association relationship between mba_p (outcome variable) and degree_p (predictor variable). Develop a simple linear regression model by estimating the model parameters. Show the regression plot. Comment on the R-square value of the model and test for significance of regression coefficient based on the summary.","","Simple linear regression model","","","13"
"738","596958","4683527","04/11/2020 12:01:37","Use the ggplot2 package to answer the below questions.
	
(i)	Draw a histogram for degree_p. Use appropriate bin width (Fig 1). Differentiate based on status (Fig 2). Using faceting introduce gender (Fig 3).		
(ii)	Draw a boxplot for degree_p (Fig4). Differentiate based on status (Fig 5). Using faceting introduce workex and gender together (Fig 6). 			
(iii)	Draw a density plot for degree_p (Fig 7). Differentiate based on status (Fig 8). Beautify the plot in Fig 7 using various arguments (Fig 9).			
(iv)	Which of the above three types of plots is more useful for interpretation and how?","","GGplot","Plots all the way !","","7"
"779","596958","4617847","04/18/2020 01:53:29","EDA



Tell a story from the data","","EDA,Visualization","","","16"
"1324","596958","388747","07/11/2020 08:01:45","Submit a model which can predict student's placement.

Using Python 2/3. Please refer to the following articles for help.

https://towardsdatascience.com/simple-machine-learning-model-in-python-in-5-lines-of-code-fe03d72e78c6

https://medium.com/@Ahmedrebai/my-first-machine-learning-program-8647051362b8

https://favouriteblog.com/first-machine-learning-project-in-python-step-by-step/","","Build a model to predict if a student gets placed","","07/14/2020 00:00:00","14"
"860","596958","4897815","05/07/2020 16:11:46","## Task Details
I found KNN Algorithm to be very easy. SoI have done some coding according to my data science and machine learning knowledge. I have uploaded my code. Suggestions are always welcome.

## Expected Submission
I welcome everyone from kaggle community to upload their codes and see the difference in output. Try different combinations of columns.

## Evaluation
No evaluation

### Further help
If new then refer:
https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/kNN.","","KNN Algorithm","","","47"
"905","596958","4959722","05/14/2020 17:55:57","Random Forest","","Random Forest","","","2"
"1142","596958","4493295","06/14/2020 20:24:34","Predict the placement status of a student using Logistic Regression.","","Logistic Regression (Predicting the placement status)","","","18"
"1624","596958","5053745","08/07/2020 15:02:32","Predict Outlier using KNN . which will give good predictions?","","Outlier detection using KNN","","09/07/2020 00:00:00","1"
"2165","881475","4688022","09/19/2020 10:04:32","## Task Details
Its interesting to find out is there a pattern for winning the game?","","Understand The Game!","Is there something consistent that we are missing ?","","0"
"2349","909148","4688022","10/06/2020 17:08:54","can we help new Patreons understand how to make a better page?","","What Make A Good Patreon Page?","can we help new Patreons understand how to make a better page?","","0"
"2318","904719","4688022","10/03/2020 17:08:42","Try and uncover underlying patterns in the dataset and understand what drives guitar players to learn a certain song.","","Find Interesting Insight","","","0"
"2277","897783","4688022","09/28/2020 17:07:55","Can we use nlp to extract features from the title and description of the book and together with the numeric data understand what makes a good programing  book.","","What are the underlying patterns which make a good programing book ?","","","1"
"1275","760477","4695261","07/07/2020 01:03:08","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

To classify emails as spam or ham.","","Classification of spam mails","","","0"
"2071","869785","4695264","09/12/2020 10:07:42","## Task Details
Some of the cities, or villages were missing either latitude/longitude data, or zip codes, due to the lack of this information on wikipedia.

## Expected Submission
An Updated file including all the Missing Data would be a great contribution to the dataset.

## Evaluation
The Best Solution is to find an alternative data source for supplying this information, with a simple methodology of how to extract the data.","","Scrap Longitude, Latitude","Rescrap null data","","0"
"709","588529","4701130","04/08/2020 01:07:02","## Task Details
Wgdesign assists the health practitioners to identify areas statistics, review areas receiving greatest and least amount of tests in the greater NYC area. We hope to compare this data with the number of hospital cases, to determine recent stay and PPE requests.

## Expected Submission
Either the Hospital Name, or the zip code for understanding the areas most effected by the virus. This may also enable Hospitals to assist with transfer patients from one site to another location.

## Evaluation
This assisting Nurses and Health Care Prof. to determine if the testing is adequate for their protection and patients.  The map was produced from this data to demonstrate the boundaries of these effected areas, showing the greatest and least amount  of testing per area.","","Covid19 Test results and Hospitals in areas","Mash up of 5 max/min test results, and hospital areas.","04/09/2020 00:00:00","0"
"2645","562352","5628750","11/07/2020 15:54:27","1. Discover which aspect of the services offered have to be emphasize more to generate more satisfied customers

2. Select a model to predict whether a future customer would be satisfied with their service given the details of the other parameters values.","","Which aspects should be improved to increase satisfaction. Modelling","","","7"
"1299","764749","4704706","07/09/2020 02:41:08","## Task Details
Basically, try to predict the amount of wins given the stats. Your model will encode weights for an estimate of how impactful each stat is in terms of winning. 
Another way to think about it is via correlation matrices.
This is a useless predictor since you will only have the stats after the games are played, but it is interesting trying to find how a team's performance in different areas affects their winningness.

## Expected Submission
Make a notebook with the dataset and show some visulization of the stats most correlated with winning.

## Evaluation
A good solution is one inspired by interest.","","Find Which Stats Contribute to Wins","","","1"
"1311","766859","4704706","07/10/2020 04:04:31","Word2Vec is currently my favorite algorithm thing. It is essentially a neural network that decides the best way to encode each word in the corpus so that it can maximize its chances of predicting surrounding words given a context. In other *words,* Word2Vec strives to get a feel for the text.

With a Word2Vec representation of the text, you can find some interesting things.

I'm going to eventually submit to this task, so you can use that for what a good evaluation looks like (just kidding).","","Word2Vec Bible","NLP application that assigns high dimensional vectors to vocab","","1"
"1450","791915","4704706","07/22/2020 03:22:32","## Task Details
Like the Iowa Housing competition, try to model the housing information and how that affects the ultimate sale price.

## Expected Submission
Submit notebook of your predictions.

## Evaluation
Man, just get close. MAE I guess.
Also, prove that you don't overfit.","","Predict sale price","obviously without LP or OLP","","0"
"1167","723461","4704957","06/18/2020 05:24:46","## Task Details
In this task, you are required to design a custom CNN Model to classify the severity of Diabetic Retinopathy. You are free to choose any Deep Learning Framework,

## Expected Submission
The Solution should contain every single detail of the model, the training parameters and the evaluation result.

## Evaluation
A good Model should achieve at least 0.90 AUC for each class.","","DR Classification","Classification using custom CNN Models","12/12/2020 00:00:00","1"
"894","649144","4714894","05/13/2020 20:54:03","## Task Details
There are 17 391 places without scores of 45 664 of all list on Alaska.

## Expected Submission
Predict scores and violations for unmarked places.

## Evaluation
The best submission is the most correct submission, which closest to 100%.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predicting of inspection results","What inspection scores and violations can be at unmarked spots?","","0"
"1737","836506","4715753","08/18/2020 13:09:34","Dog_predictions.csv contains computer-generated predictions of the images in the tweets of the twitter_dogs.csv dataset. I am looking to find a way in Python to evaluate the accuracy of the predictions.","","Evaluate image prediction accuracy","","","0"
"665","568747","4726702","04/01/2020 08:46:48","Describe the situation with statistical analysis and data visualizations","","Descriptive Statistics and Visualizations","","","0"
"666","568747","4726702","04/01/2020 08:47:46","Know how Guatemala's corona virus spread relates and compares to other countries that are a) seemingly winning the fight (China, South Korea, Singapore, etc), b) in the rush to 'flatten the curve' (Italy, Spain, France, UK, etc) and c) starting to see the impact (the rest of the world).","","Data correlation","","","1"
"667","568747","4726702","04/01/2020 08:48:34","Based on the previous two tasks (descriptive statistics and data correlation), infer information such as estimates of the real number of infections, projections on the infection rate estimates on the age-sex groups that might be affected the most to what level.","","Inference","","","0"
"2222","869126","4729207","09/24/2020 13:06:00","Develop a machine learning model to accurately classify various products into 4 different classes of sentiments based on the raw text review provided by the user","","Develop a machine learning model to classify various products into 4 different classes of sentiments","","","0"
"2188","886216","4729207","09/21/2020 17:42:42","Task is to create the machine learning component for this image similarity application. The machine learning
model should return the top K images that are most similar to this image based on a single image input.","","Task is to create the machine learning component for this image similarity application","","","0"
"1700","570440","4740297","08/15/2020 07:00:24","## Task Details
The Data set basically contains some important mechanical information about a set of cars (Number of Cylinders, Size, Fuel Consumption, etc) that were surveyed.
##Context
The main agenda behind this data set is to provide the beginners with a mindset that what is the end goal of a machine learning project and how could they help the organizations involved by giving proper results.
## Expected Submission
The users are expected to submit a model that could predict the COCO
2 Emission      (Carbon Dioxide Emission) of new cars that have new such parameters with high confidence.

## Evaluation
A good solution should contain a well train, generalized machine learning model that gives highly accurate results.The Carbon dioxide emission levels for a particular submission and we could reduce it to help save the environment and also keep the efficiency of the car's performance.","","Machine Learning Implementation.","Advanced Regression Techniques.","","3"
"1717","833557","4740297","08/16/2020 14:44:38","## Task Details
Here the main focus is to check whether a deep learning model can predict the strength of a new cement if the previously fed data is considered and can it predict the strength accurately.

## Expected Submission
The notebook should contain:
- A deep learning model with high accuracy and low loss and validation loss.
## Evaluation
A good solution is the one with at least 90% accuracy with loss being less than 1 and validation loss being less than 0.9.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/prathamtripathi/regression-with-keras-using-basic-neuralnetworking?scriptVersionId=40851682","","Calculating Concrete Strength","Advanced Regression with Neural Networking","","5"
"1789","841045","4740297","08/22/2020 12:17:33","## Task Details
The main aim is to create a model that accurately predicts customer category.
## Expected Submission
ML model which is fairly generalized and not over- trained.

## Evaluation
An f1_score of 95 - 97 at least.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/prathamtripathi/classification-accuracy-97-5
- https://www.kaggle.com/prathamtripathi/regression-neural-network","","Prediction of CustCat","A model to predict customer category","","2"
"1698","830916","4740297","08/14/2020 17:58:32","## Task Details
Here the given data set has a series of parameter of the patient like Sex, Drug Type etc.

## Expected Submission
The solution should contain a well-made machine learning model with high confidence in predicting the drug type (A, B,C,X,Y) that should be given to a particular patient based on their characteristics and which drug type will suit them best.

## Evaluation
A better solution would be the the one with highest accuracy score, highest r2 score with a generalized supervised learning model.","","Classification Problem","Machine Learning","","25"
"638","571427","4741859","03/26/2020 02:48:46","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another??","","2020 O Ano da Vit√≥ria","","03/27/2020 00:00:00","1"
"664","581994","4741859","04/01/2020 05:07:47","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?!","","Solyoh21","https://www.kaggle.com/data/140222#793310","","1"
"787","612805","4741859","04/20/2020 13:37:36","https://www.cnbc.com/2020/04/20/oil-markets-us-crude-futures-in-focus-as-coronavirus-dents-demand.html","","USA   Covid 19     Live Updates","USA     Corona Virus     Live Updates","04/21/2020 00:00:00","1"
"788","614101","4741859","04/21/2020 11:55:47","https://medium.com/personal-growth/6-secrets-of-highly-motivated-people-150234ae834e


6 √© o numero da Sorte !!!","","https://medium.com/","6 the number secrets","","0"
"816","619494","4741859","04/24/2020 20:30:29","More mistakes from an influential virus model.
There‚Äôs good news today for people hoping that politicians will just declare victory against the coronavirus‚Äîor whatever they think they need to say‚Äîand let us have our economy back. A Nobel Prize-winning chemist is encouraged by the latest global infection and mortality data. Some state governments are moving toward economic revival. And new research raises more questions about a series of virus models which have influenced the decisions of nervous politicos.

As for the need to liberate Americans to pursue their livelihoods, University of Chicago economist Casey Mulligan has a new website tracking the massive daily economic cost of the virus response. Of course the news contained in his charts is awful, but combined with other research it may help flatten the curve of belief in popular virus predictions.

On this point, this column recently noted the work of researchers at Australia‚Äôs University of Sydney, working with colleagues at America‚Äôs Northwestern and the University of Texas at El Paso. They detailed some of the flaws in predictions made by the oft-quoted Institute for Health Metrics and Evaluation (IHME), which has become famous for its coronavirus forecasts.

Today the team from Sydney, Northwestern and UTEP is publishing a new update to its working paper. Unfortunately the IHME predictions may not be getting any better, and in some ways may actually be getting worse. In their new evaluation of the predictive performance of the IHME models, the researchers report:

To assess the accuracy of the IHME models, we examine both forecast accuracy using tools from the econometric literature, as well as the predictive performance of the 95% prediction intervals provided by the IHME models. We find that the initial IHME model underestimates the uncertainty surrounding the number of daily deaths substantially. Specifically, the true number of next day deaths fell outside the IHME prediction intervals as much as 70% of the time, in comparison to the expected value of 5%. In addition, we note that the performance of the initial model does not improve with shorter forecast horizons.

Pro football scouts often say that if a quarterback is not an accurate passer when he enters the National Football League, he is unlikely to become one during his career. Perhaps a similarly healthy skepticism should be applied by politicians when consulting academic forecasters. According to the team critiquing the IHME.","","Curve Crushing","http://pandemiccosts.com/?modtag=djemBestOfTheWeb","","0"
"1122","707218","4743733","06/12/2020 01:11:38","Analyze the datasets to draw insights about the impact of Covid-19 on Canadian economic factors like GDP, CPI and workhours.","","Covid-19 Impact Analysis","","","0"
"2015","864156","4747784","09/08/2020 13:57:01","## Task Details
The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.

Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.

## Expected Submission
The following 2 files are to be uploaded.

**test_predictions.csv** - This should contain the 0/1 label for the tweets in test_tweets.csv, in the same order corresponding to the tweets in test_tweets.csv. Each 0/1 label should be in a new line.
 
**A .zip file of source code** - The code should produce the output file submitted and must be properly commented.

## Evaluation

The metric used for evaluating the performance of classification model would be F1-Score.","","Sentiment Analysis","Identify racist or sexist tweet","","1"
"2225","891418","4748515","09/24/2020 16:01:37","Use the dataset to visualize the data and come up with an answer","","Which team has won most number of matches?","","","1"
"986","645187","4751080","05/27/2020 17:37:43","## Task Details
The ascent of renewable energy sources introduces social benefits and new paradigms. In a new power distribution environment, where end users now both consume and produce energy, securing the stability of electrical distribution grids is essential. 

## Expected Submission
In my [notebook](https://www.kaggle.com/pcbreviglieri/predicting-smart-grid-stability-with-deep-learning) deep learning was employed to predict smart grid stability from simulated data. Accuracy levels in excess of 97% were obtained with a simple artificial neural network. More complex models may improve such performance - that's your task.

## Evaluation
Focus on reaching at least 99% accuracy levels with your solution.

### Further help
My code is inserted in my [notebook](https://www.kaggle.com/pcbreviglieri/predicting-smart-grid-stability-with-deep-learning) and can also be retrieved from my [Github repo](https://github.com/pcbreviglieri/data-science-smart-grid-stability).","","Help model renewable energy distribution","Train neural networks to predict stability of smart grids","06/30/2020 00:00:00","0"
"987","661308","4751080","05/27/2020 17:48:57","## Task Details
A chest x-ray radiograph is one of the most cost-effective tools to identify pneumonia. 

## Expected Submission
In my notebook the employment of a simple convolutional neural network allowed for the detection of pneumonia from chest x-ray imagery with accuracy levels in excess of 96% (validation) and 92% (testing). Also, a very low number of 'false negative' cases (only 6 out of 624 in testing) was achieved.

## Evaluation
Focus on reaching at least 98% accuracy levels on both validation and testing with your solution. More sophisticated computer vision solutions (e.g. pre-trained models) may assist you on this task. 

### Further help
My code is inserted in my [notebook](https://www.kaggle.com/pcbreviglieri/covid-19-revisiting-pneumonia-detection) and can also be retrieved from my [Github repo](https://github.com/pcbreviglieri/data-science-detecting-pneumonia).","","Enhance pneumonia prediction","Achieve high accuracy levels with sophisticated solutions","06/30/2020 00:00:00","0"
"1282","750914","4751080","07/07/2020 17:07:11","## Task Details
In a global scenario still affected by severe income distribution imbalances and chronic famine in several communities, the accurate classification of cropland pieces is of outmost strategic and economic importance.

## Expected Submission
In my [notebook](https://www.kaggle.com/pcbreviglieri/cropland-mapping-random-forest-neural-network) I compared the performance of random forest and deep learning (artificial neural network) to predict cropland mapping. Overall accuracy levels in excess of 99% were obtained with both. Would alternative models have the same success? That's your task.

## Evaluation
Focus on reaching at least 99% accuracy levels with your solution - the higher, the better.

### Further help
My code is inserted in my [notebook](https://www.kaggle.com/pcbreviglieri/cropland-mapping-random-forest-neural-network) and can also be retrieved from my [Github repo](https://github.com/pcbreviglieri/data-science-cropland-mapping).","","Pursue perfection in cropland mapping","Try alternative models to beat random forests & neural networks","","0"
"654","572974","4751478","03/29/2020 07:52:54","## Task Details
1.This data set has only the reviews and Rate the Top Restaurants based on the review

## Expected Submission
Extract reviews and their ratings for the data set

## Evaluation
Data extraction from json to get values(reviews) and find ratings depends on the results","","Reviews","Ratings are not given in the data set and challenge is find ratings based on reviews","05/10/2020 00:00:00","0"
"1109","705495","5274330","06/10/2020 12:46:50","## Task Details
Perform data analysis","","Analysis on Car Prediction","","06/11/2020 00:00:00","0"
"645","574076","4755858","03/27/2020 23:13:03","Based on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive/negative)?


### Evaluation 
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

Our team will be looking at:

1. Model Performance - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?

2. Data Preparation - How well was the data analysed prior to feeding it into the model? Are there any useful visualisations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.

3. Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Predict confirmed COVID-19 cases among suspected cases","","04/03/2020 00:00:00","93"
"646","574076","4755858","03/27/2020 23:14:14","Based on the results of laboratory tests commonly collected among confirmed COVID-19 cases during a visit to the emergency room, would it be possible to predict which patients will need to be admitted to a general ward, semi-intensive unit or intensive care unit?


### Evaluation

This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

Our team will be looking at:

1. Model Performance - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?

2. Data Preparation - How well was the data analysed prior to feeding it into the model? Are there any useful visualisations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.

3. Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Predict admission to general ward, semi-intensive unit or intensive care unit","","04/03/2020 00:00:00","47"
"1656","807638","4760409","08/10/2020 11:47:15","Data Visualization
- Imputing  missing field
- percentage of data saved  after data imputation.","","EDA for the Bank Dataset","What data is implying you?","","7"
"1606","817708","4760409","08/05/2020 19:14:31","## Create a dashboard to have real-time information about Country's Covid 19 status 
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Visulization","","","4"
"1790","836415","4760409","08/22/2020 13:46:57","## Task Details
As Analyst, I want a model to make sure it helps right countries who needs the help.

## Expected Submission
A cluster model with final results

## Evaluation
Recursive clustering model with correct information about needs in country.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create a Unsupervised model to make sure we help right country who needs help","Any model is okay but solution should be shared in way that it help the right countries","12/31/2028 23:59:00","1"
"1528","805842","4771989","07/29/2020 18:48:19","## Task Details
Find out which programming language is most required, how much experience the companies are asking for and much more.","","Find out the skills that you need to get a Data science job in 2020.","","","0"
"1209","740661","4781819","06/27/2020 00:44:43","**Task Details**
every year which book has the best , text review and high rating .

**Expected Submission**
A notebook with the solution.

**Evaluation**
A clear representation of the results of your analysis.","","Best book of every Year","","","4"
"863","618335","4393111","05/08/2020 22:49:00","## Task Details
I did it because my teacher told me to.

## Expected Submission
A file with a part of the input submission to be a part of the test to show if, a person is either healthy, dead, or has corona depending on their food lifestyle choices. 

## Evaluation
A good solution would be if you have the correct balance of a healthy population with a good recovered to death ratio.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Corona Vs Healthy Foods","Does eating healthy give a country the advantage to be more restraint or healthier? Or do they lead to more deaths?","05/15/2020 00:00:00","15"
"920","659811","4786044","05/17/2020 20:42:02","Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.","","Loan approval pediction","","","0"
"1011","683487","4786044","05/30/2020 10:26:34","Use Machine Learning Algorithms to predict CO2 emission.","","Predict the value of Carbon dioxide emission","","","2"
"2004","862865","4786510","09/07/2020 19:33:02","## Task Details
Use this dataset to get familiar with using image data for classification problems.

## Expected Submission
You could solve using either new model or pre-trained models
1. Display basic summary of model
2. Make predictions
3. Visualize metrics

## Evaluation
Evaluate on test set, try achieving accuracy greater than 98%","","Predict whether parasitized or uninfected","","","1"
"1729","835350","4789522","08/17/2020 16:53:26","Can you predict the income level of a collaborator?","","Predict The Income Level","","","1"
"2129","876947","4789522","09/16/2020 20:03:18","Can you use your data science skills to build a classification model that would help investors to predict if a startup will succeed or fail?","","Predict the success of an startup","","","0"
"2107","871938","4789522","09/15/2020 09:51:25","Use Regression Models to predict Salary of an Indian Engineering Graduate","","Regression Models to predict Salary","","","1"
"3219","757568","4793224","01/18/2021 13:04:54","## Task Details
The task is intermediate, you need to achieve most accurate solution for the same! Try increasing the data with data augmentation and using State-of-Art techniques to achieve the better and effective results! All the best!
You can use this data for your projects as well, creating a web application!

## Expected Submission
Solution can be derived using any of the algorithms user wishes too! Recommend using the State-of-Art techniques to achieve the better and effective results!

All the best!üëç","","Create a Accurate Classifier using your Deep learning skills.","Try create a most accurate classifier which classifies each amount of currency, also you can work on some web application detecting the Indian currency!","","1"
"2276","896499","4795937","09/28/2020 16:27:17","This task requires you to use the K Nearest Neighbours algorithm to make a prediction on the 'PrvPub' column of the dataset. You are encouraged to explore the different parameters you can work with in your model and understand the importance of data understanding and feature selection.
(Note: Beginners can use the ""KNeighborsClassifier"" class available under ""sklearn.neighbors"" )
Expected Submission

The submission must be a Notebook containing the process of Exploratory Data Analysis and making of the model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'accuracy' metric.
Evaluation

The aim is to understand the KNN algorithm and its parameters, and evaluation would be based on the accuracy of the model.","","Top Personality Dataset","","","10"
"2667","898072","5132765","11/10/2020 04:17:36","## Task Details
Predict the crime per capita of this dataset .

## Expected Submission
Use a 80-20 train-test split over the dataset and predict the test data.
submission data should have a id column, crime per capita and the absolute difference of predicted and real value.

## Evaluation
The task is on lowering the MSE (Mean-squared error).

## Sample Submission Data
https://github.com/sagnik1511/Real-Estate-Data-analysis/blob/main/sample_submission.csv    you have to create an output file like this.","","Crime prediction : best fitted model","","","11"
"2275","897669","4795937","09/28/2020 16:14:37","Task Details

This task is suitable for beginners who have just stepped into the world of data science. It requires you to use the K Nearest Neighbours algorithm to make a prediction on the 'PrvPub' column of the dataset. You are encouraged to explore the different parameters you can work with in your model and understand the importance of data understanding and feature selection.
(Note: Beginners can use the ""KNeighborsClassifier"" class available under ""sklearn.neighbors"" )
Expected Submission

The submission must be a Notebook containing the process of Exploratory Data Analysis and making of the model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'accuracy' metric.
Evaluation

The aim is to understand the KNN algorithm and its parameters, and evaluation would be based on the accuracy of the model.","","Survey Data","School Science Survey Data Using Classification Model","","3"
"976","675901","4808962","05/26/2020 09:41:59","Step 1. Read the Data set
Step 2. Perform Univariate Analysis
Step 3. Perform Bivariate Analysis
Step 4. Perform Multivariate Analysis
Step 5. Perform Inferential Statistics","","Perform Data Analysis","","01/31/2021 00:00:00","0"
"977","675938","4808962","05/26/2020 10:53:53","1. Analyze Rapes in India
2. Analyze Accidents in India
2. Analyze Murders in India
3. Analyze the most crime-prone areas in India","","Analyze Crimes in Different States in India","","11/30/2021 00:00:00","2"
"975","675651","4808962","05/26/2020 08:51:47","Step 1. Read the Data, check the Head, Tail etc
Step 2. Then Perform Descriptive Analysis
Step 3. Then Perform Inferential Statistics
Step 4. Perform Data Cleaning
Step 5. Perform Feature Engineering
Step 6. Data Analysis
Step 7. Grouping Operations","","Perfom EDA","","01/31/2021 00:00:00","0"
"895","650763","4811794","05/14/2020 02:13:05","## Task Details
What insights can be drawn from this data regarding the following details:

March 18- Federal Government announces that travellers from certain countries would no longer be allowed to enter Nigeria until the Coronavirus pandemic was over.
March 20- Nigerian government announced the closure of tertiary institutions, secondary and primary schools.
March 29- Government announced the closure of land borders.
March 30, 11 pm- Lockdown begins in FCT, Lagos and Ogun states.
May 11 - Lockdown is lifted in FCT, Lagos and Ogun states




## Expected Submission
Since this is not a formal competition, you're not submitting a single submission file, creativity and inquisitiveness is encouraged. Feel free to use this dataset plus any other data you have available.

## Evaluation
This is not a formal competition,  what I'd like to see is a well-defined process and decent insights (evaluated by yourself).

##Acknowledgements
The data set was scrapped from the Nigerian Center for Disease Control COVID-19 website.","","What Insights can be drawn from the Dataset (possibly with Visuals)","Predict the Spreading of Coronavirus","","1"
"1803","839167","4818011","08/23/2020 18:36:03","## Task Details
The Lockdown helped us see what a lot of prediction models could/would have predicted about cutting vehicular movement for a few weeks. 

There are groups of activists appealing to the Government to declare a day in the month as a cycle day and promote cycling. Other appeals include designating a few areas as a no-vehicle-zone on certain days.

The effects we want to achieve, are the ones that are, in part, indicated by the data collected during the lockdown.

**Why SilkBoard? **
The area around the Central Silk Board  sees the movement of lakhs of vehicles every single day. Most of the people who work in South Bangalore, are in some way required to cross the traffic junction at Silk Board sometime.  This area is a representation of the traffic volumes that Bangalore sees, and even though many projects have been undertaken to solve traffic congestion, the traffic density keeps rising, as in most parts of Bangalore.
This place is a major traffic intersection and a major source of pollution is in the form of vehicular traffic. We saw a major change in its scenery during the lockdown. 

## Expected Submission

1. The effect of a lockdown on the air quality parameters, against the data that contains measurements from February, about 6 weeks before the lockdown.
2. Model driven observations and suggestions, about what would be required to create the similar pollution curbing effects with minimum interference on vehicular traffic.
3. Data driven suggestions on what all data driven arguments could be presented, in order to keep our air cleaner at Central Silk Board, Bangalore.

## Evaluation

A good solution would be one that would predict similar air quality measurement trends, in a usual day, without a lockdown, but with some tweaks in vehicular movement or other administrative intervention.

A good solution would also include any other sources of data or links of such that could add a dimension of argument towards solutions.

I will be adding more data added in the coming few days, to include more dimensions to this problem.  Apart from being open to ideas, I am looking to include data about vehicular movement, year on year respiratory related illnesses reported in South Bangalore.","","Analyse the pollution trend at Central Silk Board, Bangalore during lockdown","","10/01/2020 23:59:00","0"
"1090","691203","4822067","06/08/2020 06:23:38","## Task Details
You need to just identify the pose in the hockey playing:)","","Identify the pose","","","0"
"942","664424","4830039","05/21/2020 04:41:28","Get this dataset and train your model to predict where news is fake or not","","Take the data and predict run various news to predict if it is fake or not.","","","0"
"2181","885911","4833212","09/21/2020 11:27:08","**# TASKS**
Create inventory and product distribution locations for a chain of stores.

**Task 0:** EDA

**Task 1:** only in state capitals (capital = 'admin')

**Task 2:** If you expand the network to the 10 largest cities that are not state capitals, will it be necessary to create another distribution point?","","Hosts Distribution","","","0"
"752","595885","2453377","04/15/2020 04:07:19","## Task Details
In the absence of immunity, the only way to prevent the spread of COVID-19 is to provide a physical barrier or distance between humans. Governments, corporations and other bodies have made various announcements, implemented shut down or distancing laws, deployed enforcements of varying duration and [stringency](https://www.bsg.ox.ac.uk/research/research-projects/coronavirus-government-response-tracker) to try to limit human-to-human contact and disease spread.

A large number of ""natural experiments"" have occurred across the planet in the last 3 months, testing various distancing efforts and their effects on key population health outcomes including case counts, hospitalizations, critical care admission, mortality and immunity. 

This task aims to find relationships between distancing efforts and public health outcomes. This is important now for a few reasons:
1. Policy-makers need feedback. COVID-19 happened quickly. Some acted swiftly and with great might. Some did not. Which strategy worked?
2. As we pass the peak in many jurisdictions, the question on everybody's mind is how to we open up. If we can keep some distancing measures will relaxing others, perhaps we can limit the impact on return to jobs and the economy.
3. We will likely see a future of oscillations between disease spread and containment. Which interventions can we use and expect what impact?

## Starting point suggestions 
- There is a lot not known about COVID-19 and Canada's response. Basic descriptive entries are encouraged. This could include visualization of NPIs over time by region or an estimate of reproductive number over time.
- Consider the timing and stringency of interventions as important features (they are not all the same). Scoring government responses based on these two features would be interested.
- Look for reference jurisdictions where public health outcomes were positive (South Korea, Singapore) and less fortunate (Italy, Spain, New York) to anchor your analysis.
- Consider model parameters of ""the curve"" as target variables, including reproductive number, rate of spread, and doubling time.
- Consider mobility data as a marker of early societal response to distancing interventions.
- Simulation of the effects of different distancing measures on health and economic outcomes could be pursued by expert participants.

## Suggested Reading:
[https://medium.com/@tomaspueyo/coronavirus-the-hammer-and-the-dance-be9337092b56] (https://medium.com/@tomaspueyo/coronavirus-the-hammer-and-the-dance-be9337092b56)
[Economics in the Age of COVID-19, forthcoming book by Joshua Gans](https://economics-in-the-age-of-covid-19.pubpub.org/)
[Mathematical models to characterize early epidemic growth: A Review by Chowdell et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5348083/)
[Estimating the effect of physical distancing on the COVID-19 pandemic using an urban mobility index by JP Soucy et al] (https://www.medrxiv.org/content/10.1101/2020.04.05.20054288v1)
[Temporal variation in transmission during the COVID-19 outbreak](https://epiforecasts.io/covid/)
[R Epidemics Consortium](https://www.repidemicsconsortium.org/projects/)
[Can the COVID-19 epidemic be controlled on the basis of daily test reports?](https://arxiv.org/pdf/2003.06967.pdf)
[Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period] (https://science.sciencemag.org/content/early/2020/04/14/science.abb5793)
[Estimating COVID-19's $R_t$ in Real-Time by Kevin Systrom] (https://github.com/k-sys/covid-19/blob/master/Realtime%20R0.ipynb)

## Expected Submission
A single annotated Jupyter-style notebook describing the methodology and containing compelling visualizations should be submitted. Team submissions welcome.

Speed &gt;&gt; perfection. But please feel free to let us know if you are working on a complex analysis that may take longer than the alloted time. 

## Evaluation
Relevance (5 points)
    * Is this relevant to Ontario decision-making?
Accuracy (5 points)
    * Did the participant accomplish the task?
    * Did the participant discuss the pros and cons of their approach?
Documentation (5 points)
    * Is the methodology well documented?
    * Is the code easy to read and reuse?
Presentation (5 points)
    * Did the participant communicate their findings in an effective manner?
    * Did the participant make effective use of data visualizations?","","1. Quantify the impact of different distancing measures on public health outcomes","","04/26/2020 00:00:00","4"
"753","595885","2453377","04/15/2020 04:34:23","## Task Details
Who has or might have the disease? In the absence of information, the only way to shut down the disease is to enforce mass isolation and shut down the economy. With full person-level information on test and exposure status, more targeted strategies (isolate only infected / exposed people) become available.

A large number of ""natural experiments"" have occured  across the planet in the last 3 months, testing various testing and tracing and their effects on key population health outcomes including case counts, hospitalizations, critical care admission, mortality and immunity.

This task aims to find relationships between testing / tracing efforts and public health outcomes. This is important now for a few reasons:
1.  Policy-makers need feedback. COVID-19 happened quickly. Some acted swiftly and with great might. Some did not. Which strategies worked?
2. As we pass the peak in many jurisdictions, the question on everybody's mind is how to open up. What testing and tracing strategies work and how well? 


## Starting point suggestions
* Consider the ramp up and peak of daily testing relative to local disease onset 
* Look for reference jurisdictions where public health outcomes were both positive (South Korea, Singapore) and less fortunate (Italy, Spain, New York) to anchor your analysis.
* Consider model parameters of ""the curve"" as target variables, including reproductive number, rate of spread, and doubling time.



Reading:
[https://medium.com/@tomaspueyo/coronavirus-the-hammer-and-the-dance-be9337092b56] (https://medium.com/@tomaspueyo/coronavirus-the-hammer-and-the-dance-be9337092b56)
[Economics in the Age of COVID-19, forthcoming book by Joshua Gans](https://economics-in-the-age-of-covid-19.pubpub.org/)
[Mathematical models to characterize early epidemic growth: A Review by Chowdell et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5348083/)
[Estimating the effect of physical distancing on the COVID-19 pandemic using an urban mobility index by JP Soucy et al] (https://www.medrxiv.org/content/10.1101/2020.04.05.20054288v1)
[Temporal variation in transmission during the COVID-19 outbreak](https://epiforecasts.io/covid/)
[R Epidemics Consortium](https://www.repidemicsconsortium.org/projects/)
[Can the COVID-19 epidemic be controlled on the basis of daily test reports?](https://arxiv.org/pdf/2003.06967.pdf)
[Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period] (https://science.sciencemag.org/content/early/2020/04/14/science.abb5793)
[Estimating COVID-19's $R_t$ in Real-Time by Kevin Systrom] (https://github.com/k-sys/covid-19/blob/master/Realtime%20R0.ipynb)

## Expected Submission
A single annotated notebook describing the methodology and containing compelling visualizations should be submitted. Team submissions welcome.

Speed &gt;&gt; perfection. But please feel free to let us know if you are working on a complex analysis that may take longer than the alloted time. 

## Evaluation
Relevance (5 points)
* Is this relevant to Ontario decision-making?
Accuracy (5 points)
* Did the participant accomplish the task?
* Did the participant discuss the pros and cons of their approach?
Documentation (5 points)
* Is the methodology well documented?
* Is the code easy to read and reuse?
Presentation (5 points)
* Did the participant communicate their findings in an effective manner?
* Did the participant make effective use of data visualizations?","","2. Quantify the impact of different testing and tracing strategies on public health outcomes","","04/26/2020 00:00:00","9"
"754","595885","2453377","04/15/2020 05:48:38","## Task Details
If we could instantly test every single person in Ontario, we would have complete information and could selectively isolate a tiny portion of the population and allow the rest to return to work. On the flip side, with no testing, complete lock down is the only strategy that prevents health system collapse - devastating the economy.

Inspired by early work of Paul Romer, this task asks the question: how do testing properties and tracing strategies impact the number of people who must remain isolated versus return to work? Amongst other things, he modeled 2 policies: random isolation versus testing + isolation and measured the proportion of a simulated population (dots in a box) that would have to be removed from the workforce to keep cumulative infection below a critical threshold.

This inspires further thinking along these lines: Should we be swabbing the nasopharynx of only symptomatic people with a test with 20% false negative rate to determine who should be isolated (without overloading the healthcare system)? Should we random sample a large population? What level of return to work would the use of instant testing and perfect cell phone tracing enable?

This kind of guidance around testing, tracing and isolation will help decision-makers understand how many of what kinds of tests to procure and deploy with what kind of logistics and policy. The more relevant to Ontario for this challenge, the more impactful.

## Suggestions to get started
- Explore the lag time of various read world testing techniques (minutes v days) and tracing strategies (phone calls v tracing)
- Consider constraints / boundaries related to total allowable number of cases (Ontario), total ICU bed use or mortality.
- Consider creating an interactive model


## Required reading:
https://paulromer.net/


## Expected Submission
A single annotated notebook describing the methodology and containing compelling visualizations should be submitted. Team submissions welcome.

Speed &gt;&gt; perfection. But please feel free to let us know if you are working on a complex analysis that may take longer than the alloted time. 

## Evaluation
Relevance (5 points)
    * Is this relevant to Ontario decision-making?
Accuracy (5 points)
    * Did the participant accomplish the task?
    * Did the participant discuss the pros and cons of their approach?
Documentation (5 points)
    * Is the methodology well documented?
    * Is the code easy to read and reuse?
Presentation (5 points)
    * Did the participant communicate their findings in an effective manner?
    * Did the participant make effective use of data visualizations?","","3. Simulate the effects of different testing and tracing parameters / policies on distancing","","04/26/2020 00:00:00","4"
"755","595885","2453377","04/15/2020 05:55:00","## Task Details
This is the leftover bucket. Feel free to take something home to chew on.

1. This pandemic isn't going anywhere for a while. What are the measures of public health and economy we should be using to monitor our response? Can you conceive, justify and validate a ""digital biomarker"" from publicly available data?
2. Related, can you create and validate a scorecard that ranks governments on their ability to act quickly / decisively to flatten the curve?


## Expected Submission
A single annotated notebook describing the methodology and containing compelling visualizations should be submitted. Team submissions welcome.

Speed &gt;&gt; perfection. But please feel free to let us know if you are working on a complex analysis that may take longer than the alloted time. 

## Evaluation
Relevance (5 points)
* Is this relevant to Ontario decision-making?
Accuracy (5 points)
* Did the participant accomplish the task?
* Did the participant discuss the pros and cons of their approach?
Documentation (5 points)
* Is the methodology well documented?
* Is the code easy to read and reuse?
Presentation (5 points)
* Did the participant communicate their findings in an effective manner?
* Did the participant make effective use of data visualizations?","","4. Open research questions","Grab bag of other ideas","04/26/2020 00:00:00","1"
"2097","873431","4852445","09/14/2020 17:03:24","## Task Details
Gain some visual insights about the data. Let your visualisations tell a story. Use a variety of plots to answer questions such as:

1. What commodity, in total, was the most expensive?
2. Are commodities imported into the USA more expensive than local commodities?
3. How did the price of gold change overtime?
4. What was the cheapest commodity before 2000?

## Expected Submission
A notebook of the working code


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Commodity Prices EDA","","","0"
"2098","873431","4852445","09/14/2020 17:05:16","## Task Details
Pick your favourite commodity, analyse it with time series analysis tools, and make predictions for the price of it. You can compare your predictions with the dataset from here: https://www.imf.org/en/Research/commodity-prices

## Expected Submission
A notebook showing all your workings

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Commodity Price Prediction","","","0"
"2078","870101","4852445","09/12/2020 15:23:27","## Task Details
Use different plotting libraries and tools to gain a better understanding of the data. Ask questions such as:

- What crop was produced the most in total?
- Is there any location that produced a significant amount of crops?
- What year produced the most crops worldwide?
- What continent produced the most crops before 2000?

## Expected Submission
A notebook showing your work


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Crop Production EDA","","","1"
"2079","870101","4852445","09/12/2020 15:35:31","## Task Details
Try predict what crop production will be worldwide from 2026 onwards until 2028. Use and try different ML and DL models.

## Expected Submission
A notebook of your solution

## Evaluation
Use cross validation methods to evaluate different models

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict Worldwide Crop Production from 2026 onwards","","","1"
"1982","860990","4852445","09/06/2020 16:48:17","## Task Details
Analyse the data, look for correlations, use different plotting libraries and time series methods to see if there is any connection between the columns. Make predictions for worldwide meat consumption using all the data.

## Expected Submission
A Notebook, showing the code used for EDA and making predictions


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and Consumption Predictions","","","2"
"5520","860990","8001886","08/03/2021 14:49:48","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

The cohesion between the elements of a group has a great influence on the success of any collaborative work between the elements of that group. Hence the importance of the feelings shared in the group. It will therefore be a question of bringing together elements ready to collaborate together and having a minimum of sympathy between them.","","system-for-the-grouping-of-learners-according-to-the-feelings-expressed-during-collaborative-work-","Sentiment analysis","","0"
"1735","836416","4857635","08/18/2020 11:53:59","Your task is to explore the data and extract as much information and patterns from it as you can. There is a lot to explore, and you will definitely have fun playing with this data.","","Exploratory Data Analysis","","","1"
"1393","782034","4857635","07/17/2020 08:05:27","## Task Details
This is a practice dataset, a kind of a simulation where you perform extensive data analysis to deliver insights on how the company can increase its profits while minimizing the losses.","","Increase the Profits","Find out how you can increase the supermarket profits.","","3"
"1485","798723","4857935","07/25/2020 18:45:16","## Task Details
Experimental data used for binary classification (room occupancy) from Temperature,Humidity,Light and CO2
With the help of various EDA skills ,predcit the target variable occupancy:
1-For occupied status.
0-For not occupied
## Expected Submission
Users have to solve this task primarily using Notebooks .
The solution must contain a resultant dataframe consisting of actual and predicted values.

## Evaluation
A classification model with higher accuracy score.","","Predic the room occupancy status","","","3"
"1440","790444","4857935","07/21/2020 09:22:55","## Task Details
I came up with this dataset in order to know that how long a player can play based on the previous summary stats.
In this dataset there are 21 features describing the performance measures of each player or you can say the summary of each player.
Your task is to predict the target variable.
Target Variable:
1-Whether a player's career is equal to or greater than 5 years.
0-Career is shorter than 5 years

 Expected Submission
You have to  solve the task primarily using Notebooks 

Evaluation
 Use various Classification Algorithms to predict the target variable with higher accuracy score.","","Career Prediction","","","4"
"1492","763288","4857935","07/27/2020 06:55:46","## Task Details
As we all know that all political parties receive heavy funds from corporate firms .
In order to know more about their funds ,donators,mode of payment use various EDA skills","","Donations received by Indian Political Parties","","","1"
"1340","771783","4857935","07/12/2020 18:18:57","This Dataset contain 1 column with raw text ,you have to do some Preprocessing to extract the target Labels from the column.

The Labels are in the range 1 to 8.
The task is to predict the document with correct labels and improve the accuracy of the model
You can submit your highest accuracy score on the Dataset with the predicted labels.","","Document Classification","Classifying labels","","4"
"1118","708347","5274044","06/11/2020 17:07:05","## Task Details
Perform data visualization on the dataset","","Visualize Dataset","","06/12/2020 00:00:00","0"
"812","617646","4858774","04/23/2020 15:31:30","To use this data set and apply supervised learning algorithms. SVM, Random Forest and K-nearest.","","Artificial Character Data Set","","","0"
"1430","789655","4860182","07/20/2020 22:06:07","Are certain words or topics associated with more child posts than others?","","Does the title of a post influence User Engagement?","","","0"
"1510","804019","4860182","07/28/2020 18:09:23","Chipotle Restaurants aren't scattered across the US haphazardly - each location is picked for a specific reason. The metrics they look at are likely a concoction of different indicators like population, median income, property value, real estate cost, etc. Can you conduct a similar analysis to find Chipotle's next prime location?","","Identify New Potentially Viable Chipotle Locations","Can you identify the most likely location that could support a new Chipotle?","08/28/2020 00:00:00","6"
"1377","778310","4860182","07/15/2020 20:34:36","## Task Details
As of late, my GPS watch has been a bit on the fritz, and the heart rate reading will be totally wrong. I'm looking for a way to help identify outliers, so I can determine if I should trust the heart rate reading or not

## Expected Submission
For each run, determine how likely it is the heart rate monitor is working correctly. You don't have to do it this way, but perhaps assign a score of 0 to 1 to each run, where 0 means very likely to be accurate and 1 means very unlikely to be accurate.

## Evaluation
A good submission identifies the most likely runs that have incorrect data and thoroughly explains their methodoloy.","","Identify Heart Rate Monitor Issues","Identify where the heart rate data is most likely to be incorrect","","2"
"1613","819517","4860182","08/06/2020 17:32:12","Identify under and over serviced markets in the United States","","Shake Shack Geospatial Analysis","Location Based Analysis of Shake Shack Restaurants","","0"
"1535","807367","4860182","07/30/2020 15:15:49","Each Chopped Episode has a carefully selected basket of ingredients the contestants must include in their meals. The ingredients are picked purposefully so as not to make them too compatible nor too incompatible. Can you use NLP to automate this process?","","Use NLP to automate the ingredient selection process","Can you use NLP to create a basket of ingredients for each meal?","","1"
"1546","809416","4860182","07/31/2020 19:57:13","Part of the excitement of the show is that the chefs that challenge Bobby Flay often choose their signature dish that they've perfected over years and years, leaving Bobby Flay at a severe disadvantage. Despite this, Bobby Flay makes it out on top over 63% of the time! Can you figure out which dishes stand the best (and worst) chance at beating Bobby Flay?","","Find the most and least likely cuisine to beat Bobby Flay","Can you determine which type of cuisine has the best shot at beating Bobby Flay?","","0"
"869","644811","4881098","05/09/2020 18:11:01","## Task Details
	As the name suggests, the online store specializes in selling different 
        varieties of wines.
	The online store receives a decent amount of traffic and reviews from its 
        users.
	Leverage the ‚Äúreviews‚Äù data and draw actionable insights from it.

## Expected Submission
	Build a predictive model for predicting the wine ‚Äúvariety‚Äù. Provide the output along with all features to a CSV file. Both Training & test data is provided.

## Evaluation
       The Evaluation will be done maximum accuracy(avoid overfitting).","","Predictive Model","","","0"
"1746","836793","4886900","08/18/2020 19:52:25","keep exploing the data according to our summary.","","Explore it...","","","0"
"1213","742180","4888348","06/27/2020 20:18:24","Read the documentation and familiarize yourself with the dataset, then write some python code which returns a line graph of the record high and record low temperatures by day of the year over the period 2005-2014. The area between the record high and record low temperatures for each day should be shaded.","","Line plot for period 2005-2014","","","1"
"1214","742180","4888348","06/27/2020 20:20:16","1. Read the documentation and familiarize yourself with the dataset, then write some python code which returns a line graph of the record high and record low temperatures by day of the year over the period 2005-2014. The area between the record high and record low temperatures for each day should be shaded.
2. Overlay a scatter of the 2015 data for any points (highs and lows) for which the ten year record (2005-2014) record high or record low was broken in 2015.","","Overlay scatter plot on line plot","","","1"
"2163","880266","4892590","09/19/2020 08:30:55","## Task Details
Using Wikipedia's [**database**](https://en.wikipedia.org/wiki/Category:Lists_of_countries) , the user can look for a criterion of his interest, and process it from scraping until visualization.

### Further help
An example scraping process can be found [**here**](https://www.kaggle.com/daniboy370/tutorial-web-scraping)

An example visualization kernel can be found [**here**](https://www.kaggle.com/daniboy370/starter-world-data-by-country-2020/edit)","","Data extraction from Wikipedia","Choose a desired criterion and process it","","0"
"2265","895979","4892590","09/27/2020 14:07:11","## Task Details
The dataframe contains several columns that might shed light on the shake-up occurence. Therefore, the challenge will be to spot them and point them out.","","Can you predict shake-up ?","What are the principle factors that correlate to the shake-up phenomenon","","0"
"2230","891801","4895752","09/25/2020 08:46:15","![](https://www.usnews.com/dims4/USNEWS/d00168a/2147483647/thumbnail/640x420/quality/85/?url=http%3A%2F%2Fmedia.beam.usnews.com%2F58%2Fd4%2Ffdd33c2f4a2cb3e104c66e52a8df%2F200221-trump2-editorial.jpg)

## Task Details
Donald Trump prefers to reach voters by huge rallies that attract a big audience. It would be interesting to analyze the speeches he gives at the rallies and find patterns, repeated sentences and perhaps what sort of topics he brings up.

## Expected Submission
Make an NLP investigation of his rallies and find the most frequent words, themes, interesting phrases. Calculating the readability index and polarirty of his speeches could be great too.

## Evaluation
First submission that answers these basic NLP questions will be accepted.

### Further help
NLP with Spacy and TXT files: https://siddhantmaharana.github.io/blog/introduction-to-nlp-using-python-and-spacy/
Other great NLP notebooks: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification","","What does Trump talk about?","Main topics and areas of interest in 2019 and 2020","","10"
"2523","891801","4895752","10/23/2020 15:55:29","![](https://www.usnews.com/dims4/USNEWS/d00168a/2147483647/thumbnail/640x420/quality/85/?url=http%3A%2F%2Fmedia.beam.usnews.com%2F58%2Fd4%2Ffdd33c2f4a2cb3e104c66e52a8df%2F200221-trump2-editorial.jpg)

## Task Details
Donald Trump prefers to reach voters by huge rallies that attract a big audience. It would be interesting to know if his speeches are mostly positive or negative. Which things does he mention in a positive light? Which things does he talk negatively about?

## Expected Submission
Make a kernel with focus on sentiment analysis (positive/negative words). Figures should be easy to read with a large enough font. Make sure to include your reasoning in the kernel, why you're doing what you do and describe your results.

## Evaluation
First submission that answers the questions raised in the task details wins.

### Further help
NLP with Spacy and TXT files: https://siddhantmaharana.github.io/blog/introduction-to-nlp-using-python-and-spacy/
Other great NLP notebooks: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification","","Sentiment analysis of Donald Trump","Make a kernel with focus on sentiment analysis (positive/negative words)","","7"
"1488","800154","4895752","07/26/2020 16:24:35","## Task Details

![](https://www.maritime-executive.com/media/images/article/Photos/Wreckage_Salvage/Original/estonia-lifeboat.b733a3.jpg)

The death toll of the MS Estonia disaster is well above 80%. Not many people made it out of the ship in time. If we were to make a naive baseline classifier that simply assumed all passengers aboard the MS Estonia died, it would be right about 86% of the time. Can you make a more sophisticated classifier that beats the baseline?

See the baseline as a notebook: https://www.kaggle.com/christianlillelund/the-baseline-classifier-nobody-survives

## Expected Submission
989 rows of passenger information provided.

## Evaluation
Beating the baseline classifier.

### Further help
The Titanic data set is similar to this: https://www.kaggle.com/c/titanic","","Beating the baseline","A trained classifier vs the baseline","","6"
"1907","848968","4895752","08/31/2020 20:29:21","## Task Details
Use NLP tools to determine, if the speeches at the 2020 Democratic convention are negative or positive, generally speaking.

## Expected Submission
A solution that answers the question will be accepted.

## Evaluation
Good arguments and also plots are very welcome.

### Further help
If you need additional inspiration, look here:
https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4","","Sentiment Analysis / Polarity","Are the speeches positive or negative?","","0"
"1537","807673","4895752","07/30/2020 19:24:10","![](https://picresize.com/images/tscreen-shot-2018-09-07-at-30041-am.jpg)

## Task Details
The Joe Rogan Experience with Elon Musk is a famous interview, where a lot of things were said and debated. A few grams of weed also went up in smoke. It would be interesting to know, what kind of stuff they actually talked about.

## Expected Submission
Make a simple investigation into what was said and how the stuff relates.

## Evaluation
A good, profound investigation and some cool NLP will get my vote.

### Further help
Watch the interview here: https://www.youtube.com/watch?v=ycPr5-27vSI","","Make an investigation of the interview","What did they say?","","1"
"1713","833303","4895752","08/16/2020 11:12:48","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4895752%2F7edb14e7952ac7e4ddea9313f9180afe%2Fcsgo-breaks-record-for-highest-player-count-all-time.jpg?generation=1597649765621262&alt=media)

## Task Details
The data is a bulk of CS:GO snapshots of the current round state accompanied with the winner of that round. Try to make an EDA of the data and show various distributions, correlations and make outliers in the data. Also ask the question which features follows a Gaussian, Erlang or Power law distribution.

## Expected Submission
Some solid EDA that is easy to understand and view. Maybe have a go on new popular DataViz tools like Plotly.

## Evaluation
Be the first to examine all the features and how the attribute to whoever wins the round.

### Further help
Look at Counter-Strike's wiki page for more information about the game: https://en.wikipedia.org/wiki/Counter-Strike:_Global_Offensive","","EDA of the data","What characteristics describe the data?","","2"
"1818","833303","4895752","08/24/2020 18:30:56","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4895752%2F7edb14e7952ac7e4ddea9313f9180afe%2Fcsgo-breaks-record-for-highest-player-count-all-time.jpg?generation=1597649765621262&alt=media)

## Task Details
Ensemble learning is on the rise. Ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. It would be interesting to know whether a single or an assemble works best on this dataset.

## Expected Submission
Accuracy/ROC plots of our model's performance for comparison. Also consider including the precision and recall scores for your models.

## Evaluation
Your task is to implement, test and evaluate a classic single-model approach and one with multiple models (ensemble) to determine what works best on the data.

### Further help
Some useful tools to get started:

- Look at Xgboost/LightGDM/CatBoost models. They are all ensemble based.
- You can combine Sklearn models using soft voting with a simple VotingClassfier: https://scikitlearn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html
- For stacking, see: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html","","Single model vs ensemble","Should you build one or multiple models?","","2"
"1820","833303","4895752","08/24/2020 19:18:33","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4895752%2F7edb14e7952ac7e4ddea9313f9180afe%2Fcsgo-breaks-record-for-highest-player-count-all-time.jpg?generation=1597649765621262&alt=media)

## Task Details
CS:GO is a competitive first-person shooter that pits two teams against each other: the Terrorists and the Counter-Terrorists. Both sides are tasked with eliminating the other while also completing separate objectives. In game, player's have health bars what must never drop to zero, armor that make them more robust and money they earn every round to buy bigger and better guns. But which attribute is the most import for winning rounds?

## Expected Submission
Make a notebook on the dataset that explains the relationship between available features such as previous rounds won (score), team health, team armor and team guns and the probability of winning a round. Which have the strongest correlation? Also experiment with modelling and feature importance.

## Evaluation
A quantitative approach the compares features using a common metric and evaluates them separately either during training (best subset selection) or afterwards (feature importance). Perhaps a correlation plot is a good place to start.

### Further help
Perhaps a correlation plot is a good place to start?","","What makes you win?","Find the determining factors for winning rounds in CS:GO","","8"
"4194","833303","6985605","04/26/2021 22:51:48","accuracy : 81,56%","","Predicting Game Outcomes for CS:GO","(Optimizer)","04/28/2021 23:59:00","1"
"1110","706231","4910145","06/10/2020 17:11:20","One criticism of farmers markets are that they are inaccessible to Americans who live in certain parts of the country or are of low socio-economic status. Does the data reflect this criticism?","","How accessible are farmers markets?","","","6"
"1447","727236","4910500","07/21/2020 16:16:46","Predict the chances of resignation for employees","","Employee Resignation Prediction","","","1"
"1342","756433","4912080","07/12/2020 19:54:03","## Task Details
Hi Friends,

I have practiced Loan Prediction Problem.I wish to explore more from this dataset  to improve loan prediction accuracy using different methods.

## Expected Submission
I am reading kernels those who already worked for the problem.

## Evaluation
I want to understand more about evaluation metrics to improve the prediction accuracy.

### Further help

I welcome your support to learn more from this dataset.","","Feature Engineering","Feature Selection for improving prediction accuracy","07/31/2020 00:00:00","1"
"2019","823542","4930501","09/08/2020 18:25:01","Use predictive modeling to predict future cases","","Predict future confirmed cases, deaths and recoveries.","Use predictive modeling to predict future cases","","0"
"1551","809521","4930706","08/01/2020 10:27:35","## Task Details
First perform analysis using NLP in this dataset taking into account lags of about 7-10 days (from when spread occurs to the infection appears in the official case list).
Use other datasets (used in various notebooks in kaggle) to complement this and create a more informative model.

## Expected Submission
Notebook containing created model and results/performance obtained in any method deemed appropriate by user.

## Evaluation
This is more of an exploratory task and there is no common evaluation strategy. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)","","India covid-19 cases modelling with social network data","Model covid-19 cases taking into account this dataset","08/30/2020 00:00:00","1"
"910","655715","4932016","05/15/2020 13:25:12","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

Task Details Users can perform Eda technique visualize data in different plots Build statistical models parameters that can be predicted ??","","DATA ANALYSIS/VISUALIZATION","","","1"
"2216","890429","4940231","09/24/2020 08:43:08","Submit in time","","Submit best solution","","09/29/2020 23:59:00","1"
"1602","761595","5493065","08/05/2020 08:20:34","## Task Details
Tell a story from the data","","EDA,Visualization","","","0"
"1261","757303","4949147","07/05/2020 12:14:19","Find which year has the most release. Task for beginners","","Year which has maximum production","","","0"
"1056","694299","4954447","06/05/2020 04:33:07","## Task Details
Find the cutest cat named meow or name sounding similar to meow per town. Cuteness level is in the range between 1 to 10. 10 being most cutest. If two cats meow and cameow are both have the same cuteness, pick the cat named meow.

### Sample Dataset
| id | name    | town      | cuteness |
|----|---------|-----------|----------|
| 1  | meow    | meerut    | 9        |
| 2  | tom     | delhi     | 10       |
| 3  | cameow    | delhi     | 10       |
| 4  | meow    | delhi     | 4        |
| 5  | meow  | delhi     | 10       |
| 6  | meowses | meerut    | 4        |
| 7  | cameow  | bangalore | 8        |
| 8  | tiger   | bangalore | 3        |
| 9  | meowses | mumbai    | 7        |
| 10 | meow    | mumbai    | 9        |

## Expected Submission
| id | name    | town      | cuteness |
|----|---------|-----------|----------|
| 5  | meow    | delhi     | 10       |
| 1  | meow    | meerut    | 9        |
| 10 | meow    | mumbai    | 9        |
| 7  | cameow  | bangalore | 8        |


### Further help
Solution using elasticsearch: https://medium.com/activeai/finding-meow-the-cutest-cat-from-each-town-using-elastic-search-29a9417bc24d

Solution described here: https://medium.com/@sharmasha2nk/pandas-to-look-for-the-cutest-cat-per-town-fbaffc7363c9","","Find the cutest cat named meow (or sounding similar to meow) per town","","","0"
"1357","775253","4954907","07/14/2020 11:00:27","## Task Details
Hello!! I'm looking to create some interesting data visualizations and metrics around a set of about 200 stock market predictions. I have about 200 raw predictions and access to an API for historical data, and I'd like to make some compelling visualizations about the effectiveness. Would this be the type of project you'd be suited for? as an example this website does a similar display, https://www.aistockfinder.com/back-testing, Main goal is to specifically, would be nice to know average return in 1 day, 1 week, and 1-month segmentations in the graph/chart form. Looking forward to your thoughts! Thanks.
The historical stock prices to create metrics around the predictions can use the API

**The goal is to display potential returns by day, week and month.for each prediction**

So for a given prediction, I'd like to see how much potential gain in a day, week, or month. So traders who use those time-frames can understand the probability of success.

In Data set id(s) column is not useable you can drop this column.","","Data Visualization","Interesting data visualizations and metrics around a set","07/18/2020 00:00:00","3"
"1415","785855","4955095","07/18/2020 23:42:26","## Task Details

I used the following database to do sentiment analysis on Bitcoin. Feel free to use it as you prefer, and if possible share your results.","","Bitcoin Reddit Sentiment Analysis","","","0"
"936","664164","4960762","05/20/2020 03:34:13","## Task Details
Wrangle the datasets and help to analyze the clinical data to know how the cases are spreading and which factors contribute the most to spread of the cases. Any public data-set can also be used as a part of this task.

## Expected Submission
Final Submission Deadline : June 1, 2020
Evaluation would be on a rolling basis happening each day.

## Evaluation
Ease of implementation (5 marks)
Data Storytelling (2 marks)
Conclusive Results (3 marks)","","Understanding Clinical Trials","","05/31/2020 00:00:00","1"
"1063","664164","4960762","06/05/2020 16:34:15","## Task Details
Wrangle the datasets and help to analyze the clinical data to know how the cases are spreading and which factors contribute the most to spread of the cases. Analyze which factors in the clinical trials data seek a HCP Requirement. Any public data-set can also be used as a part of this task.

## Expected Submission
Final Submission Deadline : June 1, 2020
Evaluation would be on a rolling basis happening each day.

## Evaluation
Ease of implementation (5 marks)
Data Storytelling (2 marks)
Conclusive Results (3 marks)

### Further help
If you need additional inspiration, check out these existing high-quality datasets:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)","","Which parameters contribute to HCP Requirements?","Can we predict HCP requirements in COVID-19 on the basis of the clinical trials data?","06/06/2020 00:00:00","3"
"939","665218","4967265","05/20/2020 16:51:05","Use this data to analyse what factors are most imported in predicting case growth globally.","","COVID-19 Cases Growth","COVID-19 Cases Growth","05/21/2020 00:00:00","0"
"964","670134","4969123","05/25/2020 06:49:39","## Task Details
I was dealing with the fact that there are too many genres to fully understand, so a more generalized approach was needed: Creating super-genres out of genres! In this task, you are expected to understand how clustering works, how a prediction is made, and the evaluation methods.

## Expected Submission
In this task, the submission notebook should be clearly demonstrating:
- Which clustering approach did you choose
- Which hyperparameters did you tune (Optional)
- How accurate your prediction is (Silhouette score)
Additionally, it is a lot fancier if you publish your super-genre groups via a json file. An exemplary json file seems like this:

&gt; {
  1: [Genres type 1],
  2: [Genres type 2],
  ...
}

## Evaluation
There is a trade-off in this task: If the number of super-genres is too low, then the each group has too many genres, which is a problem. If the number of super-genres is nearly as high as genres, then there is no difference in using super-genres. Also, the silhouette score should be relatively high. (in between 0.5 and 1 is well appreciated)

### Further help
If you need additional inspiration, check out:
- How clustering works
- Which clustering algorithms are for better use in which case
- [Clustering using sklearn](http://scikit-learn.org/stable/modules/clustering.html) and [sklearn.cluster module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster)","","Differentiate genres","Cluster different genres according to audio features","","12"
"961","670134","4969123","05/24/2020 16:56:26","## Task Details
This task is an extension for my currently working genre-based recommender system. I am mostly looking for ways to extend the whole system by implementing other features. 

## Expected Submission
Hereby, you are expected to create an all-features-inclusive (genre: optional) content-based recommendation system which takes from user a list of artists with their own user-defined ratings (e.g. 0-10 or 0-100 scoring) and returns the top results (e.g. top 5 or top 10).  If you feel overwhelmed, build only a genre-based recommendation engine using linear algebra. At the end, you are supposed to submit your recommendation engine via a notebook. The complete list of tasks:

1. Decide on which features you want to include to your recommender system
2. Build your recommendation engine according to the expected criteria
3. Publish your kernel

## Evaluation
All engines returning reasonable results are well appreciated, there is no expected accuracy criteria. The only criteria is that you need to demonstrate your programming skills and your analytical thinking.

### Further help
If you need additional inspiration, check out these sources:
- [How a content-based recommender system works](https://youtu.be/n0QiBS6A7vM)
- Search for unsupervised machine learning algorithms if necessary
- See [docs](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/) for additional information on Spotify audio features","","Recommend artists","Build a content-based recommendation engine which suggests artists","","16"
"969","670134","4969123","05/25/2020 14:13:21","## Task Details
The data.csv file is the least biased towards year of releases, because it is created using year queries, each with the same size of data. You can perform time-series analysis with minimal effort and minimal bias.

## Expected Submission
In this task, you are expected to perform time-series analysis on ""data.csv"" data. You can work on visualizations or statistical analysis and even a forecast program for upcoming tracks in the next ten years. You can publish  your solutions via a notebook.  

### Further help
If you need additional inspiration, check out:
- How to perform time-series analysis, [a well-explained article](https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775)
- More on Spotify audio features, see [docs](https://developer.spotify.com/web-api/get-audio-features/)
- Visualizations on [matplotlib](https://matplotlib.org/)","","Perform time-series analysis","Analyze the trends of songs over the course of a century","","40"
"2777","670134","1493056","11/26/2020 01:36:43","## Task Details
Many artists gain recognition by featuring other artists who are already well known.  Explore different networks of artists (based off of their appearances together in songs) and determine who the most influential artists are in each of them (based off of popularity).","","Artist Networks","Find Influential Artists in Different Genres","","5"
"2173","670134","3418069","09/20/2020 17:53:00","On the basis of current data, try analyzing which features play the most important role in determining the popularity of the song. Divide the popularity into 10 categories. And determine the rating outcome based on the other features.","","Predict Popularity rating","Predict popularity of song based on other features","","17"
"1883","836054","4971832","08/29/2020 18:20:55","## Task Details
**People have the mindset of taking simple things so seriously like visiting a doctor for even milder symptoms of coronavirus. The problem here is that in this process they‚Äôre prone to the risk of contracting the disease. This is where our project comes in handy and even people who lack proper awareness regarding the pandemic can get benefitted by taking our survey. **
***This is for my project in Btech so your help and ideas are most welcome***

## Expected Submission
The best model with a robust solution and also a reliable model which can help predict the status (Deep Learning Solutions most welcome)

## Evaluation
As any model better prediction speed and more accuracy are appreciated.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
Materials Related to Classification, feature selection, and Deep Learning will help a lot.
## Task Details
People have the mindset of taking simple things so seriously like visiting a doctor for even milder symptoms of coronavirus. The problem here is that in this process they‚Äôre prone to the risk of contracting the disease. This is where our project comes in handy and even people who lack proper awareness regarding the pandemic can get benefitted by taking our survey. 

## Expected Submission
The best model with a robust solution and also a reliable model which can help predict the status (Deep Learning Solutions most welcome)

## Evaluation
As any model better prediction speed and more accuracy is appreciated.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
Materials Related to Classification, feature selection, and Deep Learning will help a lot.","","Better Solution","Converting to better robust model","09/21/2020 23:59:00","1"
"1922","836054","5241074","09/02/2020 13:24:46","## Task Details
I have historical information on data for more than 5 years (January 2015 to March 2020), with two columns ""programmed production"" and ""executed production"", each of these columns has information on the production of a bottle company every 30 min (48 records per day).
I wish to make the forecast of the ""executed production"" of the day (that is, to predict the consumption every 30 min throughout the day), having as input the information of the ""programmed production"" of the day. It is worth emphasizing that the values ‚Äã‚Äãof the ""programmed production"" is information that is obtained in the early hours of the day and the ""executed production"" the next day.

## Expected Submission
My first premise is to take this task as time series, and I want to implement a neural network with an output layer of 48 neurons, however, it is not yet clear to me how I should perform data preparation, and generate a sliding window to perform a ""Forecast of future production"".
I am using Tensorflow in Python so I would appreciate that these same tools are kept for its development.

## Evaluation
RMSE, validation set prediction, future data prediction

Dataset in: https://github.com/Jairo-cm/Produciton/","","Prediction of bottle production","","09/14/2020 23:59:00","1"
"828","628740","4992784","04/30/2020 11:03:24","## Task Details
You can select up to 20 words as features that should be used to predict the genre of the movie. We suggest to use 17/20 of the data as training set and the remaning  data for testing.","","Predict the movie genre from 20 words spoken within the movies","","05/13/2024 00:00:00","0"
"2128","875357","4995695","09/16/2020 19:13:18","Based on the 10 years of production data, predict next year production","","Predict annual production","","09/30/2020 23:59:00","1"
"2396","875357","4995695","10/10/2020 15:27:35","The dataset contains crop production information from 1995-2014. using the data determine which region does produce which crop best","","which region is good for which crop?","","","0"
"1604","805168","4996066","08/05/2020 15:09:57","Your task should be just to identify the mood of a person with its face image like sad, happy, excited, angry, etc.","","mood-recognition-","","","0"
"1578","785492","4589594","08/03/2020 09:05:12","## Task Details

Spark Analytics on COVID 19 DATASETS


/user/edureka_918210/Deb_DataSets/COVID19_DataSet
=================================================

[edureka_918210@ip-20-0-41-164 ~]$ hadoop fs -ls /user/edureka_918210/Deb_DataSets/COVID19_DataSet
Found 6 items
-rw-r--r--   3 edureka_918210 hadoop    1109964 2020-07-28 03:39 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/countries-aggregated.csv
-rw-r--r--   3 edureka_918210 hadoop      11006 2020-07-28 03:39 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/key-countries-pivoted.csv
-rw-r--r--   3 edureka_918210 hadoop     403028 2020-07-28 03:40 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/reference.csv
-rw-r--r--   3 edureka_918210 hadoop   70182395 2020-07-28 03:48 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/us_confirmed.csv
-rw-r--r--   3 edureka_918210 hadoop   73477670 2020-07-28 04:12 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/us_deaths.csv
-rw-r--r--   3 edureka_918210 hadoop       9566 2020-07-28 03:43 /user/edureka_918210/Deb_DataSets/COVID19_DataSet/worldwide-aggregated.csv



var myDF_COVID1=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/countries-aggregated.csv"").toDF(""DATE"",""COUNTRY"",""CONFIRMED"",""RECOVERED"",""DEATHS"")

var myDF_COVID2=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/key-countries-pivoted.csv"").toDF(""DATE"",""CHINA"",""US"",""UNITED_KINGDOM"",""ITALY"",""FRANCE"",""GERMANY"",""SPAIN"",""IRAN"")


var myDF_COVID3=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/reference.csv"").toDF(""UID"",""iso2"",""iso3"",""code3"",""FIPS"",""Admin2"",""Province_State"",""Country_Region"",""Lat"",""Long_"",""Combined_Key"",""Population"")


var myDF_COVID4=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/us_confirmed.csv"").toDF(""UID"",""iso2"",""iso3"",""code3"",""FIPS"",""Admin2"",""Lat"",""Combined_Key"",""Date"",""Case"",""Long"",""Country/Region"",""Province/State"")


var myDF_COVID5=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/us_deaths.csv"").toDF(""UID"",""iso2"",""iso3"",""code3"",""FIPS"",""Admin2"",""Lat"",""Combined_Key"",""Population"",""Date"",""Case"",""Long"",""Country/Region"",""Province/State"")


var myDF_COVID6=spark.read.format(""csv"").option(""delimiter"","","").option(""header"",""false"").option(""inferSchema"",""true"").load(""/user/edureka_918210/Deb_DataSets/COVID19_DataSet/worldwide-aggregated.csv"").toDF(""Date"",""Confirmed"",""Recovered"",""Deaths"",""Increase_rate"")



myDF_COVID1.show(false)
myDF_COVID2.show(false)
myDF_COVID3.show(false)
myDF_COVID4.show(false)
myDF_COVID5.show(false)
myDF_COVID6.show(false)




scala&gt; myDF_COVID1.show(false)
+----------+-------------------+---------+---------+------+
|DATE      |COUNTRY            |CONFIRMED|RECOVERED|DEATHS|
+----------+-------------------+---------+---------+------+
|Date      |Country            |Confirmed|Recovered|Deaths|
|2020-02-06|30802    |1487     |634   |11.427847918098614|
|2020-01-22|Afghanistan        |0        |0        |0     |
|2020-01-22|Albania            |0        |0        |0     |
|2020-01-22|Algeria            |0        |0        |0     |
|2020-01-22|Andorra            |0        |0        |0     |
|2020-01-22|Angola             |0        |0        |0     |
|2020-01-22|Antigua and Barbuda|0        |0        |0     |
|2020-01-22|Argentina          |0        |0        |0     |
|2020-01-22|Armenia            |0        |0        |0     |
|2020-01-22|Australia          |0        |0        |0     |
|2020-01-22|Austria            |0        |0        |0     |
|2020-01-22|Azerbaijan         |0        |0        |0     |
|2020-01-22|Bahamas            |0        |0        |0     |
|2020-01-22|Bahrain            |0        |0        |0     |
|2020-01-22|Bangladesh         |0        |0        |0     |
|2020-01-22|Barbados           |0        |0        |0     |
|2020-01-22|Belarus            |0        |0        |0     |
|2020-01-22|Belgium            |0        |0        |0     |
|2020-01-22|Belize             |0        |0        |0     |
|2020-01-22|Benin              |0        |0        |0     |
+----------+-------------------+---------+---------+------+
only showing top 20 rows


scala&gt; myDF_COVID2.show(false)
+----------+-----+---+--------------+-----+------+-------+-----+----+
|DATE      |CHINA|US |UNITED_KINGDOM|ITALY|FRANCE|GERMANY|SPAIN|IRAN|
+----------+-----+---+--------------+-----+------+-------+-----+----+
|Date      |China|US |United_Kingdom|Italy|France|Germany|Spain|Iran|
|2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-23|643  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-24|920  |2  |0             |0    |2     |0      |0    |0   |
|2020-01-25|1406 |2  |0             |0    |3     |0      |0    |0   |
|2020-01-26|2075 |5  |0             |0    |3     |0      |0    |0   |
|2020-01-27|2877 |5  |0             |0    |3     |1      |0    |0   |
|2020-01-28|5509 |5  |0             |0    |4     |4      |0    |0   |
|2020-01-29|6087 |5  |0             |0    |5     |4      |0    |0   |
|2020-01-30|8141 |5  |0             |0    |5     |4      |0    |0   |
|2020-01-31|9802 |7  |2             |2    |5     |5      |0    |0   |
|2020-02-01|11891|8  |2             |2    |6     |8      |1    |0   |
|2020-02-02|16630|8  |2             |2    |6     |10     |1    |0   |
|2020-02-03|19716|11 |8             |2    |6     |12     |1    |0   |
|2020-02-04|23707|11 |8             |2    |6     |12     |1    |0   |
|2020-02-05|27440|11 |9             |2    |6     |12     |1    |0   |
|2020-02-06|30587|11 |9             |2    |6     |12     |1    |0   |
|2020-02-07|34110|11 |9             |3    |6     |13     |1    |0   |
|2020-02-08|36814|11 |13            |3    |11    |13     |1    |0   |
|2020-02-09|39829|11 |14            |3    |11    |14     |2    |0   |
+----------+-----+---+--------------+-----+------+-------+-----+----+
only showing top 20 rows

scala&gt; myDF_COVID3.show(false)
+---+----+----+-----+----+------+--------------+-------------------+---------+----------+-------------------+----------+
|UID|iso2|iso3|code3|FIPS|Admin2|Province_State|Country_Region     |Lat      |Long_     |Combined_Key       |Population|
+---+----+----+-----+----+------+--------------+-------------------+---------+----------+-------------------+----------+
|UID|iso2|iso3|code3|FIPS|Admin2|Province_State|Country_Region     |Lat      |Long_     |Combined_Key       |Population|
|4  |AF  |AFG |4    |null|null  |null          |Afghanistan        |33.93911 |67.709953 |Afghanistan        |38928341  |
|8  |AL  |ALB |8    |null|null  |null          |Albania            |41.1533  |20.1683   |Albania            |2877800   |
|12 |DZ  |DZA |12   |null|null  |null          |Algeria            |28.0339  |1.6596    |Algeria            |43851043  |
|20 |AD  |AND |20   |null|null  |null          |Andorra            |42.5063  |1.5218    |Andorra            |77265     |
|24 |AO  |AGO |24   |null|null  |null          |Angola             |-11.2027 |17.8739   |Angola             |32866268  |
|28 |AG  |ATG |28   |null|null  |null          |Antigua and Barbuda|17.0608  |-61.7964  |Antigua and Barbuda|97928     |
|32 |AR  |ARG |32   |null|null  |null          |Argentina          |-38.4161 |-63.6167  |Argentina          |45195777  |
|51 |AM  |ARM |51   |null|null  |null          |Armenia            |40.0691  |45.0382   |Armenia            |2963234   |
|40 |AT  |AUT |40   |null|null  |null          |Austria            |47.5162  |14.5501   |Austria            |9006400   |
|31 |AZ  |AZE |31   |null|null  |null          |Azerbaijan         |40.1431  |47.5769   |Azerbaijan         |10139175  |
|44 |BS  |BHS |44   |null|null  |null          |Bahamas            |25.025885|-78.035889|Bahamas            |393248    |
|48 |BH  |BHR |48   |null|null  |null          |Bahrain            |26.0275  |50.55     |Bahrain            |1701583   |
|50 |BD  |BGD |50   |null|null  |null          |Bangladesh         |23.685   |90.3563   |Bangladesh         |164689383 |
|52 |BB  |BRB |52   |null|null  |null          |Barbados           |13.1939  |-59.5432  |Barbados           |287371    |
|112|BY  |BLR |112  |null|null  |null          |Belarus            |53.7098  |27.9534   |Belarus            |9449321   |
|56 |BE  |BEL |56   |null|null  |null          |Belgium            |50.8333  |4.469936  |Belgium            |11589616  |
|84 |BZ  |BLZ |84   |null|null  |null          |Belize             |17.1899  |-88.4976  |Belize             |397621    |
|204|BJ  |BEN |204  |null|null  |null          |Benin              |9.3077   |2.3158    |Benin              |12123198  |
|64 |BT  |BTN |64   |null|null  |null          |Bhutan             |27.5142  |90.4336   |Bhutan             |771612    |
+---+----+----+-----+----+------+--------------+-------------------+---------+----------+-------------------+----------+
only showing top 20 rows


scala&gt; myDF_COVID4.show(false)
|32 |AR  |ARG |32   |null|null  |null          |Argentina          |-38.4161 |-63.6167  |Argentina          |45195777  |
+---+----+----+-----+----+------+-------------------+------------------+----------+----+--------+--------------+--------------+
|UID|iso2|iso3|code3|FIPS|Admin2|Lat                |Combined_Key      |Date      |Case|Long    |Country/Region|Province/State|
+---+----+----+-----+----+------+-------------------+------------------+----------+----+--------+--------------+--------------+
|UID|iso2|iso3|code3|FIPS|Admin2|Lat                |Combined_Key      |Date      |Case|Long    |Country/Region|Province/State|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-22|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-23|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-24|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-25|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-26|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-27|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-28|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-29|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-30|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-01-31|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-01|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-02|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-03|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-04|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-05|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-06|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-07|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-08|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|2020-02-09|0   |-170.132|US            |American Samoa|
+---+----+----+-----+----+------+-------------------+------------------+----------+----+--------+--------------+--------------+
only showing top 20 rows




scala&gt; myDF_COVID5.show(false)
+---+----+----+-----+----+------+-------------------+------------------+----------+----------+----+--------+--------------+--------------+
|UID|iso2|iso3|code3|FIPS|Admin2|Lat                |Combined_Key      |Population|Date      |Case|Long    |Country/Region|Province/State|
+---+----+----+-----+----+------+-------------------+------------------+----------+----------+----+--------+--------------+--------------+
|UID|iso2|iso3|code3|FIPS|Admin2|Lat                |Combined_Key      |Population|Date      |Case|Long    |Country/Region|Province/State|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-22|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-23|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-24|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-25|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-26|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-27|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-28|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-29|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-30|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-01-31|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-01|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-02|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-03|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-04|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-05|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-06|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-07|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-08|0   |-170.132|US            |American Samoa|
|16 |AS  |ASM |16   |60.0|null  |-14.270999999999999|American Samoa, US|55641     |2020-02-09|0   |-170.132|US            |American Samoa|
+---+----+----+-----+----+------+-------------------+------------------+----------+----------+----+--------+--------------+--------------+
only showing top 20 rows


scala&gt; myDF_COVID6.show(false)


+----------+---------+---------+------+------------------+
|Date      |Confirmed|Recovered|Deaths|Increase_rate     |
+----------+---------+---------+------+------------------+
|Date      |Confirmed|Recovered|Deaths|Increase rate     |
|2020-01-22|555      |28       |17    |null              |
|2020-01-23|654      |30       |18    |17.83783783783784 |
|2020-01-24|941      |36       |26    |43.883792048929664|
|2020-01-25|1434     |39       |42    |52.39107332624867 |
|2020-01-26|2118     |52       |56    |47.69874476987448 |
|2020-01-27|2927     |61       |82    |38.196411709159584|
|2020-01-28|5578     |107      |131   |90.57055005124701 |
|2020-01-29|6166     |126      |133   |10.541412692721405|
|2020-01-30|8234     |143      |171   |33.53876094712942 |
|2020-01-31|9927     |222      |213   |20.5610881709983  |
|2020-02-01|12038    |284      |259   |21.2652362244384  |
|2020-02-02|16787    |472      |362   |39.45007476324971 |
|2020-02-03|19887    |623      |426   |18.466670637993683|
|2020-02-04|23898    |852      |492   |20.16895459345301 |
|2020-02-05|27643    |1124     |564   |15.670767428236672|
|2020-02-06|30802    |1487     |634   |11.427847918098614|
|2020-02-07|34395    |2011     |719   |11.664826959288357|
|2020-02-08|37129    |2616     |806   |7.948829771769153 |
|2020-02-09|40159    |3244     |906   |8.160736890301381 |
+----------+---------+---------+------+------------------+
only showing top 20 rows

===============================================================================================================================================================================

myDF_COVID1.show(false)
myDF_COVID2.show(false)
myDF_COVID3.show(false)
myDF_COVID4.show(false)
myDF_COVID5.show(false)
myDF_COVID6.show(false)

myDF_COVID1.select(""DATE"").intersect(myDF_COVID2.select(""DATE"")).show

myDF_COVID1.join(myDF_COVID2,myDF_COVID1.col(""DATE"")===myDF_COVID2.col(""DATE"")).show(false)

+----------+-------------------+---------+---------+------+----------+-----+---+--------------+-----+------+-------+-----+----+
|DATE      |COUNTRY            |CONFIRMED|RECOVERED|DEATHS|DATE      |CHINA|US |UNITED_KINGDOM|ITALY|FRANCE|GERMANY|SPAIN|IRAN|
+----------+-------------------+---------+---------+------+----------+-----+---+--------------+-----+------+-------+-----+----+
|Date      |Country            |Confirmed|Recovered|Deaths|Date      |China|US |United_Kingdom|Italy|France|Germany|Spain|Iran|
|2020-01-22|Afghanistan        |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Albania            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Algeria            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Andorra            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Angola             |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Antigua and Barbuda|0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Argentina          |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Armenia            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Australia          |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Austria            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Azerbaijan         |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Bahamas            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Bahrain            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Bangladesh         |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Barbados           |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Belarus            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Belgium            |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Belize             |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
|2020-01-22|Benin              |0        |0        |0     |2020-01-22|548  |1  |0             |0    |0     |0      |0    |0   |
+----------+-------------------+---------+---------+------+----------+-----+---+--------------+-----+------+-------+-----+----+
only showing top 20 rows


========================================================================================================

myDF_COVID1.join(myDF_COVID6,myDF_COVID1.col(""DATE"")===myDF_COVID6.col(""DATE"")).show(false)


========================================================================================================

myDF_COVID1.createOrReplaceTempView(""countriesaggregated"")
myDF_COVID2.createOrReplaceTempView(""keycountriespivoted"")
myDF_COVID3.createOrReplaceTempView(""reference"")
myDF_COVID4.createOrReplaceTempView(""usconfirmed"")
myDF_COVID5.createOrReplaceTempView(""usdeaths"")
myDF_COVID6.createOrReplaceTempView(""worldwideaggregated"")


spark.sql(""select * from countriesaggregated"").show
spark.sql(""select * from keycountriespivoted"").show
spark.sql(""select * from reference"").show
spark.sql(""select * from usconfirmed"").show
spark.sql(""select * from usdeaths"").show
spark.sql(""select * from countriesaggregated"").show

========================================================================================================
spark.sql(""select avg(Lat),avg(Long_) from reference group by iso2"")show

+------------------------+--------------------------+
|avg(CAST(Lat AS DOUBLE))|avg(CAST(Long_ AS DOUBLE))| ============================== due to inferschema
+------------------------+--------------------------+
|                 28.0339|                    1.6596|
|                 21.9162|                    95.956|
|                 55.1694|                   23.8813|
|                    7.54|                   -5.5471|
|                  21.694|                  -71.7979|
|                 40.1431|                   47.5769|
|                61.92411|                 25.748151|
|                 46.8852|                  -56.3159|
|                 -4.6796|                    55.492|
|       48.82878571428571|        30.794999999999998|
|                 45.9432|                   24.9668|
|              -13.133897|                 27.849332|
|                8.460555|                -11.779889|
|                19.85627|                102.495496|
|        52.2484443076923|         5.528057230769231|
|               25.025885|                -78.035889|
|                -22.3285|                   24.6849|
|                 46.8625|                  103.8467|
|                 40.0691|                   45.0382|
|                -21.1151|                   55.5364|
+------------------------+--------------------------+

scala&gt; myDF_COVID3.printSchema
root
 |-- UID: string (nullable = true)
 |-- iso2: string (nullable = true)
 |-- iso3: string (nullable = true)
 |-- code3: string (nullable = true)
 |-- FIPS: string (nullable = true)
 |-- Admin2: string (nullable = true)
 |-- Province_State: string (nullable = true)
 |-- Country_Region: string (nullable = true)
 |-- Lat: string (nullable = true)       ====================================== Inferschema
 |-- Long_: string (nullable = true)
 |-- Combined_Key: string (nullable = true)
 |-- Population: string (nullable = true)","","Covid 19 Spark Work","Covid 19 Spark Work","08/04/2020 00:00:00","6"
"1736","835975","5008581","08/18/2020 12:54:08","Use any necessary tool/library/framework necessary to plot the spatial data of shipwrecks that are located across the English waters.","","Geospatial Analysis using Folium, Geopandas, Plotly, BaseMap.","Use different libraries and mapping tools to plot the shipwrecks.","","1"
"1580","814354","5008581","08/03/2020 18:56:25","## Task Details
Perform EDA and plot spatial data using any package that you find useful.","","Geospatial / Exploratory Data Analysis","Dataset contains spatial data of 1121 World Heritage Sites.","","0"
"1649","823781","5008581","08/10/2020 04:18:42","## Task Details
Plot the sites over a map and differentiate them by Country and Region/Province.

## Expected Submission
Apply Python libraries like Matplotlib, Folium, Plotly, Geopandas to plot these sites over a map.","","Geospatial Analysis","","","0"
"3105","836669","5011373","01/02/2021 17:47:35","## Task Details
Perform some advanced/detailed exploratory data analysis on different teams of Premier League.","","Detailed Analysis on Teams","","","0"
"1166","723680","5011420","06/18/2020 04:27:29","use the best approach for prediction .
and discuss with others.
minimise the loss.
the minimum loss model will get upvoted all the notebooks.","","prdict the price of diamond","","","1"
"1791","841208","5012159","08/22/2020 14:33:09","Perform exploratory data analysis (EDA) on this data set.","","Exploratory Data Analysis (EDA)","","01/01/2022 23:59:00","0"
"1583","813760","5074082","08/04/2020 10:39:44","## Task Details
Train a Machine LEarning model to recognize an image and find out whether it shows rock, paper or scissors.

## Expected Submission
The solution should be done based on the training and validation folders. Then it must be tested on the whole dataset to check.

## Evaluation
User Votes","","Detect whether an image shows rock, paper or scissors","","","0"
"1575","813262","5026355","08/03/2020 06:38:21","## Task Details
Predict whether a person having Breast Cancer: Benign or Malignant.

Dataset Information:-

1. Sample code number: id number
2. Clump Thickness: 1 - 10
3. Uniformity of Cell Size: 1 - 10
4. Uniformity of Cell Shape: 1 - 10
5. Marginal Adhesion: 1 - 10
6. Single Epithelial Cell Size: 1 - 10
7. Bare Nuclei: 1 - 10
8. Bland Chromatin: 1 - 10
9. Normal Nucleoli: 1 - 10
10. Mitoses: 1 - 10
11. Class: (2 for benign, 4 for malignant)


## Expected Submission
Submit your solutions with better accuracy and visualization.

## Evaluation
The evaluation will be based on finding out the relationship among the features with the dependent variable through visualization and the overall accuracy.","","Breast Cancer Classification: Benign or Malignant","","","3"
"1813","821288","5027739","08/24/2020 12:43:55","Try to get a ""val_acc"" of 0.60 or even more for this dataset.","","Get a Validation Accuracy of 0.60+","","","0"
"1129","711568","5029591","06/13/2020 01:25:23","Exploratory analysis of data, and comparison study using various existing predominant supervised and unsupervised learning techniques.","","Exploratory Analysis","Shared Task DSC OMG","","0"
"1130","711568","5029591","06/13/2020 01:28:21","Register for test data [link](https://forms.gle/SwDTbcMnPAv1FQWu9)
- F1-score for each class
- Micro Averaged F1 scores","","Automatic classification of tweets reporting abuse","Shared Task DSC OMG","","0"
"1787","834948","4760409","08/22/2020 03:41:09","## Task Details
As a product owner, I want a solution which can help business to better predict about leads using different machine algorithms so as my business team can work intelligently . 

## Expected Submission
A model which has balance of Accuracy , Sensitivity &  Specificity for testdata  as well as train data 

## Evaluation
A solution which  has balance of Accuracy , Sensitivity &  Specificity for testdata as well as train data  will be considered as an accepted solution. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Suggest a model who can predict the lead with  great metric scores","Accuracy , Sensitivity &  Specificity is more focused metric for the evaluation","12/31/2032 23:59:00","2"
"1544","809158","5035900","07/31/2020 16:19:04","## Task Details
Is it possible to accurately predict the outcome of a match from the first 15 minutes?","","Can you predict match outcome?","","","1"
"1545","809158","5035900","07/31/2020 16:21:09","## Task Details
Given a model that performs well on the given data, how well does it perform on other ELOs or regions?

For example, would we still have the same classification accuracy if we tried to predict Gold matches or matches from a different region?

This is more of an exploratory task and would involve the scraping of extra data.","","Given a trained model, how well does it predict outcomes for other ELOs?","","","1"
"5197","908128","4656934","07/18/2021 14:37:12","Predict Gender","","Predict Gender","","","1"
"5198","908128","4656934","07/18/2021 14:38:36","Insights of male and female gender.","","EDA on genders","","","1"
"1411","784323","5039341","07/18/2020 06:25:46","## Task Details
Predication of bike rental count hourly or daily based on the environmental and seasonal settings.



### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Regression","","","1"
"1412","784323","5039341","07/18/2020 06:37:38","## Task Details
Count of rented bikes are also correlated to some events in the town which easily are traceable via search engines.
		For instance, query like ""2012-10-30 washington d.c."" in Google returns related results to Hurricane Sandy. Some of the important events are 
		identified in [1]. Therefore the data can be used for validation of anomaly or event detection algorithms as well.



### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Event and Anomaly Detection","","","0"
"1042","691771","5043171","06/03/2020 19:35:17","This dataset can be used in conjuncture with a Job Descriptions dataset to match different jobs with their industries using NLP. I'm not entirely sure how to implement this just yet, so it'll be interesting to see how others do it!","","Linking industries and Job Descriptions","","","0"
"2314","904226","5045952","10/03/2020 10:00:09","Based on the features predict the risk of diabetes and get upvote from the community.","","Prediction","","","4"
"2315","904226","5045952","10/03/2020 10:03:45","Does the age group and gender define the risk of diabetes? If yes, which age group and gender are at high risk?

Submissions receive upvote from the community.","","Age group and Gender","","","0"
"3707","843771","5047229","03/09/2021 07:37:43","Find out the number of runs scored by each team and what's the contribution of 6s, 4s and double and single runs. Also try to extra runs in the inning with respect to the total runs scored.","","Analyze runs","study the runs pattern","","0"
"3708","843771","5047229","03/09/2021 07:40:27","Try to rank the each player based on their contribution in batting , bowling and/or fielding. Based on that select the player for Man of The match, Best bowler and best batsman awards.","","Find Man of the Match Player","rank each player based on their contribution in match","","0"
"2839","860282","5049993","12/02/2020 15:11:18","## Task Details
so for example 
- check which products have the highest reviews and ratings.
- which category of products got highest reviews and ratings.  
- check if product price had any impact on the ratings and reviews.

something like that...

## Expected Submission
it should contain different products market trends based on complete Exploratory data analysis and you can do predictions if you want to.

## Evaluation
the one who explores better can always find hidden trends better.","","Perfrom Exploratory data analysis on the amazon best seller data","find potential market in this vast data, good luck!","12/01/2022 23:59:00","0"
"908","654841","5051721","05/15/2020 03:58:40","## Task Details
Use the recall of the out-of-scope test samples to hone in on the performance of this subset of the data.

## Expected Submission
Update `oos_test.json` to include `predicted_intent`. From this, the recall can be computed.

## Evaluation
Out-of-scope recall is #(out-of-scope samples predicted as out-of-scope) / #(out-of-scope samples).","","Out-of-Scope Recall","Measure the recall of out-of-scope test samples","","0"
"1711","832967","5055505","08/16/2020 07:48:54","According to the dataset we have two csv file and that are total_death.csv and total_cases.csv .
Total cases csv file contain each and every country corona cases from 31dec-2019 to 10thaug-2020  .
How many cases are increasing every day you can find in total_cases.csv 
Similarly in total_deaths.csv you can find  number of deaths  case per day 
Your new task is to predict how much cases should increase by the end of this year for both your country and world
How much death rate should be by Dec2020
Check out and up vote now .
THANKS","","Check out when most  covid-19 cases increasing in your country or world","Prefect Project for beginners  . Apply MachineLearning algorithm to check how much cases should increase in future","12/31/2020 00:00:00","1"
"2052","868815","5055505","09/11/2020 15:27:54","Almost all information and analysis have been done by me .If any issues or more analysis you can do than create a new notebook and submit it .","","What more Data analysis you can do with Netflix?","Can you find 50+ more different analysis for this data?","09/11/2021 23:59:00","1"
"1785","834501","5058090","08/21/2020 13:33:08","I will be uploading the data for most Nasdaq-100 companies after extracting features from technical indicators. This will be a good collection to build a pipeline to predict future stock prices on multiple stocks for different forecast horizons. 


I have created a demo pipeline for training multiple models on multiple stocks, which can be trained online on any stock of your choice. It is entirely built-in python. 
Please check out the web app below for your reference -

https://stock-prediction-dashboard.herokuapp.com/

Keep on building upon this, as the sky is the limit when it comes to Stocks & Portfolio Optimization. Would love to see some great demos even better than the one I have built.

Try to use different target variables (close price, High price, close adj price, etc) and also different forecast horizons like the price after 5 days, 10 days","","Predict future Stock Price","Using the features in the dataset, try to build Machine learning/Deep learning based models to forecast future stock price for the next day","08/21/2021 23:59:00","0"
"1769","819352","5058624","08/20/2020 16:31:35","## Task Details
The data shows the results of the two exams and the result in 1 and 0, telling whether or not the student was admitted.

## Expected Submission
You are expected to plot a decision boundary and determine whether a student, with the given marks for each test, gets admitted or not.

## Evaluation
The precision of the decision boundary is to be increased.","","Decision Boundary","","","0"
"1626","816329","5073587","08/07/2020 18:23:13","## Task Details
the task is to predict high and low for the year 2020","","stock forcasting for high low prise","forcasting","01/07/2021 00:00:00","0"
"3001","808212","5074078","12/21/2020 17:35:01","## Task Details

Calligraphy is considered as one of the four ancient Chinese arts and contributes to the development of many forms of art in China. There are multiple styles of calligraphy, which mainly belong to different dynasties. Each of them has its way of shaping and arranging the character.

In this dataset, there are 4 Chinese calligraphy styles (zhuanshu, caoshu, lishu, kaishu). Your task is to build a model that is able to classify these different calligraphy styles.

### Further help
If you need additional inspiration, check out my personal attempt and tutorial:
- https://github.com/richardcsuwandi/chinese-calligraphy-classifier","","Classify Chinese calligraphy styles","","","0"
"1104","665245","5082253","06/10/2020 10:27:06","## Task Details
The given dataset has different features of various wood slabs. To make a model that can predict correctly which slab belongs to a particular wood kind.

## Expected Submission
Use the best and optimized algorithm suitable for the dataset to figure out the wood slabs.

## Evaluation
- Good Documentation
- Good Analysis
- Model Performance","","Classification of Wood Slabs based on the given parameters.","","06/11/2020 00:00:00","1"
"3744","770134","6846631","03/12/2021 14:26:34","1. Provide numerical and graphical summaries of the data set and make any initial comments
that you deem appropriate. [10 marks]
2. Divide houses based on their overall condition (OverallCond) as follows:
¬à Poor if the overall condition is between 1 to 3.
¬à Average if the overall condition is between 4 and 6.
¬à Good if the overall condition is between 7 and 10.
(a) Fit a logistic regression model which predicts the overall condition (OverallCond) of a
house. [10 marks]
(b) Carry out a similar study using a dierent classication method you learned in MA321 to
classify the house condition. [10 marks]
3. Predicting house prices:
(a) Employ two methods you learned in MA321 in order to predict house prices. Justify your
choice of model and comment on the results you obtained. [10 marks]
1 of 4
(b) Use two re-sampling methods of your choice to estimate the test error associated with
tting these particular classiers on a set of observations. Comment on the results you
obtained. [10 marks]
4. Using this data set, house-data.csv, what else would be interesting in investigating? Identify
a `research question' in relation to housing data and employ methodology you have learned in MA321 to answer this question.","","Task for coding","","03/14/2021 23:59:00","0"
"956","668851","5099053","05/23/2020 21:05:10","## Task Details
Is there a correlation between how early in the game you encounter a pokemon and how quickly it hatches? For example, does a pokemon found on route 1 typically have a lower step count for hatching than a pokemon you encounter in the postgame? Are there any outliers?

just a thought I had...

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Correlation Between How Quickly an Egg is Hatched and How Early in the Game You Encounter It","","","0"
"926","659455","5109920","05/18/2020 14:26:54","## Task Details
Classify all the images of cats and dogs using a machine learning algorithm of your choice.

## Expected Submission
Submit the notebook and the solution should have an accuracy metrics.

## Evaluation
On the basis of its accuracy your solution will be evaluated","","Classification of Dogs and Cats","Only for beginners","","4"
"1015","684809","5113196","05/31/2020 05:54:00","## Task Details
Build an Image Classification using the dataset.
This data is compatible with MNIST containing 60,000 monochrome 28x28 pixel images and their corresponding labels (from 0-12) in 1-hot encoding.
[u""‡ÆÖ"",u""‡ÆÜ"",u""‡Æá"",u""‡Æà"",u""‡Æâ"",u""‡Æä"",u""‡Æé"",u""‡Æè"",u""‡Æê"",u""‡Æí"",u""‡Æì"",u""‡Æî"",u""‡ÆÉ""]

## Expected Submission
A DNN TensorFlow/ONNX model or TensorFlow Python script.

## Evaluation
Good solutions use smallest parameter sized DNN in TensorFlow/Keras/PyTorch etc.

### Further help
Ref: https://github.com/Ezhil-Language-Foundation/acchu-tamilocr-dataset
Ref: https://tensorflow.org","","Classification","Identify classes 0-12 for each image in dataset.","","1"
"1018","685906","5113196","05/31/2020 17:15:42","## Task Details
You have two distinct Tamil word datasets: strict Tamil words vs loan words from English transliterated into Tamil script using Azhagi, Combinational and Jaffna encoding. Can you build a classifier to tell one from the other ?

## Expected Submission
Try to build a classifier based on TensorFlow or SciKit learn. Use 1-hot encoding for the two classes and train a simple model. Do you think a linear regression model would work? Why or why not ?

This dataset was used to build a Sci-Kit Learn based Multi-linear regression model with custom features of to achieve a 95% precision and 95% recall; for details see [1]. Can you do better with a simple Fully Connected-NN or a RNN/LSTM ?

## Evaluation
Can your model beat 95% precision and 95% recall ?

### References
[1] S. Abu Thahir, et-al ""Growth and Evolution of Open-Tamil,"" Tamil Internet Conference, Coimbatore, India. (2018).
[2] TensorFlow binary classification with Sigmoid : https://www.kaggle.com/realshijjang/tensorflow-binary-classification-with-sigmoid
[3] Open-Tamil https://pypi.org/project/Open-Tamil/","","Find out if a Tamil word is strictly Tamil or transliterated form English!","Build a simple binary classifier for this task","","0"
"1019","686214","5116483","05/31/2020 22:39:43","## Task Details
Use the COVID19 data to create a bar chart race

## Expected Submission
Post a file containing the bar chart race.

## Evaluation
Sort the cases in descending order. Differentiate countries by colors. Show the numbers near the bar. Auto adjust the scale as number grows.","","COVID -19 Bar Chart Race","","","0"
"927","661247","5116483","05/18/2020 14:36:52","Try to answer the following questions.

01. How many matches were played in EPL between 2009 - 2019?
      How many teams played in the EPL during these 10 seasons?
      Which are they?
02. How many games were played by each team in these 10 seasons?
03. Which teams played all the seasons in EPL from 2009 - 2019?
      Which teams played only 1 season in EPL between 2009 - 2019?
04. How much percent of the total matches were won by the home team, away team or draw?
05. How many referees officiated the EPL matches between 2009 and 2019?
      List them with the total number of matches officiated.
06. In how many games did the teams loosing at half time come back to win the game at full time (comeback wins)? List all of them.
07. Sort the comeback wins based on year.
08. Sort the comeback wins based on season.
09. Generate a data frame with the following info,
    a. Number of games per team.
    b. Home wins per team.
    c. Home defeats per team.
    d. Away wins per team.
    e. Away defeats per team.
    f. Number of draw per teams (home and away combined).
    g. Total points in EPL across 10 seasons.
10. Extract the total points gained by teams who played in all 10 seasons.
11. Which game/games produced most number of goals? List them.
12. Find out the total goals scored by each team at home, away and in total.
      Find the average goals scored per game at home and away (not combined).
13. Find the total number of dominant performances by each team. (Winning by a goal margin of 3 or more goals at Full Time).
14. Find the total points collected by each team per season. Extract the points table of the teams who played all 10 seasons.
15. Find the total shots on goal (shots and shots on target separately) made by each team.
16. Extract the shots and goals stats of the teams who played all 10 seasons.
17. Find the conversion rates (goals/shots) for the teams which played all 10 seasons.
18. Find the average goals conceded @home and @away for all the teams.
19. Find the total yellow and red cards given to home and away teams.
20. Find the total yellow and red cards given to each team home and away.
21. Find the total number of yellow and red cards awarded by each referee.
22. Find the total number of fouls committed by each team @ home and away.
23. Find the yellow card per foul in % and red card per foul in % for every team.
24. Find the corners gained/conceded during home and away games for each team.
      Also find the total corners gained and conceded during 2009 - 2019.
25. Find the EPL champions of each year with their points.
      If 2 teams have equal points in a season, find the champion by goal difference. (Total Goals Scored - Total Goals Conceded)
      Find how many teams became EPL champions during 2009 - 2019. List their names.
      Find the team which won the EPL more times between 2009 - 2019.","","EPL Data Analysis - Seasons 2009 - 2019","","","0"
"1287","763497","5130089","07/08/2020 11:08:35","Implement  Exploratory Data Analysis to bring the best insight of cases!","","Exploratory Data Analysis","","","0"
"959","666966","5130806","05/24/2020 04:55:51","# Task Details'

Identify the following cases using data analysis techniques.

- a --&gt; most active retweeter (userid) (1 points)
- b --&gt; most popular tweet --&gt; return tweet id (1 points)
- c --&gt; who posted more original tweets (using the tweets dataset) (2 points)
- d --&gt; most mentioned author (userid) by the most active account posting either original posts or retweets (3 points)
- e --&gt; identify the hashtag that received the most engagement (number of mentions) (3 points)
- f --&gt; most commonly reported location by the top 10 most active accounts posting either original posts or retweets (5 points)


### Kernel's creation

**Kernel's last time run will be considered. All outputs will be sorted by run time. Those submitted first will have a more probability to win.**","","DFRLab and DataFest Tbilisi 2020","Challenge for data analysis using python.","","1"
"1431","787368","5135796","07/20/2020 22:41:17","## Task Details
Death rates all over have been increased due to covid 19.Average of total death rates is the highest on the basis of which countries?Explain it creating pivot table on the basis of having given data sheet and making any pie catagories chart based on pivot table .

## Expected Submission
Through making pie chart based on Pivot Table,it will be very easy to know average of total deaths countries based .
## Evaluation
I think, my submission process that i have noted above is the good solution

### Further help

No need","","Statistics of culpable covid 19","Monstrous virus","07/21/2020 00:00:00","0"
"1799","841836","5139275","08/23/2020 06:04:32","## Task Details
Perform web scraping to collect the information of properties for sale.

## Expected Submission
Your submission should contain various features of the properties that are useful for data analysis and machine learning tasks later on.","","Scape information of the properties from the URLs.","","","1"
"1850","847547","5139275","08/27/2020 15:51:23","Predict the sentiment (positive or negative) of movie reviews in the IMDB dataset.","","Positive or Negative Review","","","1"
"1442","789461","5144861","07/21/2020 10:59:05","# Task Details
- You Can Visualize these things written below:- 
1. Top 10 buys in IPL history
1. Top 5 buys from Year 2014 to 2020
1. Most played 10 Venues
1. Top 10 Run Scorer in IPL history
1. Most six hitters in IPL history
1. Most four hitters in IPL history
1. Best batting strike raters in IPL history
1. Top 10 Wicket takers in IPL history
1. Best bowling average in IPL history
1. Best bowling economy in IPL history
1. Best bowling strike rate in IPL history
1. Team vs Teams data by bar plot
1. Batting 1st and Batting 2nd win percentage for top 8 stadiums
1. All time win percentage in IPL history on basis of batting first and batting second
1. All time percentage in IPL history of what teams did after wining toss

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

# Further help
If you need additional inspiration, check out these existing high-quality tasks:","","Visualize This data","","","1"
"1870","849509","5146506","08/29/2020 08:40:59","You are working for a company ABC which has decided to automate the attendance capturing system using only the picture of the employee (selfie). However, ABC company only has one or two passport size photographs of their employees in the system for reference.
Your task is to design this image based attendance capturing system using any
machine learning or computer vision approach. The system has to match an employee's selfie with the passport size photograph and output either its a match or no match.
Make sure to include your own selfies and passport size picture in the training data. Note that the images with 'script' in their name are the passport size images images or reference images which are present in the company's system.
## Deliverables:
1. Ipython notebook with all the analysis and training code
2. The trained model file
3. Python script that takes 2 images as input arguments (one selfie, one passport
size picture) and gives the output as match or no match along with the
confidence score
4. Submit all other files necessary for executing the above script
## Evaluation metric:
Your solution would be evaluated on the hidden test data in terms of;
1. Accuracy for positive matches, negative matches and overall performance
(weightage 70%)
2. Code quality (10%)
3. Processing time (10%)
4. Any innovative approach (10%)","","Face Match","","","1"
"999","681474","5148479","05/29/2020 07:23:08","## Task Details
Find the Weekdays PM Peak Traffic Volume from the Road Traffic Detector Data(SCATS)

## Expected Submission
PM Peak Traffic Volume","","Find the PM Peak Traffic Volume according to Road Traffic Detector Data(SCATS)","","","0"
"2676","888092","5153834","11/11/2020 14:59:21","## Task Details
- Balance The Dataset ( Healthy  v/s Diseased )
- Model To Predict The Diseases

## Expected Submission
Notebook with documentation of your steps of procedure and results.


## Evaluation
Model Results and Documentation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:

- Part-1 ( Preprocessing ) -&gt; [https://www.kaggle.com/shivam316/part-1-preprocessing]

- Part-2 (Class Imbalance and Spectrogams ) -&gt; [https://www.kaggle.com/shivam316/part-2-handel-imbalance-creating-spectrogram]

- Part-3 (Feature Extraction & Modeling) -&gt; [https://www.kaggle.com/shivam316/part-3-feature-extraction-modeling-95-acc]","","Respiratory Disease Detection","","","2"
"2215","890325","5155967","09/24/2020 00:45:40","## Task Details
This dataset is filled with images of clouds taken from the ground. The training dataset has folders in it specifing the labels of the clound in which you can classify.

## Expected Submission
Don't forget to clean the data.
You have to classify based on the labels given in the dataset

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classifying Images","","","0"
"1726","834930","5156765","08/17/2020 15:07:31","none","","Price Prediction of any  brand","predict the price of any specific brand (eg: H&M, WROGN)","","1"
"1727","834930","5156765","08/17/2020 16:09:47","perform EDA","","Explanatory Data Analysis","","","2"
"1468","793241","5158258","07/23/2020 17:57:44","## Task Details
Do create new artistic images and get the feel of creating and publish your work to the world.","","Artistic Neural Style Transfer","","","0"
"1218","743727","5159361","06/28/2020 17:16:46","Need to fix the formatting of the game results and scores being in the same column.","","Separate Win-Loss Column","","","0"
"1966","860338","5166622","09/06/2020 08:34:57","## Task Details
This data set includes different parameters for strength estimation of concrete.

## Expected Submission
Best model to estimate the strength.

## Evaluation
R-squared value, Mean squared error","","Concrete strength estimation model","","","0"
"2092","872565","5167145","09/14/2020 08:14:30","## Task Details
Reddit is an American social news aggregation, web content rating, and discussion website.

Registered members submit content to the site such as links, text posts, and images, which are then voted up or down by other members. Posts are organized by subject into user-created boards called ""subreddits"", which cover a variety of topics such as news, politics, science, movies, video games, music, books, sports, fitness, cooking, pets, and image-sharing. Submissions with more up-votes appear towards the top of their subreddit and, if they receive enough up-votes, ultimately on the site's front page. Despite strict rules prohibiting harassment, Reddit's administrators spend considerable resources on moderating the site.

As of May 2020, Reddit ranks as the 19th-most-visited website in the U.S. and in the world, according to Alexa Internet, with 55% of its user base coming from the United States, followed by the United Kingdom at 7.4% and Canada at 5.8%

Source Wikipedia.

There is a special Subreddit hosted for Indian discussion known as r/india. With such a large content it is essential to classify this dataset into subcategories or flairs.

This dataset contains r/india Subreddit posts. Top 1200 posts are selected along with their body, no of upvotes, flairs, comments, title etc. The task is to predict the flairs of a particular post using the data.

## Expected Submission
For each post submit a particular flair. The output file should contain the id and the flair. 

## Evaluation
For your model, We will test again a custom flair. It should predict the correct flair for that particular post.

### Further help

Notebooks to be added soon.","","Flair-it-up","Detect the flair for a particular post.","","4"
"2364","910625","5174183","10/07/2020 18:39:00","## Task Detaill
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Admission in top university","admission procedure","10/08/2020 23:59:00","0"
"1677","788604","5175856","08/12/2020 03:06:00","##Overview
Advertisement CTR prediction is the key problem in the area of computing advertising. Increasing the accuracy of Advertisement CTR prediction is critical to improve the effectiveness of precision marketing. In this competition, we release big advertising datasets that are anonymized. Based on the datasets, contestants are required to build Advertisement CTR prediction models. The aim of the event is to find talented individuals to promote the development of Advertisement CTR prediction algorithms.

##Competition Description
In this competition, we provide contestants the user behavior data of advertising tasks, basic advertisement attributes, and basic user information. Contestants are required to build Advertisement CTR prediction models based on the provided data. All the data have been masked to ensure optimal data security.

The basic datasets contain a training dataset and a test dataset. The training dataset provides the tag that specifies whether a user clicks an ad, where 1 indicates yes, and 0 indicates no. The testing dataset is used to calculate the AUC of the built models. Contestants need to provide the predictive click probability of each ad sample in the testing dataset.

##Evaluation MetricÔºöArea Under Curve (AUC)","","Advertisement CTR Prediction","","","7"
"1235","750422","5177882","07/01/2020 13:45:53","Try to classify different felines by using the best deep learning model","","Classify Different Felines","","","0"
"1497","740455","5179142","07/27/2020 14:41:00","## Task Details
The agriculture is the sector who moves the economy of various countries.
How was the credit provided to agriculture in the years of 2012 to 2018?

## Expected Submission
You can perform a credit analysis, either in Brazil or in other countries provided by the data set. A good and reasoned visual analysis is expected.

## Evaluation
Were you able to demonstrate the use of credit in agriculture? Their correlations? Did you use other references as well?","","Perceptions of rural credit","","","2"
"1152","720074","5179142","06/16/2020 14:29:39","## Task Details
Fertilizers are used daily by farmers and families to help crops and gardens grow. Whether for a small garden of flowers and plants, or a large farm with thousands of acres of crops, a wide range of fertilizers have been developed to help different crops grow in different soil and weather conditions.

Chemical ingredients help create fertilizers that promote plant growth and are cost effective, too. Commercial and consumer fertilizers are strictly regulated by both individual states and the federal government to ensure that they are safe for the people who use them, people nearby, and the surrounding environment.

So how is fertilizer being used in your country?

## Expected Submission
What is the most used fertilizer in your country over the years?
What is the most produced in your country and how is the export?
What is the best and worst rate of import and export (that is, ratio of imported and exported product) in your country over the years?

## Evaluation
Is data visualization the most suitable? Did you achieve the goal? What can you take from the analysis of the results?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","How has your country been using fertilizers?","","","6"
"2224","891379","5179142","09/24/2020 15:01:16","Amazon.com, Inc., is an American multinational technology company based in Seattle, Washington. Amazon focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence.

## Expected Submission
How has Amazon stock prices evolved over these 23 years?
Is it possible to make a prediction of the next prices?
Is there a pattern or anomaly?

## Evaluation
A good visualization, a well done analysis joining information and, if possible, the application of machine learning in this data set.","","Understanding Amazon's Actions","","","1"
"1742","836566","5180805","08/18/2020 14:41:47","## Task Details
Predict the next 30 days of sales considering the current dataset

## Expected Submission
**What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?**
Notebooks w/ Datasets

## Evaluation
**What makes a good solution? How do you evaluate which submission is better than another?**
If the developer uses the ensemble learning technique with a Linear Model + Time Series + Regression + Neural Network algorithm

### Further help
Contact me: @luisvalgoi","","Predict the next 30 days","It would be good to use Ensemble Learning","12/31/2020 00:00:00","0"
"1102","705086","5196267","06/10/2020 07:47:10","## Task Details
We have studied in the probability that the chances for the occurrence of a Head or a Tail of an unbiased coin toss is 1/2. I have created a dataset with the same understanding. Let's try out something really cool and bring out insights from this data.

## Expected Submission
Perform amazing EDA on the data and submit the notebook.

## Evaluation
The best notebook consisting of amazing plots and analysis will be featured!!

Let's do it!!","","Try our some cool EDA on the data!!","Let's bring out the hidded insights from the basic coin toss data.","12/31/2024 00:00:00","0"
"1808","842709","5200710","08/23/2020 22:13:59","You can analyze the correlations between the problem tags and other parameters. Check that the frequency of tags appearance change over time.","","Analyze the popularity of different tags over time.","","","0"
"1508","803794","5210781","07/28/2020 15:33:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
configurations in the idea of GANs and best possible hyper parameters for the image dataset
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
resulting  images should be clear 
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","generative adversarial network (GAN)","","","0"
"1026","688051","5212675","06/01/2020 21:36:56","## Task Details
We are sharing the latest BCG Atlas data and various texts related to BCG vaccine policies around the world. The goal of the task is to try to **improve and to extend the existing BCG World Atlas data** and to get a fuller picture of the administration of the vaccine. All data that is collected during this task will support the work of the BCG Atlas team, and it will also help with the analysis of the second task of the hackathon. 
The current BCG data is stored in  file.  The task is to try to find snippets that answer some of these questions. All the questions are listed as columns. Specifically, we would like to get as many accurate answers as possible to the following questions, for as many countries or regions as possible:
- Which BCG strain over the years has been used in different countries since the creation of the vaccine and during what time periods?
- Who are or were the target group of the vaccine, and when and at what age were they vaccinated?
- Have revaccinations (boosters) been given and if yes, when and at what age?
 
In the following spreadsheet are listed a couple of sample records - [BCG policy data hackathon sample google spreadsheet](https://docs.google.com/spreadsheets/d/1nrzrAy0vC43Iy9QaKA5pR83O3YW3I16pXFjnOt86j5I/edit#gid=67941157). The new records which were added in addition to the existing BCG data from ""BCG_world_atlas_data-2020.csv"" file are on line 3, 6 and 7 and are highlighted in green. Line 4 and 5 are part of the original ""BCG_world_atlas_data-2020.csv"" file. The first 4 columns are mandatory and the field ‚ÄúIs it from bcgatlas.org (Mandatory field)‚Äù should have the value ‚Äúno‚Äù for all records that are added by the participants of the hackathon. 

## Expected Submission
The expected submission will be a copy of the ""BCG_world_atlas_data-2020.csv""  file saved in Kaggle. The file should contain additional rows for a number of countries with snippets that contain answers to a specific question of a given column.  There should be no more than 2 new rows (data sources) per country. The snippet length should not be longer than 350* characters. For each additional row that has been added apart from the snippet(s), the first four columns are mandatory and should also be updated. A manual review is highly recommended in order to make sure that the snippets of the 2 selected data sources contain information about the specified country.

Participants must also accept our [competition rules](https://www.kaggle.com/bcgvaccine/hackathon). 

## Evaluation
Submissions will be scored using the following grading rubric:
- How many answers for how many countries have been found?  
- Are the snippets relevant?   
- Is the methodology well documented and if the code easy to read and reuse?  

## Timeline
Start date (June 8, 2020)
Submission deadline (July 25, 2020) at 11:59pm UTC.**

## Prizes
- First prize $3,000
- Second prize $2,000
- Third prize $1,000

## Winners
- First place - Dimitrina Zlatkova - contributed 57 additional
entries data points
- Second place - Marouane Benmeida - contributed 33 additional
entries data points


[*] 22 June, updated the length from 120 to 350 characters.","","Extend the BCG World Atlas Database for BCG Vaccine Policies and Practices","Prize Pool $6000; Submission deadline - July 25","09/21/2020 00:00:00","14"
"1027","688051","5212675","06/01/2020 22:12:22","## Task Details
The initiative is prompted by the suggestion that there may be a link between reduced rates of infection and lower case fatality rates associated with COVID-19 in [countries that recommend  BCG vaccine](https://www.embopress.org/doi/full/10.15252/emmm.202012661) for all as opposed to countries that recommend BCG only for specific high-risk groups. We hope that the analysis done as part of this task might help discover useful information about the [BCG - COVID-19 clinical trials](https://docs.google.com/spreadsheets/d/16ISrVB2OL9pOLLPyTRziciTjSwFH7RI-aVExadCJ25o/edit#gid=821329594).  For example, some insights that may come from this analysis is whether factors such as the strain of BCG, the age at which people have been vaccinated, revaccination, or how long ago people have been vaccinated are important. 
 
Using machine learning and other technologies, we hope to be able to provide support for the role of BCG vaccinations or an [alternative hypothesis](https://naturemicrobiologycommunity.nature.com/channels/2549-coronaviruses-past-present-and-future/posts/64892-universal-bcg-vaccination-and-protection-against-covid-19-critique-of-an-ecological-study). We would like to get answers to the following questions:
- Is BCG vaccination causally related to reduced COVID‚Äê19 mortality or other factors like lockdown and average age of the population are responsible for the different mortality rates?
- If BCG vaccination reduces COVID-19 mortality, what are the key factors, for example:
- How long does the immunity engendered by BCG last after vaccination?
- Which BCG strain has been used?
- What is the optimal time to vaccinate?


## Expected Submission
The expected submission must be contained in one or more public notebooks before the submission deadline including the     publicly available data that has been used for the analysis as well as the result data in CSV format. 

The participants must also accept the [competition rules](https://www.kaggle.com/bcgvaccine/hackathon). 

## Evaluation
Submissions will be scored according to the following grading rubric:
- Did the participant accomplish the task?  
- Are the results relevant?  
- Is the methodology well documented and is the code easy to read and reuse? 

## Files relevant to Task #2:
- task_2-BCG_strain_per_country-8Nov2020.csv (contains the latest http://bcgatlas.org data including the one that was gathered in the first task)
- BCG-Strain.csv
- task_2-COVID-19-death_cases_per_country_after_fifth_death-till_22_September
- task_2-COVID-19-death_cases_per_country_after_first_death-till_22_September
- BCG_COVID-19_scientific_papers.csv (the latest scientific papers can bee seen in this [spreadsheet](https://docs.google.com/spreadsheets/d/1y9kGMqyl0AGAH7fzwNNrTsUU0cRaKApEOk4a5vbcOds/edit#gid=0) )
- BCG_country_data.csv
- BCG_COVID-19_clinical_trials-2020_11_11.csv (clinical trials for BCG and COVID-19 - it has been updated on 11th November, but we keep updating this data in the following spreadsheet - https://docs.google.com/spreadsheets/d/16ISrVB2OL9pOLLPyTRziciTjSwFH7RI-aVExadCJ25o/edit#gid=821329594)
- task_2-Gemany_per_state_stats_20June2020.csv
- task_2-Tuberculosis_infection_estimates_for_2018.csv
- task_2-owid_covid_data-21_June_2020.csv

## Submission deadline
-  December 31, 2020 at 11:59pm UTC.

## Prizes
- First prize $3,000
- Second prize $2,000
- Third prize $1,000

## Winners
- First place - Arpit Solanki - [BCG Effectiveness in combating COVID-19](https://www.kaggle.com/arpitsolanki14/bcg-effectiveness-in-combating-covid-19) 
- Second place - Martha Berg - [Mandated BCG vaccination flattens COVID-19 spread - Tables and Figures](https://www.kaggle.com/marthaberg/berg-et-al-2020-script)","","""BCG - COVID-19"" - Find Insights that Could Help the Clinical Trials","Prize Pool $6000; Submission deadline - December 31","02/28/2021 00:00:00","22"
"1534","803100","5216831","07/30/2020 13:31:39","This data set is taken from [PHL (Planetory Habitability Laboratory) website](http://phl.upr.edu/projects/habitable-exoplanets-catalog/data/database). Database field descriptions can be found in the link itself. I have also mentioned about them in my notebook. The challenge is to identify features responsible for deciding habitability category of the exoplanets. Also, the task is to classify the planets in test data set partition to one of the three classes: non-habitable, conservatively habitable, optmistically habitable.","","Multilabel Classification of Exoplanets","","","0"
"1639","819653","5556552","08/08/2020 18:52:16","## Task Details
Visualizing the performance of Players at different stadiums.

## Expected Submission
I have used seaborn barplot and matplotlib subplot for visualising. Try using other tools to get more attractive results. 

## Evaluation
The notebook which gives better visualisation and insights about the given dataset.","","Visualizing the Stadium Wise Performance of Players","","","2"
"2630","899487","5221270","11/05/2020 05:40:03","Beat Guanshuo Xu on the private leaderboard. This should be possible due to open private leaderboard.

https://www.kaggle.com/c/rsna-str-pulmonary-embolism-detection/leaderboard","","Beat State of the Art for PE classification","","","1"
"1627","820971","5221311","08/07/2020 19:10:46","## Task Details
Visualize the Growth of the Access To Electricity of The USA and  The world over Time And Predict the Future of The Data(The world) using a machine learning Algorithm.
## Expected Submission
You should submit Graphs and code
## Evaluation
The shortest and the fastest Code is the Best.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
-https://www.kaggle.com/khalilbrick/coronavirus-covid-19-visualization
- https://www.kaggle.com/khalilbrick/demographic-analysis","","Predict the Future of The Access To Electricity Data","","07/07/2022 00:00:00","0"
"1306","766106","5221311","07/09/2020 15:32:31","Use Matplotlib and pandas To visualize this data from 2014 to 2020 using a line plot or a scatter plot","","Data Visulization of the price of Bitcoin(Open)","","10/10/2022 00:00:00","1"
"2935","900950","5223111","12/14/2020 00:56:53","#Well, how else could you do it though?","","Use a Transformer to Produce a Text Generator","","12/17/2020 23:59:00","0"
"6960","709715","8862792","11/29/2021 08:59:41","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
zheshishiyan","","weixingshibie","lake_airplan","07/08/2022 23:59:00","0"
"1375","763806","5224933","07/15/2020 17:34:14","Training a neural network that reaches more than 96% accuracy with a validation set of at least 50'000 datapoints","","Neural Network training","Neural Network with +96% accuracy","","1"
"1045","691896","5228362","06/03/2020 21:29:03","humanitrian, 

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","humanitrian","humanitrian, to the world","06/30/2020 00:00:00","0"
"1577","813365","5231941","08/03/2020 08:40:15","## Task Details
Given the features in the CSV files pokemon_numeric & pokemon_categorical, predict whether a pokemon is legendary or not

## Expected Submission
Submission files must contain 2 columns. The index and IsLegendary which is a boolean indicating whether a pokemon is legendary or not.

## Evaluation
Evaluation is a simple accuracy_score of the number of predictions you got right.

### Further help
Will be added soon","","Is a Pokemon Legendary ?","Use both numeric and categorical data","","0"
"2889","693956","5233251","12/08/2020 13:13:24","## Task Details
Due to various factors, daily-cases reports of COVID-19 in Indonesia are often late, can be 1 day, 3 days, 1 week, 2 weeks, or even more. Thus, sometimes the daily-cases reports are accumulated data of cases from the previous days. This causes the data on daily-cases reports of COVID-19 in Indonesia cannot describe the real situation.

Therefore we need a model that can estimate the actual number of daily cases of COVID-19 in Indonesia.

## Expected Submission
You can use this ""[COVID-19 Indonesia Dataset](https://www.kaggle.com/hendratno/covid19-indonesia)"". The expected output is an estimate of the actual daily-cases number of COVID-19 , both at the country level (Indonesia) and at the provincial level (34 provinces).","","Estimate the actual number of daily cases of COVID-19 in Indonesia","","","7"
"1641","799198","5236787","08/08/2020 23:28:56","**Apply sentiment analysis for characters in the show SKINS.**","","Sentiment Analysis","Sentiment Analysis for character in show SKINS","","0"
"1979","860997","5249501","09/06/2020 16:30:45","**About the Dataset**
These dataset contains sub folder, train and test. In each folder fire images with annotated xml files are included.

**TASKS**
Detect fire in images or video","","Detect Fire from camera feed","","","0"
"1244","752156","5259370","07/02/2020 15:50:38","Create an  image classification model by using train data (including .csv & .jpg) and use it to classify the test data (including .csv & .jpg)
the outcomes come with .csv 2 column (1.the images name  2.the categories)","","Image Classification","","","0"
"1097","702185","5263152","06/08/2020 23:38:11","## Description

To better reflect the innovative university programs, the Organizing Committee of the Second Conference of the Hanseatic League of Universities has formed a new university ranking system based on the 400 programs. Here on Kaggle, we are releasing 117 programs in text around 48 universities under the four categories mentioned below:  

1. Industrial applications, rather than the traditional ways of counting research papers and lecture-type teaching;
2. Value-creating startups and entrepreneurship, rather than a traditional focus on the number of jobs filled; 
3. Social responsibility, ethics, and integrity, rather than a focus on knowledge and skills just for material success; 
4. Student mobility and openness for exchange and collaboration between schools and across national borders, rather than an independent yet closed system.

## Task Details
At this stage, we are issuing a call for action to the world‚Äôs artificial intelligence experts to share their idea on how to develop the attached text data for the next rounds of projects next year to improve this ranking system using the Kaggle. 
Currently, this project has been conducted by expert surveys and text informational analysis manually. What we hope to achieve by opening up our data this year is to receive productive feedback from the experts on how qualitative assessments on universities‚Äô innovative programs can be rendered globally at a massive scale. 
We believe a lot of these works are suitable for text mining. However, given this is an annual task, there must be a condition to measure within the annual time frame.
Also, a list of key criteria can be found under the Tasks section of this dataset. 

## Evaluation
1. Innovativeness: Evaluators should first look at how innovative the program is. This could be with the content of the program itself or with the process the institution has demonstrated in making their program more effective. The evaluators may consider one of these aspects or both.

2. Implementability: Although the program may be innovative, evaluators should also examine its Implementability or doability. This can be assessed by measuring the costs and benefits in initiating the program.

3. Impact: Evaluators should consider both the scope and intensity of the program. In terms of scope, it should be judged whether the impact has been limited to only one department, for example, or whether it has been widespread across the entire university. Additionally, the impact of the program can have a varying degree of intensity which must be weighed up carefully.","","Share ideas to improve the WURI ranking system","","","1"
"2258","895249","5264912","09/27/2020 05:53:45","## Task Details
You are invited to prove with data whether countries with better immunization rates fared better in the fight against CoViD or otherwise.

## Expected Submission
You are expected to analyse the immunization dataset with public CoViD datasets containing country-wise data and arrive at a conclusion whether these data are related or not through notebook submissions. [Please note that immunization data from UNICEF is available from 1980 only and not all data is available for all countries for all vaccine types/doses for all years. So it is meaningful to compare statistics only for people below 40 years as of 2020.]

## Evaluation
Back your analysis with data and reach a logical conclusion regarding the two datasets.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","CoViD vs Immunization","Find whether any correlation can be established between CoViD spread and immunization rates in countries worldwide","","0"
"1939","857298","5276687","09/04/2020 00:48:15","Predict how severe a patient's cancer will be.","","Predict the Level of a Patient's Cancer","","","3"
"1854","841709","5276687","08/27/2020 23:45:55","Try to predict the number of cases in the future given the past data.","","Predict Total Number of Cases in the Future","","","1"
"1143","710024","5285932","06/15/2020 06:52:00","**Can i select more relavent 5-7 calsses to detect ?**
Yes, if you want, you can select all classes, but points will only be given on mask and no-mask. 

- **I have almost completed the problem. I only need to create a submission.csv file for all the examples. The thing is, it is really very difficult to work on kaggle now because the notebook lasts 9hour and even after saving the model i was not able to reuse it in kaggle. So I need to train again, which takes many hours, and I am left with very less time to write code for creating the sumbission.csv file.  Apart from that now I won't be able to train because soon I will exhaust my GPU and tell me how can i possibly work on it now?**
You can choose to download the model after training and upload it again next time you want to move forward. You can also choose to save the model into your Google Drive. If you exhaust GPU hours, You can look at continuing training using Google Colab

- **I would like to report an error in ""train.csv"" file on kaggle dataset, column headers are messed up, it should be like this (in Image), please revise it, thank you!!!**
That is not an error. That is done deliberately. The dataset is intended to be cleaned before using it.

- **can i train my model offline and generate the csv file  then submit it through google doc link ?**
You can train it offline or using Google Colab, but the jupyter notebook link has to be shared via the Google Docs Link

- **I have a small query, can i use existing model and retrain them or should i design my own model?**
You are free to use any model that you wish. 

- **(External Data is not allowed it's a repository or library.) Does this line mean we cannot use any pre-trained models for our task?**
You can use pre-trained model, but you cannot use any additional datasets. 

- **Can I use any Opensource repository for reference ?**
Yes, any approach that delivers results for the given dataset, works.

- **do we have to make a model which will predict all 20 classes like all the types of mask etc. or just the classes : face_with_mask and face_no_mask. Because the title is Face mask detection model. Because at last, we have to detect if a person is wearing mask or not.**
We will only be checking for two classes: face_with_mask and face_no_mask. It is up to you how you want to train the model using the existing classes. 

- **Is it mandatory to create our own model for mask detection or we can use pre trained yolo model for detection and fine tune it on our dataset**
You can use pre-trained model, but you cannot use any additional datasets. 


- **I had downloaded the dataset from kaggle. The annotations start from 1801, for images indexed from 0001 ...... Can you kindly confirm if there are any annotation files missing or if I am supposed to train with the available ones ?**
You are supposed to train on the available ones and run inference on the ones that are missing the annotation and submit the same as CSV.

- **I have completed the task. But somehow im not able to download my result submission.csv file that i have generated. I used some methods shown in kaggle but it shows : "" This XML file does not appear to have any style information associated with it. The document tree is shown below. ""**
You can upload the CSV file to your google drive using Google Drive API and then download it from there.","","FAQs - Update (15/06/2020)","","","1"
"1123","710024","5285932","06/12/2020 11:03:29","## Rules
This is a Kernels-only dataset.

Detect mask categories in submission_images.csv file.
Submissions to this competition must be made through Kernels. Your kernel will re-run automatically against an unseen test set and needs to output a file named submission.csv. You can still train a model offline, upload it as a dataset, and use the kernel exclusively to perform inference.

The sample submission file is submission.csv

In order for the ""Submit to Competition"" button to be active after a commit, the following conditions must be met:

CPU or GPU Kernels &lt;= 9 hour run-time.
Training and Testing have to be done in a single notebook or script.
Models have to be trained on Kaggle Only.
External Data is not allowed it's a repository or library.
Kernels have to be shared with organizers.


## Detailed General Instructions
- The dataset zip that is provided contains images and their corresponding labels in a CSV.
- The CSV labels&nbsp;are provided for all images except the first 1800 images. Which means that those are to be considered as test images.
- For the rest of the&nbsp;images, their corresponding labels are provided in the existing CSV.&nbsp;
- The candidate(s) are required to first train a model on the images for the label that is provided (Excluding the 1800 images that are unlabelled).
- Once a trained model is obtained, the task is to run inference on the first 1800 test images (whose labels are not provided) and save the model predictions in a CSV file, in the exact same format as the training CSV is provided. (The format can be found here: [Link](https://www.kaggle.com/wobotintelligence/face-mask-detection-dataset?select=train.csv)).
- Once the CSV is generated, the candidate is required to submit&nbsp;the same via the Google Form, [Linked here](https://forms.gle/1io1bsypdFTdks646).
- If the candidate is unfamiliar with how to generate a CSV, he/she can refer to&nbsp;[My First Kaggle Submission - YouTube](https://www.youtube.com/watch?v=68l47Zu4Yxg)&nbsp;or any other tutorials available online.

## Points to note:
- The dataset may require preprocessing and data cleaning
- Only two classes will be checked: with and without a mask. the candidate can choose to exclude the other classes,  merge the other classes, that is up the candidate.

Submissions are to be made in this Google Form Link: https://forms.gle/jRxXAQaEAhuP8BkA8","","Generate Submission","","06/16/2020 00:00:00","17"
"1761","838683","5287922","08/20/2020 11:42:52","In the scientific research, the characterization of nanoparticles has been widely used in Nanoscience domaine. Scanning electron microscopy is one of the most frequent characterizations used for determine the size and the density of nanoparticles. 

The traditional approach to determine the size distribution of nanoparticles is to count manually and calculate the density and the size distribution. The disadvantage using [imageJ ](https://imagej.nih.gov/ij/download.html)is you have to treat each image manually. Can we use the computer vision to automatically determine the size (nm) and the density (/cm^2) of nanoparticles?

The challenge part is to increase the precision and avoid the dusts. 

I expect to obtain a notebook, which can automatically treat the images and obtain the size and the density of nanoparticles with an error.  

In order to evaluate the notebook, you can download the images and use imageJ to count the nanoparticles. The results counted by ImageJ can be a reference. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Determine the size distribution of nanoparticles","","","0"
"2316","904263","5316667","10/03/2020 11:22:07","## Task Details
You can get a clear understanding about different testing centres across India state-wise.
You can visualise the data and analyse the number of testing centres of each state depending upon its population, land area, spread of covid-19 and many more factors. You can also relate the number of government testing centres with the private testing centres varying in ratio state-wise.","","Analysis of data","Try to perform different data analysis.","","0"
"2023","856709","5328462","09/09/2020 02:56:34","## Task Details
 Study the dataset carefully. Describe different aspects of this data via 5 most
interesting plots. These must include
- scatter plots
- histograms
- box and whisker type plot
For each plot, write a paragraph in your notebook showing the interesting stuff the visualization reveals.

## Expected Submission
Submit a notebook and write at least 3 data modelling/analysis questions that come to your mind after looking at this data.

## Evaluation
- Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of data and visualizations.
- Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought-provoking, and fresh all at the same time.
- Documentation - Are your code, a and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high-quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible
### Further help
https://www.cia.gov/library/publications/resources/the-world-factbook/docs/rankorderguide.html","","Visualization","","10/31/2020 23:59:00","0"
"1345","731634","5330621","07/13/2020 05:40:50","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

<img src=""x"">","","<img src=""x"">","<img src=""x"">","","0"
"2135","877335","5332545","09/17/2020 05:24:44","![association rules example](https://miro.medium.com/max/1467/1*--iUPe_DtzKdongjqZ2lOg.png)","","Association Rule Mining","Identify the association rules for Market Basket Analysis","","30"
"2178","885773","5332545","09/21/2020 09:41:17","This dataset has various NA values, replace them with appropriate values and identify the outliers","","Data Cleaning","Clean the dataset by removing outliers and NA values","","5"
"2179","885773","5332545","09/21/2020 09:42:56","Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.","","Exploratory Data Analysis","Analyse the data and show insightful visualizations","","1"
"2350","909176","5332545","10/06/2020 18:00:13","The solution should have a comparative analysis of number of COVID cases with respect to time and region.","","Data Analysis","Analyse time series data","","1"
"2289","900644","5332545","09/30/2020 17:37:21","Remove the stop words from a corpus and extract the context of a text corpus.","","Text Preprocessing","NLP","","0"
"2300","901429","5332545","10/01/2020 10:31:56","Your task is to come up with an interactive visualization that can do what no static graph can. You are on a mission to use this data and tell a story of user interactions with the Airbnb platform: switching devices, spending time on the platform, doing different actions, and effecting user outcomes.

### The challenge

We see a few tracks to the challenge. You are free to define yours. The ultimate goal is to build an interactive visualization that helps tell a compelling and crisp story of user behavior on Airbnb.

The time aspect of user interaction with Airbnb. How do the time spent on user sessions, and the intermediate actions taken by the user on his or her journey, relate to user outcomes? Is there seasonality to how users interact with the platform? Any favorite times in the day? Are there any user types defined by when and for how long they use the platform?

Devices and apps used to access the platform and their impact on user outcomes. Do users who use a few devices and apps to access the platform exhibit different patterns than those who use one or two? What is the dynamic of switching from one device/app to the next, how does it relate to user actions and outcomes?

Your track. Find your own way of looking at the data, and share it with the world!","","Data Analysis","","","1"
"1189","730407","5335192","06/21/2020 11:46:38","## Task Details

Para o desenvolvemento dos informes individualizados, necesit√°bamos s√≥ eses 4 cursos, por iso non se completaron m√°is.

Proponse completalos a 1¬∫ de BAC e 4¬∫ da ESO.","","Extensi√≥n a 4¬∫ da ESO e 2¬∫ de BAC","","08/31/2020 00:00:00","0"
"1889","850380","5337747","08/29/2020 22:49:19","Classify Stack Overflow questions based on their content quality. I have included a target column that has three values:

**HQ:** High-quality posts with 30+ score and without a single edit.
**LQ\_EDIT:** Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.
**LQ\_CLOSE:** Low-quality posts that were closed by the community without a single edit.","","Which Stack Overflow questions should be edited or closed?","Classify questions into three quality types.","","14"
"2030","850380","5291112","09/09/2020 15:41:24","## Expected Submission
It could be interested to compare deep learning and others algo?","","Predict tags according to the text and title.","","","7"
"2183","871327","5337747","09/21/2020 13:46:44","Can you classify these images into 12 classes?","","Classify store items by color","","","0"
"2286","900181","5338635","09/30/2020 13:20:22","## Task Details
Predict 'SalePrice' feature for the TestData set. 

## Expected Submission
The submission includes CSV file with ID number of the TestData and it corresponding prediction.

## Evaluation
The submissions can be compared by comparing RMSE score

### Further help
Feel free to contact me!","","Predict SalePrice","Predict 'SalePrice' feature for the TestData set.","","0"
"1265","758750","5342732","07/06/2020 06:49:20","## Task Details
I have created this for the easy recommendation of movies in all aspects of the genre.

## Expected Submission
I have already done the recommendation based on the genre and ratings provided by the used,you can see that in the kernel.
Now I would like some one to do the recommendation based on the ""tags"" and ""links""
so that the recommended movie has a IMDB id and tags
## Evaluation
Recommendation with  categories such as links,tags should be added to my older.
### Further help
For help in clear cut visual of the program
visit https://github.com/karthikband1/Movie-recommendation-system","","recommendation extension with links and tags","","","0"
"3516","757364","5342793","02/16/2021 09:15:02","## Task Details
Arranged to do research and development assignments on K-Nearest Neighbors science

## Expected Submission
The task is solved using data sets and instead. I can also take part in internet surfing.

## Evaluation
It is valuable to me that you interpret data differently and analyze things that are not visible or perceived. :)","","KNN Algorithm Dataset","K-Nearest Neighbors Lesson","02/17/2021 23:59:00","2"
"2121","875981","5345489","09/16/2020 07:03:13","## Task Details
Well , provided in the dataset is the information of Indian Super League .  Can you figure out the most expensive player? Which team wins the most ? And every other snippet of information you could find counts!!!!

## Expected Submission
The submission should be detailed and explained , please just don't only paste code. However , every submission counts .

## Evaluation
Evaluation is based on how well the exploratory analysis is done 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis of ISL","","","0"
"5346","733642","5351571","07/26/2021 10:47:32","## Task Details
This dataset is all about tweets and comments passed by politicians, big names in the industry, famous personalities around the globe. I have created this task to visualize the different genres contained in the dataset collectively. Identify which attribute should be consider for further use in ML pipeline

## Expected Submission
Expecting to see notebook containing visuals on the provided datasets.","","Exploratory data analysis","Visualizing News data","","0"
"1758","820401","5351571","08/19/2020 18:44:41","## Task Details
I have done a small project on sentiment analysis using the dataset provided but I used one of the NLP library to achieve the result. As far as I get insights from my project, It wasn't capable  enough to decode sarcastic text properly. Now, I am looking for how  other algorithms can outperform **TextBlob** and achieve more accuracy than that.

## Expected Submission
It would be great if one can make end-to-end project using this dataset and provide with the algorithms that are capable of performing excellent on text data as well as sarcastic text.

## Evaluation
Accuracy achieved by your algorithm is one of the parameters to evaluate. Means, How accurately your model is able to differentiate between positive and negative comments including sarcastic ones.

### Further help
- You can refer to this article to get started with the task.
Predicting US Presidential Election Result Using Twitter Sentiment Analysis with Python (https://medium.com/datadriveninvestor/predicting-us-presidential-election-using-twitter-sentiment-analysis-with-python-8affe9e9b8f)","","Predicting Election Result","Predict result of election using sentiment analysis on text provided.","08/20/2020 23:59:00","0"
"4191","777903","2964640","04/26/2021 11:41:26","## Task Details
This data set contains 416 liver patient records and 167 non liver patient
records. The data set was collected from north east of Andhra Pradesh, India. Selector is a class label used
to divide into groups (liver patient or not)

## Expected Submission
You're free to submit whenever you want :)

## Evaluation
Keep an eye on the final metrics

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","ILPD Classification","","","0"
"1777","839243","5355951","08/20/2020 21:42:42","Try to find any correlation between the variables! Good luck and thanks for using the dataset! Please upvote if you liked it.","","Is there any relation between the way the package was purchased to the hotel or the original resort?","","","3"
"1923","855327","5356082","09/02/2020 14:36:02","## Task Details
show your EDA skills.make it as informative as possible
notebooks with most number of upvotes are declared as winners","","Exploratory data analysis","","","1"
"1364","775845","5360852","07/14/2020 16:08:24","## Task Details
Find if there are any clear trends in the bike crashes observed e.g. are they always in places near or away from traffic control.

## Expected Submission
Visualisation of the data to highlight some insights.

## Evaluation
Finding a trend which has a clear conclusion regarding bike safety.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Bike crash insights","","","0"
"1279","756263","5360852","07/07/2020 10:48:13","## Task Details
Any football fan will have noticed that games feel very different without a crowd presence - it'd be interesting to see if this translates through to the game stats.

## Expected Submission
Some sort of angle regarding change in performance, which includes nice visualisation of the data.

## Evaluation
Good visualisation methods that make the conclusions drawn clear and indisputable.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Difference in team performance before and after COVID","","","2"
"1258","750216","5360852","07/04/2020 13:05:42","## Task Details
The reason I collected this data set was to make a regression model that could inform whether the car he wanted to buy was good value in relation to the market in general. I've extended this data set to include lots of other makes, so it would be cool to extend the car value prediction model.

## Expected Submission
Some sort of value regression model.

## Evaluation
Low error, and easy to understand output. Should be able to put in custom car details and get a value estimate.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Car Price Regression model","make a tool to predict car value","","95"
"4774","750216","5276714","06/14/2021 15:30:32","## Task Details
This task asks you to produce a robust, generalisable regression model for predicting the sold price of Ford Cars. We have used Ford as it provides the best distribution of price and greatest number of training values.

## Expected Submission
You should split the dataset into two, Train (70%) and Test (30%) before doing **any type of scaling/model pre-processing.** This will allow you to easily compare the power of your model against others.

Feel free to use the splitting code from the example notebook to maintain standards.

Or to just take the split data directly from [here](https://www.kaggle.com/munumbutt/ford-car-price-regression-task)


## Evaluation
The final score will be your RMSE obtained on the Test set. The R^2 test is terrible for any kind of non-natural phenomena (when does linear regression ever work when dealing with human behaviour?) and thus we will use the RMSE instead.","","Ford Car Price Regression (RMSE)","More Specific Task For Better Models, Evaluation Metrics: RMSE","","6"
"1477","796167","5366520","07/24/2020 17:36:00","Hy there..!!
This dataset is extracted from Magicbricks, so all the credit goes to them.
this dataset contain many columns which will be useful in predicting price of the houses in Delhi, I changed a little bit from the original dataset, I created a column named districts, which contains the name of all the districts of that particular property/house. This column will help to get more precise results, as the column 'Locality' will not help in any case.

This is my first project on Kaggle..Happy to work...","","House price prediction in Delhi, District wise","","","0"
"1423","788052","5371624","07/20/2020 05:33:59","Translator is to be created that converts the input sentence to output. EIther English to French or French to English.

Translator would be evaluated based on its accuracy.","","Creation of language translator","","","0"
"5678","910631","4912604","08/11/2021 17:21:16","Explain the approach in detail for all the problems.","","Create an EDA","","","0"
"1638","767979","5392036","08/08/2020 18:33:41","## Task Details
Create models that could identify character of [Aksara Jawa](https://en.wikipedia.org/wiki/Javanese_script)

## Expected Submission
No so high and not so low, My ideal expected results are the model could identify the character based on the training data.

## Evaluation
I am going simple, the models could identify the character from a real handwritten image. 

### Datasets
* [https://www.kaggle.com/vzrenggamani/hanacaraka](AksaraJawa/Hanacaraka dataset)","","Hanacaraka Image Classification","","","0"
"1938","857185","5401349","09/03/2020 21:23:20","## Task Details
Exploratory Data Analysis for Formula 1","","Exploratory Data Analysis","","","0"
"1237","751098","5401511","07/01/2020 21:28:35","Which politicians have increased / decreased holdings of stocks in controversial industries such as tobacco, oil & gas, and defense contractors?","","Controversial Industries","","","0"
"2045","822964","5403836","09/11/2020 04:54:05","Can you classify the vehicles with good accuracy!","","Vechicle Classification","","","0"
"1280","757988","5406706","07/07/2020 13:01:18","## Task Details
I'm missing &lt;50 doe network missing cases - go through and fill that data out and re submit for matches


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Fill out all doe network data and re upload possible matches","","","0"
"1391","781898","5406870","07/17/2020 07:46:54","## Expected Submission
A notebook with your insights

## Evaluation
Graphics, EDA and explainations","","Exploratory Data Analysis","EDA Personality System","08/20/2020 00:00:00","2"
"2435","901339","5409891","10/14/2020 08:39:29","## Task Details
I created this task in order to exercise myself after [this Coursera specialization](https://www.coursera.org/professional-certificates/tensorflow-in-practice). Task is to create a Neural network to classify watermarked and not watermarked images. Dataset was creted by myself.

## Expected Submission
The solution should contain a notebook with the code of the model. 

## Evaluation
A good soultion should be better[ than the current ](https://www.kaggle.com/felicepollano/notebook9ed147da1f?scriptVersionId=44627886)
that has an accuracy of **0.995** and a validation accuracy of **0.924**","","Classification of Watermarked and not Watermarked Images","","","3"
"1317","767013","5418520","07/10/2020 15:06:51","## Task details
Let's all of us do prediction and who knows something nice can happen
## Expected Submission
You can submit the predictions on the test.csv that you have achieved by predicting the value or you can simply write a notebook, I will send the answer.csv after the season ends

## Evaluation
We will see the CSV file or the notebook and will give every one rank and virtual JSON medal üèÖ
every participant will get a rank and will have to download their own medals
## Task datail
Let's all of us do prediction and who knows something nice can happen
## Expected Submission
You can submit the predictions on the test.csv that you have achieved by predicting the value or you can simply write a notebook, I will send the answer.csv after the season ends

## Evaluation
We will see the CSV file or the notebook or pickle file and will give every one rank and virtual json medal üèÖ
every participants will get to know their
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict_The_Production","","","1"
"2043","867676","5420310","09/10/2020 21:12:43","You are providing with dataset and ask to create a recommodation using this dataset .","","Build a Recommodation system of english words","","","1"
"1388","778797","5421882","07/17/2020 02:31:32","Hi There,
I am complete noob with this ML/AI thing.
I am learning Python for data analysis.
Can someone help me with basic analysis methods/steps to go about organizing the steps.

Attached - Cancer data for registered patients / by year/gender/cancer codes/ deaths.

I would like someone to guide me with some analysis and some prediction.","","Cancer Data NZ","Cancer deaths by cases registered","08/31/2020 00:00:00","1"
"1839","845892","5435539","08/26/2020 12:00:55","## Task Details
We developed a project - Ottoman Document Parser, our project had everythin, great translation envorinment, a giant document data set for searchers to work on. But our Character recognition algorithm was weak, we only could recognize 60% of the letters on a document (not seperated characters), so you could show us how you define a model which recognizes the characters.

## Expected Submission
Define a model with high recognition rate. Expected submissions do not need to manipulate the data set. 

## Evaluation
High recognition rates are our priority, if two models X and Y has the same success rate in recognition, faster model will be considered as ""Better"".","","Character Recognition","Character recognition specialized for Ottoman Turkish","","0"
"1524","804948","5436400","07/29/2020 10:33:06","Data Cleaning and Visualization","","Data Visualization","","","0"
"1406","782892","5451525","07/17/2020 17:41:55","Create a dashboard to quickly gain insights into the most important aspects of NEFT transactions.","","Dashboard","Creative Dashboard by using python packages","","0"
"1407","782892","5451525","07/17/2020 17:45:26","**Identify:**
Small, Medium and Large banks
Raising and falling banks
High in volume, low in value transaction and vice versa.","","Clustering","Identification of failing and rising banks using Clustering.","","0"
"1592","792174","5459251","08/05/2020 04:06:25","## Task Details
Dummy Task

## Expected Submission
Notebook uses python library numpy to perform mathematical operations

## Evaluation
Pass/Fail base on the answer key sheet","","My 2nd Task","","","1"
"1593","792174","5459251","08/05/2020 06:19:33","## Task Details
Creating a Dummy task to test a QA script

## Expected Submission
Need to submit any submission using Python or R

## Evaluation
To check the creation of a task","","My 3rd Dummy task","","","0"
"1591","790898","5459251","08/05/2020 03:44:11","## Task Details
Square root of 25

## Expected Submission
A Kaggle notebook Submission uses python library numpy to perform mathematical operations

## Evaluation
a pass/fail grade based off of whether the answer is the same as the answer in our answer key

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Creating a Task1","","","0"
"1594","790898","5459251","08/05/2020 06:59:16","## Task Details
Dummy Task

## Expected Submission
Dummy Task


## Evaluation
Dummy Task","","Dummy Task3","","","2"
"1595","790898","5459251","08/05/2020 07:00:08","## Task Details
Dummy Task

## Expected Submission
Dummy Task

## Evaluation
Dummy Task","","Dummy Task4","","","0"
"1596","790898","5459251","08/05/2020 07:01:09","## Task Details
Dummy Task

asdfsdfs
## Expected Submission
Dummy Task


## Evaluation
Dummy Task","","Dummy Task5","","","1"
"1598","790898","5459251","08/05/2020 07:06:18","## Task Details
Dummy Task


## Expected Submission
Dummy Task


## Evaluation
Dummy Task","","Dummy Task7","","","0"
"1600","790898","5459251","08/05/2020 07:14:36","## Task Details
Dummy task 8

## Expected Submission
Dummy task 8

## Evaluation
Dummy task 8

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Dummy task 8","","","1"
"2096","825452","5544078","09/14/2020 15:28:15","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","This a a testing task","Hi This a testing task","","1"
"4872","818680","7698873","06/21/2021 08:32:55","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.  

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","testing","dataset","","0"
"1558","811560","5462549","08/02/2020 15:08:39","Your target is determinations of materials.","","Determinations of materials","","","1"
"2923","821698","5484076","12/13/2020 07:23:08","### Task Details
Create a notebook to represent the exploratory data analysis using the diabetes dataset by downloading and you can use it directly. 

### Expected Submission
Submit a notebook that represents the complete Exploratory Data Analysis with an effective report on it.


#### Further help
If you need additional inspiration, check out these existing high-quality tasks:

- https://www.kaggle.com/ruchi798/book-crossing-starter-notebook-and-eda (üìö Book-Crossing: Starter notebook and EDA by @ruchi798 )

- https://www.kaggle.com/ankitaguha/time-series-analysis-with-prophet-covid19 (üìà Time Series Analysis with Prophet: Covid19 by Ankita Guha)

- https://www.kaggle.com/vikasukani/video-game-sales-eda-visualizations-ml-models (üéÆ Video Game Sales EDA, Visualizations by @vikasukani )","","Perform EDA Analysis Report on Diabetes Dataset","With the help of Diabetes Dataset. Make a end to end Exploratory Data Analysis. Also generate a beautiful analysis report on it.","","1"
"3183","821698","6443899","01/13/2021 16:16:46","## Task Details
Use any model to get prediction of diabetes with highest accuracy

## Expected Submission
A notebook with the solution having prediction values close to 1.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Diabetes Prediction","Make a prediction when a person can get diabetes","","6"
"1563","812753","5493065","08/02/2020 21:05:46","Explore the Data.","","Time series forecasting for the Amazon Stock Prices.","","","1"
"1776","839238","5493065","08/20/2020 21:30:14","Indian road analysis","","Explore the data","","","0"
"1773","839119","5493065","08/20/2020 18:43:44","UFO Sightings","","Explore the data","","","0"
"1730","835354","5493065","08/17/2020 17:12:25","For Exploring the Weapon's Functionality","","Exploratory data analysis","","","1"
"1564","798902","5493065","08/02/2020 21:11:00","Explore it, try to Predict it.","","Predict the Future cases - Visualization and Time series forecasting","","","1"
"2259","842162","5494099","09/27/2020 06:36:20","# The data available here has details of sales and items available across various shops of the e-retail Shoppee. Now the task is to create a visualization to find sales patterns across various shops and explore features which dominate the sales.","","Shoppee Sales Data","","","0"
"1456","793168","5510958","07/22/2020 16:31:51","The linked dataset (https://usda.library.cornell.edu/concern/publications/hd76s004z?locale=en&page=5#release-items) has data predating this dataset. We didn't include them because only PDF records are available. We should process the pdfs and extract the data","","Extract pre-2001 data from PDFs","OCR Text Extraction","","0"
"6149","793602","5510958","09/21/2021 15:29:24","## Task Details
Explore the data and estimate the college equity level across the country.


## Expected Submission
Please submit notebooks to present the results discovered.


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Explore the Equity across Countries","","","1"
"1513","795813","5513209","07/29/2020 01:44:49","## Task Details
Our purpose is estimating the epitope region of SARS-CoV by only analyzing the B-cell dataset. 

We are addressed the problem of classifying peptides into two categories in this task: antibodies with inducing properties (positive) and antibodies without inducing properties (negative), similar to the paper we have published.

Using the data provided, predict the antibody valence of the SARS-CoV data. It has a target value, so you can experiment and train normally to develop a model. 

## Evaluation
1. Area Under the Curve(AUC)
    - AUC of epitope prediction for SARS using the Bcell dataset as the training set and the SARS dataset as the test set.

2. Documentation Quality
    - Reusable and easy-to-understand code","","Task 1 : SARS prediction with B-cell data","","","14"
"1514","795813","5513209","07/29/2020 01:58:46","## Task Details
Our purpose is estimating the epitope region of SARS-CoV2 spike protein (the cause of coronavirus disease 2019 (COVID-19)) utilizing SARS-CoV and B-cell dataset. You can utilize the information that there is a similarity between sequence of SARS-CoV and that of SARS-CoV2.

We are addressed the problem of classifying peptides into two categories in this task: antibodies with inducing properties (positive) and antibodies without inducing properties (negative), similar to the paper we have published.

Using the data provided, predict the antibody valence of the SARS-CoV2 data. It does not have a target value, so this is the challenge point of this task.

## Evaluation
1. Prediction results for SARS-CoV-2 dataset
    * List of epitope predictions for SARS-CoV-2 using the B-cell and SARS datasets as the training set and the SARS-CoV-2 dataset as the test set.

2. Documentation Quality
    * Reusable and easy-to-understand code","","Task 2 : Covid-19 prediction with B-cell and SARS data","","","16"
"2327","904743","5515417","10/04/2020 18:04:42","A company is interested in filtering HTML files in order to only find the ones that concern their business' field.
Those tenders are unstructured data out of which word frequencies can be extracted. 

Using those apparition's frequencies,  classify the tenders in main categories, subdivided into more precise categories.","","HTML files classification","Study on Keyword's Frequencies","01/25/2021 23:59:00","1"
"2002","839939","5521351","09/07/2020 15:12:47","# Task Details
Find out the distribution of attributes like ""views"", ""comments"",""likes"" and ""dislikes"".

# Expected Submission
A notebook with visual representations of the distribution of the attributes you have selected. This will be in the form of histograms.

# Evaluation
Clear visual representations of your results.","","Data distribution of different attributes","","","0"
"1871","821948","5521351","08/29/2020 09:17:27","I welcome everyone from Kaggle community to upload their codes and see the difference in output. Try different combinations of columns.

## Task Details
Develop machine learning model which help political parties to answer the crucial question ""How likely candidate will win the election ?""

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Develop a model to predict the chances of candidate to win in election","Help parties to answer the crucial question ""How likely candidate will win the election ?""","","0"
"1872","821948","5521351","08/29/2020 09:19:04","I am a big fan of Data Visualization, and it would be awesome to see different Notebooks, with different Style of Charts to represent the Lok Sabha Candidates :)","","Data Visualization","Lok Sabha Election Candidate list (2004 to 2019)","","0"
"1646","821948","5286036","08/09/2020 13:30:48","## Task Details
Create Data plots and tables to make the Dataset easier to analyse.

## Expected Submission
1) Sort the data based upon number of criminal cases.
- City with highest crimes.
- Candidates with most number of criminal cases.

2) Create a plot with education degree distribution.
- What is the percentage wise distribution of education qualification.
- Candidates with lowest and highest qualification.

Happy Visualizing :)","","Data Exploration and Visualization","Plots and Tables","","0"
"1905","852454","5522145","08/31/2020 14:00:44","## Task Details
Create a model in which it can perfectly evaluate if a member with given attributes is likely to come to the party or not

## Expected Submission
Yes or no answer

## Evaluation
the number of coding lines

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Party attendance estimation","there is a table full of names with their features. run a model to decide whether they would like to take part in the party or not","10/30/2020 23:59:00","0"
"2119","854926","5529272","09/16/2020 06:30:19","## Task Details
All questions can be categorized if applied NLP properly!

## Expected Submission
A new column that defines at least 5 categories!  

## Evaluation
A submission.csv file that has 500 rows of questions and a column that has categories in it.","","Use NLP to categorize questions","","","0"
"6926","854926","4665148","11/24/2021 23:16:01","Improve the quality of discussions online in Qura by detecting toxic content.

## Task Details
Specially, we want to build a predictive model that labels questions asked on Quora as either insincere or not.

Questions can be labeled as insincere because they can have a non-neutralImprove the quality of discussions online in Qura by detecting toxic content.

Specially, we want to build a predictive model that labels questions asked on Quora as either insincere or not.

Questions can be labeled as insincere because they can have a non-neutral tone, they can be aggressive and they could include sexual content or false information.

## Expected Submission
Notebooks and Datasets are welcomed.","","Improve the quality of discussions online in Quora by detecting toxic content.","","","0"
"2137","877539","5529272","09/17/2020 08:27:42","## Task Details
TO visualize and analyze the data

## Expected Submission
User Enters a year to predict the net migrants.","","EDA and Baseline linear regression analysis","","","0"
"1576","813339","5529272","08/03/2020 07:45:07","## Task Details

This task is created so that we may predict citations as well as impact factor, which may provide the researchers with much information to improve their research on basis of predictions.

## Expected Submission

Submission could be predictions using notebook or a much readable and data.

## Evaluation

As the expectations, the more accurately predictions and/or a much analyzed data.","","Predictions of citations.","The predictions of citations will enhance the accuracy of the predictions on impact factor and their rankings.","","4"
"2120","838613","5529272","09/16/2020 06:33:58","## Task Details
Time series may give a very good analysis of population growth.","","Use Time Series to analyse the population growth","","","0"
"1589","816362","5531429","08/04/2020 18:01:18","## Expected Submission
Use the data from the dataset and create a model that can predict the rest of the 2019-20 season games. Good luck!","","Predict the rest of the 2019-2020 season games","","","1"
"1759","837828","5540055","08/19/2020 19:20:24","With ML techniques, we can categorize the stars between main sequence stars and giants.

## Tasks you can do:

### Preprocess the Data
* Clean the Data (Null values, Errored values)
* Create `Amag` column via the equation
* Create a label column via `SpType`

### Analysis and Predict Star Type
* Try with Logistic Regression and create a decision boundary
* Performing deep neural network and create a decision boundary","","Predict Star Type","Using Absolute Magnitude and B-V Color Index to Identify Giants and Dwarfs","","1"
"1579","814490","5540518","08/03/2020 18:55:48","Trying backward elimination model","","Backward Elimination","","","0"
"1741","836546","5541622","08/18/2020 14:33:14","## Task Details
As we see our capabilities in space travel exploration and utilization increase could we possibly see the private sector begin to dominate the dare I say 'space'? my question to you is can you demonstrate the current trajectory between eh governmental agencies and Private companies and make any predictions as to how this trend will continue?","","Is the future of Space Private?","","","1"
"1532","806536","5549846","07/30/2020 05:38:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
K
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict next no","","","0"
"1539","808111","5554766","07/31/2020 05:05:45","Mental illness has been prevalent in the world, depression is one of the most common psychological problem i know and i would like to help as much as i can. Being a fan of Anthony Bourdain and Robin Williams, It has propel me to explore in this study. With the use of the Large amount of data tweets and Facebook post online i can use machine learning to data mine it and be able to produce a meaningful and useful outcome.

Social media generates countless data every day because of millions of active users share and communicate in entire community, it changes human interaction. For this project, I will be using Python and various modules and libraries.
The aim of the project is to predict early signs of depression through Social Media text mining. Below are the steps to run the python codes using the data sets uploaded in the repositories or you can download your own.

## Evaluation
What the result could mean? Postive, This mean that person is unlikely to have depression or anxiety. Neutral, This is the middle level wherein the user may or may not have depression but may also be more prone to being depress. At that stage the user may display some depression like symptoms. lasty, Negative is the lowest level where depression and anxiety symptoms are being detected through the users tweets. The more negative words the user uses mean the more negative emotion the tweet has.
This study is not yet perfect and im still aiming to improve it.

- Use Contextual Semantic segmentation
- Use Stopwords to increase accuracy of model
- Eliminating features with extremely low frequency
- Use Complex Features: n-grams and part of speech tags","","Detection of depressing tweets","","","3"
"2282","888492","5557330","09/29/2020 17:51:08","## Task Details
Predict IMDB rating.
## Expected Submission
Notebook

## Evaluation
Good Parameter selection will be +point

### Further help","","Data Visualization","","","0"
"1767","822223","5560753","08/20/2020 16:11:03","## Task Details
The task is to predict the next best move on the basis of data (most probably ML).","","Predict the ""Next"" best move","","","0"
"1768","822223","5560753","08/20/2020 16:12:28","## Task Details
The task is to predict the first best move on the basis of data.","","Predict the ""First"" best move","","","1"
"2210","889428","5563568","09/23/2020 13:43:07","## Task Details
There are over 50 relevant pieces of data in this dataset. Many of the fields seem to have a pattern with other statistics. 

## Expected Submission
Submit a notebook containing all the analysis of this dataset.

## Evaluation
The notebook will be judged upon the the observations mentioned and the visualisations provided and the 

### Further help
If you need additional inspiration, check out these existing high-quality notebook:
- https://www.kaggle.com/kaushiksuresh147/ipl2020-twitter-analysis-eda (EDA of IPL Tweets)","","Statistics Exploratory Data Analysis","Analyse the data with visualisations to see how the pattern of the statistics of the players","","0"
"1557","812147","5556763","08/02/2020 15:01:10","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.a

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","New Task2<a href=""http://evil.com"">here</a>","Click <a href=""http://evil.com"">here</a>","","0"
"2673","824859","5575832","11/11/2020 03:32:36","## Task Details
 Is an investigation of outliers or correlation key to understanding this data? Or does this dataset need a simple EDA? What information can you put together from the EDA?

## Expected Submission
Notebooks consisting of an EDA of the shuttle data is all I am looking for right now? 
BUT if you can add: (that would be great too!)
1. Is this data Normal or does it follow another distribution? Poisson, Weibull? 
2. Can you tell me about the outliers within this set?
3. Is there any correlation between the features and should any be ruled out?

## Evaluation
1. An excellent evaluation of this data would look into how other papers and investigators have used this data? See Further Help below.
2. A Very good evaluation of the data will provide more than a table of charts or even visualizations. It should include a very brief set of observations and then conclusions made.
3. A good workup will give just a set of graphics and/or tables describing the dataset.

### Further help
If you need additional inspiration, check out these papers:
- https://archive.ics.uci.edu/ml/datasets/Statlog+%28Shuttle%29","","Statlog (Shuttle) Data Set: EDA Needed","EDA or is Outlier Detection and Correlation needed?","","2"
"2089","872127","5581490","09/13/2020 23:24:19","## Task Details
Classify whether some images in a testset contain litter or not.","","image classification","","","0"
"2247","873109","5592707","09/26/2020 06:47:57","## Task Details
Predict the sales","","Predict the sales","","","0"
"2152","879378","5592707","09/18/2020 14:02:30","# Task
Show the trends how the population grows","","Predict the population for upcoming years","Show the trends how the population grows","","0"
"2257","883852","5592707","09/27/2020 03:48:52","## Task Details
Predict the population for upcoming years","","Predict the population for upcoming years","","","0"
"2109","869845","5592707","09/15/2020 13:51:43","## Task Details
To find what factors affect the price of used car and on what basis it is calculated","","Analyze on what basis the used car price fixed","","","0"
"2110","865705","5592707","09/15/2020 13:54:29","## Task Details
To find out how to classify ads vague or not","","How do we classify an ad as vague or not","","","0"
"1762","838759","5598413","08/20/2020 11:49:34","This task is an example for reinforcement learning. The data gives the no. of times a person clicked on an add which was shown to him.
You have to decide which is the best and the most effective add and in the future if an show add are the chances of it being clicked are high or low","","Decide the best add","","","0"
"2263","895817","5607372","09/27/2020 12:18:46","## Task Details
Explore the data and visualize where the money is going, when, and on what.

## Expected Submission
We welcome Notebook visualizations. We also welcome ML insights.

## Evaluation
Solutions with unique insights and striking visualizations are preferred. 

### Further help
If you need additional information, please reach out!","","Where is the money going?","Tracking expenditure across time and product","","0"
"1672","826272","5607633","08/11/2020 15:53:04","**Task Details
**The outbreak of Covid-19 has developed into a major international crisis, and it has severely impacted important aspects of daily life. For example:

Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether.
Grocery stores: In highly affected areas, people are starting to stock up on essential goods.
A strong model that predicts how the virus could spread across different countries and regions may be able to help mitigation efforts. The goal of this task is to build a model that predicts the progression of the virus throughout for the further months.

**Expected Submission
**Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

With this model, you should produce a table in the following format for all future days of March (similar to covid_19_data.csv)

ObservationDate: Observation date in mm/dd/yyyy
Country: Country
Confirmed: Cumulative number of confirmed cases
Deaths: Cumulative number of deaths cases
The notebook should be well documented and contain:

Any steps you're taking to prepare the data, including references to external data sources
Training of your model
The table mentioned above
An evaluation of your table against the real data. Let's keep it simple and measure Mean Absolute Error.

**Evaluation
**This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

Our team will be looking at:

Accuracy - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?
Data Preparation - How well was the data analyzed prior to feeding it into the model? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought provoking, and fresh all at the same time.
Documentation - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.
This task is not strictly about getting the best submission score. Since it involves data that changes in real-time, the most accurate solution would only require uploading the real historic data on the last day of March, but that‚Äôs besides the spirit of this Task. We‚Äôre looking for genuine approaches to building models on a real problem that can serve as learning examples for our community","","Predict the future cases of coronavirus","We can help with the future planning of lockdown measure by predicting its spread","","0"
"2177","885648","5622272","09/21/2020 08:27:05","## Task Details
The task requires you to develop a model to classify the input image as that of a male or female sex. The input could be given through the file input method or using the webcam frames as real time input

## Expected Submission
A sample frame/input picture with the classifier model's output in frame.

##Optional extension
Display the count of male and female faces given as real time input through the webcam.","","Male and Female Face classifier","","","1"
"2016","860945","5625251","09/08/2020 15:47:01","## Task Details
This data was gathered just to fulfill my curiosity. Not useful for ML but still fun to play around with if you're interested in the topic.

## Expected Submission
Is the reality merely an a priori adjunct of non-naturalistic ethics?","","Architectural Offices in Finland","","","1"
"1706","832228","5627494","08/15/2020 17:33:17","## Task Details
you have to do a complete regression analysis on the given data and found out how these variable are related to each other then forecast the value of next years using Excel as your primary software .  

## Expected Submission
take your time
## Evaluation
The solution should be understandable for those people also who are not from mathematics background and be presentable.","","Perform regression analysis","","","0"
"2095","872854","5635484","09/14/2020 12:48:13","Do an exploratory data analysis of the data sets.
Try to find any insights of the data of players.
Determine the fantasy team to be chosen based on the data","","Create a Fantasy Team IPL","","","29"
"2126","876662","5636979","09/16/2020 14:29:03","Please use this dataset to predict disease based on symptoms.
And please help me how can using this dataset we can predict disease anything will be helpful.","","Disease Prediction","","","1"
"1731","835787","5638089","08/17/2020 23:50:00","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
heello
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","taskqone","","08/20/2020 00:00:00","0"
"2219","891129","5645048","09/24/2020 12:24:23","## Task Details
This task is just to get clear understanding about hospitals in India.","","Visualize the Data","Try to visualize the given data.","","0"
"2221","891144","5645048","09/24/2020 12:32:40","## Task Details
This task is just to get clear understanding about hospitals in India.","","Visualize the Data","Try to visualize the given data.","","0"
"2217","889621","5645048","09/24/2020 12:10:48","## Task Details
This task is just to get clear understanding about hospitals in India.","","Visualize the Data","Try to visualize the given data.","","4"
"2218","891082","5645048","09/24/2020 12:12:08","## Task Details
This task is just to get clear understanding about hospitals in India.","","Visualize the Data","Try to visualize the given data.","","0"
"2298","870833","5645048","10/01/2020 03:05:33","## Task Details
This task is just to get clear understanding about population in Tamil Nadu.","","Visualize the Data","Try to visualize the given data.","","0"
"2100","873567","5648058","09/14/2020 19:14:23","Task Details
Try to predict what is the next cumulative sum ('R') for each number.
Or, try to predict what is the next number to be played.","","Next winning number","","","1"
"1788","840769","5661079","08/22/2020 07:11:11","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Need 35+ queries","","Advanced historical","","08/23/2020 00:00:00","0"
"1911","849863","5661479","09/01/2020 09:17:52","## Task Details
Could this data extract crucial and required elements for business understanding?","","Create the best possible visualization with the attached data","","","0"
"2526","871712","6010363","10/24/2020 07:21:57","Good morning Fatima,
I've been reading your post and the provided dataset.
I think it is very complete.
I was interested in knowing which parameters are measured with the different sensors of the dataset.
Can you give me a little more information?
Thank you very much","","Sensors Dataset","","","1"
"7016","845444","7579904","12/05/2021 18:26:12","a.	Study the dataset, identify the independent and target variables. 

b.	Check for missing values in the dataset and take action accordingly.

c.	Visualize outliers for each variable using boxplots. Explain how you would set C parameter value in SVC() (or any other SVM classifier you are using) to handle the outliers properly. You may relate your answer to setting soft/hard margins by C parameter and how it can affect classification of outliers.

d.	Check the distribution of the target variable and explain whether it is a balanced or imbalanced dataset. Take proper action accordingly.

e.	Then, fit a SVM classifier on the training data with gamma = auto and C/kernel according the table below. 

kernel	linear	rbf	sigmoid	poly
                   1	          1    	1        1
C               100	100	     100	100
	          500	500	      500	500


f.	Predict the target variable on the test dataset.

g.	Compare the performance e.g., Accuracy of classifications in step (e).

h.	Comment on how changing C affect the performance. Tip: you may look at the performance of the algorithm while kernel is fixed and C is variable.

i.	Can you identify any overfitting in classifications from step (e)? Please explain.","","Use SVC on pulsar star dataset","","12/07/2021 23:59:00","0"
"2294","893837","5681768","09/30/2020 20:04:37","Hi All, Let's explore the dataset and find the expanded Airport and the maximum profitability of the Airline's industry","","Airline Company with Maximum Profitiblity","","","1"
"2291","900781","5681768","09/30/2020 19:58:24","Netflix is known for its strong recommendation engines. They use a mix of content-based and collaborative filtering models to recommend tv shows and movies. In this task, one can create a recommendation engine based on text/description similarity techniques.","","What shall we watch on Netflix ?","","","0"
"2074","869864","5685523","09/12/2020 11:03:07","Explore and Visualize the data that's available for Global Land and Ocean Temperature.","","Exploration and Visualization","","","1"
"1954","858803","5713982","09/05/2020 08:38:56","## Task Details
your task is to create model which predict the production value. also firstly you have to preprocess the data and remove some less usable variable. production variable is a dependent variable. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Gujarat Crops Production","","","0"
"2018","864553","5740870","09/08/2020 18:23:16","Do a good EDA on the Dataset also Build a model to evaluate the insight automatically.

Important Points:-
All the key fields like: Platform, Type, Medium, Sub Channel, Audience, Creative have already been mapped to the data.
                Platform: Marketing platforms on which campaigns are running majorly: Google Ads and Facebook Ads 
                Type: Type of campaign, In this data only Gogole search and Facebook Conversion campaigns have boon considered
                Medium: The way we are connecting to people in our Marketing campaigns either via some Keywords or Creatives.
                Sub Channel: Sub channel is under Google Search which type of keywords have been targeted ot in Facebook which on subchannel we are targeting
                Audience: Multiple Type of audiences are getting targeted in different campaigns an dthose have been encrypted as Audience 1,2,3
                Creative: This if for facebook what type of Image/Video/Carousel we are using in our Ads.","","Carry out the EDA & Build a model to evaluate the insight automatically","","","0"
"2246","893744","5758242","09/26/2020 06:43:18","## Task Details

Generate a new episode script.","","Script Writer","","","0"
"2260","895468","5789078","09/27/2020 08:02:33","### Updates required for ""***Coursera Project Network Resources"" ***","","Coursera Project Network Resources","","","0"
"2145","876817","5789078","09/18/2020 04:40:33","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
UPdate","","Update","","09/19/2020 23:59:00","0"
"2288","900595","5802952","09/30/2020 16:25:08","The file GOOG.csv contains the stock prices of the Google company for three years.
(i)	Convert the series into a time series and choose only the close stock price.
(ii)	Check out for the stationarity and if it is not, then make it stationary.
(iii)	Apply ARMA (p.q) model and find out the best values for p and q.
(iv)	Carry out the residual analysis and check the goodness of fit.
(v)	Predict for the next 20 days for the given data and plot the result.
(vi)	Find out the p,d,and q in ARIMA model","","Google dataset Regression","","10/01/2020 23:59:00","0"
"2326","899974","5817479","10/04/2020 15:33:23","## Task Details
Stats are a way to gauge how a stand might stack up against the bunch but which stand among these stand above the rest?

The stats rating hierarchy is:

 None    &lt;   E    &lt;   D   &lt;    C   &lt;   B    &lt;    A    &lt;    Infi

The following are the 3 tasks:
1. Top 10 stands in their respective categories with their ratings (Compare with other stats in case of identical values).
2. Top 10 Stands with the best overall ratings with their ratings (Do not consider DEV for this).
3. Top 10 Stands with potentially best ratings with their ratings (Consider DEV for the changes in attributes but not for overall values)

** DEV criteria (for task 3):

* DEV is an interesting stat this can be considered as a supporting attribute. 
The way this stat would work is that it can be used to upgrade the rating of any other stat (to a max ranking of A) while reducing the rating of the DEV stat (Until value reaches E).
* None value cannot be increased.
* Stands with DEV infi can be used any number times to upgrade any stat upto a max of rating A
For eg consider the following attributes,

PWR=B SPD=B RNG=B PER=B PRC=B DEV=B

DEV can be used to boost 3 times from B--&gt; C--&gt; D--&gt; E

One form of updated attributes could be,

PWR=A SPD=A RNG=A PER=B PRC=B DEV=E (3 stats have been manipulated)

## Expected Submission
Provide solutions to these tasks in a notebook format.

## Evaluation
I Will not be checking it for any correct answers (Since I'm a noob at this as well üëÄ ). But will definitely give my thoughts on your submissions. Who knows perhaps the community might share their views on this as well. üôå

Thanks!

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Best Stands among the Bunch ?!","","","1"
"2186","886369","5820347","09/21/2020 15:17:19","Annalyse the data, you will learn a lot of things about your favorite history.","","Analyse","Professional Analyse","11/28/2021 23:59:00","1"
"2184","886310","5820542","09/21/2020 15:15:00","## Task Details
Make it public and play around!

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Send link to the professor","","","0"
"2185","886341","5820546","09/21/2020 15:15:26","This is just a test","","Test_1","","","0"
"2387","892312","5841049","10/08/2020 22:58:34","## Task Details
As of September 2020, this data set contains over 18 thousand airbnb listings in Milan. The purpose of this task is to predict the price of Milan Airbnb rentals based on the data provided and any external dataset(s) with relevant information

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price:

## Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict Milan Airbnb Rental Prices","","","2"
"2388","892312","5841049","10/08/2020 23:02:15","## Task Details
As of September 2020, this data set contains over 18 thousand airbnb listings in Milan. The purpose of this task is to predict the price of Milan Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

## Expected Submission
Users should submit a csv file with each listing from the data set and the model-predicted price","","Analysis, Visualization, Prediction","","","1"
"2285","899984","5865719","09/30/2020 08:08:36","## Task Details
Predict the concentration of PM2.5 in 24 hours.

## Expected Submission
Please share the technique you used, the predictions you obtained and, optionally, the extra insights you obtained during the analysis.

## Evaluation
1. The evaluation metric for the predictions is MSE. 
2. Interpretable models are preferred: the final goal is to explain the phenomenon of air pollution to citizens, to raise awareness and promote positive actions.
3. Code quality/maintainability. We are gathering new data on air quality, so other datasets similar to this one will come out on a regular basis. As a consequence, the possibility of updating the code is highly valuable.","","Forecast time series","","","2"
"2309","903033","5866812","10/02/2020 13:07:40","## Task Details
Data work wonders, if it is processed into insights. Gaining insights from data depends on how tidy a data is. 

The task here is to tidy this data, which is in wide format into a tidy (long) format using the Tidyverse in R.

## Expected Submission
Please submit a CSV file of the tidied dataset, along with R codes used.

## Evaluation
A good solution is one where the dataset has been successfully converted into a tidy form, has a long format and has informative column headings. Succinct and commented R codes is also expected.","","Data tidying","Converting into tidy data format using R Tidyverse","10/24/2020 23:59:00","0"
"2301","901414","5557330","10/01/2020 11:04:25","Price range is a categorical variable which you need to predict","","Predict Price Range","","","0"
"2336","907232","5890776","10/05/2020 13:15:14","The set can be differentiate with 100% accuracy based on our algorithm. How close can you get?","","Find Specificity","","","0"
"2352","909270","5900883","10/06/2020 18:42:39","Try to eliminate the noise as much as possible from this audio file. Your performance is measured by the mean square error with respect to the original audio file.","","Minimize mean square error","","11/07/2020 23:59:00","0"
"3675","1128757","1902","03/06/2021 15:08:52","## Task Details
The brightness of a star is more or less determined by how far it is from us, and its spectral characteristics (i.e. color). Because we have fairly accurate measurements of distance thanks to Gaia, it should be possible to determine if a star is substantially dimmer or brighter than expected.

## Expected Submission
* The difference in brightness (in magnitudes) between what we observe and what we expected to observe.
* An standard error estimate. (The error may depend, for example, on parallax error and magnitude error.)

## Evaluation
Magnitude RMSE is what tells you how accurate the model is. But the point is not to get the most accurate machine learning model. It's up to the researcher to determine what the purpose of the model is. For example, you might include position features as part of the model, which would improve model accuracy, but it would dilute any regional anomalies, which may be of interest.

Are there line of sight artifacts? Can they be corrected? Are there other systematic dependencies that should be corrected?","","Brightness model","","","0"
"3678","1196462","1902","03/06/2021 18:09:12","## Task Details
Stars whose brightness changes at a century scale are potential SETI targets. Are stars with anomalous DASCH trends just statistical outliers, but otherwise ordinary? Are they just variable stars?

## Expected Submission
A comparison of characteristics of stars with anomalous trends with stars of a similar kind.

## Evaluation
DASCH data is affected by systematics, and the standard errors are not reflective of the observed slope variance in practice. Also, comparisons should be with stars that are similar (e.g. same distance, galactic latitude and color.) A proper analysis should be confound-aware.","","Are stars with anomalous trends different?","","","0"
"5579","1247367","5194950","08/06/2021 12:47:49","## Task Details
For centuries chess players have debated which openings provide the most advantage against stronger, equal, and weaker opposition; for slow and fast time controls; for White or Black; etc.

## Expected Submission
Solutions should contain chess openings in the dataset along with either qualitative or quantitative information.

## Evaluation
Good solutions could apply well to predicting results of games either via cross-validation or for games not included in the dataset.  Use your imagination.","","Measure opening strength","","","0"
"3642","1188571","10302","03/02/2021 16:28:28","## Task Details
Read the dataset and perform exploratory data analysis.

## Expected Submission
A Notebook is expected containing the data import, turning it into tidy data and carrying out relevant visualization.

## Evaluation
A consice solution containing all the relevant visualization would be considered good. Just plotting everything without giving thought to relevent features would not be appropriate.

The data is not in tidy format and making it tidy could be useful as well.

There is geolocation data as well. Some maps may not be out of order.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Perform EDA (Exploratory Data Analysis)","","03/10/2021 23:59:00","0"
"3505","1159053","18463","02/15/2021 15:14:48","## Task Details
How many seconds, how many words. Please describe this text to speech dataset","","Describe Speech Dataset","","","0"
"3674","1196009","18463","03/06/2021 10:24:56","Describe over the 20th and 21th centuries , the evolution ofrole of women in movies","","Women In Movies Analyze","Women In Movies Analyze","","0"
"3685","1198372","18463","03/07/2021 19:50:51","What are the mail variables related to the COVID-19 lockdown, 
pain interference, resilience, and perceived well-being between women with and without psychological trauma prior to the outbreak","","What are the mail variables related to the COVID-19 lockdown,","pain interference, resilience, and perceived well-being between women with and without psychological trauma prior to the outbreak","","0"
"3690","1198449","18463","03/07/2021 21:19:41","Use NFNets models on Kaggle Image Classification Datasets 
and show the increment of performances","","Use NFNets models on Kaggle Image Classification Datasets","and show the increment of performances","","0"
"3727","1191134","18463","03/10/2021 20:42:42","Analyze paintings by using Embeddings from CNN  and do Clustering","","Analyze paintings by using Embeddings from CNN  and do Clustering","","","0"
"3694","1199006","18463","03/08/2021 16:42:42","Make a Graphical Representation of Twitter Relations between users

From edge.csv and Nodes.csv find influencers","","Make a Graphical Representation of Twitter Relations between users","","","2"
"3693","1199007","18463","03/08/2021 16:31:42","Could you identify Anomaly in Time Series in Forex Ticks ?","","Identify Anomaly in Time Series","","","0"
"3697","1200159","18463","03/08/2021 20:01:46","Social-media can contribute to building up adolescents' relationships, but they might also bring negative exclusionary experiences. Being excluded is a subtle yet hurtful form of relational aggression, which affects people's psychological wellbeing, especially during developmental stages. In this study, we (1) analyzed the effects of social-media exclusion adapting the Ostracism Online paradigm to a cohort of Italian preadolescents (Mage = 11.47, 53% girls) and (2) tested the efficacy of two potential recovery strategies (i.e., social bonds vs. social surrogate).

Method
Inclusionary status was manipulated through the number of ‚Äúlikes‚Äù participants received on a fictitious online social network. In the exclusion condition, participants received fewer likes than everyone else. In the inclusion condition, participants received a similar number of likes of other users. Then, all participants were asked to think of a significant positive relationship with a family member (social bonds), a celebrity (social surrogate), their present moment thoughts (control).","","Analyze id Preadolescents who received fewer likes than others","reported higher levels of need threat (i.e., belong, self-esteem, meaningful existence, but not control) and negative emotions","","2"
"3698","1200238","18463","03/08/2021 21:25:25","Knowing the historical yield patterns of major commodity crops, including the trends and interannual variability, is crucial for understanding the current status, potential and risks in food production in the face of the growing demand for food and climate change. We updated the global dataset of historical yields for major crops (GDHY), which is a hybrid of agricultural census statistics and satellite remote sensing, to cover the 36-year period from 1981 to 2016, with a spatial resolution of 0.5¬∞. Four major crops were considered: maize, rice, wheat and soybean. The updated version 1.3 was developed and then aligned with the earlier version 1.2 to ensure the continuity of the yield time series. Comparisons with different global yield datasets and published results demonstrate that the GDHY-aligned version v1.2‚Äâ+‚Äâv1.3 dataset is a valuable source of information on global yields. The aligned version dataset enables users to employ an increased number of yield samples for their analyses, which ultimately increases the confidence in their findings.","","Forecasting yields for major crops for next 10 years","Historical yields for major crops 1981‚Äì2016.","","2"
"3718","1201976","18463","03/09/2021 19:38:08","Source:

Tong Wang, tong-wang '@' uiowa.edu, University of Iowa
Cynthia Rudin, cynthia '@' cs.duke.edu, Duke University


Data Set Information:

This data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver. For more information about the dataset, please refer to the paper:
Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393.


Attribute Information:

destination: No Urgent Place, Home, Work
passanger: Alone, Friend(s), Kid(s), Partner (who are the passengers in the car)
weather: Sunny, Rainy, Snowy
temperature:55, 80, 30
time: 2PM, 10AM, 6PM, 7AM, 10PM
coupon: Restaurant(&lt;$20), Coffee House, Carry out & Take away, Bar, Restaurant($20-$50)
expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours)
gender: Female, Male
age: 21, 46, 26, 31, 41, 50plus, 36, below21
maritalStatus: Unmarried partner, Single, Married partner, Divorced, Widowed
has_Children:1, 0
education: Some college - no degree, Bachelors degree, Associates degree, High School Graduate, Graduate degree (Masters or Doctorate), Some High School
occupation: Unemployed, Architecture & Engineering, Student,
Education&Training&Library, Healthcare Support,
Healthcare Practitioners & Technical, Sales & Related, Management,
Arts Design Entertainment Sports & Media, Computer & Mathematical,
Life Physical Social Science, Personal Care & Service,
Community & Social Services, Office & Administrative Support,
Construction & Extraction, Legal, Retired,
Installation Maintenance & Repair, Transportation & Material Moving,
Business & Financial, Protective Service,
Food Preparation & Serving Related, Production Occupations,
Building & Grounds Cleaning & Maintenance, Farming Fishing & Forestry
income: $37500 - $49999, $62500 - $74999, $12500 - $24999, $75000 - $87499,
$50000 - $62499, $25000 - $37499, $100000 or More, $87500 - $99999, Less than $12500
Bar: never, less1, 1~3, gt8, nan4~8 (feature meaning: how many times do you go to a bar every month?)
CoffeeHouse: never, less1, 4~8, 1~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)
CarryAway:n4~8, 1~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?)
RestaurantLessThan20: 4~8, 1~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than $20 every month?)
Restaurant20To50: 1~3, less1, never, gt8, 4~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of $20 - $50 every month?)
toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 15 minutes)
toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 25 minutes)
direction_same:0, 1 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)
direction_opp:1, 0 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)
Y:1, 0 (whether the coupon is accepted)


Relevant Papers:

Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393.","","Find relevant features why person whether he will accept the coupon if he is the driver.","Find relevant features why person whether he will accept the coupon if he is the driver.","","5"
"3721","1202117","18463","03/10/2021 07:31:27","Clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis

The data set is designed for research purpose only. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.","","Clustering with network and side information","studying influence in the citation network, finding the most influential papers, topic modeling analysis","","1"
"3723","1202805","18463","03/10/2021 11:20:23","Identify size by chromosone, number of genes, make visualization of these data

Research in the genomic sciences is confronted with the volume of sequencing and resequencing data increasing at a higher pace than that of data storage and communication resources, shifting a significant part of research budgets from the sequencing component of a project to the computational one.","","Homo Sapiens Genome Contest","","","0"
"3745","1203997","18463","03/12/2021 16:55:44","GADM provides maps and spatial data for all countries and their sub-divisions. You can browse our maps or download the data to make your own maps.

Use these datas in your project and make great vizualisations","","Use these datas in your project and make great vizualisations","","","0"
"3746","1207278","18463","03/12/2021 19:34:30","Compare Data Portals listing

Make graphical representation of Data Portals","","Compare Data Portals listing","","","0"
"3753","1209195","18463","03/13/2021 20:10:55","The Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for one‚Äôs own analysis. The database covers approximately 30,000 power plants from 164 countries and includes thermal plants (e.g. coal, gas, oil, nuclear, biomass, waste, geothermal) and renewables (e.g. hydro, wind, solar). Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. It will be continuously updated as data becomes available.

The methodology for the dataset creation is given in the World Resources Institute publication ""A Global Database of Power Plants"". Data updates may occur without associated updates to this manuscript.","","Visualize on Map the position of power plants around the world","","","0"
"3754","1209205","18463","03/13/2021 20:19:20","Train an algorithm based on this news dataset","","Train an algorithm based on this news dataset","","","0"
"3757","1209245","18463","03/13/2021 21:10:15","Open-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to ``back-off"" to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.","","Find similarity questions from this dataset","","","0"
"3758","1209258","18463","03/13/2021 21:22:17","Propose a benchmark model to identify Malware.

MTA-KDD'19
This repository contains additional material for the paper ""MTA-KDD'19: A Dataset for Malware Traffic Detection"".

Additional material
The additional material for the paper can be found here. More details about MTA-KDD'19 can be found here.

Merging Malware and Legitimate classes
url = 'http://https://github.com/IvanLetteri/MTA-KDD-19/blob/master/'

dfMTA = pd.read_csv(url+'datasetLegitimate33featues.csv')
dfLEG = pd.read_csv(url+'datasetMalware33featues.csv')
dfComplete = pd.concat([dfMTA, dfLEG])
dfComplete.describe()
To enhance performance, it is recommendable to use a more complete scikit-learn pipe that implements normalization and feature selection.

import pandas as pd
import pandas_profiling as pp
import IPython
import scipy.io
import numpy as np
#from sklearn.model_selection import cross_validate
from sklearn.model_selection import StratifiedKFold
from sklearn import svm
from sklearn.feature_selection import f_regression, mutual_info_regression

# visual libraries
from matplotlib import pyplot as plt
from matplotlib import gridspec

import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D 
plt.style.use('ggplot')

from keras.models import model_from_json
from keras import backend as K
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.models import Sequential
from keras.optimizers import Adam, SGD
from keras.utils.vis_utils import plot_model
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, confusion_matrix
from sklearn.model_selection import train_test_split
Cite
If you use this work, please cite the following paper: I. Letteri, G.D. Penna, L.D. Vita, M.T. Grifa. ""MTA-KDD'19: A Dataset for Malware Traffic Detection."", 2020, Keywords: Malware analysis, Network Traffic, Machine Learning, Malware Detection

@inproceedings{itasec2020,
  author    = {Ivan Letteri and Giuseppe {Della Penna} and Luca Di Vita and Maria Teresa Grifa},
  editor    = {Michele Loreti and Luca Spalazzi},
  title     = {MTA-KDD'19: {A} Dataset for Malware Traffic Detection},
  booktitle = {Proceedings of the Fourth Italian Conference on Cyber Security, Ancona, Italy, February 4th to 7th, 2020},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {2597},
  pages     = {153--165},
  publisher = {CEUR-WS.org},
  year      = {2020},
  url       = {http://ceur-ws.org/Vol-2597/paper-14.pdf},
  timestamp = {Mon, 27 Apr 2020 16:53:46 +0200},
  biburl    = {https://dblp.org/rec/conf/itasec/LetteriPVG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}","","Propose a benchmark model to identify Malware","","","0"
"3766","1209739","18463","03/14/2021 06:02:11","WikiTableT contains Wikipedia article sections and their corresponding tabular data and various metadata. WikiTableT contains millions of instances while covering a broad range of topics and a variety of kinds of generation tasks.","","Analyze WikiTableT (EDA, make relevant graphics)","","","0"
"3767","1209812","18463","03/14/2021 06:55:16","Visual Genome is a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language.
Explore our data:
zebra
throwing frisbee, helping, angry
108,077 Images
5.4 Million Region Descriptions
1.7 Million Visual Question Answers
3.8 Million Object Instances
2.8 Million Attributes
2.3 Million Relationships
Everything Mapped to Wordnet Synsets","","Analyze Visual Genome","","","0"
"3773","1210792","18463","03/14/2021 17:12:26","The ProofWriter dataset contains many small rulebases of facts and rules, expressed in English. Each rulebase also has a set of questions (English statements) which can either be proven true or false using proofs of various depths, or the answer is ‚ÄúUnknown‚Äù (in open-world setting, OWA) or assumed negative (in closed-world setting, CWA).

The dataset includes full proofs with intermediate conclusions, which models can try to reproduce.

The dataset supports various tasks:

Given rulebase + question, what is answer + proof (w/intermediates)?
Given rulebase, what are all the provable implications?
Given rulebase + question without proof, what single fact can be added to make the question true?
Here is a simple example from the depth-2 dataset D2(OWA):

Rulebase:

triple1: The cow is big.  
triple2: The cow needs the dog.  
triple3: The dog sees the rabbit.  
triple4: The rabbit chases the cow.  
triple5: The rabbit chases the dog.  
triple6: The rabbit is big.  
triple7: The rabbit sees the dog.  
rule1: If the cow is blue and the cow needs the rabbit then the cow needs the dog.
rule2: If the cow chases the dog then the cow sees the rabbit.
rule3: If something is big then it chases the dog.
Question with answer + proof:

Question: The cow sees the rabbit?
Answer: true
Proof: (((triple1) -&gt; (rule3 % int2))) -&gt; (rule2 % int1) ; with int1 = The cow sees the rabbit. ; int2 = The cow chases the dog.
Generate all implications (only 2 in this case, can be as high as 21 in D5 theories):

All implications: The cow chases the dog. The cow sees the rabbit.
Use abduction to find a missing fact which will make something true:

Question to make true: The dog chases the dog.
Missing fact: The dog is big.   (denoted tripleM)
Proof: (tripleM) -&gt; rule3","","Question Answering Model","","","0"
"3774","1210796","18463","03/14/2021 17:16:24","Automatic extraction of forum posts and metadata is a challenging task since forums do not expose their content in a standardized structure. Harvest performs this task reliably for many web forums and offers an easy way to extract data from web forums.","","Make a demo scraper of public forum","","","0"
"3776","1211073","18463","03/14/2021 20:58:00","WBL 2020 is the sixth in the series of biannual reports measuring gender differences in the law. In 2019 a study was released which piloted a new index that aggregates 35 data points across 8 scored indicators. The WBL index scores are based on the average of each economy‚Äôs scores for the 8 topics included in this year‚Äôs aggregate score. A higher score indicates more gender equal laws. The dataset was expanded in 2020 to include historical information dating back to 1970. This file contains Women, Business and the Law (WBL) data for 190 economies for 1970 to 2019 (reporting years 1971 to 2020). The file from 2019 study has been removed as the current file contains the most current and accurate data. The scores for previous years have been recalculated to account for data revisions and methodology changes. For more information about the methodology for data collection, scoring and analysis, visit http://wbl.worldbank.org.","","Analyze this dataset (EDA)","","","0"
"3777","1211086","18463","03/14/2021 21:05:34","DESCRIPTION
Data from the International Olympic Committee's factsheet on women in the olympic movement, 1900-2018
SUMMARY
Includes 3 tables from the 2018 edition of Factsheet: Women in the Olympic Movement:
Introduction of Women Sports
Women‚Äôs Participation in the Olympic Winter Games
Women‚Äôs Participation in the Games of the Olympiad

Source: Factsheet: Women in the Olympic Movement (Update ‚Äî October 2018)","","Analyse this dataset (EDA style format restitution)","","","0"
"3778","1211154","18463","03/14/2021 22:04:42","DESCRIPTION
Women in Power - gender inequality in the context of political appointments and parliamentary representation
SUMMARY
Original Visualization
FEMALE POLITICAL REPRESENTATION WORLDWIDE.png
Visualization provided by OperationFistula.org

About this Dataset
SOURCE ARTICLE: Proportion of seats held by women in national parliaments
DATA SOURCE: World Bank

Objectives
What works and what doesn't work with this chart?
How can you make it better?","","Analyez this dataset (EDA)","","","1"
"3787","1215377","18463","03/16/2021 16:05:48","frequence","","Project in France map the localization of 5G antenna.","","","1"
"3807","1219910","18463","03/18/2021 19:43:56","Analyze Digital Peter Dataset (EDA)","","Analyze Digital Peter Dataset (EDA)","","","0"
"3812","1221288","18463","03/19/2021 14:17:59","Compare countries by happiness and other human metrics","","Compare countries by happiness and other human metrics","","","0"
"3813","1221324","18463","03/19/2021 15:48:56","Compare countries by happiness and other human metrics","","Compare countries by happiness and other human metrics","","","1"
"3139","1081247","18463","01/06/2021 09:28:30","As the situation surrounding COVID-19 continues to evolve, we‚Äôre seeing how important trails and other active transportation infrastructure are in creating strong, healthy, resilient communities. In the last few weeks, we seen unprecedented numbers of people turning to trails and walking and biking for physical activity, respite and transportation. As a result, trail managers are facing unprecedented challenges to respond to the extreme increase in demand, while simultaneously striving to balance public health and safety in an era of social distancing. Watch our rapid-response webinar to hear from trail leaders in COVID-19 hotspots who are facing these challenges in real time.","","Vizualisation of evolution of Public transports in 2020","The Great Bicycle Boom of 2020","","1"
"2482","927977","18463","10/18/2020 19:46:26","Compare model from https://www.kaggle.com/uciml/iris (original data)
and augmented data : https://www.kaggle.com/mathurinache/iris-augmented/","","Compare performance","Use CTGan data","10/31/2021 23:59:00","0"
"2490","927981","18463","10/19/2020 14:01:16","Compare model from https://www.kaggle.com/avocado-augmented (original data)
and augmented data : https://www.kaggle.com/mathurinache/avocado-augmented/","","Compare Performances","Compare Performances","12/31/2021 23:59:00","0"
"2489","927982","18463","10/19/2020 13:51:10","Compare model from https://www.kaggle.com/adult-census-income (original data)
and augmented data : https://www.kaggle.com/mathurinache/census-augmented/","","Compare performance","Compare performance","12/31/2021 23:59:00","1"
"3974","1253000","18463","04/05/2021 19:04:58","Create EDA for Campsite Negotiation Dialogues for Automatic Negotiation Systems","","Create EDA for Campsite Negotiation Dialogues for Automatic Negotiation Systems","","","0"
"3984","1253893","18463","04/06/2021 16:57:16","Create Simpsons Image Classification","","Create Simpsons Image Classification","","","1"
"3882","1236208","18463","03/27/2021 17:35:01","Analyze European Court of Human Rights Cases (EDA approach)","","Analyze European Court of Human Rights Cases (EDA approach)","","","0"
"3883","1236505","18463","03/27/2021 17:44:17","Analyze Finnish Paraphrase Corpus","","Analyze Finnish Paraphrase Corpus","","","0"
"4032","1262521","18463","04/10/2021 10:40:40","Use this R package to Analyse Covid Outbreak","","Use this R package to Analyse Covid Outbreak","","","0"
"4050","1265401","18463","04/12/2021 08:03:31","Analyze (EDA) these datasets","","Analyze (EDA) these datasets","","","0"
"4087","1272055","18463","04/16/2021 11:11:47","Use Samanantar Datasets","","Use Samanantar Datasets","","","0"
"4119","1279902","18463","04/18/2021 16:00:25","Make A Summarization Models on Medical Studies using this dataset","","Make A Summarization Models on Medical Studies using this dataset","","","0"
"4120","1279915","18463","04/18/2021 16:02:55","Replicate code Code for paper ""Document-Level Argument Extraction by Conditional Generation"". NAACL 21'","","Replicate Code for paper ""Document-Level Argument Extraction by Conditional Generation"". NAACL 21'","","","0"
"4135","1284024","18463","04/20/2021 12:49:13","Estimate the end of Covid","","Estimate the end of Covid","","","0"
"4139","1284269","18463","04/20/2021 14:48:55","Create a best classifier","","Create a best classifier","","","0"
"4189","1297065","18463","04/26/2021 08:55:10","Create State of the Art ChestX-Det model on this dataset","","Create State of the Art ChestX-Det model on this dataset","","","0"
"4399","1343019","18463","05/15/2021 21:52:05","Use Sota Models in Computer Vision Classification Task","","Use Sota Models in Computer Vision Classification Task","","","0"
"6856","1723179","754061","11/16/2021 17:23:28","test
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","testaaa","","","1"
"7026","1765058","9107141","12/07/2021 06:59:29","The data source should be large enough (at least 10 columns and 250 rows).
Present your insights including some basic analytics and at least five different visualisations.","","Natural Disaster Data Explorer","","","0"
"2521","935942","48573","10/23/2020 12:22:02","Model to segment blood vessels trained on Fundus masks.","","Segmentation of blood vessels in retinal images","","","0"
"4031","1262487","55870","04/10/2021 10:18:27","## Task Details
Analyse the data for some interesting insights. The data has following attributes: 
- date published
- keywords
- description
- Article

It will be a fun exercise to come up with visualisations that summarise the insights that can be inferred from this data.

## Expected Submission
No submission required for this task","","Text analytics","","","0"
"3051","1060973","61943","12/27/2020 03:10:26","Find the age groups and see which age groups are targeted and what is the minimum age of engagement.","","Identify difference in the age group targeted by Democrats and Republicans","Identify if there is any specific groups targeted by Democrats and Republicans","12/27/2021 23:59:00","0"
"5416","1499592","77936","07/30/2021 04:49:19","## Task Details
As a business user, I'd like to have a performance-optimized model, so I assign credit cards to applicants with high CLTV rating only to minimize monetary loss.

## Expected Submission
From the existing notebooks 2 & 3, feel free to let them inspire you to train an AI-optimized model from them. You may tinker with parameters, train completely new models, bag & stack them, etc. You may do anything. Define how to assess your model's performance in production as a guess for you business stakeholders. How do you know, how your model performs? What is your model's metrics and how do they do?

## Evaluation
Please train this model on the target label 'cltv_value', our binary classification.

## Checklist
Please have
- one tuned AI model (binary classification) with its metrics
- compare it the our pre-existing model from Notebook #3 ""OneModel""
- Discussion on Tuning: what means to improve?

### Further help
If you need additional inspiration, check out these existing docs:
[Scikit Learn on Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)","","1) Train Model","Train your own optimized AI model","","0"
"5417","1499592","77936","07/30/2021 05:10:12","## Task Details
As a legal representative of our bank, I'd like to make the case in court how fair and unbiased our models are so we're standing ground in front of a judge. We need this to win cases based foremost on GDPR and other banking regulations.

## Expected Submission
Build a model, which is proven fair and unbiased. For this, it must be globally and locally explainable how it came about some classifications (hint: SHAP). We also need to 
- define protected attributes like gender, age, etc., which may be subject to biases,
- define and use existing metrics for data and models with acceptable bounds for fairness.
(hint: AIF360)
Discuss on your findings. How does applying means of fairness affect your model performance? How does tuning your model for better AI metrics affect your ethical metrics?

## Evaluation
A good solution comes with 
- transparency to explain how specific classifications came about (Local Explainability -- hint: SHAP),
- transparency to explain how classifications came about in general (Global Explainability -- hint: SHAP),
- measurements to prove, the resulting forecasts are unbiased (regardless of formerly biased input data -- hint: AIF360)

Additionally, you may build a Causual Model with like DoWhy or Graphical AI Models (hint: PGMs & MRFs) to infer reason to other or these models. Graphical Models are hard to do correctly.

## Checklist
Please have
- Discussion on defined reasonable attributes to protect for biases
- Fairness Metrics
- Transparency on Classifications: Local & Global
- Discussion on Tuning: AI Metrics vs Ethics","","2) Fairness & Ethics","Find biases in the provided data, mitigate them and build a fairer model","","0"
"5418","1499592","77936","07/30/2021 05:18:14","## Add-on Tasks
As a consultant, we always pursue to over-achieve on our customer expectations. When you find time or want to follow up later on, you may do something like this.

- build a 5-label classification models
- build a regression model on the customer's expected life time value
- include the credit card payment histories to improve your results
- include potential external data like GDP, economic factors for Germany and pricing index: how does this improve your models?
- combine these findings with the other tasks and explore how data, models and ethics interact with one another","","3) Add-on tasks for the slightly bored (optional)","Tinker with more target labels, build more models and more complex history","","0"
"6164","1606066","127588","09/23/2021 06:03:57","Would love to be able to breakdown award recipients by their IMDB ids.","","Match award recipient column with cast/crew ids in IMDB","","","0"
"2554","941660","131457","10/27/2020 16:38:38","## Pumpkin overflow
Some of the images contain multiple pumpkins or the same pumpkin from different angles. Can you properly identify each individual pumpkin in a picture?","","Jack-O-Lantern Detection","Can you pick out pumpkins?","","0"
"3294","1110586","147385","01/28/2021 07:34:12","## Task Details
Predict match result after end of 2nd innings. In cricket, generally fans have a good idea of a likely result given the status of teams after end of 3 innings, and sometimes even after 2 innings! How about putting a probability value for the likely results - 'Win', 'Loss' or 'Draw' given the match state after 2nd innings?","","Predict Match Outcome after 2nd innings","","","0"
"4333","1280229","152648","05/11/2021 20:58:11","Classify the flowers","","Classify the flowers","","","0"
"4332","1233923","152648","05/11/2021 20:57:03","Detect the explosions","","Detect the explosions","","","0"
"4331","1330008","152648","05/11/2021 20:54:41","Prediction","","Predict the knowledge","","","0"
"3820","1222487","168670","03/20/2021 09:02:39","The goal is to build a model to detect whether a sentence is sarcastic or not, using Bidirectional LSTMs.","","LSTM model","Detect sarcasm using bidirectional LSTM","","0"
"3428","1116181","168670","02/07/2021 15:01:35","## Task Details
Understand count of iceberg and ships in the dataset

## Expected Submission
Solution is expected to contain a barplot showing count of iceberg and ships in jupyter notebook

## Evaluation
More EDA will decide which is better solution

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","How many icebergs vs ships","Is the data imbalanced?","","2"
"4990","1205706","168670","07/04/2021 10:15:48","**Submission File Format:**

You should submit a CSV file with exactly 80,900 entries plus a header row.

The file should have exactly two columns as below

‚óè        ID (sorted in any order)
‚óè        Default (contains 0 & 1, 1 represents default)","","Predict if a particular client will default or not","The goal of the problem is to predict whether a client will default on the vehicle loan payment or not. For each ID in the Test_Dataset, you must predict the ‚ÄúDefault‚Äù level.","","0"
"3615","1185259","168670","02/28/2021 12:36:41","Do exploratory data analysis on the blogs data and build hypothesis for modeling.","","Load the dataset","Do EDA on the columns","","0"
"3439","960919","168670","02/08/2021 15:56:06","For building model it is essential to know how the stocks are trending","","Plot a few stock prices for the highly volatile stocks","See if volatility number is accurate","","0"
"3536","912577","168670","02/18/2021 18:32:06","Try different recommendation systems

1. Popularity based
2. Item-Item
3. User-User

to understand which gives better results in terms of recommendation","","Top 10 recommendation for each user","Which books will likely to be purchased by user","","0"
"2989","1047060","169335","12/19/2020 15:24:24","## Task Details
Obtain the countries with the highest and lowest divorce rates

## Expected Submission
Whenever you want, no deadlines!

## Evaluation
Obtaining the numbers is sufficient, bonus points for interesting visualizations.","","Most and least happy couples","Countries with the highest and lowest divorce rates","","0"
"6064","1584040","170572","09/12/2021 07:21:01","## Task Details
Please try to see if the below questions are answered as part of the EDA process. 

- What are the Top 10 countries where vaccinations are administered? Get View by Continent grouping as well.
- What are the Top 10 countries with fully vaccinated people? Get View by Continent grouping as well.
- What are the Top 10 countries with at least 1+ doses administered?
- What is the access to vaccines - by least wealthy and most wealthy countries? (based on GDP per capita per country data)
- What is the average daily rate of the dose administered? Which countries are Top 10 and Bottom 10? Which countries are above and below the World average?

Please feel free to add any other additional questions that EDA can help with in terms of analysis/charts/visualizations.

## Expected Submission
Please create notebooks to solve EDA aspects","","EDA on COVID19 Vaccination Tracker","","","0"
"2651","963664","180952","11/09/2020 06:57:30","There are nearly 1000 products + 5 stores for 104 weeks. 

&gt;  Is there a better way anyone recommend for EDA and more importantly visualizing the data? 
&gt; What techniques or algorithms can be used to generate the forecasts using multiple time series of 1000 products and also considering pricing as an additional feature value?","","EDA - Vizualizations","","","1"
"6944","1731148","8986029","11/27/2021 15:03:53","## Task Details
Netflix has grown to be one of the top streaming platforms. It has various movies from all parts of the world.

## Expected Submission
The countries that use netflix  more
What kind of movies are being streamed more (TV series or Film)
Directors with the highest watched or streamed movies
Please use datasets and not the notebook

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","NETFLIX ANALYSIS","How netlfix does in countries and the kind of movies that are watched in countries","12/01/2021 23:59:00","1"
"2763","991586","252874","11/24/2020 15:29:38","![Narendra Modi on stage](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Visita_Primer_Ministro_de_la_Rep%C3%BAblica_de_la_India%2C_Narendra_Modi_%2827277251430%29.jpg/800px-Visita_Primer_Ministro_de_la_Rep%C3%BAblica_de_la_India%2C_Narendra_Modi_%2827277251430%29.jpg)

Hailing from a modest background PM Modi has been a CM for [Gujarat ](https://en.wikipedia.org/wiki/Gujarat) from 2001 to 2014. 
In 2014 he led the BJP to a full majority, does becoming the 14th Prime Minister of Independent India. 
In 2019 he was elected back to office with the BJP gaining a large majority (303 seats in Lok Sabha) 

*Mann ki Baat* (English translation: Matter of contemplation) is a monthly program with which he tries to reach out to a larger audience. 

Via analyzing the text of this program we will try to understand the topics of interest.","","What does PM Modi talk about?","Identify trend in topics from 2014 to 2020","","0"
"2764","991586","252874","11/24/2020 15:35:23","![Narendra Modi in a rally in Varanasi](https://upload.wikimedia.org/wikipedia/commons/5/58/Narendra_Modi_at_a_rally_in_Varanasi_on_7_May_2014.jpg)

Perform sentiment analysis of each speech. 

Can we track contemporary events with how the topic of the speech changes. 
This will require additional efforts like getting a time line of major events. And then measuring its importance in the topic of the speech.","","Sentiment analysis & correlation of speeches with contemporary events","","","0"
"3154","1085561","252874","01/08/2021 07:16:37","1. Temporal changes and patterns
2. Geographic changes and patterns
3. New admission trends, including age brackets
4. Severity of disease, such as patients in ICU or on ventilators
5. Capacity constraints, including the relative impact of COVID-19 patients on the facility
6. Impact to the emergency department
7. Trends and impacts at the individual facility level that are not recognizable in county or state views
8. Patterns in influenza and coinfections of influenza and COVID-19","","Identify patterns in the dataset","","","0"
"2420","918242","259790","10/12/2020 23:01:35","Discover top-10 strongest schools at the highest all-Russian competition level","","Discover top-10 strongest schools","","","1"
"2421","918242","259790","10/12/2020 23:03:18","Find schools with wide paricipation coverage of the subjects for Olympiads (with big number of school subjects for Olympiads they participate in)","","Find schools with wide Olympiad paricipation","","","1"
"2422","918242","259790","10/12/2020 23:05:05","Determine ""Opening of the year"" - schools that surprisingly (without any historical successful results) captured the pedestal  (either winner place, or prize-winner) during Olympiad history.","","Opening of the year","","","0"
"2423","918242","259790","10/12/2020 23:05:47","Visualizae skill profiles of schools - strong skills and weak points for improvement","","Data Visualization - Schools profiles","","","1"
"2424","918242","259790","10/12/2020 23:06:42","Determine schools with declining Olympiad results from year to year","","Competition crisis","","","1"
"2425","918242","259790","10/12/2020 23:08:47","Determine best talent managers - schools that steadily increase their place in the leaderbord / number of successful participation / number of subjects they participate in","","Find strongest talent managers","","","0"
"3946","1249230","265431","04/03/2021 15:27:41","## Task Details
train a model to classifiy images according to yoga pose. This dataset consists of 82 type of different yoga pose. 



## Expected Submission
Notebooks/kernel with classification model","","Yoga pose image classification","","","0"
"3260","1114582","268812","01/23/2021 18:38:37","## Task Details
Recognizing handwritten digits using a variant of MNIST represents the most popular starter test for both new and established machine learning algorithms.  This grayscale (28x28 pixel) imagery provides another challenging object recognition task, labeling objects in overhead satellites.  To take advantage of the vast machine learning literature on digit recognition, we mirror the format of the original MNIST closely a and thus like other extensions (Fashion-MNIST), the goal is to provide the research community with a new drop-in replacement for benchmarking.   

For these 10,000 training and testing images (90:10 ratio), the original data was heavily processed as labeled objects in 10 classes (car, harbor, helicopter, oil_gas_field, parking_lot, plane, runway_mark, ship, stadium, storage_tank). The CSV has labels in alphabetic order (0-9) to mimic the digit recognizer classes. The data is balanced +/- 5% between the 10 classes with 1000 examples per class. Three different formats are provided for download, including comma-separated values files, JPEG images sorted by class (10 total), and the original MNIST binary format (idx-ubyte) from LeCun, et al.  These formatting options should cover most all published MNIST solutions with minimal modifications.

## Expected Submission
Users can solve the task primarily using Notebooks or Datasets.  

## Evaluation
A good solution includes a Notebook and a confusion matrix for multiple algorithm types ranging from linear regression, support vector machines, trees or deep learning methods.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/zalando-research/fashionmnist (Predict the Fashion Item in 10 Classes)
- https://www.kaggle.com/datamunge/sign-language-mnist (Predict the Hand Gesture in American Sign Language)
- https://www.kaggle.com/crawford/deepsat-sat4 (DeepSat (SAT-4) Airborne Dataset)
- https://spacenet.ai/datasets/ (SpaceNet Datasets)","","Overhead-MNIST","Satellite Dataset to Extend Previous Work on MNIST","","0"
"3610","1182827","268812","02/26/2021 21:45:45","## Task Details
Explore an image classification dataset consisting of 10 executable code varieties and approximately 50,000 virus examples. The malicious classes include 9 families of computer viruses and one benign set.  The image formatting for the first 1024 bytes of the Portable Executable (PE) mirrors the familiar MNIST handwriting dataset, such that most of the previously explored algorithmic methods can transfer with minor modifications. The designation of 9 virus families for malware derives from KMeans clustering that excludes the non-malicious examples. The work generalizes what other malware investigators have demonstrated as promising convolutional neural networks originally developed to solve image problems but applied to a new abstract domain in pixel bytes from executable files. 

## Expected Submission
Recent interest in applying the same techniques to anti-virus and malware detectors motivates the present work to score a similar formatted problem and compare the algorithmic performance.  The Portable Executable dataset contains 51,880 examples of the first 1024 bytes (32x32 pixel values) of the header along with its MD5 hash for the entire file and binary class label as either malware or not (e.g. beneware).  If formulated as a two-class problem, the imbalanced ratio of malware to beneware equals approximately 20:1 (49,364:2516).  Without rebalance, a 95% correct solution would simply declare all cases as malware. To recast the image dataset as a MNIST-formatted alternative, we performed a standard cluster analysis using the KMeans algorithm; we assigned cluster number equal to 9 based on the 1024 column byte vector but excluding the identifying file hash.  We attempted to assign dominant families to each derived cluster based on the known MD5 hashes but found multiple names and sample diversity when querying VirusTotal.  To reduce the amount of code changes comparing a larger or smaller multi-class problem with existing MNIST infrastructure, we opted to use the assigned 9-cluster result as a fully unsupervised example.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
Based on the fascinating work by Oliveira, A:
- Oliveira, Angelo (2019). Malware Analysis Datasets: Raw PE as Image. IEEE Dataport. https://dx.doi.org/10.21227/8brp-j220, https://ieee-dataport.org/open-access/malware-analysis-datasets-raw-pe-image and Kaggle, https://www.kaggle.com/ang3loliveira/malware-analysis-datasets-pe-section-headers 
- Oliveira, Angelo (2019). Malware Analysis Datasets: PE Section Headers. IEEE Dataport. https://dx.doi.org/10.21227/2czh-es14 , https://ieee-dataport.org/open-access/malware-analysis-datasets-pe-section-headers and Kaggle, https://www.kaggle.com/ang3loliveira/malware-analysis-datasets-raw-pe-as-image 
- Oliveira, Angelo (2019). Malware Analysis Datasets: Top-1000 PE Imports. IEEE Dataport. https://dx.doi.org/10.21227/004e-v304 , https://ieee-dataport.org/open-access/malware-analysis-datasets-top-1000-pe-imports and Kaggle, https://www.kaggle.com/ang3loliveira/malware-analysis-datasets-top1000-pe-imports","","Virus MNIST Detection","Methods for Malware Identification from Images","","0"
"3732","1205311","268812","03/11/2021 14:50:37","## Task Details
The use of convolutional neural networks for network attack classification depends on converting tabular feature sets into thumbnail images. This dataset renders the UNSW attack set as 256 pixel images (16x16). The categorical inputs are one-hot encoded and all numerical inputs are scaled from 0-255. The baseline UNSW dataset yields 194 values and the images are right padded with black (255) values for all labels.  The column labels are also shown for the train and test sets in CSV format.

See additional details here
https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/

## Expected Submission
The expectation is that all the tips and tricks of both deep learning and statistical machine learning (trees, etc.) may assist in the task. It shares many characteristics of the traditional MNIST dataset and thus can build quickly on those findings for algorithm comparisons.

## Evaluation
Several problems to solve include simply binary classifiers for attack vs. normal traffic.  Like MNIST digits, there are 10 categories shown (0=normal; 1-9 various attacks). 

### Further help
The details of the UNSW-NB15 dataset are published in following the papers:

- Moustafa, Nour, and Jill Slay. ""UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)."" Military Communications and Information Systems Conference (MilCIS), 2015. IEEE, 2015.
- Moustafa, Nour, and Jill Slay. ""The evaluation of Network Anomaly Detection Systems: Statistical analysis of the UNSW-NB15 dataset and the comparison with the KDD99 dataset."" Information Security Journal: A Global Perspective (2016): 1-14.
- Moustafa, Nour, et al. . ""Novel geometric area analysis technique for anomaly detection using trapezoidal area estimation on large-scale networks."" IEEE Transactions on Big Data (2017).
- Moustafa, Nour, et al. ""Big data analytics for intrusion detection system: statistical decision-making using finite dirichlet mixture models."" Data Analytics and Decision Support for Cybersecurity. Springer, Cham, 2017. 127-156.","","Classify the Network Attack by Family","","","1"
"4738","1403507","268812","06/11/2021 20:51:54","## Task Details
Well-labeled binary ELF files in three categories:
Benign, Malware, and Hackware

The first 1024 bytes are extracted and converted to decimal-integer values in grayscale jpg (32x32).  Format is in CSV for ease of use but images can be generated from this for exploration.

One should be able to build image-based deep learning algorithms to identify if IoT and embedded systems are getting valid firmware updates.

See original
https://github.com/nimrodpar/Labeled-Elfs","","Classify using MNIST Protocols","","","0"
"5759","1412520","335402","08/16/2021 09:12:02","Significant blood samples are helpful for practitioner's to care COVID-19 patient's.","","Find the most significant blood samples that are asscociated with COVID-19","","","0"
"4099","1277743","335402","04/17/2021 14:33:25","1. Classify COVID-19 confirmed patients among suspected patients'
2. Identify the most significant factors that are responsible for COVID-19","","The tasks","","","0"
"5760","1533209","335402","08/16/2021 09:14:49","It is possible to classify dead and alive COVID-19 positive patients based on patients previous diseases and current symptoms.","","Classify death and alive COVID-19 patient's and find the significant comorbidities and symptoms","","","1"
"5029","1414868","344436","07/07/2021 07:56:03","## Task Details
The address of a property can help to provide so much additional information. Extracting the components that make up an address can be a real challenge. The aim is to perform string manipulation to be able to gain more insight into where the properties are located.

## Expected Submission
Be able to use the method to extract any address contained within the dataset. 

## Evaluation
The address can be used to find the location details by Latitude and Longitude","","Extracting addresses","","","0"
"3899","1239882","355417","03/29/2021 17:46:23","## Task Details
How has the share price of the company changed with the rise in COVID? Also, India exports COVAXIN to countries like Canada - has this had an positive impact on the share price?","","Impact of COVID on stock Prices","","","1"
"4297","1324035","355417","05/07/2021 12:28:08","## Task Details
India is seeing a huge rise in the cases during the second wave. Election Rallies held in different parts of the country and the belief that we have defeated COVID is said to be  the primary cause for such an meteoric rise. Can we use the mobility Data along with COVID Data to understand whether there is an impact of mobility on rise of COVID cases?","","Impact of Mobility of COVID Cases","","","1"
"3779","1209921","356439","03/15/2021 00:18:14","## Task Details
Toxic people are everywhere. We have to deal with them.

## Available Dataset
You can leverage `chats_*.csv` combined with `ban_events.csv` and `deletion_events.csv`. For example, you would treat these chats deleted by mods as toxic chats.
See [Quicklook](https://www.kaggle.com/uetchy/toxic-chat-quicklook) notebook for the starter.

## Expected Submission
Binary or multi-label toxic chat classification model that can distinguish between wholesome chat and toxic/spam chat.

## Evaluation
Accuracy always comes first, but keeping false positive low is also important.","","Toxic Chat Classification Challenge","","","0"
"4170","1292728","358785","04/24/2021 10:24:27","Try to focus on two specific stations, say from 6184 - 6015

A ""good answer"" would have an MAPE of 80+%. 

You can ignore any information outside the dataset, e.g., Weather patterns etc.","","Predict Sept 2017 daily demand based on all information up until Aug 31 2017","","","0"
"5786","1539191","359577","08/18/2021 15:17:09","## Task Details
The first thing we should do with these sequences is link them with identifiers of similar proteins in public databases, such as Uniprot.

## Expected Submission
Create a function that takes a protein FASTA sequence as input and outputs identifiers, the % coverage of the query sequence and the % identity between the query and the sequence linked to the accession. 

## Evaluation
The function should run as quickly as possible while still retrieving sequences within 80% identity. 

### Further help
If you need additional inspiration, check out these existing related resources:
- https://www.tutorialspoint.com/biopython/biopython_overview_of_blast.htm","","Fetching gene identifiers","","","0"
"5787","1539191","359577","08/18/2021 15:20:33","## Task Details
Given a gene identifier (accession, common name..), search for mentions of this gene name in the open access scientific literature, and present them in a useful form.

## Expected Submission
A function that takes an identifier or search term as input and prints PMC identifiers for papers with snippets of text from the paper mentioning the gene. 

## Evaluation
The function should run as quickly as possible. The quality of the snippet collection will influence the quality of the submission - try to include enough context to understand what is being said about the gene, without printing too much text. 

### Further help
If you need additional inspiration, check out these existing resources:
- https://www.kaggle.com/nwheeler443/literature-fetching-and-mining","","Fetching mentions of similar genes","","","0"
"5788","1539191","359577","08/18/2021 15:22:47","## Task Details
Score mentions of a gene in the literature according to how relevant they are to a biomedical concept, such as virulence or antibiotic resistance. 

## Expected Submission
A function that takes a gene identifier and a topic string as input and present snippets mentioning the gene scored for relevance to the topic. 

## Evaluation
Ideally, the topic string can be optionally expanded to synonyms, e.g. by pulling them from BioBERT. Quality of assessment of relevance will be key to the value of the solution to this task. 

### Further help
If you need additional inspiration, check out these existing resources:
- https://www.kaggle.com/nwheeler443/literature-fetching-and-mining","","Scoring the relevance of discussion of a gene to a key topic","","","0"
"6006","1571019","360751","09/04/2021 17:41:52","Exploration could include the frequency of competitions, average award, total award, number of prizes, etc.","","How did Google's acquisition of Kaggle affect payouts?","Compare monetary awards before and after","","0"
"6007","1571019","360751","09/04/2021 17:45:14","Exploration could include year-to-date earnings, all-time earnings, changes over time, etc.","","Who are the Grandmasters of Coin?","Show who won cash prizes for competitions","","0"
"3616","1185708","364772","02/28/2021 17:50:06","## Task Details
Created to find out if its possible to predict a winning pick using ML and the probability.

## Expected Submission
Users should submit a detailed Ipython notebook solution? The solution should contain a submission.csv

## Evaluation
Ability to solve the task. Others will evaluate based on efficiency","","Predicting Winning Pick Through ML","","","0"
"3497","927821","364772","02/14/2021 13:54:52","## Task Details
This task aims to analyze price movements on companies listed on ZSE. Detail if the ZSE listed companies are affected  with market information and derive market sentiment

## Expected Submission
Submit the solution detailed on Notebooks 

## Evaluation
More up votes

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Zimbabwe Stock Exchange Dataset Task","Roadmap","03/01/2021 23:59:00","0"
"2835","1010123","368997","12/02/2020 07:25:26","## Task Details
Main data file is: satellite.mat.csv
Column 'Y' is annotation for anomaly

## Baseline AUC is 0.64 [Notebook](https://www.kaggle.com/mineshjethva/anomaly-detection-lstm-isolation-forest/notebook?scriptVersionId=48335381)","","Train Unsupervised Anomaly Detection","","","1"
"3092","1071580","373959","01/01/2021 21:56:42","## Task Details
Perform pixel-wise classifiction on hand-labeled collected images","","Semantic Segmentation","","","0"
"3093","1071580","373959","01/01/2021 21:57:37","## Task Details
Predict the biomass composition (in fractions) of a subset of the biomass-labeled images","","Biomass Composition Prediction","","","0"
"3094","1072448","373959","01/02/2021 08:18:45","## Task Details
Exploratory Data Analysis of the dataset","","DataSet Exploration","","","0"
"3095","1072502","373959","01/02/2021 08:45:51","## Task Details

Build Neural Machine Translation Model","","Neural Machine Translation Model","","","1"
"3096","1072502","373959","01/02/2021 08:48:00","## Task Details
Exploratory Data Analysis of the language Corpus","","DatasSet Exploration","","","1"
"3106","1073378","373959","01/02/2021 19:11:59","## Task Details
Build Image Instance segmentation Models","","Instance Segmentation","","","1"
"3107","1073535","373959","01/02/2021 21:21:30","## Task Details
EDA of PandaSet Dataset","","Exploratory Data Analysis","","","0"
"4101","1277956","373959","04/17/2021 15:58:20","Exploratory Data Analysis of the Dataset","","Exploratory Data Analysis of the Dataset","","","0"
"4102","1277988","373959","04/17/2021 16:11:36","Exploratory Data Analysis of the data","","Exploratory Data Analysis of the data","","","0"
"4109","1279247","373959","04/18/2021 09:20:26","Exploratory Data Analysis","","Exploratory Data Analysis","","","0"
"4111","1279310","373959","04/18/2021 10:07:19","Exploratory Data Analysis","","Exploratory Data Analysis","","","0"
"4110","1279314","373959","04/18/2021 10:06:16","Exploratory Data Analysis","","Exploratory Data Analysis","","","1"
"4107","1278612","373959","04/18/2021 01:54:50","Exploratory Data Analysis","","Exploratory Data Analysis","","","4"
"4022","1261582","373959","04/10/2021 00:50:52","Exploratory Data Analysis of the dataset","","Exploratory Data Analysis of the dataset","","","1"
"3957","1251074","373959","04/04/2021 17:42:18","Exploratory Data Analysis of the dataset","","Exploratory Data Analysis of the dataset","","","2"
"3958","1251126","373959","04/04/2021 18:28:05","Exploratory Data Analysis of the dataset","","Exploratory Data Analysis of the dataset","","","3"
"3959","1251185","373959","04/04/2021 19:07:42","Exploratory Data Analysis of the dataset","","Exploratory Data Analysis of the dataset","","","2"
"3960","1251188","373959","04/04/2021 19:18:39","Exploratory Data Analysis of the dataset","","Exploratory Data Analysis of the dataset","","","1"
"6590","1651794","376354","10/24/2021 08:23:53","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Sexual Compulsivity - Warming-up! Predict the age based on the questions answers and other features","This should be pretty easy!","","1"
"6591","1651794","376354","10/24/2021 08:24:06","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Sexual Compulsivity - Find the outliers!","Find the test takers outliers!","","1"
"6592","1651794","376354","10/24/2021 08:24:21","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Sexual Compulsivity - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6593","1651794","376354","10/24/2021 08:24:36","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Sexual Compulsivity - Background analysis","Practice EDA on a simple dataset!","","1"
"6594","1651794","376354","10/24/2021 08:24:48","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Sexual Compulsivity - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6595","1651794","376354","10/24/2021 08:25:08","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Sexual Compulsivity - Data visualization","Practice data visualization on a simple dataset!","","1"
"6496","1651798","376354","10/24/2021 07:36:33","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regession algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Confidence, Adventurousness, Dominance Personality - Predict the age based on the questions answers","This should be pretty easy!","","1"
"6497","1651798","376354","10/24/2021 07:36:49","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Confidence, Adventurousness, Dominance Personality - Find the outliers!","Find the test takers outliers!","","1"
"6498","1651798","376354","10/24/2021 07:37:02","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Confidence, Adventurousness, Dominance Personality - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6499","1651798","376354","10/24/2021 07:37:16","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Confidence, Adventurousness, Dominance Personality - Background analysis","Practice EDA on a simple dataset!","","1"
"6500","1651798","376354","10/24/2021 07:37:28","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Confidence, Adventurousness, Dominance Personality - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6501","1651798","376354","10/24/2021 07:37:43","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Confidence, Adventurousness, Dominance Personality - Data visualization","Practice data visualization on a simple dataset!","","1"
"6485","1651803","376354","10/24/2021 07:30:35","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Consideration of Future Consequences - Find the outliers!","Find the test takers outliers!","","1"
"6486","1651803","376354","10/24/2021 07:30:52","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Consideration of Future Consequences - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6487","1651803","376354","10/24/2021 07:31:07","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Consideration of Future Consequences - Background analysis","Practice EDA on a simple dataset!","","1"
"6488","1651803","376354","10/24/2021 07:31:23","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Consideration of Future Consequences - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6489","1651803","376354","10/24/2021 07:31:37","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Consideration of Future Consequences - Data visualization","Practice data visualization on a simple dataset!","","1"
"6450","1651803","376354","10/24/2021 06:59:17","### Warming-up: Train a model to predict the accuracy based on the questions answers and other features
#### This actually should be pretty easy..

### Dont Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regession algorithm to make a prediction on the 'accuracy' column of the dataset. You are encouraged to explore the different parameters you can work with in your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the accuracy based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Warming-up: Train a model to predict the accuracy based on the questions answers and other features","This should be pretty easy","","1"
"6644","1651805","376354","10/24/2021 08:55:06","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Sexual Self-Concept - Warming-up! Predict the age based on the questions answers","This should be pretty easy!","","1"
"6645","1651805","376354","10/24/2021 08:55:18","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Sexual Self-Concept - Find the outliers!","Find the test takers outliers!","","1"
"6646","1651805","376354","10/24/2021 08:55:29","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Sexual Self-Concept - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6647","1651805","376354","10/24/2021 08:55:40","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Self-Concept - Background analysis","Practice EDA on a simple dataset!","","1"
"6648","1651805","376354","10/24/2021 08:55:54","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Sexual Self-Concept - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6649","1651805","376354","10/24/2021 08:56:11","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Self-Concept - Data visualization","Practice data visualization on a simple dataset!","","1"
"6554","1651810","376354","10/24/2021 08:06:37","### Warming-up: Train a model to predict the accuracy based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'accuracy' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the accuracy based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Dirty Dozen - Warming-up! Predict the accuracy based on the questions answers and other features","This should be pretty easy!","","1"
"6555","1651810","376354","10/24/2021 08:06:53","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Dirty Dozen - Find the outliers!","Find the test takers outliers!","","1"
"6556","1651810","376354","10/24/2021 08:07:06","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Dirty Dozen - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6557","1651810","376354","10/24/2021 08:07:17","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Dirty Dozen - Background analysis","Practice EDA on a simple dataset!","","1"
"4128","1282206","378285","04/19/2021 19:14:37","## Task Details

The outbreak of Covid-19-Wave 2 in Tamil Nadu major crisis in India and it's engine of Indian Economy Contributing 13 % to GDP 

Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether.
Grocery stores: In highly affected areas, people are starting to stock up on essential goods.
A strong model that predicts how the hospital facilities available overtime

## Expected Submission

Date
District
Beds with oxygen	
Beds without oxygen	
ICU beds 

Target Variable :- 
Ventilators","","Visualise the Hospital Facilities","Visualise the Hospital Facilities","04/30/2021 23:59:00","2"
"4403","1340875","378285","05/16/2021 07:07:54","## Task Details

Mean average vote percentage for Winning party and Run up party 

Which parties plays crucial role in vote splitting 

To increase the winning chances, Runup Alliance can tie up with which party ( @ Constituency wise )","","Exploratory Data Analysis & Insights ( QA )","","08/31/2021 23:59:00","1"
"3192","1092930","391404","01/14/2021 12:55:34","## Task Details
This is a comprehensive dataset consisting of airline and passenger data pertaining to U.S. International Air Traffic (1990-2020). This data when explored should be able to answer some interesting questions . One of them would be to create a list of the top 20 busiest airports in US based on volume of Inetrnational passengers. Additionally, the data should also be useful to understand how much impact the COVID-19 had on the volume of passengers and flights.","","Find the Busiest Airports by Volume of International Passengers","","","0"
"4496","1353957","396276","05/26/2021 09:44:24","## Build a binomial classifer

Build a binomial classifier which can differentiate a domain name that is random (potential DGA) and a legitimate one. 

* Data reading and exploration
* Feature Engineering
* Evaluation metric choice
* Model Training
* Model Evaluration","","Binomial Classifier","","","1"
"4246","1310838","413923","05/02/2021 10:48:01","The task is to understand customer reviews of a product. Specifically, we are asked to analyze customers reviews of ""Moto Edge +"" mobile phones from motorola, Moto E4.

A sample of 9988 customer reviews was collected from ""Flipkart"" using web scrapping techniques as on April 2021 and is stored in the csv file ‚Äúmoto e4.csv‚Äù.

There are three columns in the file as below
‚Ä¢	Rating - Customer rating for the product.
‚Ä¢	Comment ‚Äì Comments added by the customer.
‚Ä¢	Review_text ‚Äì Product review text.

As a data science enthusiastic you may be interested in knowing some basic insights from the customer reviews. You can use this data for hands on text processing, sentiment anlaysis and other NLP tasks.","","Product Reviews: Moto E4","Getting started with text processing, sentiment analysis and NLP tasks.","","2"
"4150","1257187","5081778","04/22/2021 08:55:04","## Task Details
Perform an explorative data analysis

## Expected Submission
Submit your notebook exploring the dataset

## Evaluation
Ensure good documentation and explanation of the analysis and separate steps so that it is clear for others what you're doing","","EDA - Data Analysis","","","0"
"2506","933329","417337","10/22/2020 01:12:34","I need a list of Twitter handles for all current US Senators and 2020 Senate candidates.","","US Senators Twitter Handles","","","2"
"2543","941054","417337","10/26/2020 18:17:09","Create a notebook that can explore the number and breakdown of languages in this dataset.","","Euro Parliament Proceedings Language Breakdown","","","0"
"3263","1115257","425370","01/24/2021 06:27:18","## Task Details
Data Collection and cleaning is the most important task for a data scientist. How would you clean this dataset?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Cleaning","","","0"
"4751","1404394","444204","06/12/2021 16:19:34","# Task Details
#### Understand and predict credit defaulter's behavior so that financial institution can predict lose ends upfront. As a data scientist, inform the business which loans have high risk of getting defaulted or which loans are not feasible to be issued.


# Expected Submission
#### Predicting the 'loan_status' in the following values:- 'Default', 'Charged Off', 'Does not meet the credit policy. Status:Charged Off'. All of these loans means bad news for the lending financial institution.


# Evaluation
#### Classification accuracy score. Recall for assuming a defaulted or charged off loan as a positive event.","","Understand and predict credit defaulters' behaviour","Understand and predict credit defaulters' behaviour","","1"
"3632","1187455","460920","03/01/2021 19:01:07","Create your own piece of aRt that is art with R. This might be an inspiration: [https://www.kaggle.com/jacekpardyak/invitation-to-art](https://www.kaggle.com/jacekpardyak/invitation-to-art)","","Invitation to aRt","","","0"
"3182","1095531","484516","01/13/2021 15:33:55","## Task Details

Many storms result in flooding. In order for local governments use this information to prepare for storm events and the damage that come with them. 

## Expected Submission

Identify regions where storm-related flooding has increased. Make recommendations for local governments that need to be prepared.

## Evaluation

An ounce of prevention is worth a pound of gold (or something). If you can predict where the next billion dollar disaster will be then you will become very famous.","","Areas at high risk for flooding","","","0"
"3706","1200743","496432","03/09/2021 07:11:05","## Task Details
Building a data cleaning and normalization script which helps model to understand it better. Data is merge of multiple domains. Cleaning and normalization should handle tweets, product review, and generic text handling approaches.

## Expected Submission
Submit a jupyter notebook.","","Data preprocessing pipeline","Starter code for data preprocessing","","0"
"4159","1289019","503526","04/22/2021 17:36:20","## Task Details
The dataset consists of 170,305 sentence pairs in English and Portuguese. The data comes from Tatoeba.org, which is a large database of example sentences translated into many languages by volunteers. 

Using this data, can you build a model that effectively learns to translate sentences between English and Portuguese?

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. You are constraint to use only this dataset. 

The notebook should contain:

1. Any preprocess steps of the data
2. Model definition and training
3. The translations from the sentences mentioned below

## Evaluation

These 5 sentences should be translated using your model:
- Kaggle‚Äôs community is made up of data scientists
- Machine learners from all over the world
- With a variety of skills and backgrounds
- We strongly believe that our community
- The future of the field is brighter when we embrace differences","","Machine Translation Task with 170,305 sentence pairs","Translate sentences from English to Portuguese","","0"
"2669","965950","543708","11/10/2020 12:31:44","Use the Aspect Segments to extract all the aspect terms.","","Aspect Term Extraction","","","0"
"2670","965950","543708","11/10/2020 12:32:36","Use the Aspect Sentences to extract all the argumentative spans (stance labels) and the aspect terms simultaneously.","","Nested Segmentation","","","0"
"3386","1125049","544876","02/03/2021 09:42:27","## Classification","","Classification","","","2"
"3396","1130162","562247","02/03/2021 16:42:07","## Task Details
Note: please do not claim the diagnostic performance of a model without a clinical study! This is not a kaggle competition dataset.
Please read our paper:
Loey, M., Manogaran, G. & Khalifa, N.E.M. A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images. Neural Comput & Applic (2020). https://doi.org/10.1007/s00521-020-05437-x
Khalifa, N.E.M., Smarandache, F., Manogaran, G. et al. A Study of the Neutrosophic Set Significance on Deep Transfer Learning Models: an Experimental Case on a Limited COVID-19 Chest X-ray Dataset. Cogn Comput (2021). https://doi.org/10.1007/s12559-020-09802-9

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
Loey, M., Manogaran, G. & Khalifa, N.E.M. A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images. Neural Comput & Applic (2020). https://doi.org/10.1007/s00521-020-05437-x
Loey, Mohamed; Smarandache, Florentin; M. Khalifa, Nour E. 2020. ""Within the Lack of Chest COVID-19 X-ray Dataset: A Novel Detection Model Based on GAN and Deep Transfer Learning"" Symmetry 12, no. 4: 651. https://doi.org/10.3390/sym12040651
Khalifa, N.E.M., Smarandache, F., Manogaran, G. et al. A Study of the Neutrosophic Set Significance on Deep Transfer Learning Models: an Experimental Case on a Limited COVID-19 Chest X-ray Dataset. Cogn Comput (2021). https://doi.org/10.1007/s12559-020-09802-9","","COVID-19 Chest CT image Augmentation GAN Dataset Task1","task","02/28/2021 23:59:00","0"
"3083","1067740","585548","12/31/2020 17:19:19","## Task Details
Can improved drinking water & sanitation facilities and preventive measures decrease the risk of malaria infection?
Do countries with more malaria cases reported or a high incidence of malaria (per 1000 population at risk) have more preventive measures in place?
How do poor drinking water facilities and poor sanitation contribute to a higher risk of malaria?
Do countries take more preventive measures in the next or upcoming years when the malaria cases are increasing?

## Expected Submission
A notebook showing the results of your submission and a clear presentation of the results.

### Hint
Some countries report a low incidence risk or no malaria cases reported (in some countries, malaria is not common).
LM-Model to estimate the results.
Time series analysis: do countries take more preventive measures in the next or upcoming years when the malaria cases are increasing?","","Malaria prevention","","","1"
"2998","1043158","585548","12/21/2020 13:46:02","## Task Details
Can you predict if Chicago has a white Christmas in 2020?

## Expectations
A notebook that shows the results of your analysis and a clear presentation of the results.

## Hint 
You can use binary classification to predict a white Christmas. The most commonly used methods for binary classification include: 
- Logistic regression;
- Decision trees;
- K-nearest neighbors;
- Support Vector Machines;
- Bayesian networks;
- Random forests.

Check out my notebook for some additional data cleaning, dropping missing or not defined values, the count plot of a white Christmas in the past:
https://www.kaggle.com/lydia70/dreaming-of-a-white-christmas-in-chicago","","Will Chicago have a White Christmas in 2020?","Christmas Challenge","","0"
"2999","1043158","585548","12/21/2020 14:06:03","## Task Details
Can you predict if the Chicago Christmas Temperature is rising (with time series analysis)?

## Expected Submission
A notebook showing the results of your submission and a clear presentation of the results. 

## Hint
Time Series Analysis;
Temperature forecast by LSTM Model;
Sarima Modelling;

Check out the trend in temperatures at Christmas:
https://www.kaggle.com/lydia70/dreaming-of-a-white-christmas-in-chicago","","Is the Chicago Christmas Temperature rising?","Christmas Challenge: Time Series Analysis","","0"
"2785","994450","586934","11/26/2020 16:18:31","## Task Details
It would be awesome if this merged with the data published by NUFORC [1]. 

## Expected Submission
90K rows with all fields in [1] with Full Description added. 

## Evaluation
The efficiency of the partial text-matching algorithm used to merge ""Summary"" in [1] with the full description will be evaluated qualitatively. 

### Further help
[1] https://www.kaggle.com/NUFORC/ufo-sightings","","Merge with Original Data","https://www.kaggle.com/NUFORC/ufo-sightings","","0"
"3318","1126384","590653","01/29/2021 17:22:02","How many times Trump discussed a particular country in his tweets and if we can label the sentiments? (North Korea, India, Pakistan, Mexico?)","","How many times Trump discussed a particular country?","Sentiment Analysis","","0"
"3319","1126384","590653","01/29/2021 17:26:29","How many times and ways he has insulted?","","How Many Insults?","","","1"
"3320","1126384","590653","01/29/2021 17:27:00","Can you find a link between his tweets and stock market prices?","","Can you find a link between his tweets and stock market prices?","","","0"
"3321","1126384","590653","01/29/2021 17:27:45","How many times he has downplayed Corona/Covid?","","How many times he has downplayed Corona/Covid?","","","0"
"3322","1126384","590653","01/29/2021 17:28:04","How many times he has called the election fraud?","","How many times he has called the election fraud?","","","0"
"3323","1126384","590653","01/29/2021 17:28:22","How many tweets about Hillary Clinton, Obama or Joe Biden?","","How many tweets about Hillary Clinton, Obama or Joe Biden?","","","0"
"3324","1126384","590653","01/29/2021 17:29:00","Anything else you can find that surprise us?","","Anything else you can find that surprise us?","","","0"
"3345","1126618","590653","01/30/2021 19:16:43","See if you can combine this dataset with other external sources. Like can we compute the distance from City A to City B for all the cities? Can we make a Covid vaccination distribution plan using this data? Can we add population for each unit by combining the data with Census 2017?","","Combine with External Sources","Like Census","","0"
"3309","1111101","590653","01/29/2021 15:49:20","## Task Details
Find out Top 10 selling books.

## Expected Submission
The names of Top Selling Books 

## Evaluation
Show some visualization and see if you can break the top selling books by province and city 

### Further help
Check out the datasets version discussion - https://www.kaggle.com/zusmani/gufhtugu-publications-dataset-challenge/discussion/215355

have a look at www.Gufhtugu.com","","Top 10 Books","Best Selling Titles","03/23/2021 23:59:00","27"
"3310","1111101","590653","01/29/2021 15:57:04","## Task Details
We like to find out the impact of order size (items numbers), order date and time, payment method on order status.

## Expected Submission
Tell us what's the co-relation between 
- return orders and any other given variable
- completed orders and any other given varilables
- cancelled orders and any other given variables

It would help if you can break it down to cities as well

## Evaluation
We are looking for easy to understand graphs and clear insights backed by data","","Order Status Insights","Co-relation wtih Order Status","03/23/2021 23:59:00","22"
"3311","1111101","590653","01/29/2021 15:59:37","## Task Details
How many orders we received or expect to receive in a given day. For example, we receive more orders at the start of the month and on weekends. 

## Expected Submission
Prediction on number of orders for given data/day for the year 2021. 

## Evaluation
We will compare your prediction with actual number of orders in February and March and will announce the winners of March 23rd 2021","","Number of Orders","Sort by Day of the Week","03/23/2021 23:59:00","15"
"3312","1111101","590653","01/29/2021 16:02:42","## Task Details
Every data scientist knows that data is not cleaned. We have the same issue, currently it shows 4,082 cities and that's not possible. We have many variants of city names like Karachi, Khi, KARACHI, Rawalpindi, RWP, Pindi, Lahore, Lhr, Model Town Lahore, Faisal-abad, Faisalabad, FSB, FSD etc. Can you clean it up?

## Expected Submission
Clean list of number of cities we have served. We would love to see it visualize on a Pakistani Map. Here is a [dataset with complete list of Pakistani cities](https://www.kaggle.com/zusmani/pakistan-cities-and-postal-codes), villages, towns and administrative units with zip/postal codes. 

## Evaluation
Accurate list of number of cities","","City Names Cleaning","How Many Cities We Have Served?","03/23/2021 23:59:00","15"
"3313","1111101","590653","01/29/2021 16:05:22","## Task Details
Returns is number one issue for any startup in Pakistan. It can kill the startup in no time. This is an issue with us as well. Can you predict the possibility of return for a given order? For example by city name, book name, number of items, total weight or method of payment?


## Expected Submission
The probability of an order return based on given variables

## Evaluation
We will compare your returns prediction with actual return in the month of February and March and will announce the results.","","Returns","A Big Issue","03/23/2021 23:59:00","15"
"3314","1111101","590653","01/29/2021 16:08:34","## Task Details
There are dozens of datasets available out in public that can add more value to our dataset. Can you find a few and add?

## Expected Submission
Surprise us with more data that can be used for analysis with the given dataset. Few examples

- Postal codes for a given address/city
- Population for a given city versus number of orders received
- estimated travel time to deliver the order from Islamabad to a particular city. Make a map of all cities covered with distances from Islamabad

## Evaluation
We are looking for out of box thinking and will evaluate the tasks with maximum number of insights and additional datasets","","External Datasets","Add Contect","03/23/2021 23:59:00","14"
"3346","1106196","590653","01/30/2021 19:32:46","What is the best-selling category?","","Best Selling Category","","","9"
"3347","1106196","590653","01/30/2021 19:33:20","Visualize payment method and order status frequency","","Visualize Payment Methods versus Order Status","","","5"
"3348","1106196","590653","01/30/2021 19:33:53","Find a correlation between payment method and order status","","Payment Method and Status Co-relation","","","5"
"3349","1106196","590653","01/30/2021 19:34:56","Find a correlation between order date and item category","","Order Date and Item Category","","","5"
"3350","1106196","590653","01/30/2021 19:35:18","Find any hidden patterns that are counter-intuitive for a layman","","Hidden Patterns","","","12"
"3351","1106196","590653","01/30/2021 19:35:43","Can we predict number of orders, or item category or number of customers/amount in advance?","","Predict Number of Orders","","","8"
"3686","1198341","590653","03/07/2021 19:53:09","Any individuals in the dataset more susceptible to cancer?","","Who Will Get Cancer?","","","0"
"3687","1198341","590653","03/07/2021 19:53:31","Who Will Gain Weight?","","Who Will Gain Weight?","","","0"
"3688","1198341","590653","03/07/2021 19:54:07","Identify Place of Origin","","Identify Place of Origin?","","","0"
"3689","1198341","590653","03/07/2021 19:54:53","Which gene determines certain biological feature (cancer susceptibility, fat generation rate, hair color etc.","","Responsible Genes?","","","0"
"3756","1209215","590653","03/13/2021 20:47:33","See if we can find the co-relation with climate change data or rainfall data of neighboring countries","","Co-Relation with Climate Change","","","3"
"3808","1220125","590653","03/19/2021 00:07:49","Can you co-relate stock market behavior with Pakistan political or security events?","","Co-Relation with Political Events","","","1"
"3551","1170113","590653","02/20/2021 18:52:07","Find who are the best and repeat tax payers?","","Who Are the Best Tax Payers?","","","2"
"3552","1170113","590653","02/20/2021 18:52:50","How many companies and individuals pay taxes in Pakistan?","","Who Pays Taxes?","","","3"
"3553","1170113","590653","02/20/2021 18:53:15","How many companies and individuals file null taxes?","","Null Taxes","","","1"
"3554","1170113","590653","02/20/2021 18:53:43","How many companies or individuals doing good financially as per their tax records?","","Who is doing good financially?","","","2"
"3555","1170113","590653","02/20/2021 18:54:07","Can you combine these tax records with other open datasets like City Name extracted from their CNIC numbers to see which city pay how much?","","Combine Open Datasets","","","3"
"3556","1170113","590653","02/20/2021 18:54:36","Anything else you can learn that would help the government broaden the tax net?","","How to broaden the tax net?","","","1"
"3841","1227122","590653","03/22/2021 21:08:38","What insights you can learn from this dataset?","","Pakistan Startup Census Insights","","","3"
"4321","1330786","590653","05/10/2021 06:01:52","Can you find out what contents our youth is watching?","","What Pakistan is Watching?","","","0"
"4322","1330807","590653","05/10/2021 06:25:40","What contents we are consuming and how much time we are spending on TikTok?","","What we consume?","","","0"
"4317","1330666","590653","05/10/2021 04:45:59","Let's find out what Pakistan talks about and what are our major issues as one can see from news.","","What are the main issues?","Pakistan Talks","","0"
"4318","1330705","590653","05/10/2021 05:20:48","What jobs are currently available in Pakistan?","","Available Jobs","","","1"
"4335","1335342","590653","05/12/2021 03:19:04","Find out what's the most favorite car model for Pakistanis?","","What Pakistan Drives?","","","0"
"4336","1335342","590653","05/12/2021 03:19:38","Find out what's the buyer's choice based on reviews for the best car model?","","Best Car Model","","","1"
"4337","1335362","590653","05/12/2021 03:39:21","Find the city with most car dealers?","","City with most car dealers","","","0"
"4338","1335362","590653","05/12/2021 03:39:51","Find the dealerships with offices in more than one cities?","","Car Dealership Franchise","","","0"
"4339","1335430","590653","05/12/2021 04:28:56","What kind of contents we are interested in as a nation on Instagram?","","Insta Contents","","","1"
"4377","1339951","590653","05/14/2021 10:29:07","Who talks more Rangnar Lothbrok, Athelstan, Rollo or Bjorn?","","Who Talks More?","","","0"
"4378","1339951","590653","05/14/2021 10:29:44","Do you think Floki has used the most confusing language in any cinematic recording ever? Prove it with NLP","","Understand Floki","","","0"
"4379","1339951","590653","05/14/2021 10:30:12","Who uses more romantic vocabulary - Lagertha or Princes Aslaug?","","Romance Vocabulary","","","0"
"4380","1339951","590653","05/14/2021 10:30:37","Can you use the dialogues and predict characters hidden personalities using IBM blue mix or personality insights? how about Steven Wolframs package or Apply Magic Sauce? Sci-Kit anyone?","","Predict Characters Personalities","","","0"
"4381","1339951","590653","05/14/2021 10:31:04","If the characters appear in TOEFL or GRE, can you predict their scores?","","Characters GRE & TOEFL Scores","","","0"
"4382","1339951","590653","05/14/2021 10:31:26","Do you think King Ecbert is the most articulated character of the show?","","King Ecbert","","","0"
"4383","1339951","590653","05/14/2021 10:31:49","Can you compare Floki with Atherlstan and see who is more religious?","","Atherlstan's Curse","","","0"
"4384","1339951","590653","05/14/2021 10:32:07","How many time the word Valahalla has been used in all seasons?","","Valahalla","","","0"
"4385","1339951","590653","05/14/2021 10:32:23","Can you predict anything using previous episodes scripts and compare with the predictions of seer? Can you make a digital seer?","","Digital Seer","","","1"
"4481","1364785","590653","05/24/2021 20:56:53","Can you visualize the damage and map it geographically?","","Visualize the Destruction","","","2"
"4482","1364829","590653","05/24/2021 21:20:01","How much is the human cost in terms of statistical value of life and money lost due to unavailability of resources","","Calculate the Human Cost","","","4"
"4608","1384199","590653","06/02/2021 12:52:33","How many emails are there?","","How Many Emails?","","","0"
"4609","1384199","590653","06/02/2021 12:53:03","How much time Dr. Fauci should have spent to answer all these emails?



Good Luck!","","How much time Dr. Fauci should have spent to answer all these emails?","","","0"
"4610","1384199","590653","06/02/2021 12:53:31","Can you predict Dr. Fauci's personality using IBM BlueMix or ApplyMagicSauce?","","Can you predict Dr. Fauci's personality using NLP?","","","0"
"4611","1384199","590653","06/02/2021 12:54:04","How you can classify each email, threatening, appreciating, positive, negative?","","How you can classify emails?","How you can classify each email, threatening, appreciating, positive, negative?","","0"
"4612","1384199","590653","06/02/2021 12:54:25","How the communication unfolded between US and China?","","How the communication unfolded between US and China?","","","0"
"3628","1186883","596659","03/01/2021 15:50:11","To evaluate the presumed excessive correlation among variables (i.e., colinearity),  we calculated the variance inflation factor (VIF) for each variable.","","Calculated the variance inflation factor (VIF)","","","0"
"3629","1186883","596659","03/01/2021 15:59:36","We set 59 as the seed, the partial least squares regression (PLSR) (Wold 2001, Martens 2001, Mevik and Wehrens 2007), cross-validated using 10 random segments.
The calculated minimal value that we can use to measure with less error of prediction the propagation of the remaining values around the variability between the two experiments. 
Notwithstanding, what variables are important for the variability between the two experiments remained to be assessed.","","Partial least squares regression (PLSR): the minimal root mean squared error of prediction (RMSEP)","","","0"
"2733","984172","601066","11/19/2020 23:47:42","## Task Details
Ads on haraj.com are not free text and not structured which makes it hard to compare, filter, and search. Text classification can be applied to categorize these ads as the first step to organize the sheer amount of daily ads added to the webstie.","","Ads classification","","","0"
"3254","1113073","607904","01/23/2021 00:00:54","Train machine learning model using train set and predict sales on test set.
  
  



## Submit your predictions [here](https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/#SolutionChecker)


## Submission file format
| Variable | Description |
| --- | --- |
|Item_Identifier  |  Unique product ID |
| Outlet_Identifier | Unique store ID |
| Item\_Outlet\_Sales | Sales of the product in the particular store. This is the outcome variable to be predicted.|","","Predict sales on test set","","","0"
"2677","968162","612429","11/11/2020 16:48:29","## Task Details
Generate News using

* Neural Language Model NLM 
* Recurrent Neural Language Model RNLM 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
Evaluate and compare the two models using The perplexity PPL. 

The perplexity (sometimes called PP for short) of a language model on a test set is the inverse probability of the test set, normalized by the number of words.","","Arabic News Generation","","","1"
"3233","1108926","618942","01/20/2021 23:35:06","Train a neural network to segment a person. You can use the architecture from this article: [blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66](https://blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66)","","Train a neural network to segment a person","","","0"
"3465","1152595","618942","02/11/2021 15:57:30","Train a neural network to segment a person. You can use the architecture from this article: [blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66](https://blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66)","","Train a neural network to segment a person","","","0"
"3487","1152755","618942","02/13/2021 15:27:53","Train a neural network to segment a person. You can use the architecture from this article: [blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66](https://blog.prismalabs.ai/real-time-portrait-segmentation-on-smartphones-39c84f1b9e66)","","Train a neural network to segment a person","","","0"
"4876","1409004","618942","06/21/2021 11:16:13","You need to train the neural network, which of the 4 types is the image.","","Train a neural network to detect a mask on a person's face","","","1"
"4430","1347699","619418","05/18/2021 11:24:50","EDA on data. 
Do time series forecasting on the fully_vaccinated_people and daily vaccination per hundred.","","Predict when each country will close its vaccine drive. Assume only 18+ to be vaccinated.","","","0"
"3030","1056196","655302","12/24/2020 07:08:18","## Task Details
Find popular hockey teams & the nicknames used to refer to them

## Expected Submission
share the data via google drive or any other source to update this dataset.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Add hockey teams & their nicknames","","","0"
"3413","942976","3742540","02/05/2021 17:40:16","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

do when free","","Shipping Optimization Challenge","","","0"
"4803","1415015","660302","06/17/2021 08:58:16","## Task Details
Out of 10k+ models available on HF model hub, can we identify most popular ones? 

## Expected Submission
Visualization notebooks identifying most popular or trending models.
eg: Out of BART or T5, which one is most published? Which ones are getting published recently?","","Identify trending/popular architectures","","","2"
"4804","1415015","660302","06/17/2021 09:11:11","## Task Details
README data for the 10k+ models available sometime contain a lot of information ( and sometime they're empty as well). Could the README data itself be parsed and structured to some extent? 
Useful possible outputs:
- streamlined benchmark scores
- identify training datasets
- identify training/finetuning hyperparams and duration

## Expected Submission
Any notebook output which brings out information from the READMEs in a meaningful manner. Tabularized benchmark scores would be a great submission.","","Parse Readme data","","","2"
"4638","1388749","678381","06/04/2021 13:50:37","This is small data set given for Classification of the images as Pneumonia or Normal

It is made available for ease of use","","Classify the images as Pneumonia or Normal","","","0"
"3768","1209817","681869","03/14/2021 07:00:17","Using the data set, find out the country against which Virat Kohli has the maximum batting average. Here, the batting average is given by (total number of runs Virat scored)/(the total number of matches he played) and not the average runs Virat Kohli scored before getting out.
- Plot a histogram to see where Virat Kohli has scored the most number of times.
- Whenever Virat Kohli has scored 90-100 runs (exclude 100), what has been the average strike rate?
- Using the previous histogram, find the run interval where Virat has scored the maximum number of sixes. Also, mention the number of fours he hit in the same bracket.
- Plot a pie chart to find out the approximate percentage of the times Virat Kohli has been out by LBW in all his innings. Take into account all his innings, including the ones in which he remained not out.
- Let‚Äôs say you want to visualize the consistency of the runs scored by Virat against various teams, i.e. you want to compare the spread of the runs scored by Virat against various teams. Which of the following plots will be the most appropriate for visualizing this?
- In which years have Kohli‚Äôs runs kept improving in the Q2-Q4 period given that he played at least one match in that period?
- Against which country has Virat scored the maximum aggregate runs in matches where the mode of dismissal was ‚Äúcaught‚Äù?
- What is the batting position at which Virat has the best average against England?","","Analysis of Virat Kohli ODI Matches","","","1"
"3122","1072085","691741","01/04/2021 11:53:44","## Task Details
ÂåØÂá∫2020Âè∞ÁÅ£ÊâÄÊúâÊ∞¥Â∫´ÊØèÂ§©ÁöÑÁ∏ΩÊ∞¥ÈáèÂúñ

## Expected Submission
‰∏ÄÂºµÂúñ



### Further help
ÂèØÂèÉËÄÉ Â±±Ê≤≥‰∫ã‰ª∂Á∞ø playground","","Export the daily total water volume of all reservoirs in Taiwan","","01/18/2021 23:59:00","0"
"3195","1097311","699749","01/14/2021 22:00:55","## Task Details
The GIFs are stored in json format and allow for a nested data structure. To use this dataset for a machine learning task you need to match gifs to their 16 sentiment values in tabular form. While you can query these on the fly using a Data Loader, this is supposed to be a starting point for analyzing data stored in this format.

## Expected Submission
The expected submission is a csv file or notebook.

## Evaluation
The default json library in Python may not be that great compared a json querying language like MongoDB for this task.

### Further help
[The quickstart](https://pymongo.readthedocs.io/en/stable/tutorial.html) for the Python MongoDB cilent.","","Query intersection of GIFs","","","0"
"3196","1097311","699749","01/15/2021 00:24:52","## Task Details
Producing a dataset costs time and money so to get around this you can append the title of the gif with its tags as an text mapping to a gif.

## Expected Submission
The target for this submission are the 16 sentiment labels. Analysis using images from the gif is encouraged, but optional.

## Evaluation
Once you train a model, find a text snippet and match the prediction to the closest match in the gif corpus.

### Further help
If the labels are binarized, then Jaccard score may be a good evaluation metric, but by default the gifs are scored so multi-class logloss may be a better metric to start.","","Map text to a gif","","","0"
"4635","1387995","710858","06/04/2021 07:40:54","## Task Details
Predict the book price using data-train.csv for training and data-test.csv for testing.

## Evaluation
RMSE","","Book price prediction","","","0"
"4504","1369606","728568","05/26/2021 22:00:59","Train a classifier to detect between an Imperative vs Interrogative Sentence","","Detect questions vs Statements","","","0"
"4019","1260949","728568","04/09/2021 16:04:19","A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~100k formulas and images splitted into train, validation and test sets. Formulas were parsed from LaTeX sources provided here: http://www.cs.cornell.edu/projects/kddcup/datasets.html(originally from  arXiv)

Each image is a PNG image of fixed size. Formula is in black and rest of the image is transparent.

For related tools (eg. tokenizer) check out this repository: https://github.com/Miffyli/im2latex-dataset
For pre-made evaluation scripts and built im2latex system check this repository: https://github.com/harvardnlp/im2markup

Newlines used in formulas_im2latex.lst are UNIX-style newlines (\n). Reading file with other type of newlines results to slightly wrong amount of lines (104563 instead of 103558), and thus breaks the structure used by this dataset. Python 3.x reads files using newlines of the running system by default, and to avoid this file must be opened with newlines=""\n"" (eg. open(""formulas_im2latex.lst"", newline=""\n"")).","","Add seq to seq neural net","","","0"
"4649","1390825","732424","06/06/2021 02:27:11","## Task Details
In this task you need to submit a notebook with full EDA of the current dataset.","","Prepare Exploratory Data Analysis of the dataset","","","1"
"4650","1390825","732424","06/06/2021 02:28:36","## Task Details
Please submit a notebook with your findings.","","Detect patterns for defective bearings","","","0"
"4651","1390825","732424","06/06/2021 02:30:43","## Task Details
Try to find any dependencies between good and defective bearings placed on the both positions of the device","","Check the influence between bearings on the first and the second positions","","","0"
"4652","1390825","732424","06/06/2021 02:31:27","## Task Details
Build binary classification model","","Build model for bearing classification","","","0"
"4653","1390825","732424","06/06/2021 02:32:28","## Task Details
Try to split bad bearings into groups based on the defects. Potentially use clustering.","","Find groups of defective bearings","","","0"
"4659","1393077","732424","06/06/2021 22:39:35","## Task Details
Prepare notebook with EDA of this dataset","","Prepare Exploratory Data Analysis","","","3"
"4660","1393077","732424","06/06/2021 22:40:27","## Task Details
Build neural network model to segment banners present in the dataset.","","Build banner segmentation model","","","2"
"4661","1393077","732424","06/06/2021 22:41:19","## Task Details
Build model that is able to change banners on the field to your custom banners.","","Build model for banner replacement","","","1"
"3204","1090197","738361","01/15/2021 22:23:50","What is the best suffix for a task sequence, which, if executed, will likely to lead to a desired outcome with the desired performance characteristics? We learn a deep model from prior instances
of a process that performed well. The input is a partial task sequence, and the output is a suffix","","Predict the next state based on a given prefix","","","0"
"3205","1090197","738361","01/15/2021 22:26:32","Considering the general process of handling an issue: https://docs.moodle.org/dev/Process 

What is the time remaining to completion for a process instance? We learn from a set of complete
past process instances (i.e., a complete set of process log entries that show the start time of the process as well as the end time of the process, plus the times when each task was completed).
Some process logs record start time of each task, while others record the end time, hence we will sometimes have to treat the start time of a task as an approximate indicator of when the task
was executed.","","How much time is needed to resolve a certain issue?","","","0"
"3206","1090197","738361","01/15/2021 22:29:11","What is the best set of resources to allocate to each issue? We train the learner using process
logs annotated with resource sets and QoS performance indicators (we learn only from those
instances which performed well). Here we need to prescribe a suffix of task-resource set pairs, so
the input is also a sequence of task-resource set pairs.","","Who Should handle a certain issue?","","","0"
"3714","1201407","755659","03/09/2021 13:55:58","## Task Details
3 years ago, I collected with a friend of mine some data from the web to train a fake news detector for french news articles and we had pretty good results. Recently, it just popped up to my mind to share with the community our dataset and let you build a better approach. 

## Expected Submission
The expected submission is a notebook in which you try to predict whether or not a news is fake. The prediction quality has to rely on the AUC (Area Under ROC). The training has to be done on the training dataset. 

## Evaluation
The AUC has to be computed on the test set. Greater is the AUC, better is your classification system. 

### Further help
If you need additional inspiration, check out these existing notebook as a starting point: https://www.kaggle.com/therealsampat/fake-news-detection","","FakeNewsDetector","Create a fake news detector for french news articles","","1"
"3103","1065183","61943","01/02/2021 12:17:53","## Task Details
Using train1 and train 9 data set train the algorithm to predict test 1 and test 9 game wins and kda ratio

## Expected Submission
Notebook that details all the steps in building this prediction model and the prediction score in Root mean square error format

## Evaluation
The best RMSE notebook is the best approach

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict Game wins and KDA ratios for Test 1 and Test 9","","","1"
"3838","1057426","5108943","03/22/2021 15:34:25","It will helpful for who are entering into an kaggle competition","","Converting files from one format into another format","","","0"
"3134","1049794","6456690","01/05/2021 12:10:53","Handling the data with missing data , check the outlier and remove it and bringing the relation between the dependent variable and independent variable","","Data Preprocessing and ML Model","Creating ML Model Using R","01/09/2021 23:59:00","1"
"3024","1055354","761104","12/23/2020 19:45:56","## Task Details
Predict whether a potential promotee at checkpoint in the test set will be promoted or not after the evaluation process.

## Expected Submission
Solution Notebook included prediction.  For every row and employee_id  in test set concat your prediction probability.
employee_id  | prediction


## Evaluation
The evaluation metric for this competition is F1 Score.","","Predict the probability  of eligibility of candidate","","","7"
"6877","1041311","8177959","11/19/2021 09:24:26","## Task Details
You are a junior data analyst working on the marketing analyst team at Bellabeat, a high-tech manufacturer of health-focused products for women. Bellabeat is a successful small company, but they have the potential to become a larger player in the global smart device market. Ur≈°ka Sr≈°en, cofounder and Chief Creative Officer of Bellabeat, believes that analyzing smart device fitness data could help unlock new growth opportunities for the company. You have been asked to focus on one of Bellabeat‚Äôs products and analyze smart device data to gain insight into how consumers are using their smart devices. The insights you discover will then help guide marketing strategy for the company. You will present your analysis to the Bellabeat executive team along with your high-level recommendations for Bellabeat‚Äôs marketing strategy.","","Google Data Analytics Case Study","Submit your work here!!!","","1"
"2828","1004280","761104","12/01/2020 11:26:39","## Task Details
We will start from simple methods . Matrix factorization is a known method for recommendation system. While generally collaboration filtering give us who is interested to which item MF specificlly goes further and tell us how much interested. As we have number of upvotes in the data  we can implement matrix factorization . 

- Also you might find similar users and books through  your pipeline. (e.g. 5 similar users with similarity score  )

#       
## Expected Submission
Submit a notebook including recommendation in tabular format which recommend to each user  n ranked (e.g 5 or 10) books with  **predicted vote**. 

#      
## Evaluation

It depends on **Variants of Matrix Factorization** you apply. Basically  you should apply hold-out method , and evaluate you model on unseen test data. 

#### *Note :  When splitting up the data into training and test sets, you should randomly select (user, book) pairs, not select random users or books. The whole idea is the model be able to predict ratings for books user haven't seen based on the ratings  provided for ones you have. If a user is present only in the testing set, the model cannot possibly be basing predictions based on their other ratings.*


#  
### Further help
- *[Introduction to recommender systems](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)*
- *[LightFM‚Äôs documentation](https://github.com/lyst/lightfm)*
- *[The 7 Variants of Matrix Factorization For Collaborative Filtering
](https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5)*","","Apply Matrix Factorization to recommend books","Predict how each user votes to an unseen book","","1"
"3015","1019790","761104","12/22/2020 21:59:24","## Task Details
This dataset designed to understand the factors that lead a person will work for the company(leaving current job) ,and  the goal of this task is  building model(s) that uses the current credentials,demographics,experience to predict the probability of a candidate  looking for a new job or will work for the company.

The whole data divided to train and test . Also sample submission has been provided correspond to enrollee_ id of test set  ( enrolle_ id | target)

- Note:

       - The dataset is imbalanced so it might affect your result if you dont handle it
       - Most features are categorical (Nominal, Ordinal, Binary), some with high cardinality so encoding methods and techniques will help to boost models performance
       - Missing imputation strategy might affect the results so it can be a part of your pipeline as well.

## Expected Submission
Solution Notebook  with following **mandatory** outputs:

&gt;- **Prediction**: for each row in test data the predicted probability of  candidate looking for a job, (probability of the class with the greater label).Format is given in sample_submission.csv:
     - enrollee_id        :  Unique ID for enrollee
     - target                :  probability of an enrollee looking for a job change
#  

&gt;-  **Model Evaluation**: roc_ auc_ score of (answer,prediction probability  of rows in test data)
*answer: test target values(download from : https://www.kaggle.com/arashnic/job-change-dataset-answer)
#   

&gt;-  **Model Interpretation**:  Interpret model(s) such a way that illustrate which features affect candidates decision



#  
#  

## Evaluation
The evaluation metric is [ area under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) score.

#  
You have better  first evaluate your model(s) with your selected strategy (KFold CV, hold_ out,...) and then predict test (sample_submission) and  finally compare the result for your own with test target values and find out your model performance on unseen data and  your model fitness.

*The pipeline you apply for getting result (EDA, preprocessing, feature engineering and selection , model building and evaluation strategy and so on,   will be evaluated by your model performance on unseen data(aug_test.csv)*","","Predict the probability of an candidate  looking for a new job","","07/28/2021 00:00:00","82"
"3033","1019790","6042091","12/24/2020 09:59:52","I will work with datasets to predict the output as instructions. I know this data can be classified as supervised learning.","","HR Analytics","","12/31/2020 23:59:00","51"
"2611","950652","761104","11/02/2020 09:37:08","## Task Details
Build model(s) to predict how treatment affects conversion by binary classification. the label is weather customer converted [1] or not converted [0]

## Expected Submission
For each ID , you must predict a probability for the conversion variable. 
It is a good idea to divide the data to train, validation and test(unseen) and verify your model(s) on test with labels on hands. 

## Evaluation
- Use Kfold cross validation [and prediction] evaluation .  Test both KFold and Stratified. 
- Build Confusion matrix to see how your model is useful for operational environment. 
- Because the data is highly imbalanced  i recommend  apply  **area under the ROC curve**  or **recall_score** .





#    

There are lots of technique to handle imbalanced data but at first dont forget apply right algorithm(s) and also class weights or resampling methods","","Imbalanced Binary Classification","Predict how treatment affects conversion","","2"
"2613","951698","761104","11/02/2020 15:14:14","## Task Details
Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue. 
#  
## Expected Submission

Submit a notebook with following mandatory outputs: 

&gt;1.   For each row in test.csv you should predict the probability of customer Response.  The submission format for the prediction  is a .csv file with the following format:
id,Response
57782,0.5
286811,0.5
117823, 0.5
etc.
&gt;
&gt;2. In any pipeline you follow to get the result you should evaluate your model on test target values with eval metric roc_ auc_ score. You can find answer (in numpy format) via following link: 
https://www.kaggle.com/arashnic/answer
 
&gt;3. Explain you pipeline and model(s) implementation in summary 
#  


#  
## Evaluation
The evaluation metric is  [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) between the predicted probability and the observed target.","","Predict whether a customer would be interested in Vehicle Insurance","","","4"
"2814","958032","761104","11/29/2020 12:44:42","## Task Details (Multi-classification)
You are provided by buildings structure data with ""csv_building_structure.csv"" among data files. This file includes building structure data as well as damage_grade as target. 

We're trying to predict the ordinal target damage_grade, which represents a level of damage to the building that was hit by the earthquake. There are 5 grades of the damage with value_counts(): 

&gt;Grade 5    275766
&gt;Grade 4    183844
&gt;Grade 3    136412
&gt;Grade 2     87257
&gt;Grade 1     78815

The dataset mainly consists of information on the buildings' structure . Each row in the dataset represents a specific building in the region that was hit by Gorkha earthquake.



There are 31 columns in this dataset, where the building_id column is a unique and random identifier. 

You can use other data e.g.  'csv_building_ownership_and_use.csv',  'csv_household_resources.csv' and  'csv_household_demographics.csv' and join them using  'mapping.csv' data.

## Expected Submission
Try to build model(s) with output format of two columns with the building_id and the damage_grade and submit your solution via notebook. The data type of damage_grade should be convert to : 
{'Grade 1': 0, 'Grade 2': 1, 'Grade 3': 2, 'Grade 4': 3, 'Grade 5': 4}

&gt; building_id,damage_grade
11456,1
16528,1
3253,1
18614,1
1544,1


## Recommended Pipeline and Evaluation
- Split you input data set to train , test for example by .20 test size and keep test data as unseen,   to be able  deliver score of you final model on test data
- Use KFold or StratifiedKfold cross validation on delivered train data for evaluation
-  As the data is imbalanced don't use 'accuracy' metric instead you can apply **micro averaged F1 score**

- This dataset is a good practice to apply several feature selection methods such as :


&gt;- [x] Filter
  * [x] Removing Low Variances
  * [x] Pearson Correlation
  * [x] Chi-2


&gt;- [ ] Wrapper
   * [ ] RFE, FFS, BFS
   * [x] Boruta

&gt;- [ ] Embeded (SelectFromModel)
      * [x] Random Forest, LightGBM,XGBoost, ...
   
&gt;- [x] LOFO
- [x] Permutation Importance
    - [x] Null Importance
    - [x] Target Imputation 

#### *example Boruta: At every iteration we check if a given feature is doing better then expected than random chance. We do this by simply comparing the number of times a feature did better than the shadow features using a binomial distribution.*
#   
https://danielhomola.com/feature%20selection/phd/borutapy-an-all-relevant-feature-selection-method/
#  
#   
![boruta](https://danielhomola.com/assets/images/boruta.png)





### Further help
- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html
- https://www.kaggle.com/learn/machine-learning-explainability
- https://github.com/scikit-learn-contrib/boruta_py
- https://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf","","Predict Damage Grade from Building Structure Data","Practice multi-classification with damage grades","","3"
"2557","942945","769452","10/27/2020 22:37:23","## Task Details
Some high-ranked military, intelligence or police officers are present in multiple doctoral boards, as chairmen, advisors or reviewers.
Find the most frequent apparitions.

## Expected Submission
Use a Notebook or R Markdown report to present your findings.

## Evaluation
A good solution will use relevant graphics to highlight the presence of some of the professors in multiple boards.","","Are some advisers present in multiple academies?","Find the advisors and reviewers present in multiple boards and doctoral schools","12/21/2020 23:59:00","0"
"2547","928054","769452","10/27/2020 08:23:13","## Task Details
Has the forest surface increased or decreased after 1990. It was a single trend or we can see several evolutions in parallel?

Bring more data to sustain your theory. The data in this dataset is only the official data so it might show only one side of the story.

## Expected Submission
Notebook with great visuals. Use animated geo data to support your findings.","","Analysis of Forest Surfaces in Romania","Identify trends in the forest surface evolution after 1990","01/31/2021 00:00:00","0"
"2473","926409","769452","10/17/2020 22:23:24","## Task Details
Perform a detailed exploratory data analysis of the entire data. Study the distribution per company, per year, per activity type. Plot the participants country, city and address distribution. Look to total number of contracts per company, as well as total amounts per company per year, understand why in some activities are many small contracts and in another activities a small number of contracts with a large sum of money. Explore things like if EU funds were used, if subcontractors were employed. Look as well to details as what type of European fund was used.

### Data Cleaning

#### **Missing data** 

Are there missing data? Check especially the flags like *EU Funds* or *Subcontracted* - Null value means here most probably ‚ÄùNo‚Äù.

#### **Duplicate values**

Are there multiple names for the same category (duplicate names) ? Pay attention to possible issues with misspelled names, variation for name of companies, financing methods, activity type, EU Fund used. 

#### **Companies members of a group**

Also, investigate if it is possible that one group will create dedicated companies for applying for one tender, so watch as well for companies names that have some specific word of group of words in the name.

### Data visualization

There are multiple companies applying to multiple contracting public authorities, in multiple years. Use visualizaton

## Expected Submission
Use a Notebook or a R Markdown report.

## Evaluation
A good submission would have to address extensively the data cleaning step, to look to the categories distribution, to investigate both number of contracts and total amounts per company, per activity type, per country and city, per contracting authority as well as looking to regional, activity type and time trends","","Analysis of the public tenders in Romania from 2007 to 2016","","12/18/2020 23:59:00","0"
"2480","926409","769452","10/18/2020 16:11:43","## Task Details
Perform a Data Quality Assessment with a focus on Data Cleaning.

## Expected Submission
Submit your findings in the form of a Jupyter Notebook or RMarkdown report illustrating your method, detailing your process, explaining the findings, showing the results of your data quality assessment and data cleaning process.

## Evaluation
Discuss all the details of your process","","Data Quality Assessment","Perform a Data Quality Assessment with a focus on Data Cleaning","11/18/2020 23:59:00","1"
"2546","926409","769452","10/27/2020 08:18:42","## Task Details
In the data for public tenders in Romania (2007-2016), an important number of contracts are from Health sector (Sanatate, in Romanian). This sector uses a very large number of contracts, with small amounts, closed between hospitals and companies that win such contracts. This is most probably due to the legislation that requires the hospitals to do the acquisitions directly, through various forms of contracts.

## Expected Submission
Use a Jupyter Notebook or a R Markdown report.

## Evaluation
An important aspect should be the data profiling, including an assessment of the data quality. Several hospitals might have register their name differently for different contracts or also companies might have changed slightly their name when filling the applications. Or some groups might use multiple companies (with a common word - identifying the group) in the name. 
Identify the big players (with most of the contracts or largest amount), the raising stars, the companies that lost the market share. 
Try to identify trends in the sector over the 10 years of data.","","Health sector contracts analysis","Perform an Exploratory Data Analysis on the Health sector data","12/11/2020 23:59:00","0"
"2720","980262","769452","11/18/2020 12:30:42","## Task Details
Output more data formats and save them. I will periodically inspect the added Notebooks and add new formats ideas to this dataset.","","Add more data formats","Create a Notebook to export more data formats","12/25/2020 23:59:00","1"
"2786","999729","769452","11/27/2020 00:35:44","## Task Details

Plot monthly earnings in Romania from 1991 to 2020. Compare net and gross amounts. Calculate how amount or percent of taxes changed over time. Spot the devaluation due to inflation during '90s and the valuation in the '00s.","","Plot monthly earnings in Romania","","12/25/2020 23:59:00","0"
"2994","1049638","769452","12/21/2020 00:37:55","## Task Details

Explore the tweets about the new Pfizer &BioNTech vaccine to understand the public reception, the topics discussed, the positive and negative sentiments related to the vaccine.","","Analyze tweets about Pfizer Vaccine","How the new vaccine from Pfizer and BioNTech is received by the public?","01/05/2021 23:59:00","5"
"3223","1049638","769452","01/19/2021 11:03:48","## Task Details

Recently I added a complementary dataset on Vaccination progress, [COVID-19 World Vaccination Progress](https://www.kaggle.com/gpreda/covid-world-vaccination-progress)

This tasks will require you to analyze if the vaccination campaigns progress around the World is reflected in the tweets. 

You can try to answer to such questions:  

- Are the campaigns reflected in the tweets text?  
- Is the evolution of the campaigns reflected in the tweets sentiment?  
- Are people tweeting more in geographies where the campaigns are more active?  


## Expected Submission
Your submission is expected in a Jupyter Notebook or R Markdown report.

## Evaluation
Your submission will be evaluated by the quality of the analysis, the type of data included, the additional data added into consideration (for example, you can use this data: [Country Statistics - UNData](https://www.kaggle.com/sudalairajkumar/undata-country-profiles/)","","Vaccination campaigns reflected in tweets content and sentiment","Are the vaccination campaigns around the World reflected in the tweets?","04/20/2021 00:00:00","1"
"3150","1049638","769452","01/07/2021 17:18:51","## Task Details
The tweets does not have associated sentiment labels. Use various tools and techniques to perform sentiment analysis and evaluate the polarity, subjectivity and other sentiment measurement metrics.

## Expected Submission
You are expected to submit a Python Notebook or an R Markdown report, including the following:
* Data analysis; 
* Method for sentiment analysis;  
* Results;     
* Comments and conclusions.


## Evaluation
The submission evaluation will be based on the:
* Originality of your solution;   
* Quality of data analysis;   
* Methods and techniques presentation;   
* Quality of results;    
* Comments and conclusions.","","Pfizer Sentiment Analysis","Use various tools and techniques to perform sentiment analysis on tweets data","02/04/2021 23:59:00","13"
"3152","1049638","769452","01/07/2021 21:44:20","## Task Details

Use the Tweets about Pfizer Vaccine to understand what discussion topics are included in these tweets.","","Pfizer Vaccine Tweets Topics","Explore the discussion topics in Pfizer Vaccine Tweets","02/05/2021 23:59:00","8"
"3175","1049638","2043348","01/12/2021 17:59:33","## Task Details
The task is to create custom word embedding metrics for vaccine-related tweeted words.

## Expected Submission
The solution should be a python notebook and a downloadble embedding metrics which can be leverage for any language generation assignments 

## Evaluation
There is not much on evaluation perse but compare same with google embedding metrics and should at least 50% accurate","","Create Word Embedding of tweeted words","word embedding metrics for vaccine related terms","","10"
"3501","1158238","769452","02/14/2021 23:08:06","## Task Details

This datasets contains tweets about all major COVID-19 vaccines used, as following:  
* Pfizer/BioNTech;  
* Sinopharm;   
* Sinovac;   
* Moderna;  
* Oxford/AstraZeneca;  
* Covaxin;  
* Sputnik V.  

## Expected submission

You are expected to submit a Python Notebook or an R Markdown report, including the following:

* Data analysis;  
* Method for sentiment analysis;  
* Results;  
* Comments and conclusions.  

## Evaluation

The submission evaluation will be based on the:

* Originality of your solution;  
* Quality of data analysis;  
* Methods and techniques presentation;  
* Quality of results;  
* Comments and conclusions.","","Sentiment Analysis for All COVID-19 Vaccines Tweets","Use sentiment analysis for COVID-19 Vaccines Tweets","03/31/2021 00:00:00","2"
"3518","1158238","769452","02/16/2021 19:59:02","## Task Details

This datasets contains tweets about all major COVID-19 vaccines used, as following:  
* Pfizer/BioNTech;  
* Sinopharm;   
* Sinovac;   
* Moderna;  
* Oxford/AstraZeneca;  
* Covaxin;  
* Sputnik V.  

Perform sentiment analysis on the tweets looking to things like:   

* Sentiment polarity (Negative/Positive/Neutral) in relationship with each vaccine (looking to the association with the vaccine name, as found in tweets);   
* Evolution of sentiment in time;   


## Expected submission

You are expected to submit a Python Notebook or an R Markdown report, including the following:

* Data analysis;  
* Method for sentiment analysis;  
* Results;  
* Comments and conclusions.  

## Evaluation

The submission evaluation will be based on the:

* Originality of your solution;  
* Quality of data analysis;  
* Methods and techniques presentation;  
* Quality of results;  
* Comments and conclusions.","","Sentiment analysis - per vaccine and variation in time","Evaluate sentiment per each vaccine and as well how it changes in time","03/31/2021 00:00:00","4"
"3614","1158238","769452","02/27/2021 20:30:57","## Task Details

In this task, you should use the data from two Kaggle datasets:

* [COVID-19 World Vaccination Progress](https://www.kaggle.com/gpreda/covid-world-vaccination-progress)

* [All COVID-19 Vaccines Tweets](https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets) and

The aim is to understand how sentiment toward vaccination evolved, as reflected in the tweets, in parallel with vaccination progress.

## Expected Submission

The expected submission for this task should fulfill few conditions, as following:

* Combine sentiment analysis, topic analysis, or at least frequent concepts analysis trends with vaccine programme evolution to understand relationship between public vaccination programmes actions and public reception;  
* Use data from the two datasets mentioned in the task details section;  
* Bring useful insights;  

## Evaluation

A successful submission should:

* Use data from both datasets;
* Include a detailed data analysis;  
* Bring an original perspective;  
* Have great visuals;   
* Include a discussion/conclusion section.","","Sentiments about vaccines as reflected in tweets and vaccination progress","How the progress of vaccination is reflected in the sentiments toward vaccines?","03/31/2021 23:59:00","4"
"3630","1187302","769452","03/01/2021 16:39:17","## Task Details
Perform sentiment analysis on r/VaccineMyths subreddit posts

## Expected Submission
Use a Python Jupyter Notebook or a RMarkdown report to communicate your analysis.

## Evaluation

Your submission will be evaluated based on:
* Your data cleaning process;
* The data evaluation performed;  
* Various visualizations included;  
* The results of your analysis;   
* Conclusions and final remarks;
* Originality of your approach.","","Analyze Vaccine Myths","Sentiment analysis from r/VaccineMyths subreddit posts","05/02/2021 00:00:00","15"
"3900","1187302","769452","03/29/2021 18:09:39","### Task Details

Perform topic modelling for  r/VaccineMyths subreddit posts & comments. The purpose of this exercise is to identify what are the main topics of the vaccine myths discussions.

### Expected Submission
Use a Python Jupyter Notebook or a RMarkdown report to communicate your analysis.

### Evaluation
Your submission will be evaluated based on:

* Your data cleaning process;
* The data evaluation performed;
* Data preparation;
* Implementation of algorithm for topic modelling;
* Analysis of the results and personal comments.","","Topic Modelling for Vaccine Myths","Topic Modelling for r/VaccineMyths Subreddit Posts & Comments","04/30/2021 23:59:00","6"
"4488","1187302","769452","05/25/2021 15:05:37","## Task Details
Perform a detailed exploration on the Vaccine Myths data.","","Vaccine Myths Analysis: May-July edition","Perform a detailed exploration on the Vaccine Myths data","07/31/2021 00:00:00","34"
"3176","1093816","769452","01/12/2021 18:57:14","## Task Details

Answer to questions like:   

* What vaccines are used and in which countries?   
* What country is vaccinated more people?   
* What country is vaccinated a larger percent from its population?","","Track the progress of COVID-19 vaccination","","03/25/2021 23:59:00","168"
"3222","1093816","769452","01/19/2021 08:18:39","## Task Details
Combine vaccination data with UN Data to try to capture what influences vaccination programmes, and these programmes success.

You can use for example the data from: [Country Statistics - UNData](https://www.kaggle.com/sudalairajkumar/undata-country-profiles/)

## Expected Submission
Use Jupyter Notebooks or R Markdown reports with graphics to document your findings.","","Politics, economy, demography - what are the factors that influence vaccination?","","02/20/2021 23:59:00","39"
"3897","1093816","769452","03/29/2021 15:17:47","### Introduction

This is a task about answering questions from analyzing the data. To address this task, you will have to answer to questions like:

* What vaccination schemes (combination of vaccines) are used and in which countries?
* What is the vaccine used in the largest number of countries?
* What country has vaccinated more people? 
* What country has immunized the largest percent from its population?
* What is the country that vaccinated completely most of the population?
* Can you trace the daily vaccinations dynamic?

### Expected submission

A successful submission will have to include:
* Exploratory data analysis;
* Comments on the facts revealed by various graphs, diagrams;
* Original and insightful interpretations.

### How the submission is evaluated

The submission is evaluated on:
* Quality of data analytics and graphs;
* Originality of approach;
* Quality of insights.","","Track the Progress of Covid-19 Vaccination: Spring Edition","","04/30/2021 23:59:00","19"
"3594","1093816","769452","02/25/2021 09:22:00","## Task Details

Combine the data from two Kaggle datasets:

* [All COVID-19 Vaccines Tweets](https://www.kaggle.com/gpreda/all-covid19-vaccines-tweets) and
* [COVID-19 World Vaccination Progress](https://www.kaggle.com/gpreda/covid-world-vaccination-progress)

to understand how progress of vaccination programmes around the World (or in a specific country) is received by the public, as reflected in the tweets about all vaccines.

## Expected Submission

A submission for this task should:

* Use data from at least the two datasets mentioned in the task details section;  
* Combine sentiment analysis, topic analysis, or at least frequent concepts analysis trends with vaccine programme evolution to understand relationship between public vaccination programmes actions and public reception;  
* Be a Jupyter Notebook or R Markdown report;  
* Bring interesting insights;  
* Be original;

## Evaluation

A successful submission should:

* Use data from both datasets;
* Include a detailed data analysis;  
* Bring an original perspective;  
* Have great visuals;   
* Include a discussion/conclusion section.","","Vaccination progress and public sentiment about vaccines","Combine datasets with tweets about vaccines and vaccination progress to analyze relationship between vaccination evolution and sentiment toward vaccinations","03/25/2021 23:59:00","13"
"3298","1124381","769452","01/28/2021 14:58:31","## Task Details
Use WSB Reddit posts data to understand the discussion trends in the community,

## Expected Submission
You can submit either a Jupyter Notebook or R Markdown report to analyze the data.","","Trends in Reddit WallStreetBets Posts","Analyze the trends in Reddit WallStreetBets Posts","02/28/2021 23:59:00","8"
"3411","1124381","6636583","02/05/2021 09:36:23","## **Task Details**
 Everyone wants to know who is engaging with whom on the site, and why.

## **Expected Submission**
Users need to investigate that whether activity on WallStreetBets was driven by inauthentic accounts after an anonymous spokesperson News that there was ‚Äúa large amount of bot activity in the subreddit‚Äù and that posts were being blocked by an automated moderation system.

## **Evaluation**
The evaluation of submission is better than a more reliable and satisfying result.","","‚ÄòHype Machine‚Äô Is in a Quest to Drive Out Bots","The explosive growth of users on WallStreetBets is triggering concern and finger-pointing","02/10/2021 23:59:00","4"
"4294","1248984","1024670","05/07/2021 10:18:10","## Task Details
It is very important that a vaccine is safe. For that it should have the less side effect cause rate on the population. This task is to find out the side effect cause by the vaccine. Help us to understand the different side effect cause by the vaccine .


## Expected Submission
Please solve the task primarily using Notebooks. See different machine learning/ deep learning approach to solve this task

## Evaluation
How accurately one can extract the side effect cause by the vaccine.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find the side effect cause by the vaccine.","","","0"
"5207","1429992","769452","07/19/2021 07:41:35","## Objective

Explore the Fantasy Premier League Reddit posts and comments dataset.

## Expected Submission
A rich visualization Notebook highlighting the various features of the dataset. The data has text, personal, geographical, temporal and network related features.


## Evaluation
The following will be considered for the submission evaluation:
- completeness of the analysis;
- attention to details;
- quality of visualization;
- originality;
- comments and conclusions.","","Exploration of Fantasy Premier League dataset","Explore the rich Fantasy Premier League Reddit posts and comments dataset","08/31/2021 23:59:00","0"
"4926","1434047","769452","06/27/2021 12:02:54","## Task Details

The data has the temporal information given as columns (per year, or per quarter/year, or per month/year or per day/month/year). It is more easy to pivot these columns to get instead date/value pairs. This pivot operation, using `melt` from pandas is done in the starter kernels:
* [Starter Kernel: Daily Euro Exchange Rates](https://www.kaggle.com/gpreda/starter-kernel-daily-euro-exchange-rates);
* [Starter Kernel: Annual Euro Exchange Rates](https://www.kaggle.com/gpreda/starter-kernel-annual-euro-exchange-rates).

Follow these examples to create similar starter kernels for:
* Quarterly data;
* Monthly data.

## Expected Submission

A submission should consist in a Notebook or script that will pivot the temporal data to get it in date/value pairs format.

## Evaluation

A successful submission will pivot the data, clean and transform missing data to fit to the target format.","","Create starter kernels for monthly & quarterly data","","08/01/2021 23:59:00","0"
"4927","1434047","769452","06/27/2021 12:05:55","## Task Details
Perform Exploratory data analysis for Euro exchange rates.
You can use the starter kernels examples.

## Expected submission

A successful submission would need to:
* identify missing data;
* put the data in a simple for for analysis;
* explore in detail the data;
* provide insights into the data;
* merge (eventually, if needed) with other external data (example: map currencies / countries).","","Exploratory Data Analysis for Euro Exchange Rates","","08/01/2021 23:59:00","0"
"5209","1459801","769452","07/19/2021 07:46:50","## Task Details

The dataset contains information about how disable people in Europe have access to education. The features include level of education attainment, degree of disability, sex, age group and geography.


## Expected Submission
A rich visualization Notebook highlighting the various features of the dataset. The data has geographical, education, gender, education attainment level and degree of disability information.
The submission is expected to answer to such question as: how a certain degree of disability affects the level of education attainment? How this aspect is variable across Europe's countries? Is gender an aggravating factor? How different age groups are affected? How all these aspects are interlinked?

## Evaluation
The following will be considered for the submission evaluation:
- completeness of the analysis;
- attention to details;
- quality of visualization;
- originality;
- comments and conclusions.","","How disabled people have access to education in Europe?","","08/31/2021 23:59:00","0"
"2584","930684","792567","10/30/2020 16:41:21","It would be nice to see someone use this dataset to make some easy to comprehend visualizations","","Visualizing the data","","","0"
"2585","947684","792567","10/30/2020 16:48:08","## Task Details
Use the dataset to draw insights about the World's management and disposal of solid waste. Present an analysis or comparison based on metrics of your choice.","","Analyse the Worlds approach to Waste Management","","","1"
"2410","916586","793761","10/12/2020 00:33:37","## Task Details
Create a model to classify the outcome of Cardiotocogram (CTG) exam (which represents the well being of the fetus).

## Notes
Note that this is a **multiclass** problem that can also be treated as regression (since the labels are progressive).
Also note that there is considerable class imbalance and therefore **accuracy** is not recommended as primary metric and **stratification** is recommended when splitting the data.


## Evaluation
With a testing set of size of 30% of all available data, calculate some of the recommended metrics bellow:

- Area under the ROC Curve
- F1 Score
- Area under the Precision-Recall Curve

Have fun!","","Fetal Health Classification","Assess the well being of a fetus undergoing a CTG exam","","33"
"5026","1451416","793761","07/06/2021 18:29:14","## Task Details
Using text from scientific papers create a model to automatically generate slides.
Despite the visual aspect of slides, it is actually a XML file. For this reason, this is a text2text task.


## Evaluation
For the purposes of evaluation, the metric ROUGE is recommended


### Further help
https://aclanthology.org/2021.sdp-1.11.pdf","","Generate Slides from Scientific Articles","","","1"
"2851","1014336","796185","12/04/2020 00:14:56","identifying disease by symptoms","","identifying disease by symptoms","","","0"
"2483","927258","801251","10/18/2020 23:11:21","## Task Details
The labels inside the dataset are not evenly distributed. For example there are many magnitudes more common road- or building-pixels than rare people- or traffic-sign-pixels to learn from. 

## Possible Submissions
Create a notebook and explore some approaches to counteract the scarcity of the rare labels without new simulated images.","","How To Counter Rare Labels?","There are e. g. far more building-pixels than people-pixels.","","0"
"3437","1145934","803207","02/08/2021 12:57:01","Create Visualization in any Platform, Python, Kaggle, Tableau, PowerBI, Qlik","","Create a Visualization of his Expeditions","","","2"
"3354","1107050","812596","01/30/2021 23:00:14","## Task Details
Create data visualizations that capture and communicate key insights on the dataset.

## Expected Submission
Visualizations in Notebooks.

## Evaluation
Visualizations submitted should be easy to understand and they should communicate broad patterns in the data. Brief explanations of the generated visualizations will also improve the quality of a submission.","","Exploratory Data Analysis (EDA) With Visualizations","","","0"
"4064","1269137","813217","04/13/2021 14:30:51","## Task Details

Can the links present in the Wikipedia articles of musical artists be used to calculate the similarity between them? Are cosine and distance good metrics for that task or are there any better ones? Could clustering also be a good solution in that case?","","Similarity Between Artists","","","0"
"3875","1196764","820667","03/27/2021 04:19:58","## Task Details
Open to interpretation but I'd look to join the songs (or just the artists) to a database that includes genre names. Maybe Spotify, maybe Wikipedia. Then track the ebb and flow.

## Expected Submission
You do you, I personally find visualizations an easy way to convey information. So maybe do that via a notebook?

## Evaluation
A good entry will clearly display the changing tastes in music through the years.","","How have the tastes in music changed throughout the years?","","","1"
"3939","1196764","820667","04/02/2021 16:39:30","## Task Details
Here's something that could be done - compare this dataset with one from another region. For example, the US. I believe there are datasets of the Billboard Hot 100 on kaggle so that's one possibility.","","Find songs that were hits in Europe but weren't elsewhere (and the reverse)","","","1"
"3826","1212002","824113","03/21/2021 03:07:10","## Task Details
[Shopee Code League 2021](https://www.kaggle.com/c/scl-2021-ds/overview)

### Further help
* [Data Cleaning] (https://www.kaggle.com/zeyalt/scl-2021-data-science-part-1-data-cleaning)
* [Tokenisation] (https://www.kaggle.com/chongkairu/scl-2021-data-science-part-2-tag-train-words)
* [Bilstm NER model using Keras] (https://www.kaggle.com/chongkairu/scl-2021-data-science-part-3-bilstm)
* [Imputation with exact matches](https://www.kaggle.com/chongkairu/match-poi-and-street-from-train-to-test)","","Address elements extraction","","","0"
"3193","1097064","841836","01/14/2021 14:00:08","The goal of this task is to scrap the info of 
https://en.wikipedia.org/wiki/List_of_S%26P_500_companies which has the GICS sub-industry name and add the information of the GICS table.","","Add S&P 500 stocks industry classification id","Scrap from wikipedia the info","","0"
"3212","1097064","841836","01/17/2021 16:13:22","Simple example on how to use the dataset","","Add example","Simple example on how to use the dataset","","0"
"3443","1147062","841836","02/09/2021 01:15:22","## Task Details
Take one stock an make an analysis.

## Expected Submission","","Analysis","Deep stock analysis","","0"
"4428","1329110","844176","05/18/2021 05:40:31","## $$\color{#ff35fe}{\mathbb{Task \; Details}}$$
In addition to the usual machine learning tasks (classification, generation, etc.), this database allows you to formulate a more complex task: to recognize the same symbols photographed by different devices.
For machine learning algorithms, these are different objects, in real life they are the same letters written on the same sheet of paper.
## $$\color{#ff35fe}{\mathbb{Expected \; Submission}}$$
As a solution, an algorithm, which could recognize the same objects in different images with a visual presentation of the result, can be presented.
## $$\color{#ff35fe}{\mathbb{Evaluation}}$$
Each set of archived files like  
<p>00_00_00.zip</p>
contains at least two to three photographs that are photographs of the same symbol. 
The ideal solution is to find all the ""cloned"" objects.
## $$\color{#ff35fe}{\mathbb{Further \; Help}}$$
Studying the effect of the number of ""cloned"" objects on the operation of machine learning algorithms can be an interesting line of research.","","Clone Catching","Recognition of the same symbols on different photos","","0"
"3404","1138072","858250","02/04/2021 14:23:11","1. Draw insights on how tourism has changed for different countries over time.
1. Use other world datasets to draw the possible reasons/factors affecting tourism.","","EDA and Visualisation","","","2"
"3284","1121787","858250","01/27/2021 09:09:00","Visualize the data to draw the important trends and insights.","","EDA and Visulisation","","","4"
"3285","1121787","858250","01/27/2021 09:14:08","Given the demographics, predict the risk associated with an adverse effect for any new individual.","","Predict suitable candidates for vaccination","","","17"
"3794","1121787","5010926","03/17/2021 03:15:48","## Task Details

Investigation to elucidate the recent grow in reactions alert and suspensions with more information about the most common ones.

## Expected Submission

Users should submit a prediction of high risk population groups by each vaccine, for the most common reactions for that vaccine.

## Evaluation

The submissions will be discussed with doctors and researchers from Brazil about the relevance of each prediction group and effect for the patients care.","","Abnormal Reactions Tracking","Investigation to elucidate the recent grow in reactions alert and suspensions","","1"
"3231","1108793","858250","01/20/2021 17:10:53","Using NLP, perform
1. EDA
1. Sentiment Analysis","","EDA and Sentiment Analysis","","","4"
"3275","1119858","863388","01/26/2021 11:06:29","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Naukri Job Listing Analysis","A full blown analysis of Naukri Job Listing","01/31/2021 23:59:00","0"
"3276","1120031","863388","01/26/2021 11:59:46","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Zillow Property Analysis","A full analysis of the zillow sample dataset","01/31/2021 23:59:00","0"
"3407","1138444","863388","02/04/2021 17:20:43","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Walmart Product Details Q2 Analysis","A full analysis of Walmart Products","02/28/2021 23:59:00","0"
"3408","1138475","863388","02/04/2021 17:34:33","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Amazon Product Data Analysis 2020","A full analysis of amazon product data","02/28/2021 23:59:00","0"
"3445","1147429","863388","02/09/2021 06:11:20","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Indeed Job Listing Analysis 2020","Indeed Job listing from Q2 2020","02/28/2021 23:59:00","0"
"3400","1137380","863388","02/04/2021 06:42:11","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","EaseMyTrip flight Fare Details Analysis","A full analysis of the sample dataset provided here","02/28/2021 23:59:00","0"
"3667","1194730","863388","03/05/2021 20:22:37","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of SimplyHired","","03/31/2021 23:59:00","0"
"3668","1194732","863388","03/05/2021 20:27:15","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Flipkart Data","","03/31/2021 23:59:00","0"
"3662","1193668","863388","03/05/2021 07:04:03","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of CareerBuilder Job Dataset","","03/31/2021 23:59:00","1"
"3664","1193753","863388","03/05/2021 11:01:35","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Homes.com Dataset","A full sample dataset analysis of data from Homes.com","03/31/2021 23:59:00","0"
"3631","1187537","863388","03/01/2021 18:16:05","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Fashion Products Dataset from Amazon 2020","A full analysis of fashion dataset sample from Amazon USA 2020","03/31/2021 23:59:00","1"
"3644","1189303","863388","03/02/2021 18:51:07","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Sample Data","A full analysis of Booking.com data","03/31/2021 23:59:00","0"
"2484","928894","863388","10/19/2020 11:20:21","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
The solution must contain the key inference from this sample dataset that will help you with your analysis. 

## Evaluation
A good solution is something that will blow our data warriors' minds. A full-fledged analysis in this dataset can be really a great start for the same.","","CareerBuilder Job Dataset 2020","A sample of Job listing from Jan to Mar 2020","10/31/2020 23:59:00","0"
"2487","928966","863388","10/19/2020 11:53:29","## Task Details
This task was created keeping in mind the best analysts and researchers will require data of this quality for their research and analysis. 

## Expected Submission
Users must submit a full-blown analysis of the dataset that is present in this sample.

## Evaluation
The analysis will be evaluated by our engineers and winners will get a 20% discount on all our dataset in the repository.","","Amazon Spain Sample Dataset","A sample of the dataset that is there in this dataset","10/31/2020 23:59:00","0"
"2414","917548","863388","10/12/2020 13:25:45","## Task Details
This task is being created for all the data warriors across the world. Use this sample to come up with the best possible analysis of the dataset. 

## Expected Submission
The users must submit a detailed analysis of this dataset. It has to have the potential to feature in our reports at the end of the month. 

## Evaluation
The analysis that will blow our engineer's minds is the one we are looking for. Do you have what it takes to do so? Go ahead and attempt this challenge.","","Sample Dataset Analysis","Hotel listing analysis of this sample dataset","10/31/2020 23:59:00","0"
"2415","917663","863388","10/12/2020 13:41:26","## Task Details
This task was created for the sole purpose of finding the best analysis for this sample dataset. 

## Expected Submission
Users must submit data in an analytical format and represent the data in a clean manner. 

## Evaluation
Our engineers will evaluate the analysis and award the winners with a discount on all our datasets in our repository.","","Analysis of Job data from Seek.Au","This a sample analysis of the data that is being extracted","10/31/2020 23:59:00","0"
"2567","944534","863388","10/28/2020 18:06:37","## Task Details
This task was created keeping in mind our data scientists and other various analysts across the globe. 

## Expected Submission
Users must submit a proper analysis of the sample dataset that you have here. 

## Evaluation
What makes a good solution? A good solution is something that will blow our minds. We want a full-blown analysis of the dataset that you have here.","","SimplyHired Data Task","An Analysis of the Job dataset","10/31/2020 23:59:00","0"
"2614","952284","863388","11/02/2020 18:22:08","## Task Details
This task was created keeping in mind the best data warriors and analysts out there in the world.

## Expected Submission
The submission should contain a complete analysis of the dataset. 

## Evaluation
A good solution is something that will blow our engineer's minds. We require a full analysis of the data that is present in this dataset. This is a sample but you can download the full dataset here(https://app.datastock.shop/?site_name=Walmart_Products_Data_Listing_2020).","","A Sample Analysis of Walmart Dataset","A small sample of the dataset created by us","11/30/2020 23:59:00","0"
"2705","975417","863388","11/16/2020 08:17:40","## Task Details
This task was created by our in house teams to get the best datasets in our repository. Every analysis submitted and the best will be chosen and will receive an early bird Black Friday discount on datasets of his/her choice. 

## Expected Submission
We expect a full analysis of the dataset that is present here. The sample can get you to understand the dataset better. 

## Evaluation
A good solution is something that will help you understand the dataset with more ease.","","IMDB TV Show Details Analysis","This analysis of the TV Shows across IMDB","11/30/2020 23:59:00","0"
"2708","975738","863388","11/16/2020 11:27:41","## Task Details
This task was created keeping in mind the data scientists and Researchers across the globe who would want such data at their disposal. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
What makes a good solution? A solution that makes us proud that we have such great minds among us. It should help a common man understand the interpretation of the sample dataset that we have here","","Articles Submission Analysis Dataset","To Submit a full analysis of the articles provided in this dataset","11/30/2020 23:59:00","0"
"3037","1057397","863388","12/24/2020 19:42:58","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","A Sample New Analysis","A small analysis of the sample dataset that is here","01/31/2021 23:59:00","0"
"3038","1057414","863388","12/24/2020 19:52:39","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Amazon Electronics","A Sample Analysis","01/31/2021 23:59:00","1"
"3060","1063580","863388","12/28/2020 13:06:46","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Property Listing Dataset 2020","","01/31/2021 23:59:00","0"
"3061","1063687","863388","12/28/2020 13:53:32","## Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

## Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

## Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Dataset from Amazon Furnishings 2020","","01/31/2021 23:59:00","0"
"4517","1368489","863388","05/28/2021 10:42:23","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 



This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock


###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content


###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of USA Indeed Data","","07/31/2021 23:59:00","1"
"4075","1272676","863388","04/15/2021 04:51:29","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used other than for price comparison and analysis.

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Analysis of Amazon USA Product Details","A full blown analysis of the sample dataset","04/30/2021 23:59:00","0"
"3904","1240655","863388","03/30/2021 04:58:15","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Walmart Product Reviews Dataset","A Full Analysis of the Sample","04/30/2021 23:59:00","0"
"3905","1240738","863388","03/30/2021 05:54:28","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition.","","Walmart Product Reviews Q2","A full analysis of sample data from Walmart","04/30/2021 23:59:00","0"
"5031","1453373","863388","07/07/2021 12:42:48","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Product Review Dataset","","09/30/2021 23:59:00","0"
"5065","1457846","863388","07/09/2021 14:30:28","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Naukri Job Listings","","09/30/2021 23:59:00","0"
"4948","1438013","863388","06/29/2021 11:33:12","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of USA Careerbuilder Job Listings","","08/31/2021 23:59:00","0"
"4886","1424958","863388","06/22/2021 11:56:08","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of USA Monster Job Listings","","08/31/2021 23:59:00","0"
"4732","1402533","863388","06/11/2021 11:00:06","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Ebay Product Data","","07/31/2021 23:59:00","0"
"4834","1417439","863388","06/18/2021 13:21:12","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Australia Seek Job Listings","","08/31/2021 23:59:00","0"
"4781","1410087","863388","06/15/2021 11:18:16","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Australia Seek Job Listings","","08/31/2021 23:59:00","0"
"3519","1163800","872208","02/17/2021 02:14:34","## Task Details
As we know we are on a global pandemic that impacted everybody. A few months ago I did a [causal analysis](http://the-odd-dataguy.com/timeseries-forecasting-and-causal-analysis-in-r-with-facebook-prophet-and-google-causalimpact/) on the impact on covid on crimes in Montreal so I will be really curious to see the impact covid on the bike sharing business in Washington

## Expected Submission and evalution
A well explained notebook on the impact of covid on the daily/weekly/monthly basis of the COVID.

The notebook should contains:
- the methodology of the estimation
- an explanation of the period selection
- figures well explained

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict covid impact on bike sharing in washington","Make causal analysis with bike sharing data","","0"
"2540","929361","874280","10/25/2020 16:10:11","## Task Details

Build an Image segmentation model that can detect the damages in Car.

## Expected Submission
Bounding boxes and segmentation arrays of the damages.

## Evaluation
This task is evaluated on the mean average precision at the different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:

$$ IoU(A,B) = \frac{A \cap B}{ A \cup B} $$

The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a ""hit"" if its intersection over union with a ground truth object is greater than 0.5.



At each threshold value t, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:

$$ \frac{TP(t)}{TP(t) + FP(t) + FN(t)} $$

A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:

$$\frac{1}{|thresholds|} \sum_t \frac{TP(t)}{TP(t) + FP(t) + FN(t)}  $$","","Detect Damages","Image Segmentation for Car Damages","","8"
"2541","929361","874280","10/25/2020 16:55:45","## Task Details

Build an Image segmentation model that can detect the damaged parts in the car.

## Expected Submission
List of the parts damaged.

## Evaluation
Comparison of predicted damaged parts and actually damaged parts.","","Detect Damaged Parts","Build a model to detect damaged parts","","5"
"3639","1175549","878378","03/02/2021 11:13:10","## Task Details
Can you find a model that can classify Digikala online e-commerce images based on their colors?

## Expected Submission
Use any type of machine learning algorithm to classify the images with high accuracy and less loss. 

## Evaluation
Solutions with high accuracy are the bests.","","Find a model that can detect the color of products best.","","","0"
"4794","1409140","880012","06/16/2021 20:37:44","## Task Details
We have data for Italian restaurants which include food quality, d√©cor quality, service quality, as well as price and location. 

See what we can conclude about the restaurants from their qualitative and quantitative attributes.","","Rank by location","","","0"
"3987","950193","917353","04/07/2021 03:49:22","## Task Details
Data is not cleaned. It is very important to have cleaned data before analyzing.

## Expected Submission
Please submit the cleaned data.","","Cleaning the Data","","","0"
"3988","950193","917353","04/07/2021 03:54:46","## Task Details
Here are some questions you need to answer.
1. How many participants were from Daffodil International University/DIU?
2. Which university participants had most project experience?
3. How many participants were not sure about joining the webinar?
4. Ratio of profession.
5. Which date had most event registration?","","Task for EDA","","","0"
"3877","1235536","926822","03/27/2021 06:52:12","Creation of Recommendation Engine","","Recommendation Engine Based on User Reviews","","12/31/2025 23:59:00","0"
"2703","974344","929585","11/15/2020 20:26:37","look in article on nature
https://www.nature.com/articles/s42256-020-00251-5

https://www.nature.com/articles/s42256-020-00253-3

Imho the decisiontree is overfitted
The doctors think they have an tree with prediction accuracy of 90%
I think the maximal efficiency you get from the data is 50% precision if you split the data...

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F929585%2F962ddaaff5d2ee8b0861ce8c7945d5df%2Ftree.png?generation=1605471983929507&alt=media)","","create decision tree","","","0"
"3949","1249857","940461","04/04/2021 02:30:40","## Task Details
Over the past several months, the issue of ‚Äúfake news‚Äù -- defined by the New York Times as ‚Äúmade-up
stories written with the intention to deceive‚Äù and published in formats similar to those of traditional ‚Äúreal‚Äù
news -- has arisen as a threat to high-quality journalism and well-informed public discourse. In particular,
fake news has been accused of increasing political polarization and partisan conflict in the United States
during the divisive 2016 presidential campaign and the early days of the Donald Trump administration.

The task is organized around the more well-defined problem of ‚Äústance detection,‚Äù which involves
comparing a headline with a body of text from a news article to determine what relationship (if any) exists
between the two. There are 4 possible classifications:
1. The article text ‚Äã agrees with‚Äã the headline.
2. The article text ‚Äã disagrees with‚Äã the headline.
3. The article text ‚Äã is a discussion of‚Äã the headline, without taking a position on it.
4. The article text ‚Äã is unrelated to‚Äã the headline (i.e. it doesn‚Äôt address the same topic).



## Expected Submission

A machine learning model which can be used to solve the stance detection problem with high accuracy might effectively be
used either as a tool for humans working to identify fake news (e.g., retrieving articles that agree, disagree
and discuss a headline), or as a building block for a more elaborate future AI system that would try to
identify the actual truthfulness of news stories

## Evaluation

The model thus created should be analyzed in all the parameters like precision,recall,and accuracy for each four classes as well as overall dataset.","","Creating a text  classification model for finding categorising similarity between two input text","Classify article headline and body similarity in four categories,agree,disagree,discuss,unrelated.","","0"
"3944","1249044","940461","04/03/2021 13:15:46","## Task Details

## Expected Submission
Developers should submit notebooks describing their approach and results having f1 score outputs for each of the 14 classes

## Evaluation
The parameter for best submission is the best f1 score classwise as well as overall.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/abhinavkrjha/using-vgg16-final-layer-92-acc-fast-training
- https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification/comments","","Create a Dog breed classification model","Creating the best model to classify the various breeds of dogs","","0"
"4489","1366920","972998","05/25/2021 19:21:56","Researchers continue to write new articles every day. It often takes a long time to find the right journal for their new articles. This dataset can be used to solve this problem. For example, when someone who writes an article on new drug discovery for COVID-19 enters the abstract of the article, a model can be developed that outputs the most relevant journals with that summary.","","Journal Recommendation System for COVID-19 Researches","Recommend the most relevant journals according to the abstracts.","","0"
"4490","1366920","972998","05/25/2021 19:24:54","Hundreds of new articles are published every day. Many of these articles are not even read at all. Use this data set to predict which article will be cited and which article will not be cited at all.","","Citation Count Prediction for COVID-19 Researches","Predict the number of citations an article will receive.","","0"
"4491","1366920","972998","05/25/2021 19:28:21","The dataset contains thousands of COVID-19-related article abstracts. Based on these article summaries, generate article data that are realistic but actually fake.","","Generate Fake Research Paper Abstracts for COVID-19","Create realistic but fake articles based on article summaries.","","0"
"4492","1366920","972998","05/25/2021 19:30:24","There are thousands of published article titles in the data set. Using these article titles, develop a model that generates new and original research ideas.","","Generate New Research Paper Ideas for COVID-19","Generate new article ideas using generator systems.","","1"
"3034","1056739","984523","12/24/2020 11:59:29","Already exists an analysis, should check all libraries are available on Kaggle and port.","","Import lexical analysis from GitHub.","Integration and porting.","","0"
"3007","1031194","984523","12/22/2020 09:35:53","## Task Details
More metadata about abstract contents can provide document frequency, and can be applied to a word embedding system, for example using word2vec.

## Expected Submission
A PCA or T-SNE or other visualization of clusters of concepts in this data is very interesting.

## Evaluation
Any sort of visualization of concept clusters, using pre-trained models are also good.","","Abstract mining: document frequency","Full URLs are available. Get metadata using crawling...","","0"
"2607","951253","993667","11/02/2020 06:24:15","## Task Details
Most often the best performers are projected in news but rarely are those that perform poorly for their teams. 

It's time to identify the best performers in batting and bowling(perhaps even fielding?). 

Find the best batting partnerships, the worst partnerships, best bowlers(wickets/economy/average), worst bowlers and those batsmen and bowlers that perform below their team averages/numbers. 
Also rank teams based on their batting and bowling performance. 

## Expected Submission
Use creative and beautiful visualizations as you can. You can use Tableau or such to get the best visualizations.","","Identifying Best And Worst Performers of IPL 2020","","","0"
"3931","1245844","1000833","04/01/2021 17:03:40","Some of the key feature that can be extracted from this data set are:
- Most dangerous city/province for women
- Top most reason for violence against women in Turkey
- Average age of victims at the time of crime
- Most probably perpetrator - relation involved in the most number of crimes against women in Turkey","","Visualizing the Key Features in the Dataset","Capture and showcase the key features in the available data","","0"
"4869","1422073","1000833","06/21/2021 04:56:01","## Task Details
**1. Clean the Data:**
- Taking websites (mentioned in the business description) and putting them in website column
- Find duplicate records in Business name column and remove the redundant data if found
- Remove the leading and trailing spaces, '/n' from Full address column 

**2. Data Insights:**
- What are popular categories, and business types ?
- Top 5 cities, and province with most registered businesses 
-  Number of empty values in every column (except votes/ratings/website)
-  Popular word cloud for business descriptions
-  Common First and Last Names in contact person","","Visualizations for the Given Data","","","0"
"2848","1014124","1001888","12/03/2020 21:25:13","I would love to see notebooks from the kaggle community that perform better than my sample_solution notebook in terms of the dice score.","","Perform better than sample solution","","","0"
"2849","1014124","1001888","12/03/2020 21:26:07","I would love to see solutions that are very easy to understand for students.

Context: This dataset was created as part of an [online course](https://thomasfermi.github.io/Algorithms-for-Automated-Driving/Introduction/intro.html). Students of this course would profit from different well explained approaches to the image segmentation problem.","","Create a short and easy to understand solution","","","0"
"3264","1111852","1003427","01/24/2021 10:03:00","## Task Details
Implement an algorithm for Thai Text Generation","","Text Generation","","","0"
"3436","968621","5168032","02/08/2021 11:31:27","Make predictions.","","Predictions","","","0"
"4306","1263247","1014084","05/09/2021 12:38:58","## Task Details
- Find the performance of the party.
- Find the performance of the candidates.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

- Solution should contain graphs

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help","","Analysis election data","Analysis candidate performance","","0"
"3699","1188367","1017089","03/09/2021 04:04:07","The goal is to predict the price of the second hand apartments - transaction price (total)_log
-- Evaluation metric is MAE - Mean Absolute Error","","Predict secondhand apartment prices","","","0"
"3542","1169015","1021612","02/19/2021 10:22:40","## Task Details
Train a deep learning network (autoencoder) on this dataset to create image denoiser.","","Create Image Denoiser","","03/19/2021 23:59:00","0"
"4583","1380359","1036462","05/31/2021 18:50:25","## Task Details
This challenge is about a pediatric clinic where patients are scheduled over the next 5 days, and no walk-in allowed. Patients call for an appointment and assigned to the provider based on availability and preference. Most patients show on their appointment time, others do not, and these are called no-shows which are included in the dataset as 0 minutes. In a regular weekday clinic providers work 2 sessions (or blocks), where each session takes 3.5 hours. Providers start seeing patients at 8:30 AM up until the lunch break, 12:00 PM for the morning sessions. Afternoon hours scheduled between 1:00 PM and 4:30 PM for afternoon sessions. Providers must finish serving all patients that show up. if providers work after hours, it is considered overtime.
We consider 30 minutes time slots where patients are punctual, arrive on time, and check in with the registration desk. After arrival, the patient meets with the provider whenever the provider is available.

The dataset is a make-up duration for each of the 150 patients. Again if the duration is 0 the patients did not show up.

Here the goal is to visualize scenarios in ## Task Details
This challenge is about a pediatric clinic where patients are scheduled over the next 5 days, and no walk-in allowed. Patients call for an appointment and assigned to the provider based on availability and preference. Most patients show on their appointment time, others do not, and these are called no-shows which are included in the dataset as 0 minutes. In a regular weekday clinic providers work 2 sessions (or blocks), where each session takes 3.5 hours. Providers start seeing patients at 8:30 AM up until the lunch break, 12:00 PM for the morning sessions. Afternoon hours scheduled between 1:00 PM and 4:30 PM for afternoon sessions. Providers must finish serving all patients that show up. if providers work after hours, it is considered overtime.
We consider 30 minutes time slots where patients are punctual, arrive on time, and check in with the registration desk. After arrival, the patient meets with the provider whenever the provider is available.

The dataset is a make-up duration for each of the 150 patients. Again if the duration is 0 the patients did not show up.

Here the goal is to visualize scenarios such a way that they can easily be compared. Few important variables are:
Number of patients scheduled
Total patient wait time

Using the data, I created a Gantt chart for scenario-1 and would like to compare it with other scenarios.

## Expected Submission
Current setting there are 14 patients scheduled where no-show is not considered. Here the goal is to compare scenarios with visualization if different levels of overbooking are allowed. 

There are 7 scenarios but you may add based on your creativity:

Schedule 2 extra	patients at 8:30 and 13:00-  check plotly code for this
Schedule 4 extra patients at 8:30, 10:30, 13:00, and 15:00
Schedule 6 extras  8:30, 10:00, 11:30, 13:00, 14:30, 16:00
Schedule 8 extras 8:30, 9:30, 10:30, 11:30, 13:00, 14:00, 15:00, 16:00
Schedule 10 extras 8:30, 9:00, 10:00, 10:30, 11:30, 13:00, 13:30, 14:00, 14:30, 15:30
Schedule 12 extras 8:30, 9:00, 9:30, 10:30, 11:00, 11:30, 13:00, 13:30, 14:00, 15:0, 15:30, 16:00
Schedule 14 extras 8:30, 9:00, 9:30, 10:00, 10:30, 11:00, 11:30, 13:00, 13:30, 14:00, 14:30, 15:00, 15:30, 16:00



## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Novice visualization of appointment blocks and comparing various scheduling scenarios.","","06/30/2021 23:59:00","1"
"3748","1207742","1042462","03/13/2021 03:12:26","## Task Details
The University of Vermont has made several cuts to their humanities and arts programs (see [this article](https://vtdigger.org/2020/12/03/uvm-to-eliminate-23-programs-in-the-college-of-arts-and-sciences/) by VTDigger news). Cited reasons have been due to a combination of low enrollment and budget shortfalls amidst the pandemic. My suggested task is to to Investigate how salaries have varied over time relative to different departments. Have arts and humanities been on the decline for awhile or was it mainly the pandemic? 

## Expected Submission
A notebook with visualizations in a language of your choice

## Evaluation
Insightful/interactive visualizations and documentation/story-telling","","Visualize how the university has valued their departments over time","Arts & Humanities vs. STEM fields","","0"
"3643","1169769","1061323","03/02/2021 16:42:57","## Task Details
The legal language is very special in many regards compared to regular natural language. It is highly structured, rather complicated, contains its own special terms and uses certain words differently than they are used in regular text.

Text classification is simple to define but has a myriad of possible applications and good systems can provide immense value. Common general applications of text classification include for example:
- Spam filtering
- Email priority rating
- Topic classification
And in the legal domain:
- Legal judgement prediction (predict outcome of a case based on description of case's facts)
- Legal area prediction

So in this task, you will predict the court_id based on the text of a court decision. The court_id is structured in the form of {canton}_{court}_{chamber_number} (e.g. SG_KG_002 =&gt; St. Gallen, Kantonsgericht, 002).


## Expected Submission
Please submit a csv file containing the ids of the test set and your associated predicted court_ids. 

## Evaluation
Your solution will be rated by the achieved F1-Score.","","Text Classification","Can you predict the court id based on the text?","03/25/2021 23:59:00","0"
"4349","1336488","1063067","05/12/2021 15:00:51","1. Read the dataset, clean the data and prepare a final dataset to be used for analysis.
2. Perform detailed statistical analysis and EDA using univariate, bi-variate and multivariate EDA techniques to get a data driven insights on recommending which teams they can approach which will be a deal win for them. Also as a data and statistics expert you have to develop a detailed performance report using this data.
3. Please include any improvements or suggestions to the association management on quality, variety, velocity, veracity etc. on the data points collected by the association to perform a better data analysis in future","","Tasks for Basketball.csv dataset","","","1"
"3506","1159906","1077334","02/15/2021 18:26:46","As a financial institution regulated by the FCA, Bank has the obligation to verify the identity of all customers who want to open a bank account. Each prospective customer has to go through the process by submitting a government-issued photo ID and a facial picture of themselves to our technical partner. The ‚Äúpass rate‚Äù is defined as the number of customers who pass both the process divided by the number of customers who attempt the process.

Problem: The pass rate has decreased substantially in the recent period.

You need to analyze the data and make a decision about the reasons.","","Find causes of decrease the pass rate","","","0"
"4106","1260946","1092605","04/17/2021 21:36:29","## Task Details
The aim is to build a template for machine learning stack implementation from preprocessing to prediction.

## Expected Submission
Use Palmer Penguins Dataset and try to use sci-kit learn stackingClassifier ensemble model. 

## Evaluation
Evaluate the model with accuracy_score, matthews_corrcoef, f1_score.

### Further help
If you need additional inspiration, check out these existing high-quality tasks from Kaggle's sample works.","","How to stack ML models with the palmer penguins.","","04/25/2021 23:59:00","0"
"4311","1327238","1102236","05/09/2021 14:42:17","Sentiment Analysis for each month","","Sentiment Analysis","","","0"
"4346","1327238","858250","05/12/2021 10:45:33","Capture the trends in the tweets with respect to time as the severity of Covid-19 changes.","","EDA Analysis","","","1"
"4453","1309039","1102236","05/21/2021 07:00:56","Visualise how different countries were impacted by covid.","","Visualisation","","","0"
"4457","1357799","1102236","05/21/2021 14:54:28","Visualise the preparedness of states","","Visualisation","","","0"
"4374","1339590","1102236","05/14/2021 06:38:52","Visualisation for each month","","Visualisation","","","0"
"4387","1339590","1102236","05/14/2021 14:00:41","Given Details about a trip:
&gt; Age
&gt; Month, Date, Time
&gt; Gender,
&gt; Location
**Predict the duration of trip**","","Predict Duration of Trip","","","0"
"4844","1419308","1102236","06/19/2021 13:55:44","Visualise how countries compare in environment data over the past decade.","","Visualisation","Visualise change over the years in countries","","0"
"3558","1171622","1115324","02/20/2021 19:30:59","Both column  has numerical.","","Predict the best hours for best score.","","","1"
"2815","1005129","1134139","11/29/2020 17:46:11","## Task Details
Predict the player's value using meaningful features.

## Expected Submission
A notebook.

## Evaluation
Detailed and clearly explained notebook of solution.","","Prediction of player value","","","2"
"2816","1005129","1134139","11/29/2020 17:49:34","## Task Details
Which player should you transfer for the team X? You can try to find the weakest position of a team and try to find the best possible player to fill it. You should also consider the transfer budget of the team.

## Expected Submission
A notebook.

## Evaluation
Detailed and clearly explained notebook of solution.","","Who should you transfer?","","","1"
"2817","1005129","1134139","11/29/2020 17:52:17","## Task Details
Predict player's all overall rating for every position (ST = 92, CAM = 84 etc.)

## Expected Submission
A notebook.

## Evaluation
Detailed and clearly explained notebook of solution.","","Predict player's all overall rating for every position","","","2"
"2818","1005129","1134139","11/29/2020 17:53:42","## Task Details
Get most similar 11 using player images

## Expected Submission
A notebook.

## Evaluation
Detailed and clearly explained notebook of solution.","","Create look-alike 11","","","1"
"3116","1007061","1136755","01/03/2021 11:54:13","‚Ä¢ Can you train a model that identifies damaged packages using both, top and side view?
‚Ä¢ Can you train a model that identifies damaged packages using just the top view?","","Image classification","","","0"
"3117","1007061","1136755","01/03/2021 11:54:53","‚Ä¢ Can you train a model that performs OCR to extract the serial numbers (some SN may be blurry due to simulated motion blur)","","Optical Character Recognition","","","0"
"3463","1150161","1141083","02/11/2021 08:40:31","## Task Details
Detecting the horse riding activity from the rider's movement pattern.

## Evaluation
The most important metric to evaluate against is the number of times the wrong activity is predicted. The second most important metric is the time a wrong activity is predicted.

For example if a 5 min segment was annotated as canter and this was predicted as canter, but with 4 shorter 10 second segments of trot this would mean 4 errors with an error time of 40 seconds.

The metric to beat (Equilabs current algorithm) is 562 errors with a total error time of 13 minutes in all the annotated trainings.

**Note 1**
*The exact timing of an activity is not that important and if a segment it does not have to . For example if the annotations indicate that canter started 100 seconds into the training, but the predicated canter activity started 103 seconds into the training it does not have to be counted as an error.*

**Note 2**
*It is fine for any activity to be predicted when the annotated activity is ""unknown"". Unknown is not an activity, but indicates it is unknown what activity the rider was doing at this time and could have been any of the other activities or even something else such as walking beside the horse etc.*","","Horse riding activity recognition","Detecting the horse riding activity from a smartphone sensor data","","1"
"3492","1156836","1158131","02/14/2021 05:50:31","## Task Details
Classify images into the following 6 categories-
Airplane
Candle
Christmas_Tree
Jacket
Miscellaneous
Snowman

## Expected Submission
hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/problems/

## Evaluation
hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/problems/

### Further help
hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-holidays/problems/","","Classify the images into 6 categories","","","0"
"4758","1395476","2123792","06/13/2021 07:49:53","# import fast ai vision library
from fastai.vision.all import *
from pathlib import Path
data_path = Path(""../input/bird-speciestiny"")
df = pd.read_csv(data_path/'bird-species-tiny.csv')
df.tail()
dls = ImageDataLoaders.from_folder(path=data_path, 
                                   train='train', 
                                   valid='test',
                                   shuffle=True)
dls.train.show_batch()
dls.valid.show_batch()
learn = cnn_learner(dls, 
                    resnet18, 
                    metrics=[accuracy, error_rate])


learn.fine_tune(4
interep = ClassificationInterpretation.from_learner(learn)
interep.plot_confusion_matrix()
interep.plot_top_losses(10, figsize=(35,15)) #click to enlarge

#my work https://www.kaggle.com/bouille/bird-speciestiny","","Fast ai with the birds","Use of ImageDataLoaders.from_folder","","1"
"4877","1421822","1176851","06/21/2021 12:19:04","Com base nas informa√ß√µes disponibilizadas no dataset, tente criar um modelo de aprendizado de m√°quina para perver qual time vencer√° a partida com base em suas estat√≠sticas pr√©vias √† partida.","","Predict Wins","Create a machine learning model to predict the winner","","0"
"2602","950809","1185442","11/01/2020 20:16:09","## Identify potential customers
Let‚Äôs say your client is a holiday operator. They would like to advertise their trips to potential customers online. They‚Äôve provided a sample of users that we know have bought tours from them in the past, which are saved as user_uids in conversion.csv. We would like to know which of the other users in our dataset are likely to be interested in their offers as well.

Create a model using an algorithm of your choice to predict holiday interest and populate the field p_conv in submission.csv with predictions for the respective user_uids given in the file.

## Data Intro
- data.csv contains raw data with user_uids and associated ‚Äúsegments‚Äù (variables that represent what we know about these users). Each segment is identified by an ID and you will find a corresponding label in the taxonomy.csv file. The segments are also marked as P (when a user had the opportunity to click on an option in the quiz) and A (when the user actually clicked on an option in the quiz). Some segments are represented by binary variables, others by continuous variables.
- taxonomy.csv gives category labels for the variables in data.csv. Labels have an intrinsic hierarchy delimited by ¬ß characters.
- conversion.csv represents conversion data we may receive from a client. These are customers that we know have purchased a product of interest in the past.
- submission.csv lists the user_ids that we would like predictions for.","","Identify potential online customers","","","0"
"2615","952329","1185442","11/02/2020 18:54:31","## Task
Main task is to generate actionable insights from this pet food customer orders dataset. 
As a focus area, try to address the following questions: 

- Which customers order and reorder the wet food, and when are they likely to try it? 
- Are there certain characteristics of the pets, customers or their orders that impact how likely they are to purchase the wet food, and keep ordering it? 
- Based on this data, what do you suggest we focus on to get more customers taking and continuing to enjoy our wet food? 


## Data Intro
The data set is a selection of customers and their orders, with‚Ä©some pet characteristics and details of what they got in each order.

Orders are numbered in sequence for each pet and every order contains dry food, which is the core offering of all online pet food companies. Some customers also purchase wet food (the subject of this challenge) alongside their dry food and in this dataset you also have the wet food order sequence. So if a customer has 2 dry food only orders followed by an order containing dry and wet food, then they will have a row in this dataset with petordernumber = 3, and wetfoodorder_number = 1.

This customer order dataset is derived from a subscription business. If a pet has an active subscription, then the business will make and deliver an order every 31 days, which contains dry food and whatever other products a customer has on their subscription. A customer can remove wet food and treats from their orders and still keep an active subscription, the business will just continue to make and send them‚Ä© their dry food.","","Insights from online pet food customer orders","","","2"
"2612","952011","1185442","11/02/2020 14:55:52","# Modelling / Machine Learning Car Crash Data
Data is provided on crash frequencies among various randomly selected cohorts, along with the risk variables in those cohorts. Our task is to identify correlations and model the data. 


## Data Intro
The following columns have crash frequencies:
  'Frontal Impact (per 100,000 vehicles)',
  'Rear Impact (per 100,000 vehicles)',
  'Front Offset Impact (per 100,000 vehicles)',
  'Side Impact (per 100,000 vehicles)'

The following columns have data on risk variables:
  'Average EuroNCAP Lane Deviation Score', 
  'Driver Claim History Rating',
  'No Year no claims', 
  'Gender'","","Modelling car crash data","","","0"
"2595","948814","1185442","10/31/2020 12:31:04","## Fly-tipping
- Fly-tipping is illegal dumping of waste
- Fly-tipping comes at an annual cost of over ¬£60 million a year to the Local Authorities in England
- In the year 2018-19 over 1.07 million incidents of fly-tipping were recorded in England


## Data Intro
Open data is published by the UK Government, which can be found here:
https://data.gov.uk/ 

This dataset provides a summary of fly-tipping incidents in England, by local
authority. This is the primary data for this task. It contains Local Authority names. These are well supported by national statistics published by ONS. We can  complement the fly-tipping data with data from anywhere on the web (e.g. data.gov.uk). 

## Task: 
Do exploratory data analysis of flytipping data. Build a machine learning model of your choice to explain the phenomenon.","","Flytipping data analysis and modelling, England (UK)","EDA and modelling","","0"
"2573","946164","1185442","10/29/2020 16:45:58","## Data Intro
A sample of labelled search term category data is provided in the csv file, trainSet.csv. The file contains two columns: the search term and the search term category. The search term category has been indexed. There are 606,823 examples in the datset with 1,419 different search term categories. There are roughly 427 examples in each category. 

## Task
We have to construct a classification model that can accurately categorise search terms in candidateTestSet.txt. Feel free to use whatever model you feel is appropriate for the task.","","Search terms classification","","","0"
"2832","1009882","1191029","12/01/2020 15:00:59","## Task Details
This is a very simple task. You need to clean the dataset then you need to find the future closing price.

## Expected Submission
create a simple future value submission.csv which will contain trade code and future closing value

## Evaluation
self evaluation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find the future closing value from this data set","","","1"
"3790","1215557","1191029","03/16/2021 18:04:22","## Task Details
All the data have mixed values (English, Bangla  and banglish) you need to create a model for sentiment analysis.

## Expected Submission
Submission should be negative or positive in binary form 0,1.

## Evaluation
Self-practice.

### Further help
PM me on linkedIn: https://www.linkedin.com/in/muntakim1","","Create A sentiment analysis","","","0"
"3791","1215557","1191029","03/16/2021 18:06:49","## Task Details
This data contains garbage data so clean the data before perfoming operations.

## Expected Submission
A clean data set with unique posts.

## Evaluation
Self-evaluation

### Further help
Pm me in LinkedIn: https://www.linkedin.com/in/muntakim1","","Perform a data cleaning process","","","0"
"4003","1258168","1191029","04/08/2021 09:47:11","## Task Details
Find the pattern of earthquake.","","Find any pattern","","","0"
"3884","1236550","1192971","03/27/2021 18:56:18","## Task Details

&gt; Music actors operate several tools to achieve high-quality musical artifacts. One of the typically used tools is the digital multi-effects processor, which applies a chain of non-linear transformations (effects) to audio signals. However, the large number of available configurations implies that better use of such equipment is restricted to specialists. In this context, intelligent systems for music production aim to propose automatic tools to ease and support decision making by music actors. -- Introduction of [Audio Plugin Recommendation Systems for Music Production](https://ieeexplore.ieee.org/document/8923658)

Suppose that a person has a Zoom G3 audio equipment. This person, are starting to creating a patch. Probably, the user doesn't knows the utility of all the audio plugins, then he only select some audio plugins and put they in specific positions.

A patch into the Zoom G3 is a sequence of N=6 audio plugins. This equipment has 117 audio plugins. Each audio plugin can be used in any position. Also, it's possible to use a same audio plugin in more than one position in a patch. In the following figure, each blocks correspond to the instances of the audio plugins in the position defined in the patch. 
![A patch representation](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/master/readmedata/Figure-14-dissertation.jpg).
&gt; Image extrated from the dissertation <a href=""https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical"">Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical</a> (Figure 14).

In this sense, create a recommendation system that recommends audio plugins from incomplete patch. You can try: 
 * Plugin recommendation on a single position (M = N-1);
 * Plugin recommendation on multiple positions (M &lt; N-1);

This task is represented on the following image (letter `a`).
![Audio plugins recommendation cases](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/zoom-g3-2019-edition/readmedata/Figure-6-dissertation.jpg)
&gt; Image extrated from the dissertation [Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical) (Figure 6).

## Expected Submission

Is expected solutions based on recommenders systems. The models can be trained from the `patches-filtered.csv` dataset. Try to create cool Notebooks for de system recommender. You can use the [Zoom G3 image plugins collection](https://github.com/SrMouraSilva/ZoomG3-Patches/tree/master/images) to enrich the visualization. You can also condition recommendations based on the plugins categories (see `plugins-categories.csv` file).

## Evaluation

We recommends this following metrics, [based on the original paper](https://www.researchgate.net/publication/336830006_Audio_Plugin_Recommendation_Systems_for_Music_Production):

* **Hit**@k: Verifies if the expected item is within the top-k positions ([reference](https://dl.acm.org/doi/10.1145/502585.502627)). You can use `k=1` (standard accuracy) and `k=5`, where the number 5 was chosen as an acceptable number  of  plugins  that  a  user  would  evaluate  during  a patch configuration procedure;
* **MDCG** (Mean  Discounted  Cumulative  Gain):  Indicates how  close  an  item  is  in  the  first  positions  by  assigning greater  relevance  to  the  results  whose  expected  item  is in  the  top  positions.  The  MDCG  score  is  computed  as follows:
```latex
MDCG@k = 1/k \sum_{i=1}^k (2^(rel(i)-1)) / (log_2(i+1)).
```
The `rel(i)` relevance  function  is  applied  as  an indicator function that returns `1` if the i-th element is the expected item and `0` otherwise. Use `k=117`;
* **MAP**@k:  Indicates  the  proportion  of  the `k=5` recommended  items  that  are  in  the  same  category  as  the expected  item ([reference](https://dl.acm.org/doi/abs/10.1145/1390334.1390453)).  It  is  expected  that  this  metric  will give  an  insight  into  cases  where  the  recommended  item does  not  match  what  was  expected  but  the  item  is  still eligible, due to it being sufficiently similar.

## References:

* [Audio Plugin Recommendation Systems for Music Production](https://www.researchgate.net/publication/336830006_Audio_Plugin_Recommendation_Systems_for_Music_Production)
```bibtex
@inproceedings{da2019audio,
  title={Audio Plugin Recommendation Systems for Music Production},
  author={da Silva, Paulo Mateus Moura and Mattos, C{\'e}sar Lincoln Cavalcante and de Souza J{\'u}nior, Amauri Holanda},
  booktitle={2019 8th Brazilian Conference on Intelligent Systems (BRACIS)},
  pages={854--859},
  year={2019},
  organization={IEEE}
}
```
* [Disserta√ß√£o - Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical)","","Recommends audio plugins from a incomplete patch","An audio plugins recommendation system for music production task","","0"
"3885","1236550","1192971","03/27/2021 20:17:12","## Task Details

&gt; Music actors operate several tools to achieve high-quality musical artifacts. One of the typically used tools is the digital multi-effects processor, which applies a chain of non-linear transformations (effects) to audio signals. However, the large number of available configurations implies that better use of such equipment is restricted to specialists. In this context, intelligent systems for music production aim to propose automatic tools to ease and support decision making by music actors. -- Introduction of [Audio Plugin Recommendation Systems for Music Production](https://ieeexplore.ieee.org/document/8923658)

A patch into the Zoom G3 is a sequence of N=6 audio plugins. This equipment has 117 audio plugins. Each audio plugin can be used in any position. Also, it's possible to use a same audio plugin in more than one position in a patch. In the following figure, each blocks correspond to the instances of the audio plugins in the position defined in the patch. 
![A patch representation](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/master/readmedata/Figure-14-dissertation.jpg).
&gt; Image extrated from the dissertation <a href=""https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical"">Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical</a> (Figure 14).

Create a recommendation system that recommends a full patch.

This task is represented on the following image (letter `b`).
![Audio plugins recommendation cases](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/zoom-g3-2019-edition/readmedata/Figure-6-dissertation.jpg)
&gt; Image extrated from the dissertation [Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical) (Figure 6).

## Expected Submission

Is expected solutions based on recommenders systems based on generative models. The models can be trained from the `patches-filtered.csv` dataset. Try to create cool Notebooks for de system recommender. You can use the [Zoom G3 image plugins collection](https://github.com/SrMouraSilva/ZoomG3-Patches/tree/master/images) to enrich the visualization. You can also condition recommendations based on the plugins categories (see `plugins-categories.csv` file).

## Evaluation

We recommends that the patch selection would be based on the (log) likelihood of the generated patch is in the dataset. Also, you can select the train and test patches to check the (log) likelihood. Try to compare the (log) likelihood of uniform generated samples. You can try to detects [out-of-distribution](https://paperswithcode.com/task/out-of-distribution-detection/latest) samples. 

## References:

* [Audio Plugin Recommendation Systems for Music Production](https://www.researchgate.net/publication/336830006_Audio_Plugin_Recommendation_Systems_for_Music_Production)
```bibtex
@inproceedings{da2019audio,
  title={Audio Plugin Recommendation Systems for Music Production},
  author={da Silva, Paulo Mateus Moura and Mattos, C{\'e}sar Lincoln Cavalcante and de Souza J{\'u}nior, Amauri Holanda},
  booktitle={2019 8th Brazilian Conference on Intelligent Systems (BRACIS)},
  pages={854--859},
  year={2019},
  organization={IEEE}
}
```
* [Disserta√ß√£o - Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical)","","Full patch recommendation and Out-of-Distribution Detection","An audio plugins recommendation system for music production task","","0"
"3886","1236550","1192971","03/27/2021 20:34:11","## Task Details

&gt; Music actors operate several tools to achieve high-quality musical artifacts. One of the typically used tools is the digital multi-effects processor, which applies a chain of non-linear transformations (effects) to audio signals. However, the large number of available configurations implies that better use of such equipment is restricted to specialists. In this context, intelligent systems for music production aim to propose automatic tools to ease and support decision making by music actors. -- Introduction of [Audio Plugin Recommendation Systems for Music Production](https://ieeexplore.ieee.org/document/8923658)

Suppose that a person has a Zoom G3 audio equipment. This person are creating a patch. He selects all the N=6 audio plugins, but the positioning defined by the user maybe isn't the best.

A patch into the Zoom G3 is a sequence of N=6 audio plugins. This equipment has 117 audio plugins. Each audio plugin can be used in any position. Also, it's possible to use a same audio plugin in more than one position in a patch. In the following figure, each blocks correspond to the instances of the audio plugins in the position defined in the patch. 
![A patch representation](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/master/readmedata/Figure-14-dissertation.jpg).
&gt; Image extrated from the dissertation <a href=""https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical"">Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical</a> (Figure 14).

In this sense, create a recommendation system that recommends the best positioning of the audio plugins based in the acknowledgement learned from the data train.

This task is represented on the following image (letter `c`).
![Audio plugins recommendation cases](https://raw.githubusercontent.com/SrMouraSilva/ZoomG3-Patches/zoom-g3-2019-edition/readmedata/Figure-6-dissertation.jpg)
&gt; Image extrated from the dissertation [Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical) (Figure 6).

## Expected Submission

Is expected solutions based on recommenders systems. The models can be trained from the `patches-filtered.csv` dataset. Try to create cool Notebooks for de system recommender. You can use the [Zoom G3 image plugins collection](https://github.com/SrMouraSilva/ZoomG3-Patches/tree/master/images) to enrich the visualization. You can also condition recommendations based on the plugins categories (see `plugins-categories.csv` file).

## Evaluation

The evaluation can be done by counting the proportion of the audio plugins that are correctly positioned.

## References:

* [Audio Plugin Recommendation Systems for Music Production](https://www.researchgate.net/publication/336830006_Audio_Plugin_Recommendation_Systems_for_Music_Production)
```bibtex
@inproceedings{da2019audio,
  title={Audio Plugin Recommendation Systems for Music Production},
  author={da Silva, Paulo Mateus Moura and Mattos, C{\'e}sar Lincoln Cavalcante and de Souza J{\'u}nior, Amauri Holanda},
  booktitle={2019 8th Brazilian Conference on Intelligent Systems (BRACIS)},
  pages={854--859},
  year={2019},
  organization={IEEE}
}
```
* [Disserta√ß√£o - Sistemas de Recomenda√ß√£o de Plugins de √Åudio para Produ√ß√£o Musical](https://www.researchgate.net/publication/349648078_Dissertacao_-_Sistemas_de_Recomendacao_de_Plugins_de_Audio_para_Producao_Musical)","","Plugins positioning in a given patch","An audio plugins recommendation system for music production task","","0"
"3132","1063531","1213271","01/05/2021 06:39:02","## Task Details
Using this dataset try to find the trend in the number of cases and use this dataset to predict the number of cases around new year","","Find Trend","","","0"
"3190","1093874","1232675","01/14/2021 09:15:24","The task is to create a model to predict the refractive index of a mineral only from it' chemical composition. You may use other features if it improves your model. 

You may try to predict as many properties as possible - predicting the crystal structure from the data would be impressive. 

Use this dataset to create your train, test samples. There are around 600 minerals whose refractive index is available. 

The evaluation will be based on the r2 score of prediction on the test set. The test set should contain no fewer than 100 samples that have not been by the model during training.","","Predicting refractive index of minerals from chemical formula","Create a predictive model for any of the properties of minerals - particularly refractive index - from available data","03/01/2021 23:59:00","1"
"3191","1093874","1232675","01/14/2021 09:17:22","Create a model that predicts the crystal structure of an inorganic compound from the data available - particularly the chemical formula.","","Predict crystal structure of compounds","Predict the crystal structure of inorganic compounds from the available data","03/01/2021 23:59:00","0"
"2993","989873","4765384","12/20/2020 16:41:42","Fill in the missing data, visualize the relationship between the data, and cluster","","Cleaning, visualization and clustering","","","0"
"2675","967440","1239334","11/11/2020 09:41:12","I have created this dataset. This is just now only the preliminary version.
More data about plant food will be included in the future.
This data is directly taken from Wikipedia, so I own no ownership of the data https://en.wikipedia.org/wiki/Millet.

Here what i need to study is whether consuming millets can improve neurological functions. As I rea, vitamin B12 is has a hand in improving brain functions. So, which millet has the highest Vitamin B-12 content. 

Anyone can contribute to the dataset.","","How can millets help in improving neurological functions ?","","","0"
"4405","1343930","1260510","05/16/2021 10:39:39","Explore the Data","","Conduct an EDA","","","0"
"5995","1444816","1260510","09/03/2021 20:40:24","We talk about how certain countries were under-reporting COVID statistics, how significant are those results?","","Is under-reporting a problem with COVID?","","09/30/2021 23:59:00","0"
"4301","1322414","1295006","05/07/2021 19:41:58","## Vaccination Time Slot

What is the best time slot to get vaccinated as you don't want to go when there is peak time as the probability of picking the virus is more when more people are around!","","What is the best time slot to get vaccinated?","","","0"
"3251","1030906","1295006","01/22/2021 18:42:18","## Task Details
This data contains comprehensive information about the landscape of data science in a country.

## Expected Submission
Submit a notebook which does a survey of data science landscape in your residing country.

## Evaluation
Interactive Visualizations, Hidden secrets.","","Landscape of Data Science according to your current country","","","0"
"4473","1286530","1302444","05/23/2021 18:31:19","## Task Details
Predicting waiting time for a deceased donor kidney transplant can help patients and clinicians to discuss management and contribute to a more efficient use of resources
We build a Cox model that was able to predict the time to kidney transplant.

## Expected Submission
Suggested models: Survival models like Cox or Survival Trees using R or phyton
We suggest the R some R packages: tidymodes; tidyverse and survnip


### Further help
If you need additional inspiration, check out these existing high-quality tasks:

Sapiertein Silva JF, Ferreira GF, Perosa M, Nga HS, de Andrade LGM. A machine learning prediction model for waiting time to kidney transplant. PLoS One. 2021 May 20;16(5):e0252069. doi: 10.1371/journal.pone.0252069. PMID: 34015020.

https://transplantmodels.shinyapps.io/time_list_in_tx/","","Predict Waitlist to Kidney Transplant","","","0"
"2941","1035282","1314380","12/14/2020 18:16:31","## Task Details

Re-create the following data visualization from informationisbeautiful.net:
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1314380%2F47db5c28a395939f9b76984d998ee27a%2Fbest_in_show.png?generation=1607969735409210&alt=media)

Source: https://informationisbeautiful.net/visualizations/best-in-show-whats-the-top-data-dog/


## Expected Submission

A Kaggle notebook that uses Python or R to create a chart that is similar to https://informationisbeautiful.net/visualizations/best-in-show-whats-the-top-data-dog/

## Evaluation
N/A","","Re-create the following data visualization","Dog popularity vs ratings from informationisbeautiful.net","","4"
"3252","1113000","1314380","01/22/2021 21:42:06","## Task Details

Which states distributed the largest number of vaccines last week?  

## Expected Submission

A public Kaggle notebook that generates both: (A) a list of the top 10 states that distributed the most vaccines total; and (B) a list of the top 10 states that distributed the most vaccines per capita -- for the past 7 days.

## Evaluation
N/A","","Which USA state distributed the largest number of vaccines last week?","Top 10 states that distributed the most vaccines per capita -- for the previous 7 days","","8"
"3253","1113000","1314380","01/22/2021 21:52:09","## Task Details

Which states are using their vaccine supplies most efficiently?  

## Expected Submission

A public Kaggle notebook that identifies both the top 10 and the bottom 10 states that have used up the largest portion of their vaccine supplies -- in the past 7 days.

Top 10 and Bottom 10 States for `""share_doses_used""` over past 7 days

## Evaluation

N/A","","Which states are using their vaccine supplies most efficiently?","Top 10 and Bottom 10 States for ""share_doses_used"" over past 7 days","","2"
"3241","1111039","1314380","01/21/2021 20:22:08","## Task Details

Identify the most retweeted images and videos and try to replicate the figures from https://arxiv.org/pdf/2101.08210.pdf and/or https://github.com/sTechLab/VoterFraud2020-analysis.

## Expected Submission

A public notebook shared on Kaggle

## Evaluation

N/A","","Replicate the plots from the VoterFraud2020 publication","Identify the most retweeted images and videos","","0"
"3273","1118569","1314380","01/25/2021 18:10:35","## Task Details

Recreate the [animations](https://1.bp.blogspot.com/-vYmWtLc_x7w/YA3NEe-EA3I/AAAAAAAADFs/CPoZ3mBI9WkQNwpee1ewARSKZjJMvEqhQCNcBGAsYHQ/s1400/topninexy.gif) from the inpredictable.com blog post at https://www.inpredictable.com/2021/01/nba-player-shooting-motions-data-dump.html.

## Expected Submission

A Kaggle Notebook that accomplishes the task.

## Evaluation

N/A","","Recreate the animations from the inpredictable.com blog post","Animated overhead views of three-point windups","","0"
"3315","1125174","1314380","01/29/2021 17:09:48","## Task Details

Identify the top 10 stocks with the largest gains in the past 7 days, for both the largest relative gains and also the largest total gains. 

## Expected Submission

A public Kaggle notebook that accomplishes the task.  It should save a .CSV file that lists the answers to `/kaggle/working/` and should also preview the list within the notebook itself.","","Identify the top 10 stocks with the largest gains in the past 7 days","Largest relative gains and largest total gains","","7"
"3316","1125174","1314380","01/29/2021 17:10:55","## Task Details

Identify the top 10 stocks with the largest losses in the past 7 days, for both the largest relative losses and also the largest total losses. 

## Expected Submission

A public Kaggle notebook that accomplishes the task.  It should save a .CSV file that lists the answers to `/kaggle/working/` and should also preview the list within the notebook itself.","","Identify the top 10 stocks with the largest losses in the past 7 days","Largest relative losses and also the largest total losses","","4"
"3317","1125174","1314380","01/29/2021 17:17:23","## Task Details

Create a notebook that simulates a specific market strategy.  What would your profits have been if you stuck to a specific set of rules?  How would it have performed over the past 30 days, 1 year, or 5 years?  How might it perform over the next 30 days, 1 year, or 5 years?

## Expected Submission

A public Kaggle notebook that accomplishes the task.  It should have both a narrative section (in markdown cells) in addition to some data visualizations.  What market strategy did you choose to evaluate?  What profit or loss would it have yielded?","","Create a notebook that simulates a specific market strategy","What would your profits have been if you stuck to a specific set of rules?","","51"
"3335","1125174","1314380","01/30/2021 12:40:42","## Task Details

Create a weekly updated notebook that documents the profits from your market strategy.  Pick a consistent set of rules that you think will perform profitably in the future.  What were your profits or losses?  Document everything with a table and a line graph and update your notebook once a week to see how your strategy performs against realistic market conditions.

## Expected Submission

A public Kaggle notebook that accomplishes the task.","","Create a weekly updated notebook that documents the profits from your market strategy","Pick a consistent set of rules that you think will perform profitably in the future","","3"
"3479","1154841","1314380","02/12/2021 23:13:14","# [Dubois Visualization Challenge](https://github.com/ajstarks/dubois-data-portraits/tree/master/challenge)
The goal of the challenge is to celebrate the data visualization legacy of W.E.B DuBois by recreating the visualizations from the 1900 Paris Exposition using modern tools.

This directory contains the data and original plates from the exposition; your goal is to re-create
the visualizations using modern tools of your choice (matplotlib, seaborn, plotly ggplot2, etc)

In this repo, there is a folder for each challenge, which includes the images of the 1900 original plates along with the corresponding data. You may submit your re-creations to twitter using the hash tag ```#DuBoisChallenge```

## The Challenges

* challenge01: Comparative Increase of White and Colored Population in Georgia
* challenge02: Conjugal Condition
* challenge03: Occupations of Negroes and Whites in Georgia
* challenge04: Proportion of Freeman and Slaves Among American Negroes
* challenge05: Income and Expenditure of 150 Negro Families in Atlanta, GA, USA
* challenge06: City and Rural Population 1890
* challenge07: Assessed Value of Household and Kitchen Furniture Owned by Georgia Negroes.
* challenge08: The Georgia Negro. A Social Study by W.E.Burghardt Du Bois
* challenge09: Migration of Negroes
* challenge10: Negro Population of Georgia by Counties

## References

To learn about how I re-created the visualizations using [decksh](https://speakerdeck.com/ajstarks/decksh-a-little-language-for-decks), see: [Recreating the Dubois Data Portraits](https://speakerdeck.com/ajstarks/recreating-the-dubois-data-portraits). This presentation contains the full catalog of re-creations at the end.

Also, here is a quick guide to the [""Duboisian"" style](https://github.com/ajstarks/dubois-data-portraits/blob/master/dubois-style.pdf).","","Recreate famous data visualizations from the 1900 Paris Exposition","Dubois Data Portrait Challenge","","0"
"3043","946753","1315989","12/26/2020 05:09:30","E-mail classification is a way of flagging or tagging messages as being of a certain type. Identifying spam email is very important.","","Classifiying emails","Classification of email as ham or spam","","2"
"4402","1342116","1315989","05/16/2021 07:05:29","Analysis on Covid - 19 vaccination in India can provide details on how much percentage of the population is getting vaccinated, gender insights, which time slot is the prime time people prefer","","Covid 19 Vaccination in India","","","3"
"2904","1022315","1341160","12/10/2020 19:22:42","Creating an image classifier, coded in PyTorch preferably and using pretrained VGG19 architecture","","PyTorch Image Classifier VGG19 Challenge","","","0"
"2674","966713","1350598","11/11/2020 03:37:50","Create a Classification Model to classify image","","Create Image Classification Model","","","1"
"4143","1285662","1381447","04/21/2021 11:02:21","Using an image classification model find out which cricketer has the highest resemblance with person in the input image.","","Find doppelganger","Find cricketing doppelganger using your image","","1"
"4944","1437956","1383867","06/29/2021 10:48:55","Let the tourist know about the weather of differet cites in Pakistan","","Let the tourist know about the weather of differet cites in Pakistan","","","3"
"6689","1437956","5740467","10/27/2021 10:14:58","To answer this question, the following variables (or more) must be added to the dataset:

- Geographical location (longitude, latitude)
- Province","","Cities located in the south of Pakistan have the highest temperature?","","","3"
"4945","1437976","1383867","06/29/2021 10:55:54","A rather straightforward use of this corpus could be the
style mimicking or transfer of style from the original game.
While the original game is famous we can use a text mining
technique i.e. n-gram or Markov chain for style transfer.","","Style Transfer","","","0"
"4946","1437976","1383867","06/29/2021 10:56:26","Another use of this corpus could be to use it as an input
for any machine learning algorithm and the designer can
thus reduce the training time, errors and other barriers. The
corpus could also be used to auto-complete some part of the
level which the designer has left for the machine learning
algorithms.","","Mixed-initiative Design","","","0"
"4947","1437976","1383867","06/29/2021 10:56:51","The initial application of PCG methods was for data compression.
Since there was limited disk space, the compression
of data was valuable. By using the likewise techniques, we can
efficiently store data and it could also store distinctive features
cheaply.","","Data Compression","","","0"
"4907","1429324","1393578","06/24/2021 14:30:45","The dataset contains 134,000 text snippets, each written in one of seven languages/dialects.
 
## Task Details
The original purpose of the data set was in a [workshop](https://sites.google.com/view/vardial2019/home) about classifying dialects. This task is about taking on the same challenge. 

## Expected Submission
Submit a notebook that classifies each text into one of the seven dialects. Calculate and show relevant metrics, such as a confusion matrix, overall accuracy, etc.","","Detect language and dialect","Create a multiclass classifier to detect which of the seven languages is used","","0"
"2794","995685","1396467","11/27/2020 05:53:04","Yes Bank Limited has been in news for quite some time for the Rana Kapoor Fraud that happened in 2018. At that point, the stock prices of the organization took a major dip. Keeping that in mind, try predicting the Closing Stock Prices of the Organisation for the next year. Here are a few questions to seek answers for:

1. How does the fraud impact the predictive ability?
2. What methods/algorithms work best to predict such data? Time series, Regression, Neural Networks, or something else?
3. Can any additional parameters improve the predictions?
4.  Is Auto ARIMA the correct solution to such cases?","","Predict Closing Stock Prices for the year 2021","","","2"
"2462","926008","1430847","10/17/2020 13:04:06","## Task Details
Dive deeper into understanding the product range from Nike and adidas","","Competitive analysis between Nike and Adidas","","","1"
"2463","926008","1430847","10/17/2020 13:05:06","## Task Details
A clear EDA with good visualization approaches","","EDA on the products provided by nike and adidas","","","3"
"2464","926008","1430847","10/17/2020 13:06:24","## Task Details
Conduct an extensive clustering to identify the common characteristics between the products of Nike and adidas","","Clustering based on the products","","","1"
"3483","1155801","1430847","02/13/2021 13:58:18","## Task Details
Create Notebooks for sentiment analysis from Covid Vaccine tweets","","Tweets sentiment analysis","Sentiment analysis for tweets about Bitcoin","","1"
"3484","1155801","1430847","02/13/2021 13:59:26","Analyze the dynamic of hashtag tweets (temporal, spatial)","","Dynamic in time and space of the tweets","","","1"
"3488","1156273","1430847","02/13/2021 18:33:13","Create Notebooks for sentiment analysis from Covid Vaccine tweets","","Tweets sentiment analysis","Sentiment analysis for tweets about Bitcoin","","0"
"3589","1178072","1430847","02/24/2021 09:14:37","## Task Details
Experts say that ethereum has a huge potential in the future. Do you believe it? Well, let's find it by building our own creative models to predict if the statement is true.","","Let's predict the Prices of ETH","","","0"
"3747","1207554","1430847","03/12/2021 21:42:41","## Task Details
Experts say that Matic has a huge potential in the future. Do you believe it? Well, let's find it by building our own creative models to predict if the statement is true.

Predict the MATIC prices for the next 6months, end of 2021 (For collaborative research work)","","Let's predict the Prices of Matic","","04/30/2021 23:59:00","0"
"4175","1293866","1430847","04/24/2021 23:12:03","## Task Details
Create EDA notebooks on the tweets","","EDA on tweets","","","2"
"4176","1293866","1430847","04/24/2021 23:13:24","## Task Details
Classify the tweets using advanced models","","Tweet classification","","","1"
"4062","1268560","1430847","04/13/2021 10:00:17","## Task Details

Explore and visualize different crypto coins and identify trends and patterns","","EDA Notebooks","","","0"
"4468","1360225","1444085","05/23/2021 12:39:29","## Task Details
Build Machine Learning Model to predict Injuries for Runners

## Evaluation
Official Scoring Metrics - F1 Score","","Build Machine Learning Model to predict Injuries for Runners","","","1"
"3751","1208878","1444085","03/13/2021 16:04:56","Develop Machine Learning Algorithm to detect Android Malware.","","Classification","","","0"
"3059","1063134","1444085","12/28/2020 08:35:00","Predict Airfares for NZ and Jetstar Airlines","","Predictive Analysis for Airfares","","","0"
"3784","1204069","1463717","03/16/2021 01:12:11","## Task Details
This dataset has key information about the evaluation process of thousand of projects. We can identify some features that are related to result of an approval or denied permission.

## Expected Submission
For this, an evaluation using Jupyter notebook is expected. You can create amazing plots and maps to find relevant features of data.","","Find relevant patterns","Find most important findings inside this dataset","","0"
"2634","919713","1483392","11/05/2020 19:56:28","## Task Details
Create English translations so that there could be option for creating translation seq2seq models. 

## Evaluation
Create translation notebook and test using BLEU score.","","Add English Translations","Add English translations to the dataset so that it's use could be extended.","","0"
"2439","919713","1483392","10/15/2020 05:51:08","## Task Details
Hinglish is the language of communication when it comes to texting and communication through text in India. It is being used in almost all domains of chatbots and conversational agents. So with this dataset, I am planning to achieve a baseline dataset for anyone to create and build his language models in order to use it in his own unique application. 

## Expected Submission
Please help me create baseline notebooks either for language modeling or any other task of your own with the dataset.","","Create a new notebook","This dataset needs notebooks explaining what can be done using the dataset.","","0"
"2440","919713","1483392","10/15/2020 05:53:23","## Dataset Details
Hinglish is the language of communication when it comes to texting and communication through text in India. It is being used in almost all domains of chatbots and conversational agents. So with this dataset I am planning to achieve the a baseline dataset for anyone to create and build his language models in order to use the it in his own unique application.

## Task details
The dataset is very brief and requires more data (Obviously). So please scrap and add more data or add a sources list so that I could extract it from the internet.","","Add more data to the dataset","Require to add more sources or data to the dataset","","0"
"4562","1377358","1500668","05/30/2021 13:58:44","## Task Details
Please use the data to predict the nifty 50 data for the next 1 month.

## Evaluation
Please use different forecast metrics and compare your performance.","","NIFTY 50 Forecasting","","","1"
"2443","921390","1505251","10/15/2020 07:03:48","## Expected Submission
Check the `joined` column in *_rankings.csv and from that, you can evaluate avg time required to become in the top 1% in competitions, notebooks  or rankings leaderboard.","","Avg time required a person to be on top 1% in rankings","Find how much time a user required to come to top leagues.","10/31/2020 23:59:00","0"
"3021","1055290","1510254","12/23/2020 17:05:25","## Task Details
The world of gaming is constantly evolving. Help fellow Data Scientists stay cognizant of emerging and past trends by syphering through data like only you can!

## Expected Submission
* A vibrant narrative about trends and user reception of 3DS games via EDA

## Evaluation
* An eye-catching notebook that tells a story from the data
* Feel free to be creative an implement ML models for your analysis
    + Suggestions include maybe creating a classification model to categorize 
        games based on performance/reception or regression to predict Metacritic 
        score of the title based on user review.","","EDA on videogame data","Build a notebook for EDA and test your insights with ML(optional)","","0"
"2961","1040511","1510254","12/16/2020 09:10:29","## Task Details
With over a 100 respondents and 30 survey questions to choose from. The dataset has untapped potential for inferential tests to be conducted.

## Expected Submission
Users can apply a wide range of techniques. Some examples are:

* ANOVA test along Race/Ethnicity to determine if there are statistically significant differences in response scores grouped by Race

* t-tests on Demographic information vs. Question group construct(eg: Do married respondents have higher mean response scores among Financial Questions, i.e are married people more likely to believe that women defer pregnancy owing to financial reasons)

* Chi-Squared tests either for Goodness of fit or Independence.

## Evaluation
Ideally the following requirements are to be met:

* Are the null and alternative hypotheses clearly stated at the onset of analysis/testing?

* Is the significance level(p-value) of the test statistic present?

* Are the inferential conclusions of the test stated in the notebook?


### Further help
In R: (https://rstudio.com/resources/rstudioconf-2018/infer-a-package-for-tidy-statistical-inference/)

In Python: (https://www.statsmodels.org/v0.10.1/)","","Stat Inference on Survey respondents","Perform Statistical tests to make observations","","0"
"2962","1040511","1510254","12/16/2020 09:22:26","## Task Details
Since the dataset includes a diverse group of respondents via gender, employment status, age and race, it would be beneficial to examine if demographic factors contribute to or correlate with response scores.

## Expected Submission
Users should apply an ML Classification algorithm to pertinent demographic and survey response information and classify respondents into the following categories:

* Socially Conservative
* Moderate
* Socially Liberal

## Evaluation
An AUC score greater than 0.5 would be satisfactory.

### Further help
* Initial Classification labels (Socially Conservative, Moderate, Socially Liberal) can be generated using a Clustering algorithm (k = 3) to segment the data according to response scores and thereby the Cluster groups can be renamed to the categorical variable names above.

* After the labels are generated. Split the data into train and test sets and apply any classification algorithm you like.

* Verify model performance on testset via Confusion matrix and Share results.

Happy Coding!","","Classify survey respondents by social attitude","Apply an ML algorithm to classify respondents into 3 groups","","0"
"4548","1375029","1516883","05/29/2021 12:55:25","## Task Details
Build a model using the given data to predict the human activity in the test video.

## Expected Submission
The output video file and activity label for each frame of the video.

## Evaluation
For how long the model is predicting the activity correctly. (correct frames / total frames)

### Further help
Will be adding soon","","Activity Recognition using RNN","Building a LSTM model to predict human activity in a video","","0"
"2681","968714","1520352","11/12/2020 04:28:52","## Task Details
Can the CO2 concentration, room air humidity, room temperature, and luminosity data be used to identify whether a room has occupants or not? The passive infrared (PIR) sensor measures the occupancy in a room (target label). Approximately 6% of the PIR data is non-zero, indicating an occupied status of the room. The remaining 94% of the PIR data is zero, indicating an empty room. You can refer to (https://citris-uc.org/about/sutardja-dai-hall/about-facilities/floorplans/) to find out more about the geospatial positioning of the rooms in the building.

## Expected Submission
The user should submit all data visualization, data cleaning, preprocessing, feature engineering, model building, model training and evaluation code. They may use either a notebook or a script. The solution should ideally contain the F1 scores. The user may also include accuracy, precision, recall, confusion matrix, AUC - ROC curve.

## Evaluation
A good solution contains as high an F1 score as possible.

## Contact
If you have any queries regarding the task, the dataset or find some interesting insights from this dataset, please feel free to reach out at rrchowdh@eng.ucsd.edu.","","Occupancy Prediction","Classify the status of a room.","","5"
"4509","1370713","1522152","05/27/2021 10:14:51","## Task Details
I would like code to clean a text document, and then have user type in a word or phrase the code should then return ID'SOR row numbers that have the word or phrase in it .

## Expected Submission
I have sample text&nbsp;of a movie plot Dataset. For this project, we want to useThe plot column as our input data. A person should be able&nbsp;to search a phrase or word.. say someone wants to see which movie has plots that deal with Mental health. The person should be able to type the phrase&nbsp; ""mental health"" and the program should return movie titles and year it was released.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Text Analysis. Create a Mini Search Engine","","","0"
"2406","915781","1526569","10/11/2020 12:37:04","## Task Details
Every row contains data from one ECG (2000 point, 250 Hz). Each row has a label (0, 3).
The proposed task is to develop a model to classify each ECG in one of the 4 classes.

## Expected Submission
a csv file with the file name and probability for each one of the classes.

Also, evaluation of the model on a test set (a subset of the file, extracted before training)


## Evaluation
Sensibility and specificity for each class","","Develop a model to classify ECG in the four label classes","","","1"
"2901","1021364","1526569","12/10/2020 05:25:26","## Task Details
Each image is a CXR with a label (0,1) that specifies if the CXR contains signs of disease.

The suggested task is to develop an image classifier that output the probability (0,1) that the image is not normal.

## Expected Submission
The user should submit a JPEG image 256x256. THe image must be packed in a Tensor N, 256, 256, 3) where N is the number of images (samples)

## Evaluation
A good model is a model with AUC, as evaluated on test set, higher than 0.95","","Train a model to check if a CXR contains abnormalities","","","2"
"2534","938082","1541942","10/25/2020 01:18:05","## Task Details
A couple days ago, I recorded a road on my way back home. In hazy condition, I barely see potholes. Therefore, I drove carefully in order to avoid those obstacles. I found that potholes in hazy road is dangerous.  

## Expected Submission
Make a detection using any techniques you know in order to detect potholes in this image. Submit your notebook in this dataset.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
Any similarity with the other notebooks will not be tolerated.","","Pothole Detection","Final Project Digital Image Processing Course (PCD)","11/05/2020 23:59:00","5"
"4987","1435153","1541942","07/04/2021 03:07:07","## Task Details
1. Tugas berbentuk jupyter notebook

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Tugas 1 Memprediksi Harga Real Estate","","","2"
"4343","1325940","1554925","05/12/2021 07:40:31","## Task Details
How do overtime payments vary based on department and role? What other factors help predict how much overtime someone makes?","","Overtime payments","Who gets them?","","0"
"4880","1423768","1562454","06/21/2021 19:25:52","Phase 1 (Development): Participants are provided with training data (which includes reference data) and validation data (without reference data until phase 1 concludes) to train and validate their algorithms. Participants can submit prediction results for the validation set to the codalab competition website to get feedback on the performance from April 15 to May 14, 2021. The performance of the best submission from each account will be displayed on the leaderboard. 
- Phase 2 (Test): Participants receive the validation set reference data for model tuning and test data set (without the corresponding reference data) to generate predictions and submit their binary classification maps in numpy array format from May 15 to June 30, 2021. After evaluation of the results, three winners will be announced on July 1, 2021.","","Generate Segmentation Masks","","","0"
"2535","938448","1567143","10/25/2020 07:51:53","Tiny ImageNet Challenge is the first course project of Introduction to Deep Learning. It runs similar to the [ImageNet challenge] and [Stanford CS231n].
[ImageNet challenge]:http://www.image-net.org/challenges/LSVRC/2014/
[Stanford CS231n]:https://tiny-imagenet.herokuapp.com/

## Dataset
The Tiny ImageNet dataset has 100 classes. Each class has 1,000 training images, 100 validation images. The test set contains 10,000 images in total. We have released the training and validation sets with images and labels. The test set is released without labels. You are asked to predict the class label of each test image.

In detail, all training and validation images are from ImageNet training set. For test set, 5,000 images (id 0~4999) are selected from ImageNet validation set, while another 5,000 images (id 5000-9999) from [ImageNet-A]dataset, which called **Natural Adversarial Examples**. Modern deep CNNs achieve very low accuracy on these natural adversarial examples. ImageNet-A examples cause mistakes due to occlusion, weather, and other complications encountered in the long tail of scene configurations. You are recommended to read the [original paper] for more information.
[ImageNet-A]:https://arxiv.org/pdf/1907.07174.pdf
[original paper]:https://arxiv.org/pdf/1907.07174.pdf

For this course project, you need to consider how to achieve high classification accuracy on both general ImageNet images and natural adversarial examples.

## Submission
We use the test set accuracy to measure the performance. To submit your predictions on the test set, you need to upload a CSV file, which should be a two-column file with 10,001 lines (the first row is a header). Each line contains a pair of test image filename and its predicted class id (0-99). An example might look like:
```
Id,Category
0.jpg,0
1.jpg,1
2.jpg,2
...
9999.jpg,99
```
After the deadline, you are required to submit all codes and a short report (no less than one page). Plagiarism and labeling test images manually will be penalized.

## Grading Policy
Total marks for this project is 20:
- 14 marks based on rankings
   - 1st place: 14 marks
   - 2nd - last place: between 7 and 14 marks, in descending order
last place: 7 marks
- 6 marks based on the final report
   - correctness
   - novelty

Members in the same group get the same grade.

## References
Natural Adversarial Examples. https://arxiv.org/abs/1907.07174.
ImageNet LSVRC challenge. http://www.image-net.org/challenges/LSVRC.
Tiny ImageNet Visual Recognition Challenge. https://tiny-imagenet.herokuapp.com/.","","Tiny ImageNet Challenge","The second homework of Introduction to Deep Learning, hosted by Prof. Xiaolin Hu and TAs.","","1"
"2960","1009351","1567578","12/16/2020 07:39:51","## Task Details
Latent Dirichlet Allocation (LDA) is a widely used topic modeling technique to extract topic from the textual data.

While my starter notebook [Keywords Extraction Using TF-IDF Method](https://www.kaggle.com/rowhitswami/keywords-extraction-using-tf-idf-method) on NeuIPS Papers gives a head-start in Topic Modelling, but still there are some limitations of TF-IDF that LDA improves upon.

One of the major difference is that TF-IDF works at the word level. For example, if a document that is about **macOS** might be far from a document which is about **MacBook Air** when represented using TF-IDF. On the other hand, since **macOS** and **MacBook Air** frequently co-occur in articles they are likely to come from the same topic, so the LDA representation of these documents would be close.
LDA is a good option if you want to find the groups of related words, but you can't do it with TF-IDF.

## Expected Submission
You're required to implement topic modelling on all NeuIPS Papers, so the expected submission would be a **CSV** file with columns:
1. `source_id` -  Unique ID of the published paper
2. `keywords` - Extracted keywords from a paper

Additionally, you can also use LDA visualization tool [pyLDAvis](https://pypi.org/project/pyLDAvis/) to better understand the results.


## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself). Please note that the notebook should be well documented and contain any steps you're taking to prepare the data, including references to external data sources. A great entry will be informative, thought provoking, and fresh all at the same time.","","Topic Modelling of ML Papers using LDA (Latent Dirichlet Allocation)","Extract top keywords from NeuIPS Papers using LDA.","","44"
"3798","1216620","1571639","03/17/2021 11:04:47","## Task Details
Build a sentiment classifier using the reviews to either categorize any review into positive and negative class","","Sentiment Analysis","","","0"
"6087","1149230","1571785","09/15/2021 08:24:12","## Task Details
Network / Graph analytics is a popular technique used to mine customer behaviors from a large transaction dataset. 

## Expected Submission
A notebook with nice story about the graph / network-related trends","","Perform Network Analysis to identify Transaction Trends","","","0"
"3815","1220173","1572216","03/20/2021 00:22:08","## Task Details

This task has the goal to receive a message in text and plot it using Pokemon Unowns.

## Expected Submission

The users should use the Pokemon GO Unown Letters as a dataset to provide the images to visualize the message.

## Evaluation

This task is for fun, so try to play with many messages trying to plot and visualize than inline with matplotlib, pillow, and pandas.","","Text to Pokemon Unown Images","Decode text in to Pokemon Unown images","","1"
"3816","1220173","1572216","03/20/2021 00:29:13","## Task Details

This task has the goal to decode a Caesar Cipher message and, after that, plot the message using Pokemon Unown Plots.

## Expected Submission

The users should use the Pokemon GO Unown Letters as a dataset to provide the images to visualize the message. The message is: ""P HT H RHNNSL NYHUKTHZALY!""

## Evaluation

This task is for fun, so try to play with many messages trying to plot and visualize than inline with matplotlib, pillow, and pandas. Decode the message and plot in Pokemon Unown Letters, and maybe you become a Kaggle grandmaster!","","Decode the Caesar Cipher in to Pokemon Unown Plots","","","1"
"2625","953917","1601258","11/03/2020 20:53:01","The notebook should guide the audiences on how to use the data as well as providing a brief of descriptive statistics.","","Add a notebook to guide the datasets","","11/15/2020 23:59:00","0"
"3874","1214263","1606049","03/26/2021 20:48:10","## Task Details
It would be interesting to know what proportion of songs in this dataset can be matched to a dataset such as the [Spotify audio features dataset](https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks). Is there any way to improve the match rate by cleaning the song and artist names? Is there another dataset of song attributes which would provide a better match rate? 

## Expected Submission
Submit a notebook which shows how to match the songs in this dataset to another dataset which provides additional metadata about the song (e.g. genre, audio features, popularity).","","Match songs to Spotify audio features dataset","","","0"
"3265","1115136","1633969","01/24/2021 10:40:56","## Task Details
My best AUROC score on this data is 0.749 as seen in the notebook __random\_forest\_example__. I train the model on 80% of the data and scored it on the other 20%.

## Expected Submission
Use a notebook!

## Evaluation
Just the score!","","Beat my AUROC score!","","","1"
"2717","979950","1634816","11/18/2020 06:21:58","Use calibration parameters from `sequences/00/calib.txt` to calculate the distance between left (`P2`) and right (`P3`) cameras.

Calibration parameters are given in the form of 3x4 matrix, which is equal to:  
`P = K @ T`  
Where `K` is the intrinsic camera matrix, and `T` is the translation matrix.  

`P` is called extrinsic camera matrix.","","Calculate distance between cameras","","","0"
"3124","1071541","1636313","01/04/2021 14:55:46","eCommerce behavior  data from multi-category store is a dataset containing the user behavior ( (view/add to cart/ purchase) of an ecommerce shop over 7 months.

Events: 
* View, AddToCart, Purchase

Timeframe: 
* Oct-2019 - April 2020

As the dataset contains only interactions (positive samples), we need to define a goal / target to predict. There is a lot of literature about how to construct negative samples from the dataset in order to make the goal easier or harder to predict.

We decided that the goal is to predict if a user purchased an item:
* Positive: User purchased an item
* Negative: User added an item to the cart, but did not purchase it (in the same session)

**NOTE**
We split the dataset in train, validation and testset by the timestamp:
* Training: October 2019 - February 2020
* Validation: March 2020
* Test: April 2020","","Make A Classifier","Predict Whether A User Will Purchase An Item In Their Cart","","2"
"5578","1177384","5194950","08/06/2021 12:46:33","## Task Details
For centuries chess players have debated which openings provide the most advantage against stronger, equal, and weaker opposition; for slow and fast time controls; for White or Black; etc.

## Expected Submission
Solutions should contain chess openings in the dataset along with either qualitative or quantitative information.

## Evaluation
Good solutions could apply well to predicting results of games either via cross-validation or for games not included in the dataset.  Use your imagination.","","Measure opening strength","","","0"
"4912","1430984","1671177","06/25/2021 11:24:16","## Task Details
Items in the invoices need to be extracted in structured format, preferably in excel.

## Expected Submission
Generate an excel containing list of items provided in the invoice so that exact item details can be known

## Evaluation
Overlapped items, correct values, text over lines - if correct then it makes correct submission","","Extract tabular data from invoices","","","0"
"3172","1086951","1708165","01/11/2021 16:08:31","## Task Details
Generate photo-realistic face images from ambiguous sketch images or segmentation maps

## Expected Submission
Model(s)
Generated face images

## Evaluation
Model(s) -&gt; model size
Human reviewer -&gt; Generated images' quality

### Further help
Good start point, [pixel2style2pixel]( https://github.com/eladrich/pixel2style2pixel#segmentation-map-to-face)","","Conditional Image Synthesis","using smaller than 512 MB model(s)","","0"
"3064","1063605","1710629","12/28/2020 14:24:28","Regularly, people talk about UFOs, some of us believe having seen one of them. These phenomena last since the WW2. UFO sightings across the world is a nice dataset to discover and experiment data manipulation and correlation.

UFO datasets, the truth is out there...
Regularly, people talk about UFOs, some of us believe having seen one of them. This phenomenon lasts since the WW2.

There is a UFO dataset which represents 18 thousands of UFO sightings over the world, with a date, a location, and some extra info.","","UFO Trends (EDA) and prediction","","","1"
"2608","951310","1715390","11/02/2020 07:06:57","## Task Details
Build segmentation models that accurately and efficiently segment polyps from endoscopic colonoscopy frames using Kvasir-SEG dataset. 

## Evaluation
Models are evaluated based on their Test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Kvasir-SEG Polyp Segmentation from Endoscopic Colonoscopic Frames","","12/28/2021 23:59:00","3"
"2561","943153","1715390","10/28/2020 11:22:26","Task Details
Train models to perform 3D object multi-class classification on ModelNet10 dataset.

Evaluation
Models are evaluated based on (i) overall accuracy (ii) per-class accuracy scores.","","Classify 3D Objects [ModelNet10 Dataset]","","12/31/2021 23:59:00","1"
"2638","960082","1715390","11/06/2020 17:30:36","## Task Details
Build Salient Object Detection (SOD) models that accurately and efficiently identify salient regions in the test input images using DUTS training and test datasets. 

## Evaluation
Models are evaluated based on their Precision, Recall and Accuracy scores. Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Salient Object Detection on DUTS Dataset","","12/30/2021 23:59:00","2"
"2529","937211","1715390","10/24/2020 11:16:53","## Task Details
Perform Single Image Dehazing (Indoor Scenario) on [RESIDE-ITS](https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=0) dataset and evaluate models on [SOTS](https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=0) dataset.

## Evaluation
Models are evaluated based on PSNR / SSIM scores.","","Single Image Dehazing (Testing on RESIDE-ITS data)","","12/30/2021 23:59:00","4"
"2528","937212","1715390","10/24/2020 11:15:34","## Task Details
Perform Single Image Dehazing (Indoor Scenario) on [RESIDE-ITS](https://sites.google.com/view/reside-dehaze-datasets/reside-standard?authuser=0) dataset.

## Evaluation
Models are evaluated based on PSNR / SSIM scores.","","Single Image Dehazing (Training on RESIDE-ITS data)","","12/30/2021 23:59:00","3"
"2496","930614","1715390","10/20/2020 12:06:56","## Task Details
Build segmentation models that accurately and efficiently segment polyps from endoscopic colonoscopy frames. 

## Evaluation
Models are evaluated based on their Test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Polyp Segmentation from Endoscopic Colonoscopic Frames","","12/29/2021 23:59:00","4"
"2459","925704","1715390","10/17/2020 09:49:48","## Task Details
Perform Image-to-Image Translation between Monet Paintings & Natural Photos

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Monet Paintings & Natural Photos","","12/15/2021 23:59:00","2"
"2461","925857","1715390","10/17/2020 11:15:50","## Task Details
Perform Image-to-Image Translation between Summer & Winter Photos

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Summer & Winter Photos","","12/28/2021 00:00:00","2"
"2476","926989","1715390","10/18/2020 08:04:29","## Task Details
Perform Image-to-Image Translation between Shoe Edges & Shoes

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Shoe Edges & Shoes","","12/30/2021 23:59:00","2"
"2478","927102","1715390","10/18/2020 08:34:31","## Task Details
Perform Image-to-Image Translation between Cityscapes Road Scene Segmentations & corresponding Dashcam Images

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Cityscapes Road Scene Segmentations & corresponding Dashcam Images","","12/30/2021 23:59:00","2"
"2467","926166","1715390","10/17/2020 15:32:55","## Task Details
Perform Image-to-Image Translation between Building Facades and their Segmentations

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Building Facades and their Segmentations","","12/28/2021 23:59:00","3"
"2468","926287","1715390","10/17/2020 17:08:12","## Task Details
Perform Image-to-Image Translation between Apples & Oranges

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Apples & Oranges","","12/30/2021 23:59:00","2"
"2469","926321","1715390","10/17/2020 17:33:50","## Task Details
Perform Image-to-Image Translation between Horses & Zebras

## Evaluation
Models are evaluated based on (visually) realistic image-to-image translations","","Image-to-Image Translation b/w Horses & Zebras","","12/30/2021 23:59:00","2"
"2413","916809","1715390","10/12/2020 04:36:31","## Task Details
Build segmentation models that accurately and efficiently segment the various classes in the dataset.

## Evaluation
Models are evaluated based on their test Mean [Intersection over Union (IoU) or Jaccard Score](https://en.wikipedia.org/wiki/Jaccard_index). Models that are lightweight and fast (takes less memory footprint and inference times) are preferred.","","Semantic Segmentation","","12/28/2021 23:59:00","1"
"2457","916809","1715390","10/17/2020 08:17:27","## Task Details
Perform Domain Adaptation on MNIST & MNIST-M datasets while using BSDS500 dataset as background imagery for MNIST-M data.

## Evaluation
Accuracy Score of target models in classifying digits would be the evaluation metric.","","MNIST-M Domain Adaptation Using BSDS500 for Background Imagery","","12/01/2021 23:59:00","1"
"2687","968300","1733636","11/12/2020 19:58:30","## Task Details
It would be great to see if this data fits for automatic multilingual machine translation based on GAN's.","","Multilingual Machine Translation","","","1"
"5181","950156","6900019","07/18/2021 07:37:10","## Task Details
For each Kickstarter project, use the available data to forecast success

## Expected Submission
Submit a classification of success / failure per project

## Evaluation
Try to improve your accuracy!","","Classification of Kickstarter projects","Will it succeed or fail?","","0"
"3368","1128700","1738700","02/01/2021 07:57:07","## Task Details
Please contribute!!
https://github.com/JEF1056/R5","","Pretrain a Reformer model on this dataset","","","0"
"2996","1050099","1759475","12/21/2020 06:53:40","## Task Details
It would be nice to predict the top 4 (that will play Libertadores da America, another important competition in South America) as well as the bottom 4 teams (descending to B Series) *as the tournament progresses*, i.e., as teams are playing each other.","","Predict champion as the tournament progresses","","","0"
"2449","924544","1764858","10/16/2020 12:09:19","## Task Details
DO PCA analysis of FGSM and NON ADVERSARIAL DATA

## Expected Submission
4 plot of PCA data, Kmeans, LDA, TSNE","","Cluster FGSM VS non adversarial data","","10/17/2020 23:59:00","0"
"3651","1190742","1787261","03/03/2021 13:05:07","## Task Context
This task was given as a Take  Home task by a major multinational corporation.

## Task Details
Mars Mission Control needs a good data-driven system for predicting Zeta Disease infection on the International Mars Colony.
Use the `_zeta-disease_training-data_` dataset to build a model that can predict who will be infected by Zeta Disease.
Apply your model to the `_zeta-disease_prediction-data_` dataset to predict who will be infected by Zeta Disease.

### DATASET

The dataset includes 9 columns with information on 800 people.
1.	age : in years
2.	weight : body weight in pounds (lbs)
3.	bmi : Body Mass Index (weight in kg/(height in m)2)
4.	blood_pressure : resting blood pressure (mm Hg)
5.	insulin_test : inuslin test value
6.	liver_stress_test : liver_stress_test value
7.	cardio_stress_test : cardio_stress_test value
8.	years_smoking : number of years of smoking
9.	zeta_disease :
              1 = yes;
              0 = no","","THE CHALLENGE: Zeta Disease Prediction","","","0"
"3760","1209563","1788694","03/14/2021 03:52:11","Identify the voxels of potential tumors in the lungs using PyTorch to implement
a technique known as segmentation. This is roughly akin to producing a heatmap
of areas that should be fed into our classifier in step 3. This will allow us to focus
on potential tumors inside the lungs and ignore huge swaths of uninteresting
anatomy (a person can‚Äôt have lung cancer in the stomach, for example).","","Identify potential voxels of tumors in lungs","","","2"
"3761","1209563","1788694","03/14/2021 03:53:06","Classify candidate nodules as actual nodules or non-nodules using 3D convolution.
This will be similar in concept to the 2D convolution we covered in chapter 8.
The features that determine the nature of a tumor from a candidate structure are
local to the tumor in question, so this approach should provide a good balance
between limiting input data size and excluding relevant information. Making
scope-limiting decisions like this can keep each individual task constrained,
which can help limit the amount of things to examine when troubleshooting","","Classify candidate nodules as actual nodules or non-nodules using 3D convolution","","","1"
"3762","1209563","1788694","03/14/2021 03:54:16","Similar to the nodule classifier in the previous step, we will attempt to determine whether the nodule is benign or malignant based on imaging data alone. We
will take a simple maximum of the per-tumor malignancy predictions, as only one
tumor needs to be malignant for a patient to have cancer. Other projects might
want to use different ways of aggregating the per-instance predictions into a file
score. Here, we are asking, ‚ÄúIs there anything suspicious?‚Äù so maximum is a good
fit for aggregation. If we were looking for quantitative information like ‚Äúthe ratio
of type A tissue to type B tissue,‚Äù we might take an appropriate mean instead.","","Diagnose the patient using the combined per-nodule classifications.","","","0"
"4188","1296332","1788694","04/26/2021 01:13:06","We often find in data science that the objects we wish to analyze are textual. For example,
they might be tweets, articles, or network logs. Since our algorithms require numerical
inputs, we must find a way to convert such text into numerical features. To this end, we
utilize a sequence of techniques.
A token is a unit of text. For example, we may specify that our tokens are words, sentences,
or characters. A count vectorizer takes textual input and then outputs a vector consisting of
the counts of the textual tokens. A hashing vectorizer is a variation on the count vectorizer
that sets out to be faster and more scalable, at the cost of interpretability and hashing
collisions. Though it can be useful, just having the counts of the words appearing in a
document corpus can be misleading. The reason is that, often, unimportant words, such as
the and a (known as stop words) have a high frequency of occurrence, and hence little
informative content. For reasons such as this, we often give words different weights to
offset this. The main technique for doing so is tf-idf, which stands for Term-Frequency,
Inverse-Document-Frequency. The main idea is that we account for the number of times a
term occurs, but discount it by the number of documents it occurs in","","creating hashing vectorizer","","","0"
"4256","1313437","1788694","05/03/2021 13:56:02","classification between files that are obfuscated and non obfuscated.","","classification","","","0"
"4460","1359264","1788694","05/22/2021 09:53:08","classify iot devices based on its network characteristics","","IOT device Identification","","","3"
"4429","1347855","1788694","05/18/2021 05:56:04","Create a captcha breaker","","Creating a Captcha breaker","","","0"
"4410","1345188","1788694","05/17/2021 01:26:42","Load the dataset and try determining the veracity of the news.","","Classification","To check whether a news item is fake or not.","","0"
"4178","1294136","1788694","04/25/2021 01:21:55","Creating Markov Chains on this model. You can follow first chapter of this book https://github.com/PacktPublishing/Machine-Learning-for-Cybersecurity-Cookbook.","","Creating Markov Chain","","","0"
"4167","1292260","1788694","04/24/2021 05:46:49","## Task Details
Creating a train test split of the database","","test train split data","","","0"
"4168","1292312","1788694","04/24/2021 06:24:45","Standardize the data, sample notebook provided. This is taken from chapter 1 of machine learning cookbook.","","Standardizing the data","","","0"
"3876","1235405","1792385","03/27/2021 05:22:56","## Task Details
Nasa Exoplanets

## Expected Submission
Mid April

## Evaluation
Better Results

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA of dataset","Visualization","04/22/2021 23:59:00","0"
"3823","1213260","1798416","03/20/2021 20:40:23","## Task Details
It is interesting to investigate whether it is possible to train the model on a standard training dataset MNIST (70 thousand images) so that it can be used with acceptable accuracy to classify digits from screenshots from text editors (different fonts in italic format)?

The digits are written in one row ""0 1 2 3 4 5 6 7 8 9""  in the form of one image for all 10 at once, with spaces, for the convenience of their recognition and marking.
The markup is needed to determine the correct classification.

It is proposed to experiment with those 10 images that are already in the dataset, but the dataset is planned to be replenished. 

## Expected Submission
Submission is a list of 10 labels predicted for one of the images in the dataset using a model trained on the standard training dataset MNIST (70 thousand images).

## Evaluation
Good solution is a list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. 
Recommended metric: accuracy_score

### Further help
See a notebook example [MNIST model testing : typographic digits](https://www.kaggle.com/vbmokin/mnist-model-testing-typographic-digits) that:
- recognizes the digits in the image, smoothes them, thickens the lines, forms each digit in a typical MNIST format [28, 28, 1],
- set target for all digits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
- trains a simple model based on CNN (0.93-0.95),
- makes a prediction for one of the given images: accuracy_score is 0.3-0.6 for different fonts,
- displays confusion matrix and analyzes emissions.","","MNIST model testing : try to recognize typographic digits","Try to recognize the typographic digits by a model trained for MNIST","","0"
"3814","1220102","1798416","03/19/2021 18:07:41","## Task Details
It is interesting to investigate whether it is possible to train the model on a standard training dataset MNIST (70 thousand images) so that it can be used with acceptable accuracy to classify digits written by hand with a pen or marker and photographed by a smartphone?

The digits are written in one row ""0 1 2 3 4 5 6 7 8 9""  in the form of one image for all 10 at once, with spaces, for the convenience of their recognition and marking.
The markup is needed to determine the correct classification.

For interest, the digits are written in different conditions:
- pen with blue paste or black marker, 
- by the left or right hand,
- in a comfortable or an uncomfortable position.

It is proposed to experiment with those 10 images that are already in the dataset, but the dataset is planned to be replenished. 

## Expected Submission
Submission is a list of 10 labels predicted for one of the images in the dataset using a model trained on the standard training dataset MNIST (70 thousand images).

## Evaluation
Good solution is a list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]. 
Recommended metric: accuracy_score

### Further help
See a notebook example [MNIST model testing : user handwritten digits](https://www.kaggle.com/vbmokin/mnist-model-testing-user-handwritten-digits) that:
- recognizes the digits in the image, smoothes them, thickens the lines, forms each digit in a typical MNIST format [28, 28, 1],
- set target for all digits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
- trains a simple model based on CNN (0.93-0.95),
- makes a prediction for one of the given images: accuracy_score is 0.5-0.6 for different options,
- displays confusion matrix and analyzes emissions.","","MNIST model testing : try to recognize user handwritten digits","Try to recognize the user's handwritten digits by a model trained for MNIST","","5"
"2784","998206","1798416","11/26/2020 15:10:35","## Task Details
Classify the text from the dataset as accurately as possible for each of the 5 target binary features of the English-language dataset or/and Ukrainian-language dataset. 

## Expected Submission
Submit a notebook that processes the data by NLP methods, classifies it, and evaluates the accuracy of classification for each target feature separately.

Don't use or incorporate information from hand labeling or human prediction of the validation dataset or test data records. 

You may use external data with data from this dataset to develop and test your models, but only public external data indicating the source.

## Evaluation
The maximum value of the [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for each target feature of the dataset (English or/and Ukrainian) which determined on a pre-selected part of the test data in the amount of at least 40% randomly selected data of the total data in the relevant language.

### Further help

* [A Visual Guide to Using BERT for the First Time](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)
* [NLP for EN : BERT Classification for Water Report](https://www.kaggle.com/vbmokin/nlp-for-en-bert-classification-for-water-report)
* [NLP for UA : BERT Classification for Water Report](https://www.kaggle.com/vbmokin/nlp-for-ua-bert-classification-for-water-report)","","Text Multilabel Binary Classification","Environmental Reports & News Classification","","5"
"6372","998206","1798416","10/15/2021 16:02:32","## Task Details
Create a new table with text and marked (for training) or unmarked (for testing) data for the dataset.

## Expected Submission
The notebook must download from the web resource (where such a download is allowed with reference to the source!) text data (in English or Ukrainian) on water bodies and form from them a dataset as a table with columns: 
`['text', 'env_problems', 'pollution', 'treatment', 'climate', 'biomonitoring']`

## Evaluation
The best solution should contain as many text lines as possible (50-300 characters each). 
In addition, it is desirable to cite such sentences that contain geographical names from the basin of the Southern Bug (Booh).

### Further help
[Web scraping from HTML - BUWR SB site-parser](https://www.kaggle.com/vbmokin/web-scraping-from-html-buwr-sb-site-parser) - 21 lines for the basin of the Southern Bug.","","Create new data tables for a dataset","Web-scraping and text data parsing","","1"
"2470","926453","1815526","10/17/2020 19:40:11","## Task Details

Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. 

Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.

Given the abstracts for a set of research articles, predict the tags for each article included in the test set. 
Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: 

1. Computer Science
2. Mathematics
3. Physics
4. Statistics

List of possible tags are as follows:

[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]
 
# Data Dictionary 

- train.csv

| Column |Description  |
| --- | --- |
|id| Unique ID for each article|
|ABSTRACT	|Abstract of the research article|
|Computer Science|Whether article belongs to topic computer science (1/0)|
|Mathematics	|Whether article belongs to topic Mathematics (1/0)|
|Physics	|Whether article belongs to topic physics (1/0)|
|Statistics|	Whether article belongs to topic Statistics (1/0)|
|Tags	(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|

- test.csv

| Column |Description  |
| --- | --- |
|id| Unique ID for each article|
|ABSTRACT	|Abstract of the research article|
|Computer Science|Whether article belongs to topic computer science (1/0)|
|Mathematics	|Whether article belongs to topic Mathematics (1/0)|
|Physics	|Whether article belongs to topic physics (1/0)|
|Statistics|	Whether article belongs to topic Statistics (1/0)|

- sample_submission.csv

| Column |Description  |
| --- | --- |
|id| Unique ID for each article|
|Tags	(TARGET) |There are 25 columns of possible tags with (1/0) :1 : if article belongs to that tag 0 : if article doesn't belong to that tag|

## Evaluation Metric
Submissions are evaluated on micro F1 Score between the predicted and observed tags for each article in the test set","","Topic Modeling for Research Articles","Natural Language Processing","","5"
"2931","916045","628840","12/13/2020 17:18:56","## Task Details

This notebook simplifies the steps to solve github bug predition through a typical workflow using BERT base uncased model. Before we get started with the workflow , the first question that comes into our mind is how do we get started. Well we will get there but let's first understand what exactly the problem statement is and what is expected here?


## Expected Submission

MachineHack a Biggest Machine Learning Community For Data Science & AI Enthusiastshas comeup with one of the popular Hackathon on GitHub bugs prediction.

Detecting defects in software systems is an evergreen topic, since there is no real world software without bugs. There are many algorithms that has come up recently to help developers find bugs in software. The challenges of having curated datasets for own domain specific software , or code relates bug finding is rare. You can go through the research paper for gaining more ideas on Bug Prediction Databases and related algorithms that can be used.


## Evaluation

The prediction shold  be accurate about a specific github description. 

### Further help

Read more about Basic BERT here.

In this Notebook we will be using BERT Base Uncased, from Hugging Face Library.

Would like to thank and give credits to the Kaggle Kernel by Purva Singh. I have a taken the ideas and workflow as a guide to bhild my first Kaggle Notebook. üòäüëèüëèüëèüëèüëèüëèüëèüëè","","GitHub Bugs Prediction using BERT Model","GitHub Bugs Prediction using BERT Model","12/14/2020 23:59:00","1"
"2408","916045","1815526","10/11/2020 15:40:45","# [Machine Hack Submission link](https://www.machinehack.com/hackathons/predict_github_issues_embold_sponsored_hackathon/submission)

# Evaluation
Evaluation Stages
In this hackathon, the evaluation comprises of two steps. Please refer to the below stage-wise evaluations for more details

## Stage - 1 - Model Evaluation
Compete like a regular Machinehack hackathon on the live leaderboard using the accuracy score as a metric
Once the hackathon has ended the leaderboard will be closed
Machinehack team will reach out to Top-20 participants based on the accuracy score on the private leaderboard for sharing the Embold scorecard
Only Top-20 participants will be asked for the Embold scorecard

## Stage - 2 - Code Quality Evaluation
Top-20 participants will have to share their Embold scorecard downloaded using the Embold code analysis platform
Check out this quick-start guide on how to generate the Embold scorecard
Please check this guide to understand more on how to analyze the code repository
It's a mandatory step for the winner's to share their Embold scorecard, failing to do so may lead to disqualification
What is the Metric In this competition? How is the Stage-1 Evaluated ??
The submission will be evaluated using the Accuracy metric. One can use sklearn.metric.accuracy_score to calculate the same
This hackathon supports private and public leaderboards
The public leaderboard is evaluated on 30% of Test data
The private leaderboard will be made available at the end of the hackathon which will be evaluated on 100% Test data

# Synopsis
- Registration
Now open for Submissions.
# Hackathon Starts
Participate after 8th Oct, 12:00 PM IST
# Hackathon Closes
## Stage-1 Evaluation
 - It will be based on the participants standing on the private leaderboard.
- The private leaderboard uses the 30% of provided test.json dataset to evaluate.
- Final standing will reflect on 18th Oct, after 7:00 AM IST.
# Finalize Top-20
## Stage-2 Evaluation
- Machinehack team will select the Top-20 participants from private leaderboard.
- Top-20 participants will be notified for sharing there Embold Score Card, please refer the quick-tour video by Embold.io under OVERVIEW section to understand more.
# Winners Announcement
Final winners will be notified via an email, based on an aggregate score of their private leaderboard rankings and the Embold Score Card score.","","GitHub Bugs Prediction","Natural Language Processing - TF-IDF, CountVectorizer, BERT","","3"
"4316","1330226","1815526","05/09/2021 23:34:26","## Task Details
To segregate the test data between genuine and false conversions by identifying the maximum possible leads generated by the malignant technique.

## Expected Submission
Refer to Sample submission file

## Evaluation metric
Accuracy","","Predict Conversion Fraud using Click Log data","","","4"
"2730","981292","1822946","11/19/2020 11:50:34","## Task Details
Apart from the classical tasks that one can conduct on the Mini-DDSM data set  (i.e., disease classification/prediction, tumor/lesion localisation),  age  estimation from mammography is a new challenging venue. Age estimation  has  attracted  attention for its  various  medical  applications.  There  are  many  studies  on  human  age  estimation  from biomedical images. However, there is no research done on mammograms for age estimation, as far as we know (except our recent published work [1]). The purpose of this task is to devise a deep/machine learning model for estimating age from mammogram images.  In [1], we extracted deep learning features from the collected data set (Mini-DDSM), by which we built a model using Random Forests regressor to estimate the age automatically. The performance assessment was measured using the mean absolute error values. The average error value out of 10 tests on random selection of samples was +/- 8.11 years and r = 0.618, can you beat this base-line? I bet you can.

## Expected Submission
Since this is not a formal competition, and since it is based on voluntary work, I would just rely on calling upon your enthusiastic spirit to engage in this meaningful study. If you are up to this challenge, then you need to **construct an age prediction model** based on the following partitions: 70% training and 30% test (randomly selected). 

What we would like you to submit is the following:

**1) Your whole approach to building the model**
    1.1  Data preparation and pre-processing steps
    1.2 Detailed description of the model's architecture to ease replication

**2) Performance metrics**
    2.1 The average MAE (mean absolute error) value over 10 runs (samples  
          randomly selected based on 70% / 30% partitioning
    2.2 The Pearson correlation coefficient (r) value over 10 runs (samples randomly 
          selected based on 70% / 30% partitioning

## Evaluation
Average MAE and r (Pearson correlation coefficient)

### Further info
[1] Check the conference pre-print at the bottom of this page:
https://ardisdataset.github.io/MiniDDSM/","","Challenge: Automatic Age Estimation from Mammograms","Help estimate age in mammography cohorts comprising missing age values (i.e., NaN)","","1"
"3086","1050220","1842206","01/01/2021 03:50:09","## Task Details
1. Improve  definition
2. add more context
3. improve/ add visualizations

## Expected Submission
Analysis and/ or appropriate visualization with the data","","Continue Working On Notebooks and Visualizations","","","0"
"3120","1050220","1842206","01/04/2021 03:10:56","## Task Details
Add King Levels to both players

## Expected Submission
Dataset should have king levels of players","","Add King Levels to both players","","","0"
"3333","1127808","1842206","01/30/2021 11:04:25","## Task Details
Run Sentiment Analysis

## Expected Submission
Quantified Sentiment Analysis on the tweets","","Maybe you guys can create run sentiment analysis on this dataset?","","","0"
"3901","1240454","1842648","03/30/2021 03:02:27","## Task Details
Find which party has highly educated candidates? 
Find which party has high criminal candidates? 
Find which party has rich candidates? 

## Expected Submission
Just a simple notebook with clean code and plots.","","Find which party has highly educated candidates?","","04/06/2021 23:59:00","2"
"2563","921440","1846196","10/28/2020 13:43:50","Build a video game recommender","","Video Game Recommender","","","0"
"4459","1359157","1849478","05/22/2021 08:09:44","## Task Details
Tell Your story

## Expected Submission
Detailed EDA

## Evaluation
Insights and Recommendations

### Further help","","Do a detailed EDA and try to understand if we can use whatsapp for student learning","","","2"
"4434","1351301","1852232","05/18/2021 18:42:46","## Task Details
The task involves the prediction of the restaurant average ratings based on the data provided in the ""TripAdvisor European restaurants"" dataset.

## Expected Submission
The solution should contain the model that is capable of predicting in the most accurate way the restaurant average ratings.

## Evaluation
The predictive models should be evaluated using the Mean Absolute Error (MAE)","","Average rating prediction","","","1"
"2397","912162","1852232","10/10/2020 17:10:20","## Task Details
The task is about finding, using the 'players_21' dataset, the ideal transfers to complete on FIFA 21 Career Mode in order to get the 11 players with the highest overall ratings given a budget constraint of ‚Ç¨ 100M.

## Expected Submission
The users should submit the list of players and their associated price (taken from the field 'value_eur', so that it is possible to see that their total cost is below ‚Ç¨ 100M and their total overall rating is the highest.

## Evaluation
The solution is evaluated by seeing the lineup of the 11 players purchased for less than ‚Ç¨ 100M.

### Further help
The task can be carried over also for older FIFA version (from version 15 to version 20) and the experiment can be alternatively done with the potential ratings instead of the overall ratings.","","Most optimized FIFA 21 transfer window","Find the best 11 players to buy given a budget constraint of ‚Ç¨ 100M","","2"
"2659","964336","1863042","11/09/2020 14:22:53","## Task Details
This is a very simple beginer  level dataset clean and small very apt to practice the skills that you have gained recently, show us what you got!!","","Explore and practice","practice your skills now","","1"
"2718","964336","1863042","11/18/2020 07:59:20","#Practice

Just practice your new skills in this dataset","","Prediction of future fuel prices","Practice your Machine learning skills","","1"
"2648","962693","1863042","11/08/2020 15:48:21","## Task Details
Explore the dataset try to come up with insights over the evolution of production and supplies of cannabis over time.

## Expected Submission
A beautiful Notebook which explores the dataset and comes up with conclusions that would give insights over the data.

## Evaluation
Nothing to Evaluate, go crazy its time for exploration.","","Explore and Analyse the dataset","explore and come up with beautiful insights about evolution of cannabis over time","","1"
"3272","1118500","1864580","01/25/2021 17:52:40","People around the world are bound to have different opinions, concerns and rationale regarding the newly elected US President and his team. 

Some of the things to look into: 

- What are people looking forward to?
- What are the general concerns?
- What are the global opinions? 

Be Creative!","","Sentiment Analysis, EDA and more!","Find out the global opinions about the new US Administration","","0"
"3496","1157395","1864580","02/14/2021 13:37:11","## Task Details
Use this as an index to normalize the currencies of different countries and generate comparative visuals.","","Standardize Finance Data","","","0"
"3715","1201421","1873296","03/09/2021 14:20:22","Makine √∂ƒürenmesi veya derin √∂ƒürenme algoritmalarƒ± ile sƒ±nƒ±flandƒ±rƒ±nƒ±z.","","Dilleri Siniflandirma","","","0"
"2905","954101","1873296","12/11/2020 00:24:16","M√º≈üteri duygularƒ±na g√∂re, ikili (binary) g√∂r√º≈ü sƒ±nƒ±flandƒ±rmasƒ± yapalƒ±m. Makine √∂ƒürenmesi ya da derin √∂ƒürenme teknikleri kullanƒ±labilir.","","G√∂r√ºs Siniflandirma","","","0"
"2903","923536","1873296","12/10/2020 16:43:36","M√º≈üteri g√∂r√º≈ülerini, duygulara g√∂re, farklƒ± teknikler kullanarak sƒ±nƒ±flandƒ±ralƒ±m. Makine √∂ƒürenmesi ya da derin √∂ƒürenme teknikleri kullanƒ±labilir.","","G√∂r√ºs Siniflandirma","","","0"
"3858","1230665","1873296","03/24/2021 15:48:21","Make a Translation Algorithm.","","Machine Translation","","","0"
"4298","1312942","1895217","05/07/2021 13:04:10","## Task Details
Attia et al 2019 showed that it is possible to predict age and gender with high accuracy using Convolutional Neural Networks on ECG data (12-lead resting ECG).

The objective is to reproduce this result with this data set and optional model architecture. 

## Expected Submission
The user should submit a 10-fold cross-validated result based on the data set.

## Evaluation
A good submission contains nice visualizations

### Further help
https://www.ahajournals.org/doi/full/10.1161/CIRCEP.119.007284","","Do age and gender prediction bases on ECG waveform","","","0"
"3965","1251943","1900190","04/05/2021 08:43:59","## Task Details
Exploratory data analysis of the data.","","Exploratory Data Analysis","","","0"
"3967","1251943","1900190","04/05/2021 08:59:05","## Task Details
For movies in every region, find out the average rating, number of movies and many more insights.","","Details of movies region wise","","","1"
"4171","1292934","1900190","04/24/2021 12:04:46","Exploratory Data Analysis of the Tweets","","Exploratory Data Analysis of the Tweets","","","0"
"4136","1284112","1900190","04/20/2021 13:38:39","Perform ARIMA to predict the population growth for upcoming years","","Perform ARIMA to predict the population growth for upcoming years","","","0"
"4115","1279817","1900190","04/18/2021 15:08:52","Visualize the trend in mortality rate over the years for various countries","","Visualize the trend in mortality rate over the years for various countries","","","0"
"4116","1279817","1900190","04/18/2021 15:09:41","Comparison of trend of mortality rate for a year for both male and female.","","Comparison of trend of mortality rate for a year for both male and female.","","","0"
"4117","1279817","1900190","04/18/2021 15:10:32","Top 5 Countries with highest mortality rate  both male and female","","Top 5 Countries with highest mortality rate both male and female","","","0"
"4118","1279817","1900190","04/18/2021 15:11:08","Top 5 Countries with highest mortality rate both male and female","","Top 5 Countries with lowest mortality rate both male and female","","","1"
"4025","1258894","1900190","04/10/2021 04:30:54","Top 5 countries which are most interested in Football Word Cup","","Top 5 countries which are most interested in Football Word Cup","","","0"
"4026","1258894","1900190","04/10/2021 04:32:29","Analyze how the interest of people around the world has changed over time","","Analyze how the interest of people around the world has changed over time","","","0"
"4027","1258894","1900190","04/10/2021 04:33:15","Common words/phrases searched over time","","Common words/phrases searched over time","","","0"
"4515","1372571","1900190","05/28/2021 09:26:52","Perform Time Series Analysis to predict future values","","Perform Time Series Analysis to predict future values","","","0"
"3785","1214972","1900190","03/16/2021 12:43:13","Find out the Average rating of a restaurant in a city","","Average rating of restaurant in a city","","","0"
"3786","1214972","1900190","03/16/2021 12:44:11","Generate Word cloud of Cuisine in a city","","Word cloud of Cuisine in a city","","","1"
"4300","1323179","1914932","05/07/2021 14:00:24","Analyze data to give insights about dogecoin.","","Insights about DogeCoin","","","0"
"2426","918207","1914939","10/12/2020 23:31:01","## Task Details
The main objective is to show step-by-step how to analyze and visualize different features from the dataset to have a better understanding of the bank industry and how it behaves.

## Expected Submission
A notebook that accomplishes the task.

## Evaluation
Try to answer the following questions:

* What is the max Close price for each bank's stock throughout the time period?
* On what date did Citigroup stock reach its highest price?
* Why does the first row have NaN values?
* Is there a stock that stands out?
* Did anything significant happen on 2009-01-20?
* Which stock would you classify as the riskiest over the entire time period?
* Which would you classify as the riskiest for the year 2015?
* What is the rolling 30 day average against the Close Price for Bank Of America's stock for the year 2008?","","Exploratory Data Analysis - Help Investors Predict New Trends","","","9"
"2491","927999","1914939","10/19/2020 15:05:03","### Task Details
The main objective is to see how the pandemic has affected tech stocks.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Try to answer the following questions:

**Which are the companies that have been harmed the most by the COVID-19 Pandemic?**
**Which are the companies that have benefited the most from the COVID-19 Pandemic?**
**How to invest during the COVID-19 Pandemic?**","","How have the tech stocks performed during the COVID-19 Pandemic?","","","6"
"2485","927522","1914939","10/19/2020 11:51:38","### Task Details
The main objective is to show step-by-step how to analyze and visualize different features from the dataset to have a better understanding of the stock market and how it behaves.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Try to answer the following questions:
**In which economic sectors does Berkshire Hathaway invest the most?**
**In which companies does Warren Buffett invest the most?**","","Exploratory Data Analysis ‚Äì New Trends","","","7"
"2486","927522","1914939","10/19/2020 11:52:54","### Task Details
The main objective is to see how the pandemic has affected Berkshire Hathaway.

### Expected Submission
A notebook that accomplishes the task.

### Evaluation
Try to answer the following questions:

**Which are the companies that have been harmed the most by the COVID-19 Pandemic?**
**Which are the companies that have benefited the most from the COVID-19 Pandemic?**
**How to invest during the COVID-19 Pandemic?**","","How have Warren Buffett's portfolio performed during the COVID-19 Pandemic?","","","7"
"4037","1263194","1923205","04/10/2021 17:59:02","Investigate whether global conflict has been rising since 1979.","","Has global conflict been rising since 1979?","","","0"
"3250","1063843","1943362","01/22/2021 17:16:43","## Task Details
I have many low res images that I would like the improve the quality of. For this task, you should create a model that makes the image 4x larger.

You can try a model like SRGAN. I have created a notebook that implements and trains SRGAN.

## Expected Submission
Create a notebook that trains a super resolution model and saves it. Also, provide some code for testing on random images.

## Evaluation
A good model should be able to create a super resolved image that looks like the original, but is larger and the image should be more detailed.","","Super Resolution","Create a super resolution model with this dataset","","1"
"3525","1165536","1956324","02/17/2021 17:09:57","## Task Details
Take the list of all 2090 stock tickers, get news from stocknewsapi.com and create a submission for Numerai Signals.","","Compute a submission to Numerai Signals using all tickers","Submit to Numerai Signals using news article from stocknewsapi.com","","0"
"3082","1069796","1958282","12/31/2020 16:59:36","Create better visualization","","Create better visualization","","12/01/2021 23:59:00","1"
"4187","1293800","1979955","04/25/2021 17:14:49","An E-commerce company has run a A/B tests for its new webpage. Your job is to analyze data and come to a conclusion whether company should go ahead with new webpage or should stick with old/present webpage.","","Should company implement new webpage or keep the old/present page?","","","0"
"3458","1123919","1996326","02/10/2021 11:56:48","## Task Details

Telegram groups' data/messages is used to understand the discussion trends in the crypto community

## Expected Submission

Either a Jupyter Notebook or R MarkdownYou can be submitted to analyze the data.","","Trends in crypto space","","","0"
"3568","1173032","2002616","02/21/2021 14:57:02","Chess is a two-player strategy board game played on an 8 √ó 8 checkered board with 64 squares. The history of chess can be drawn back to around 1500 years, it was started in the North of India and spread throughout the world. Each player begins with 16 pieces one king, one queen, two rooks, two knights, two bishops and eight pawns. Each type of piece moves uniquely of which the most powerful is the queen and the least powerful is the pawn. Although the rules of the game have been amended multiple times the main objective is to checkmate the opponent‚Äôs king by placing it under an inescapable threat of capture. The invention of databases and chess engines in the 20th century revolutionized chess.
The goal of the project is to predict the number of moves (from 0-16) required to win for white with a king and a rook, given the position of white king, white rook and black king. It is a multiclass classification problem. The dataset has 28,056 rows and has the following attributes
1. White King file (column) - X1
2. White King rank (row) - X2
3. White Rook file -X3
4. White Rook rank -X4
5. Black King file - X5
6. Black King rank -X6
7. optimal depth-of-win for White in 0 to 16 moves, otherwise drawn {draw, zero, one, two, ..., sixteen}. -Y
As a team we are fascinated with the game. The rich history and the complexity (10 to the power 120 games possible) of the game dragged us to this project. The chess Engine came to limelight when deep blue, IBM Engine beat Garry Kasparov (in year 1997) the Russian Grand Master and world champion at that time. After that chess Engines took a huge turn and engines like stock fish were developed which is based on brute force algorithms. In late 2017 Google developed an AI based chess Engine which learned the game, in just 4 hours playing with itself and beat stockfish8.","","Aanalyzing checkmate pattern using King Rook King","","","0"
"3466","1152769","2018468","02/11/2021 17:34:36","## Task 
Every now and then we have an IPO of a company coming up. So, how should the investor decide whether to apply for an IPO or not with the **aim of earning a Listing gain**?

## Task details
Every now and then we have an IPO of a company coming up. So, how should the investor decide whether to apply for an IPO or not with the **aim of earning a Listing gain**?
There are a lot of parameters in the RHP (Red Herring Prospectus) which can help one decide this but is there any other pattern that can be looked into? 

The data provided in this task has a list of all the past IPOs of Indian companies. 
It has details like -  Issue Size (in crores) of the IPO, Subscription rate of QIB, HNI and RII categories, Issue price, link to the details of the company, etc. These columns can provide interesting insights and when used correctly can lead to the building of robust models for the task.

So, how should one decide if an IPO is worth applying or not, just by looking at the listing gain? It's pretty simple - If the listing gain is positive, then the IPO is worth applying, otherwise not. Thus it's a classification problem (Target variable? A derivative of the listing gain).

It is to be noted that the columns - Listing Open, Listing Close and Listing
Gains(%)  should not be used in the Classifier model. CMP and Current Gains (%)  should not be used either.


NOTE: We are not concerned about long term gains.

## Expected Submission
The solution can be a notebook containing a visualisation of important patterns in data. This can be further modified by adding a classifier model (1 for +ve listing gain positive and 0 for -ve listing gain).

## Evaluation
The Evaluation for the classifier model will be based on 5 fold CV score. Notebooks are required.","","Apply for IPO?","Whether one should apply for the IPO of a company or not (for listing gains)?","","1"
"3879","1235835","2026986","03/27/2021 10:28:59","## Task Details
You don't have to know about the whole universe of stocks to make money. You can earn if you can predict movement of one index only. It will suffice. Here is the data of Nifty 50 index. It has volume in millions. You have a chance to make an algorithm with this data and win in the markets.

## Expected Submission
1. Find patterns in the data and generate a buy/sell signal
2. Tell the % profit/loss your algorithm made in last 5 year and how it performed in last 1 year.
3. List the winning streak and losing streak 
4. List the maximum profit/loss from your algorithm
5. Make an algorithm for intraday trading

## Evaluation
1. Algorithm which gives maximum profit with minimum loss would be considered good.
2. Algorithm with losing streak of less than 3 would be considered good

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Finding Intraday Patterns","","12/25/2021 23:59:00","0"
"3666","1193776","2041784","03/05/2021 19:52:38","translated geojson","","Add translated gejson","","","0"
"4918","1432330","2052355","06/26/2021 09:13:00","## Task Details
Make a Machine Learning Model and deploy over Flask Server, including
1. Perform an Exploratory Data Analysis on the data
Univariate analysis - Bivariate analysis - Use appropriate visualizations to identify the patterns and insights - Come up with a customer profile (characteristics of a customer) of the different packages - Any other exploratory deep dive

2. Illustrate the insights based on EDA
Key meaningful observations on individual variables and the relationship between variables

3. Data Pre-processing
Prepare the data for analysis - Missing value Treatment, Outlier Detection (treat, if needed- why or why not ), Feature Engineering, Prepare data for modeling

4. Model building - Bagging
Build classifier based on random forest, and decision tree.

5. Model performance improvement - Ensemble
Comment on which metric is right for model performance evaluation and why? - Comment on model performance - Can model performance be improved? check and comment

6. Actionable Insights & Recommendations
Conclude with the key takeaways for the business - What would your advice be to grow the business?

7. Flask Server
Deployment of model with Flask Server enabling the company to use the tool as a web-application

## Expected Submission
Your submission must contain Jupyter Notebook file for Modeling and Flask Server Code with Templates and Static Files in Zip over Github along with your Flask Server URL","","Predict the Tour Package Customer may Buy","Use of Ensemble Techniques","06/30/2021 23:59:00","5"
"4743","1404389","2052355","06/12/2021 11:17:51","## Task Details
We have a restaurant dataset with us. You need to create a Machine Learning Model to predict the revenue. Also deploy the model into Python GUI application to fetch realtime queries over it.

## Column Information
Data fields
Id : Restaurant id. 
Open Date : opening date for a restaurant
City : City that the restaurant is in. Note that there are unicode in the names. 
City Group: Type of the city. Big cities, or Other. 
Type: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile
P1, P2 - P37: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.
Revenue: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values.","","Predict the Revenue on GUI App","Machine Learning (Regression Task)","06/14/2021 23:59:00","2"
"3422","1108326","2055480","02/06/2021 15:33:31","Edit (23/07/21): The [starter]((https://www.kaggle.com/cdminix/starter-us-drought-meteorological-data) and [baseline](https://www.kaggle.com/cdminix/lstm-baseline) notebooks have been updated. We now use a 180-day window of past data for predictions, and include previous drought values, static data, and meteorological data from the year prior. We also now evaluate on 6 future weeks of predictions. While the baseline model is still very simple, it performs much better using this additional input data.

## Task Details
For this task, try to predict drought categories using the 180-day window of weather data given for each sample.

## Expected Submission
Submit a notebook (based on [the starter notebook](https://www.kaggle.com/cdminix/starter-us-drought-meteorological-data)) predicting drought categories better than the baselines in the notebook.

## Evaluation
Due to the imbalanced classes, the primary metric is **macro f1** (on the validation set), anything above ``0.164`` is better than a random stratified classifier! Here are the baseline results from the starter notebook:

###  Baseline Scores
| Score | Week 1| Week 2| Week 3| Week 4| Week 5 | Week 6
| -------- | ------- | ------ | ----- | --- | --- | -- |
| F1 | 82.1| 71.9 | 65.8 | 58.3 | 54.5 | 49.7 |
| MAE | 0.133| 0.2 | 0.257 | 0.312 | 0.359 | 0.401 |

## ""Leaderboard""
I will add submitted notebooks to this highly unofficial leaderboard. Let me know if there is a better way to do this!

| Rank | User | *Model/Notebook* | Macro F1 Mean | MAE Mean |
| ----- | -----| --------- | ---------- |
| 1       | @cdminix | [LSTM Baseline](https://www.kaggle.com/cdminix/lstm-baseline) | 63.9 | 0.277 |","","Predict Drought Categories (Beat the Baseline)","Using the 180 days of weather data provided in the train dataset, predict drought categories in validation.","","7"
"3473","1153510","2067080","02/12/2021 05:22:11","## Build a Regression model 

Build a Regression/Ensemble model and predict some top 10 product prices.","","Make Prediction for some 10 random Products!","","","0"
"3637","1181967","2067080","03/02/2021 09:48:36","Using this training data, please identify whether a consumer will make a transaction in September 2019. Once model is built, use hold out data for final validation.","","Predictive task","","","0"
"3638","1181967","2067080","03/02/2021 10:11:11","Derive Business Insights using Multiple EDA methods.","","Business Insights","","","0"
"4093","1276832","2067080","04/17/2021 04:23:10","## Task Details
Create a generalized function to decompose the **Opening/Closing price** column using ***Discrete Wavelet Transform**.

For more, refer this: [DWT-Pywat Library](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html)","","DWT Decomposition","","","0"
"4094","1276832","2067080","04/17/2021 04:25:05","## Task Details
Refer the PDF for MODWT Explanation and create a generalized function to decompose the opening/closing price column using MODWT technique.

Once decomposed, create a model to make predictions over those coefficients and upcoeff them to final prediction.","","Implement MODWT Function","","","0"
"3613","1182746","2069310","02/27/2021 14:34:39","## Task Details
Build and train a model that's given a picture of a face and find a famous iconic woman who's the most similar to that person.","","Iconic Doppelganger","Match a face with an iconic woman","","1"
"2574","946022","2072595","10/29/2020 16:59:35","## Task Details
This task is to find an algorithm that helps in locating the license plates.

## Expected Submission
People should play with the data as they want. They can submit their weight files or their notebooks does not really matter.

## Evaluation
A good solution will be a good test accuracy for the co-ordinates.","","Object Detection","Find the best algorithm for license plate detection","","1"
"3235","1109924","2077168","01/21/2021 08:53:40","Create Insights of NIFTY Equity Stock Rice Trend Prediction.","","Predict Trend of Nifty Equity Stocks","","","0"
"4214","1302873","2089138","04/28/2021 16:35:46","Forecast the load","","Time Series Forecasting","","","0"
"3414","1134563","2092170","02/05/2021 19:24:47","## Task Details

Perform EDA on the data, like top cities with most post office, area wise post office, plotting coordinates on the map.

## Expected Submission
Names of the Cities with most General Post Office.

## Evaluation
Show some visualization and see if you can break the top cities by area

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/kingabzpro/story-of-gufhtugu-publications
- https://www.kaggle.com/tahminashoaib86/pakistan-post-office-with-postal-code-and-gps","","EDA of the Data","Data Visualization","03/31/2021 23:59:00","0"
"3339","1128256","2103971","01/30/2021 15:14:38","Compare large restaurants with small ones. The criterion for allocation to one of the groups may be sales in 2020, and the aspects to compare are sales increases, locations or the specificity of restaurants.","","Small businesses versus large companies?","What are the differences?","","9"
"3433","1128256","4999450","02/08/2021 06:43:34","yoy_units means increased amount of units?","","what is YOY_Uuits?","","","38"
"3266","1115711","2103971","01/24/2021 11:57:11","Were users as a whole speaking more positively or negatively about UFC257?
What words did the tweeters write about which character?
Did the sentiment change over time and was it different during and after the event?","","What emotions did Twitter users feel about the fight?","Negative or positive?","","0"
"3218","1003985","2111725","01/18/2021 12:19:01","## Task Details
Skoltech Anomaly Benchmark (SKAB) designed for evaluating the anomaly detection algorithms.

Currently the SKAB v0.9 is available, but soon v1.0 will be published (expected &gt;350 files with the anomalies).

We expect issues with the data (leaks and other problems) to be found.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation.

We expect participants to apply unsupervised anomaly detection algorithms. For the purposes of model fitting, either the fault-free dataset (located in a separate folder) or fault-free interval in the beginning of each dataset can be used.

## Evaluation
SKAB allows working with two main problems (outlier detection, changepoint detection). That's why we propose metrics for each problem:
- Outlier detection: False Alarm Rate (FAR), Missing Alarm Rate (MAR);
- Changepoint detection: Numenta Anomaly Benchmark (NAB) evaluation algorithm.

In the description and in [this readme](https://github.com/waico/SKAB) we propose the leaderboard for SKAB v0.9 both for outlier and changepoint detection problems.

This task is strictly about getting the best submission score.","","Find the Anomalies in the Data","Let us solve the outlier detection or changepoint detection problems with the best quality","","0"
"2570","945122","2142457","10/29/2020 04:57:40","## Task Details
Give the size of the user according to the input values

## Expected Submission
The expected submission would be to give the expected size of the cloth that the user should buy.","","Predicted Size","Give size of the user according to the input values","10/31/2020 23:59:00","4"
"5165","945122","4656934","07/17/2021 12:02:56","For a given age,weight and height predict the size.","","Predict the size of the cloth.","","","2"
"4001","1257909","2145476","04/08/2021 07:09:41","Spam filtering is a beginner‚Äôs example of document classification task which involves classifying an email as spam or non-spam (a.k.a. ham) mail. Spam box in your Gmail account is the best example of this. So lets get started in building a spam filter on a publicly available mail corpus.

We will walk through the following steps to build this application :

Preparing the text data.
Creating word dictionary.
Feature extraction process
Training the classifier

1. Preparing the text data.
 
The data-set used here, is split into a training set and a test set containing 702 mails and 260 mails respectively, divided equally between spam and ham mails. You will easily recognize spam mails as it contains *spmsg* in its filename.

In any text mining problem, text cleaning is the first step where we remove those words from the document which may not contribute to the information we want to extract. Emails may contain a lot of undesirable characters like punctuation marks, stop words, digits, etc which may not be helpful in detecting the spam email. The emails in Ling-spam corpus have been already preprocessed in the following ways:

a) Removal of stop words ‚Äì Stop words like ‚Äúand‚Äù, ‚Äúthe‚Äù, ‚Äúof‚Äù, etc are very common in all English sentences and are not very meaningful in deciding spam or legitimate status, so these words have been removed from the emails.

b) Lemmatization ‚Äì It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. For example, ‚Äúinclude‚Äù, ‚Äúincludes,‚Äù and ‚Äúincluded‚Äù would all be represented as ‚Äúinclude‚Äù. The context of the sentence is also preserved in lemmatization as opposed to stemming (another buzz word in text mining which does not consider meaning of the sentence).

We still need to remove the non-words like punctuation marks or special characters from the mail documents. There are several ways to do it. Here, we will remove such words after creating a dictionary, which is a very convenient method to do so since when you have a dictionary, you need to remove every such word only once. So cheers !! As of now you don‚Äôt need to do anything.


Creating word dictionary.
 It can be seen that the first line of the mail is subject and the 3rd line contains the body of the email. We will only perform text analytics on the content to detect the spam mails. As a first step, we need to create a dictionary of words and their frequency. For this task, training set of 700 mails is utilized. This python function creates the dictionary for you.

3. Feature extraction process.
 
Once the dictionary is ready, we can extract word count vector (our feature here) of 3000 dimensions for each email of training set. Each word count vector contains the frequency of 3000 words in the training file. Of course you might have guessed by now that most of them will be zero. Let us take an example. Suppose we have 500 words in our dictionary. Each word count vector contains the frequency of 500 dictionary words in the training file. Suppose text in training file was ‚ÄúGet the work done, work done‚Äù then it will be encoded as [0,0,0,0,0,‚Ä¶‚Ä¶.0,0,2,0,0,0,‚Ä¶‚Ä¶,0,0,1,0,0,‚Ä¶0,0,1,0,0,‚Ä¶‚Ä¶2,0,0,0,0,0]. Here, all the word counts are placed at 296th, 359th, 415th, 495th index of 500 length word count vector and the rest are zero.

4. Training the classifiers.
 
Here, I will be using scikit-learn ML library for training classifiers. It is an open source python ML library which comes bundled in 3rd party distribution anaconda or can be used by separate installation following this. Once installed, we only need to import it in our program.

I have trained two models here namely Naive Bayes classifier and Support Vector Machines (SVM). Naive Bayes classifier is a conventional and very popular method for document classification problem. It is a supervised probabilistic classifier based on Bayes theorem assuming independence between every pair of features. SVMs are supervised binary classifiers which are very effective when you have higher number of features. The goal of SVM is to separate some subset of training data from rest called the support vectors (boundary of separating hyper-plane). The decision function of SVM model that predicts the class of the test data is based on support vectors and makes use of a kernel trick.","","SMS Spam Detection","Spam Detection using NLTK","04/09/2021 23:59:00","1"
"3911","1242096","2156555","03/30/2021 18:18:41","Can you use your skills to generate new and meaningful inspiration quotes?","","Generation of new quotes","Can you use your skills to generate new and meaningful inspiration quotes?","","0"
"3941","1242096","2156555","04/02/2021 19:17:02","To find the gold hidden in this data.","","Exploratory Data Analysis","","","0"
"2939","1035018","2167573","12/14/2020 15:08:56","Check if there is any relation between Maternal Age and smoking habits.
Justify the reason for the hypothesis.","","Is there any Relationship between Smoking and Maternal Age?","","12/16/2020 00:00:00","4"
"2940","1035229","2167573","12/14/2020 17:14:52","Verify that, there is a 75% chance that it will have purple flowers and a 25% chance that the flowers will be white, regardless of the colours in all the other plants.","","Mendel's Pea Flowers - Hypothesis Testing.","","12/18/2020 23:59:00","0"
"2756","991988","2189188","11/23/2020 15:25:07","asd","","Multyclassification task","Create a simple model for classification by product name","11/27/2056 23:59:00","0"
"4479","1364504","769452","05/24/2021 19:36:01","## Task Details
Push the exploratory data analysis for this rich tweets dataset one step further and include  topic modelling

## Expected Submission
You ca use either a Jupyter Notebook or a R Markdown report.

## Evaluation
A good submission will need to include:
- A comprehensive exploratory data analysis;
- Expressive graphs;
- Interpretation of results","","Perform Topic Modelling on Kenyan Political Tweets","Discover the subjects discussed in Kenyan Political Tweets","06/15/2021 23:59:00","0"
"4480","1364504","769452","05/24/2021 20:15:09","### Task description

Perform sentiment analysis of Kenyan Political Tweets","","Sentiment Analysis","Perform sentiment analysis of Kenyan Political Tweets","06/15/2021 23:59:00","0"
"3963","1251863","2200451","04/05/2021 07:34:25","Task Details
Our top priority in this health problem is to identify patients with a stroke.

Expected Submission
Solve the task primarily using Notebooks.","","Predict Stroke","","","0"
"3149","1084273","2227464","01/07/2021 14:02:13","The task requires increasing the accuracy of the dataset, by any technique, e.g. use different classifiers, clean the data with better techniques, or using neural networks.","","Increase Accuracy","Apply Different Techniques to get Better Accuracy","01/31/2021 23:59:00","0"
"3831","1225408","2236152","03/21/2021 23:11:03","which recommended system will work better? ¬øcollaborative filtering, based on context or hybrid?

With a recommended system based on context, which feature are the most valuable for the system? synopsis? genre? studio?","","Recommended system task","","","4"
"4938","1225408","2236152","06/28/2021 23:23:59","Try to merge the information of this dataset with [https://www.kaggle.com/hernan4444/animeplanet-recommendation-database-2020](https://www.kaggle.com/hernan4444/animeplanet-recommendation-database-2020).

## Expected Submission

New [anime-recommendation-database-2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020) with tag from [animeplanet-recommendation-database-2020](https://www.kaggle.com/hernan4444/animeplanet-recommendation-database-2020)","","Merge 2 anime datasets","","","0"
"4937","1435738","2236152","06/28/2021 23:23:35","Try to merge the information of this dataset with [https://www.kaggle.com/hernan4444/anime-recommendation-database-2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020).

## Expected Submission

New [anime-recommendation-database-2020](https://www.kaggle.com/hernan4444/anime-recommendation-database-2020) with tag from [animeplanet-recommendation-database-2020](https://www.kaggle.com/hernan4444/animeplanet-recommendation-database-2020)","","Merge 2 anime datasets","","","0"
"4455","1357233","2256936","05/21/2021 08:51:07","detect vehicle plates in images.","","Vehicle plate detection","","","0"
"2520","935724","2258136","10/23/2020 09:36:03","You can use all of the data to do statistics and analysis about inspection in US.
To go further you can create ML tool to try to predict the score of a business regarding its information. 
You can also create two categories of restaurants to do so, one with bad scores and one with good scores. 

Enjoy and have a great experience !","","Inspection score prediction -  good or bad outcome","","","0"
"3115","1062213","2261575","01/03/2021 11:23:37","## Task Details
The behavior of a person is recognized by the various factors. Even it is not easy for people to get correctly understand the personality of some one by first meeting. Here is the dataset of 110 writers in English and Hindi containing all the alphabets. Design a deep neural network that is able to classify the personality traits using the handwriting of the persons.

## Expected Submission
Please submit a notebook containing the preprocessing operations  and then 

## Evaluation
The criteria for the good solution is accuracy.","","Classification of Personality Traits through Handwriting Analysis","","","0"
"4354","1317291","2291146","05/12/2021 23:00:01","Title says it all. Good luck, speak soon.","","Find a winning trading strategy and share exclusively with the dataset owner",":p","","1"
"3040","1058763","2308010","12/25/2020 16:08:56","## Task Details
Since this dataset has all the voting details from 2012, it can be used to rank the footballers over the whole time period


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Best Footballers over the years","","","3"
"3679","1193223","2319869","03/07/2021 00:43:16","## Task Details
The NBA has evolved somehow, someway throughout the years. Let's try to encapsulate that change through compelling visuals.","","Examine different eras and unique style of play","","","0"
"5088","1428089","2334976","07/12/2021 04:35:06","The data was collected using the PushShift API which does not update the variables frequently, updating them using the reddit API would help to make the data more precise.

The reddit API can use the ID of a submission in order to get its most recent variables.","","Update scores and number of awards using the reddit API","","","0"
"3391","1116669","2337246","02/03/2021 12:41:58","Use the data to draw some useful visualization to help understand how the population of Pakistan increasing over time.","","Create insightful Visualization","","","0"
"3050","916012","2337246","12/27/2020 01:35:51","## Task Details
**Bi-Gram**
A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.

The task is very simple you just have to create a bi-gram model and create a prompt to test it out.
Further details:
https://github.com/MrAsimZahid/William-Shakespeare-Plays-BiGram-Model-From-Scratch-in-Python/blob/master/Pythons%20Scripts.pdf

## Expected Submission
**Model**
Create a bi-gram model file. It could be in any format(Choose the best format yourself).

**Generate Statement**
Given your model and a prompt, generate sentences of various lengths.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:

- https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9","","Create Bi-Gram Model","","02/28/2021 23:59:00","0"
"3430","1124418","2347619","02/07/2021 17:50:37","## Task Details
This task is what my notebook related to this dataset answers

## Expected Submission
Feel free to answer this however you would like, I used statistics to answer it. It's a simple dataset, so this limits what you can do.

## Evaluation
Good non-ambiguous explanations are all I would expect with evaluations.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Were the Fantasy Football Results Statistically Significant or due to Random Chance?","","","1"
"3544","1169473","2347619","02/19/2021 14:33:26","## Task Details
Answer the following questions from this dataset:
- Which languages are spoken in the U.S.?
- Where are these languages spoken within the U.S.?
- Which states have the most language diversity?
- Which foreign language speakers are most fluent in English?
- How have the languages spoken changed over time?

## Expected Submission
The solution should answer the questions listed above. Be creative in your responses.

## Evaluation
A good solution is creative and thorough in answering the listed questions.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","U.S. Languages","Answer some questions about languages in the U.S.","","1"
"3164","1088794","2352583","01/10/2021 03:59:09","Find insights about distribution efficacy across various regions based on community comments","","Vaccine Distribution Efficacy/Speed","Find insights about distribution efficacy across various regions based on community comments","","0"
"2575","946174","2352583","10/29/2020 17:11:50","Whether you are using Logistic regression, RNNs or transformers, please report your results here!","","Report your F1 Score here","","","2"
"3377","1083622","2357264","02/02/2021 09:03:04","## Predicting the sales of products across stores of a retail chain
I. A large Indian retail chain has stores across 3 states in India: Maharashtra, Telangana and Kerala. These stores stock products across various categories such as FMCG (fast moving consumer goods), eatables / perishables and others. Managing the inventory is crucial for the revenue stream of the retail chain. Meeting the demand is important to not lose potential revenue, while at the same time stocking excessive products could lead to losses.
In this problem you are tasked with building a machine learning model to predict the sales of products across stores for one month. These models can then be used to power the recommendations for the inventory management software at these stores.

##II. The datasets are provided as cited below
Target attribute: ""sales"" (continuous)
traindata.csv: ‚óè date : The date for which the observation was recorded ‚óè productidentifier : The id for a product
‚óè departmentidentifier : The id for a specific department in a store ‚óè categoryof_product : The category to which a product belongs
‚óè outlet : The id for a store
‚óè state : The name of the state
‚óè sales : The number of sales for the product

##Auxiliary Datasets:
‚óè productprices.csv : The prices of products at each store for each week ‚óè datetoweekidmap.csv : The mapping from a date to the weekid
‚óè samplesubmission.csv :The format for submissions ‚óè The testdata.csv file has all the attributes of the train_data.csv file excluding the sales (target) column
You are expected to create an analytical and forecasting framework to predict the sales of the products based on the quantitative and qualitative features provided in the datasets. You may derive new features from the existing features and also from the domain knowledge, which may help in improving the model efficiency

The evaluation metric for this problem is the ##RMSE or the ## Root Mean Squared Error.","","Predicting the sales of products of a retail chain","Predicting the sales of products of a retail chain","","2"
"3370","1100036","2357264","02/01/2021 14:38:03","**Description
This is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not.
**
Abstract: This is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not.(Target variable)
Source:
Daniel Whiteson daniel '@' uci.edu, Assistant Professor, Physics & Astronomy, Univ. of California Irvine
Data Set Information:
The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.
Attribute Information:.(Target variable)
The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature see the original paper.
Relevant Papers:Baldi, P., P. Sadowski, and D. Whiteson. ‚ÄúSearching for Exotic Particles in High-energy Physics with Deep Learning.‚Äù Nature Communications 5 (July 2, 2014).
Citation Request:
Baldi, P., P. Sadowski, and D. Whiteson. ‚ÄúSearching for Exotic Particles in High-energy Physics with Deep Learning.‚Äù Nature Communications 5 (July 2, 2014).
Description
The dataset has been built from official ATLAS full-detector simulation, with ""Higgs to tautau"" events mixed with different backgrounds. The simulator has two parts. In the first, random proton-proton collisions are simulated based on the knowledge that we have accumulated on particle physics. It reproduces the random microscopic explosions resulting from the proton-proton collisions. In the second part, the resulting particles are tracked through a virtual model of the detector. The process yields simulated events with properties that mimic the statistical properties of the real events with additional information on what has happened during the collision, before particles are measured in the detector.
The signal sample contains events in which Higgs bosons (with a fixed mass of 125 GeV) were produced. The background sample was generated by other known processes that can produce events with at least one electron or muon and a hadronic tau, mimicking the signal. For the sake of simplicity, only three background processes were retained for the Challenge. The first comes from the decay of the Z boson (with a mass of 91.2 GeV) into two taus. This decay produces events with a topology very similar to that produced by the decay of a Higgs. The second set contains events with a pair of top quarks, which can have a lepton and a hadronic tau among their decay. The third set involves the decay of the W boson, where one electron or muon and a hadronic tau can appear simultaneously only through imperfections of the particle identification procedure.
Due to the complexity of the simulation process, each simulated event has a weight that is proportional to the conditional density divided by the instrumental density used by the simulator (an importance-sampling flavour), and normalised for integrated luminosity such that, in any region, the sum of the weights of events falling in the region is an unbiased estimate of the expected number of events falling in the same region during a given fixed time interval. In our case, the weights correspond to the quantity of real data taken during the year 2012. The weights are an artifact of the way the simulation works and so they are not part of the input to the classifier. For the Challenge, weights have been provided in the training set so the AMS can be properly evaluated. Weights were not provided in the qualifying set since the weight distribution of the signal and background sets are very different and so they would give away the label immediately. However, in the opendata.cern.ch dataset, weights and labels have been provided for the complete dataset.
The evaluation metric is the approximate median significance (AMS):
\[ \text{AMS} = \sqrt{2\left((s+b+b_r) \log \left(1 + \frac{s}{b + b_r}\right)-s\right)}\]
where
‚Ä¢	$s, b$: unnormalised true positive and false positive rates, respectively,
‚Ä¢	$b_r =10$ is the constant regularisation term,
‚Ä¢	$\log$ is the natural log.
More precisely, let $(y_1, \ldots, y_n) \in \{\text{b},\text{s}\}^n$ be the vector of true test labels, let $(\hat{y}_1, \ldots, \hat{y}_n) \in \{\text{b},\text{s}\}^n$ be the vector of predicted (submitted) test labels, and let $(w_1, \ldots, w_n) \in {\mathbb{R}^+}^n$ be the vector of weights. Then
\[ s = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{s}\} \mathbb{1}\{\hat{y}_i = \text{s}\} \]
and
\[ b = \sum_{i=1}^n w_i\mathbb{1}\{y_i = \text{b}\} \mathbb{1}\{\hat{y}_i = \text{s}\}, \]
where the indicator function $\mathbb{1}\{A\}$ is 1 if its argument $A$ is true and 0 otherwise.
For more information on the statistical model and the derivation of the metric, see the documentation.","","Higgs bosons and a background process","Higgs bosons and a background process which does not (classification problem)","","3"
"3913","1242521","2361275","03/31/2021 01:55:44","## Task Details
Perform an exploratory data analysis.","","Exploratory Data Analysis","","","0"
"4195","1298871","2361700","04/27/2021 03:26:04","You could do EDA and any ML option you feel would be best for this data.","","Any kind of machine learning","","","1"
"4646","1390204","2386669","06/05/2021 10:17:38","## Task Details
This is dataset that I found online for my project. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help","","License Plate Digit Recognition","Feel free to make License Plate Digit Recognition using this dataset","10/05/2022 23:59:00","0"
"4681","1390140","2386669","06/08/2021 12:18:53","## Task Details
Nowadays, a lot of people moving to live in urban areas - with dense population where social and transport security becomes an important issue in comfort and safe life in urban environments. In dense populated areas, there are a lot cars and transport vehicles that form transportation ecosystem of the city. In order to be able to guarantee safety of citizens, local government should be able to limit the speed of moving vehicles in an efficient way. One of the efficient ways to help in guaranteeing the speed limit obeyance is use of Computer Vision and Deep Learning. Imagine this scenario:

![Speed Detection](https://images.app.goo.gl/2HQmSw28ExMDUEcq6)


## Expected Submission
Submissions are preferred in Notebooks format. 

## Evaluation
Evaluations are made based on primarily on score results. Well documented notebooks are also very welcomed!

### Further help
Feel free to create new discussion topic if you have any questions so that people and I could answer them. Happy Coding!:D","","License Plate Detection with Tensorflow","Create a License Plate Detection using Tensorflow","07/05/2021 00:00:00","2"
"2565","944249","2390050","10/28/2020 17:06:49","## Task Details
I just wanted to understand tweet correlation.
## Expected Submission
Just a visualization notebook.

## Take a look at any notebook related to this
https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12","","Are people Tweeting about the right things this election?","Visualize what people are tweeting about the election.","","0"
"2761","957212","2390050","11/24/2020 12:43:09","## Task Details
Create a notebook having analysed the text data in the given files.

## Expected Submission
A notebook. 
Eg: a notebook exploring sentiment analysis of each tweet or Visualizations on how many tweets a team has put out.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","what happened in this year's IPL?","","","0"
"2781","959018","2390050","11/26/2020 08:06:56","## Task Details
Tesla makes super safe cars and thats for the people sitting inside of it, well for the people outside it is super safe too other than this dataset of anomalies that ive curated. ü•á 

## Expected Submission
A notebook exploring all tesla accidents and fatalities.


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Are Not All Teslas super safe?","","","0"
"2660","963637","2390050","11/09/2020 15:15:07","## Task Details
Take a look at this dataset and Explore it.
## Expected Submission
Data exploration.

## Evaluation
Still hafto figure it out.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","So-fish-ticated Image classification.","Classify fish images","","1"
"2762","964536","2390050","11/24/2020 12:48:59","## Task Details
Pro-kabaddi has an average viewership of 316 million and although viewership is in decline, I guess its not a bad time to see how the league has performed till date.

## Expected Submission
A notebook exploring the data, with visualization would be great.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Is Cricket too mainstream?","","","0"
"2707","975304","2390050","11/16/2020 11:09:58","## Expected Submission
A notebook exploring text data based on this dataset.


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Joker movie sentiment analysis","","","0"
"2689","970312","2398395","11/13/2020 09:55:36","Which songs or Artist tend to always be in the Top40? üé∂ üî•","","Which Songs / Artists are Immortal?","","","0"
"3359","1044706","2411256","01/31/2021 13:19:42","Predict the stock prices using Machine Learning","","Stock Price Prediction","","","1"
"3343","1115964","2411256","01/30/2021 16:46:51","Use your skills to solve math problems written in a natural language","","Solve Math Problems Written In a Natural Language","","","0"
"3256","1113708","2411256","01/23/2021 09:47:21","Build a machine learning model that would help you to categorize cognitive processes involved in storytelling - Imagined, Recalled or Retold.","","Storytelling Processes","","","2"
"3342","1127912","2411256","01/30/2021 16:24:45","Target 
Benign - 0
Malware - 1","","Classify The Malwares","","","0"
"3344","1127920","2411256","01/30/2021 17:20:06","Classify the microorganisms","","Classification of Microorganisms","","","1"
"3660","1141564","5648246","03/04/2021 20:31:52","Ovarian Cancer has a survival rate of: 
93% if caught in stage 1
75% if caught in stage 2
30% if caught in stage 3 or 4.

Find the strongest features and build a classification model to decrease time/spending and therefore speed up diagnosis and treatment initiation.
 
Publish your model on a public server or upload a notebook to show the specs and scores of your model.","","Predict Ovarian Cancer","Classification Model","","0"
"3464","1151863","2411256","02/11/2021 08:46:43","Predict the CST values","","Predict CST","","","0"
"3494","1157198","2411256","02/14/2021 10:31:37","predict the reduced glass transition temperature","","Predict the reduced glass transition temperature","","","0"
"3605","1182702","2411256","02/26/2021 18:49:55","Use different time series techniques to predict the DGSR","","Time Series Analysis","","","0"
"3606","1182717","2411256","02/26/2021 19:08:24","Predict the bike trip duration considering various atmospheric factors","","Bike Trip Duration","","","1"
"3607","1182486","2411256","02/26/2021 19:09:53","Predict bike sharing demand considering the various atmospheric factors","","Predict Bike Sharing Demand","","","1"
"3801","1217491","2411256","03/17/2021 17:07:02","Predict the diving behaviour of seabirds","","Animal Behaviour Prediction","","","0"
"3890","1237568","2411256","03/28/2021 11:57:12","identify snail trails and hot spot failures","","identify snail trails and hot spot failures","","","0"
"3891","1237568","2411256","03/28/2021 11:57:15","identify snail trails and hot spot failures","","identify snail trails and hot spot failures","","","0"
"4259","1315408","2411256","05/04/2021 07:45:27","Predict the popularity of media","","Popularity Forecasting","","","0"
"4389","1340873","2411256","05/14/2021 18:49:30","Classify according to sentiments","","Classify according to sentiments","","","0"
"4392","1341732","2411256","05/15/2021 07:48:42","Analyze the data","","Analyze the Data","","","0"
"4370","1339084","2411256","05/13/2021 20:35:15","Tell a story using this data","","Analyze the Data","","","0"
"6886","1438735","8669470","11/20/2021 17:29:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","FirstTask","","11/30/2021 23:59:00","0"
"5555","1457743","6123027","08/05/2021 07:42:58","## Task Details

After mangoes were harvested they bring back to warehouse and washed it. Then each mangoes were send on conveyor belt. Using a high resolution camera and GPU enable device we need to give prediction of quality of mangoes in real time. It can be a binary classification.


## Expected Submission
Create a convolutional neural network model that can do binary classification for a input image.


## Evaluation
use any relevant accuarcy  measurement technique to measure it","","Classify mangoes according black patches","if a mongo without black patches then it high quality mango and if mango with black patches will be low quality mango.","08/07/2021 23:59:00","0"
"3531","1144865","2434598","02/18/2021 01:13:50","## Task Details
The shape of the board is one of the most influential characteristics in surfing. But how does shape influence performance?

## Expected shipment
What are the features that most define performance?
What is more relevant: width or length of the surfboard?

## Assessment
How easily can the insights found be translated into day-to-day knowledge?","","The importance of shape","","","0"
"3119","1074867","2437200","01/03/2021 14:15:26","## Task Details
Predict the gender of the person whose hand is in the image.

## Expected Submission
Submit a notebook which employs a robust representation of the gender detection problem and solution.
## Evaluation
Evaluation will be based on usefulness and upvotes on the notebooks.","","Gender detection","Predict gender given a dorsal or palm image.","","1"
"3645","1189453","2446721","03/02/2021 20:09:17","Come up with vizs for interesting stats","","Interesting IPL stats","","","0"
"2897","1020642","2447166","12/09/2020 15:30:53","## Task Details
Hi everyone,

Below are some of the suggestions you could use in your notebook: 

1. Descriptive analysis on the given dataset. 
2. Inferential statistical analysis: 
    - Correlation between different attributes such as to determine statistical 
       relationship between time spent on sleep and time spent on fitness.
    - Association between age of the students (group them into '7-17', '18-22', and 
       '23-') and different attributes including health issue, change in weight, etc. 
       (Pearson Chi Square test).
    - Non parametric tests to check significant differences between the distribution 
      of age of students/ region of residence with time spent on different activities.

Feel free to code.","","To do list","","","4"
"6153","1058693","2448949","09/22/2021 18:34:53","## Task Details
Build an Exploratory Data Analysis for one of the files of a power distribution company of your choice.

## Expected Submission
Solve using a Notebook, and be creative!

## Evaluation
Solutions that show correlations between variables and good graphical analysis are considered the best in this case.","","Wind Energy Dataset EDA","Analyzing Wind Energy Data","","0"
"3189","1080264","2448949","01/14/2021 05:42:35","## Task Details

Energy generations datasets have very close values, what makes easy for regression to do its magic.  But it is still necessary to apply several techniques to improve the performance of the models to the maximum.


## Expected Submission

Create a notebook using feature selection, dimensionality reduction or even other machine learning or statistics algorithms that can increase the accuracy rate above 80%.

## Evaluation
The solution is measured by the accuracy of the model and the quality of the predictions.

### Further help
You can begin from here: https://www.kaggle.com/jorgesandoval/hydropower-generation-knn-for-regression","","Hydropower Model Accuracy","Create a model with accuracy greater than 80%","","0"
"4668","1391277","2450509","06/07/2021 15:02:43","## Context
UFC 263 will see the best middleweights in Israel Adesanya and Marvin Vettori battling it out for UFC Gold. 

## Expected Submission
Your task? Find out interesting stats in the history of the middleweights. Stats could range from records held in the division to how different the middleweights operate from the rest of the roster.

## Evaluation
Interesting stats. For example, in the heavyweight division, Daniel Cormier has gone 6-0 when he lands a minimum of two takedowns in his fights. Wouldn't want to see the usual ""longest winning streak"", ""longest title reign""
Brownie points if you use Plotly to build a dashboard and not just skewered graphs.","","UFC 263: The Battle of the Middleweights","","06/12/2021 23:59:00","0"
"4669","1391277","2450509","06/07/2021 15:10:04","## Task Details
The Ultimate Fighter is the primary reason why the sport of mixed martial arts has evolved and has become a global attraction.

## Expected Submission
An analysis of how the contenders have performed in their UFC careers. Ranging from winners to second-round eliminations who have made their way to the organization, find out how much an Ultimate Fighter Alumni has given back to the company.
P.S. You can find the list of contenders in every season of The Ultimate Fighter by having a text filter on the event name.

## Evaluation
Props for really interesting insights, brownie points for building a Tableau or a Plotly dashboard, and not just interesting graphs.","","The Ultimate Fighter: A Statistical Analysis","","09/15/2021 23:59:00","0"
"3862","1232774","2485486","03/25/2021 17:37:20","## Task Details
For time series with Signals best for feature engineering should be Timeseries

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
With 120 GB of DDR3 this should create a lot of features? Amount of features/time?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Use Timeseries","","03/27/2021 23:59:00","0"
"4801","1414564","2489013","06/17/2021 04:03:30","Time series prediction","","Time series prediction","","01/17/2024 23:59:00","0"
"5042","1454910","2489013","07/08/2021 07:02:08","Create a model using deep learning or classical forecasting methods.","","NIFTY PREDICTION","","12/08/2026 23:59:00","0"
"5044","1455053","2489013","07/08/2021 08:03:32","Use deep learning or classical forecasting methods","","BANKNIFTY PREDICTION","","11/08/2025 23:59:00","0"
"5038","1454707","2489013","07/08/2021 05:08:43","Train a model using deep learning or classical forecasting methods.","","Stock Market Prediction","","10/29/2025 23:59:00","0"
"5039","1454711","2489013","07/08/2021 05:09:12","Train a model using deep learning or classical forecasting methods","","NIFTY Prediction","","06/08/2026 23:59:00","0"
"4901","1427931","2489013","06/23/2021 17:47:54","Train a model","","Stock market prediction","","10/23/2025 23:59:00","1"
"4897","1426870","2489013","06/23/2021 11:34:30","Feel free to use any of the methods","","Stock market prediction","","05/23/2025 23:59:00","2"
"3278","1120523","2490236","01/26/2021 16:47:12","Can we find subset of driver species responsible for autism?","","Determine driver species","","","1"
"2965","1041487","2490236","12/16/2020 18:11:08","## Task Details
Diabetes proved to be difficult to predict based on species abundance features using RF or XGB. Can we find a better model/approach? Reduce number of features further to lower dimensionality? Create some new features? Ensemble different models?

## Expected Submission
Submit a notebook that implements data preparation/augmentation, and model creation and evaluation.

## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).<br>
It seems that AUC doesn't really tell the story in this case, so we need to look at F1 and recall/precision scores to be sure the prediction makes sense.","","Better prediction for diabetes","Find a better approach to predict diabetes","","3"
"2955","1035997","2490236","12/15/2020 17:20:05","## Task Details
Diabetes proved to be difficult to predict based on species abundance features using RF or XGB. Can we find a better model/approach? Reduce number of features further to lower dimensionality? Create some new features? Ensemble different models?

## Expected Submission
Submit a notebook that implements data preparation/augmentation, and model creation and evaluation.

## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).<br>
It seems that AUC doesn't really tell the story in this case, so we need to look at F1 and recall/precision scores to be sure the prediction makes sense.","","Better prediction for diabetes","Find a better approach to predict diabetes","","36"
"3976","1218020","2507257","04/05/2021 21:23:50","## Task Details
Since not everyone may be familiar with how to query data from `SQLite` databases using `Python`, a notebook needs to be created illustrating the possible ways to build a dataset from the database. 

## Expected Submission
The solution for this task should be a notebook that is a mix of `Markdown` and code cells. These cells should explain, with examples, how one may go about querying data from the database. How to create various data structures from the database, such as `Pandas DataFrames`, should be included.

## Evaluation
A good solution is one that optimal in reaching the goal of delivering a helpful notebook for users looking to utilize `SQLite` databases within ***Kaggle***. This means that is should be grammatically correct, have numerous examples from different perspectives and with different tools, and be succinct yet thorough with its explanations. 


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Database Query & Dataset Formation Python Kernel","Create a kernel with examples of how to query the SQLite database data using Python","","1"
"3977","1218020","2507257","04/05/2021 21:25:31","## Task Details
Since not everyone may be familiar with how to query data from `SQLite` databases using `R`, a notebook needs to be created illustrating the possible ways to build a dataset from the database. 

## Expected Submission
The solution for this task should be a notebook that is a mix of `Markdown` and code cells. These cells should explain, with examples, how one may go about querying data from the database. How to create various data structures from the database, such as `Data Frame`, should be included.

## Evaluation
A good solution is one that optimal in reaching the goal of delivering a helpful notebook for users looking to utilize `SQLite` databases within ***Kaggle***. This means that is should be grammatically correct, have numerous examples from different perspectives and with different tools, and be succinct yet thorough with its explanations. 


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Database Query & Dataset Formation R Kernel","Create a kernel with examples of how to query the SQLite database data using R","","6"
"3978","1218020","2507257","04/05/2021 21:37:02","## Task Details
Use a notebook to explore this dataset. This can include statistical measurements, visualization (Univariate, Bivariate, Multivariate), and more!

## Expected Submission
A notebook of mixed `Markdown` and code cells that include your exploratory data analysis on this dataset

## Evaluation
Ensure the submission is grammatically correct, statistically & mathematically sound, robust in its explanations of decisions made, visually appealing, well-annotated, and interesting. This can include, explaining assumptions and chosen models/visualizations/decisions, making sure to label visualizations informatively with axis labels, titles, legends, and annotations, and using dashboards, widgets, and other interactive functionality.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Basketball Exploratory Data Analysis","Apply EDA to the dataset in a cool way","","1"
"4622","1387282","2520701","06/03/2021 20:46:51","## Task Details
The prices for each index are quoted in terms of the national currency of the country each stock exchange is located in. By translating all of the indexes to a common currency, it would be easier to compare them.","","Adjusting Currencies","Convert all prices to one currency","","29"
"3270","1117950","2554646","01/25/2021 12:40:33","‚Ä¢ Download examples of spam and ham.
‚Ä¢ Unzip the datasets and familiarize yourself with the data format.
‚Ä¢ Split the datasets into a training set and a test set.
‚Ä¢ Write a data preparation pipeline to convert each email into a feature vector.
Your preparation pipeline should transform an email into a (sparse) vector that
indicates the presence or absence of each possible word. For example, if all
emails only ever contain four words, ‚ÄúHello,‚Äù ‚Äúhow,‚Äù ‚Äúare,‚Äù ‚Äúyou,‚Äù then the email
‚ÄúHello you Hello Hello you‚Äù would be converted into a vector [1, 0, 0, 1]
(meaning [‚ÄúHello‚Äù is present, ‚Äúhow‚Äù is absent, ‚Äúare‚Äù is absent, ‚Äúyou‚Äù is
present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of
each word.
You may want to add hyperparameters to your preparation pipeline to control
whether or not to strip off email headers, convert each email to lowercase,
remove punctuation, replace all URLs with ‚ÄúURL,‚Äù replace all numbers with
‚ÄúNUMBER,‚Äù or even perform stemming (i.e., trim off word endings; there are
Python libraries available to do this).
Finally, try out several classifiers and see if you can build a great spam classifier,
with both high recall and high precision.","","To make an effective span classifier","","","0"
"4766","1406598","2589790","06/13/2021 16:52:12","## Task Details
Identify the serial killer with the most deranged method of killing his/her victims.","","Most Deranged Serial Killer","","","1"
"3381","1134694","2590287","02/02/2021 18:34:21","## Task Details
Any Economic Research Project for the United States can be supplemented with this data.","","Consumer Price Index 1947 to 2020","Prefect Supplemental Data For Economic Research","","0"
"5557","1036775","2610418","08/05/2021 12:54:14","## Task Details
the task is very simple to predict the next coordinates for the vehicle. if you are using a google map and suddenly your network is gone then your google map stops. how to know where are you go after that your past vehicle history based pattern your prediction show you the way of place. 
 



### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

our is","","road map coordinates","predict next cordinates for the vehical","12/05/2021 23:59:00","0"
"4741","1337505","2613513","06/12/2021 04:12:11","## Task Details
Cleaning Option chain Data

## Expected Submission
Code for Option chain EDA

## Evaluation
- Neat code with proper comments
- Relevant plots

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA OptionChain","","","0"
"3227","1108013","2618090","01/20/2021 09:09:42","The aim is to swap the faces of the images provided in the image directory onto faces of videos provided in the videos directory.","","Task expected to be done","Face swap challenge","02/28/2021 23:59:00","0"
"3624","1185257","2618677","03/01/2021 08:00:42","## Task Details

This dataset is only for getting access to EHR datasets which is difficult to find these days. Task is to convert huge number files into a data frame by 
consuming majority of data and perform EDA.","","Exploratory Data Analysis","Data Transformation of all json into a dataframe","","0"
"2550","941357","2627994","10/27/2020 12:03:17","##Exploratory Data Analysis: 
Some suggestions for what to do with the data:

You can start analyzing this dataset based on factors such as the number of TV shows by category or Home, top 10 best TV shows,botton 10 TV shows, etc.

##Basic questions :

- What TV shows do users like the most?
- Which TV shows have low scores?
- Number of series by gender
- What is the genre that Ororo TV users consume the most?
- What is the HOME with the best score?

##Optional Challenges

- Would you dare to merge this dataset with others in order to compare IMDB ratings (Netflix, IMDB website, and Ororo)?
- What do these top TV shows have in common?
- Why do people like them?
 
##Submission Format

The submission should be a notebook based on any one of the above-mentioned points - either EDA or solve the optional questions.","","What do these top TV shows have in common?","Why do people like them?","12/01/2020 23:59:00","0"
"2505","932930","2627994","10/21/2020 18:09:49","## Task Details

**Exploratory Data Analysis:** Some suggestions for what to do with the data:

- To analyze this dataset based on factors such as the number of orders by the minute or hour, orders by restaurants, orders by category, revenue by delivery, etc.

##Optional questions : 

- Which restaurants need more Rappitender? 
- Where should we lower off ordering out pricing to help the sales commerces?


## Optional Challenges

**Would you dare to cluster restaurants in order to offer discounts?**


##Submission Format

The submission should be a notebook based on any one of the above-mentioned points - either EDA or solve the optional questions.","","Sales Analysis & orders","Which restaurants need more Rappitender?","12/31/2020 23:59:00","0"
"3121","1075882","2643735","01/04/2021 06:08:34","WBCs image classification are currently being used in numerous  practical situations in medicine.
Comparing among different classification approaches using as many as possible evaluation measures.
which one was the relative better than others based on the method's accuracy , time cost and generalization capability etc?","","leukocyte classification","automatic classify WBCs applying computer vision technology","12/31/2021 23:59:00","0"
"2699","972946","2646279","11/14/2020 16:20:25","RNA-Seq is a particular technology-based sequencing technique which uses next-generation sequencing (NGS) to reveal the presence and quantity of RNA in a biological sample, analyzing the continuously changing cellular transcriptome.

https://en.wikipedia.org/wiki/RNA-Seq

We download SRA and FASTQ data from NCBI and prepared 128 RNA-Seq datasets. RNA-seq reads were mapped to the human genome (hg38) with gene annotation. The abundances of individual transcripts were quantified by Salmon. Finally, We applied tximport to create an output file for analysis.

Let‚Äôs find important transcripts in COVID-19 by using machine learning.","","COVID_RNA_Seq","Let‚Äôs find important transcripts in COVID-19 by using machine learning","","0"
"4420","1346596","2646597","05/17/2021 14:23:53","Classify the feeling according to the text.","","Sentiment Analysis","Portuguese Sentiment Analysis","","0"
"3129","1077665","2669041","01/04/2021 19:08:08","## Task Details
The task is to find the most frequently used words. 

## Expected Submission
The Submission can be represented as a list of Top K words across different ratings and overall or in the form of Word Cloud.","","Analyse Frequently used Words","","","0"
"3130","1077665","2669041","01/04/2021 19:11:25","## Task Details
The Task is to predict the rating given by a customer corresponding to the review. 

## Expected Submission
The Submission must consist of Mean Accuracy over 10 Folds of data.","","Prediction of Rating","Classification","","0"
"2788","973252","2670233","11/27/2020 03:25:49","## Task Details
A variational autoencoder (VAE) is a generative model that maps input data to a latent distributional representation. Then, it can generate output data from the latent representation which matches the input, hence the ""autoencoder"" ability. Your task is to build such a VAE on this dataset of Pokemon images.

## Expected Submission
Submit a trained model and the accompanying notebook required to load the model, load the dataset, and evaluate the model's reconstruction error on that dataset.

## Evaluation
Your model will be evaluated on its reconstruction error on a withheld set of Pokemon images. 

### Further help
If you would like help implementing a VAE, check out this Tensorflow tutorial: https://www.tensorflow.org/tutorials/generative/cvae.","","Pokemon VAE","A variational autoencoder for Pokemon","","1"
"2771","973252","2670233","11/25/2020 03:51:04","## Task Details
Given the images of these pokemon in their corresponding folders, wouldn't it be nice to train a model that can identify which pokemon it is just based on this?

## Expected Submission
Submit a trained, saved model and the accompanying Notebook that can load that model and evaluate its performance. A withheld dataset of Pokemon images will be used to evaluate the performance of the model.

## Evaluation
Solutions will be judged based on their classification accuracy on the withheld dataset.

### Further help
This is an almost impossible task to perform very well on, given how few images there are for each Pokemon. Nonetheless, I hope this gives you something to practice with in implementing a machine learning model on images!","","Pokemon Classifier","Apply supervised learning towards labeling Pokemon based on their images.","","1"
"3828","981722","2675197","03/21/2021 11:59:05","## Task Details
This is a task I want to explore myself. I want to know if taking a smaller sample size from the total observation (e.g., 2 blocks of 50% of the total number, 5 blocks of 20% of the observations) changes considerably the number of clusters found in the total dataset (which is 3).

## Expected Submission
A plot of the percentage of the observation (i.e., 10%, 20%, until 100%) in the x-axis versus the number of optimal clusters found.","","Subsampling effect on the number of optimal clusters","Does the optimal number of cluster changes when taking smaller samples?","","0"
"2577","946482","2676226","10/29/2020 22:08:25","Due to the nature of accelerometers,  they must be filtered inorder to get usable results.  This filter may take the place in the form of some sort of moving average filter, or something more complicated such as a kalman filter.  Once you have the filtered acceleration data you can derive the position estimate by integrating the acceleration twice.","","Develop filter and integration algorithm to derive position from acceleration","","","1"
"2915","1030856","2679105","12/12/2020 12:29:45","Let's change the style of the map using GAN.","","CycleGAN task","Change the map style","12/31/2030 23:59:00","0"
"4545","1366867","5794402","05/29/2021 10:28:39","## Task Details
Create a face recognition model that compares your face features to the data set and gives you back which anime character looks like you the most.","","What anime character are you?","","","0"
"3892","1237363","2679105","03/28/2021 14:12:47","Let's discover bias using various dimensionality reduction techniques on emoji.

ex) 

- [Bokeh : Emoji tsne](https://holoviews.org/gallery/demos/bokeh/emoji_tsne.html)
- [Latent Space Cartography: Visual Analysis of Vector Space Embeddings](https://yangliu.life/build/misc/lsc.pdf)","","Dimension Reduction using Emoji","Can you find bias in emoji?","01/04/2025 23:59:00","1"
"3213","1102778","2680939","01/17/2021 17:24:53","Thank you!","","Graph it","Use matplotlib / Seaborne to plot it","","0"
"3658","1192712","2680939","03/04/2021 16:15:04","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
Thanks
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Make Correlation","","","0"
"3716","1201877","2681031","03/09/2021 19:01:15","## Task Details
Create a submission to detect the different chess piece and put then in 5 different classes","","Chess Piece Dectection","","","2"
"3620","1185637","2681031","02/28/2021 23:10:02","Detect the people wearing helmets and the ones who don't.
Best looking submission will get pick.","","Bike Helmet Detection","","","1"
"3924","1243688","2681031","03/31/2021 14:30:04","## Task Details
Take the data from this dataset and analyze the data","","Analyze","","","0"
"3975","1252816","2681031","04/05/2021 20:15:47","## Task Details
Use the data from the dataset to determine what year or what genre makes the most money.","","Classify the data","","","1"
"4367","1316421","2681031","05/13/2021 15:30:37","## Task Details
Analysis the dataset and find which oil is the best choice for eating purposes.","","Find which Oil is the Best One","","","0"
"2624","953973","2685252","11/03/2020 20:15:03","Explore environmental and social factors in Berlin, Germany","","Environmental Justice","","","1"
"3044","1059449","2693690","12/26/2020 05:51:43","## Task Details
The task is to find out the match percentage between each user and create an ML model that will suggest a new user about their top 10 match preference.

## Expected Submission
The submission file is required to be in a matrix format. For example, if the number of users in the dataset provided is 1000, then the submission.csv file must contain a matrix of size 1000x1000.  
You can refer to the 'sample submission.csv' file for the sample dataset provided in the dataset folder.
Note: Ensure that 'user_id' of the users is mentioned correctly in the submission.csv file.

***Evaluation metric***
The evaluation metric that is used is the root mean square error metric. The score is calculated using the following:
score = max(0, 100-root_mean_squared_error(actual,predicted))

## Evaluation
The closer the score to 100 the better the model is.
You can evaluate your score by choose a file and then click on the submit & evaluate button. (https://www.hackerearth.com/problem/machine-learning/predict-the-match-percentage-25-818cf487/)","","Predict the match percentage","Please write your approach in the first cell of the notebook you submit.","","0"
"3236","1109015","2702372","01/21/2021 12:07:58","Has Covid impacted the Express Entry Process?","","Covid and Express Entry","","","0"
"2436","920599","2723137","10/14/2020 09:33:11","## Task Details
Create a classifier that classifies a given image into the pose that is being performed in it.

## Expected Submission
Notebooks/Repositories with scripts and an example with correct classification of the pose.","","Yoga Pose Image Classification","Detect and classify the pose being performed in the image.","","1"
"4865","1421204","2727996","06/20/2021 17:08:49","Participants are suppose to do analyzing of data which is of COVID 19 of INDIA.","","Analyzing data","","04/20/2022 23:59:00","0"
"3214","1102816","2736298","01/17/2021 17:46:45","## Task Details
Predict the future closing price of the wheat commodity in the stock market.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Future price prediction of Wheat Commodity","","","0"
"4463","1357368","2738235","05/23/2021 03:49:30","A major drawback of the microstrip patch antenna is its narrow bandwidth and less return loss. The design of the antenna can be improved using neural networks. As per recent studies, not enough work has been done regarding this field using machine learning. Some basic ANN algorithms have been already used like feedforward backpropagation. Studies related to decision trees and random forest has still not been done yet rigorously. So looking forward to some ideas for the scope of improvement.","","Improve the S11 (return loss) and bandwidth using neural networks.","","","0"
"3305","1126322","2739275","01/29/2021 14:28:43","## Task Details
Subreddit classification

## Evaluation
Accuracy","","Classify subreddits","","","0"
"3306","1126322","2739275","01/29/2021 14:30:10","## Task Details
Predict the number of upvotes

## Evaluation
RMSE","","Number of upvotes prediction","","","0"
"3274","1119614","2748867","01/26/2021 08:30:10","## Task Details
Perform different regression techniques using this datasetüòÑ","","Regression practice","","","0"
"2914","1030364","2749991","12/12/2020 07:29:46","Use this dataset to create face mask detection models. And use it to predict whether people are wearing mask or not either real time or via images or both.","","Creating a face mask detection model","","","6"
"3460","1151114","2770804","02/10/2021 20:45:31","## Task Details
Finding that one perfect fragrance for a special occasion can be very a difficult task for a novice. 
Using the perfume description & fragrance notes present in the dataset, create a content-based recommendation system for the users in search of that ""perfect"" fragrance for themselves.","","Content-based Recommendation System","","","0"
"3866","1233703","2773025","03/26/2021 07:43:54","## Task Details
Build a RBM using this dataset to predict whether a particular user will like a movie or not. It is kind of building a recommendation system for recommending movies to users based on their previous watch history. This dataset is different from others because it is larger in size and thus more data is available for the model to be trained.","","Build a RBM using this dataset","","","2"
"3983","1254539","2781854","04/06/2021 14:59:36","Predict loan defaulters based on the data set of employment status, loan amount and bank balance","","Predict loan defaulters","","","3"
"4239","1308671","2781854","05/01/2021 09:30:05","Check which factors lead to attrition","","Analyse the reason for attrition","","","4"
"2982","1045835","2781854","12/18/2020 22:44:04","## Task Details
Look at the data and find out if there is a gender pay gap 
a) in the company as a whole
b) for some departments","","Gender pay gap","","","0"
"3056","1061915","2796256","12/27/2020 16:41:09","## Task Details
Extract all .yml files and convert them to a single csv file for each .zip file

## Expected Submission
csv file for each zip file

## Evaluation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Unzip and Convert data to csv","","12/27/2021 23:59:00","0"
"2970","1042140","2796818","12/17/2020 04:16:21","need to develop and implement an algorithm to detect clouds in INSAT satellite images and predict the location of clouds in subsequent images","","Detecting clouds and predicting their movement from INSAT imagery.","","","0"
"2971","1042175","2796818","12/17/2020 04:32:45","Need to develop algorithm for automatically classifying vegetation areas as either farm or tree/plantation by analysing seasonal changes apparent in multi-temporal satellite imagery.","","Automated mapping of trees/plantation and farmland","Design and Implement algorithm for automated mapping of trees/plantation and farmland from 5m multispectral multi-temporal (LISS IV) data.","","0"
"2827","1008573","2809468","11/30/2020 22:37:38","## Task Details
Visibility on the road is determined using the dataset so as to get the clear directions of choosing the roads for driving or not.

## Expected Submission
The solution contains a machine learning model determining the visibility accuracy and precision on the following dataset.

## Evaluation
A good solution has more precision and accuracy with less errors.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Can you test the visibility on the roads?","Visibility on the roads are tested due to foggy weather.","","0"
"2596","948856","2831853","10/31/2020 13:25:15","## Task Details
Time is money, and as a business and a customer, both want to minimse time wasted by only recommending the most relevant products.

## Expected Submission
The solution should be able to predict accurately the relevance of each product to each customer

## Evaluation
How to minimise RMSE when predicting relevance of each product to the customer","","Product Recommendation","Recommend the right products to each user","","1"
"3563","1172808","2865337","02/21/2021 12:47:01","Create ML based classifier from the given dataset with best prediction accuracy per class.","","Emotion Classifier","","12/31/2021 23:59:00","0"
"4408","1344722","2875991","05/16/2021 18:47:08","## Task Details
Use unsupervised anomaly detection methods to detect anomalies (""o"" in the target attribute Outlier_label). 

## Expected Submission
Whatever you like 

## Evaluation
Precision, Recall, F1-score, ROC AUC

### Further help","","Unsupervised Anomaly Detection","","","0"
"4096","1268650","2884225","04/17/2021 06:38:08","## Task Details
The dataset contains *Cooking Time* and *Preparation Time* for most of the recipes. Can you predict the same for the recipes with missing information ?

## Expected Submission
Create a notebook that contains an ML algorithm that takes into consideration - the ingredients for each recipe, predicts the cooking and preparation time on the basis of available data, and then fills up the missing values in the Cooking Time and Preparation Time columns.

## Evaluation
Let's see the accuracy of your algorithm at different randomizations.","","Predict Cooking Time and Preparation Time based on Ingredients","","","0"
"4312","1165452","6776149","05/09/2021 15:07:33","Yo, classify different fish on this big dataset, I know you can do it, leave your notebooks here","","Classification fish","","","16"
"4708","1165452","2916096","06/09/2021 17:54:01","## Task Details
Hello guys! I would like to thank all who are interested in our dataset. I want to give you an example task to use this dataset efficiently. 

The first step of food and beverage quality assessment depends on the classification of the samples. Before classifying the food types, it is usually difficult to detect spoilage or diseases, since the same illnesses can cause different symptoms in diverse food types. Therefore, it is crucial to identify the food type before detecting the diseases or any early spoilage. 
 
In this task, you need to design an SVM classifier to compare 3 different approaches to see & decide if there is any benefit of the segmentation and feature extraction in these classification problems.   
1. Classify images without any segmentation and feature extraction via SVMs.
2. Classify segmented images without feature extraction via SVMs.
3. Classify segmented images by extracting features for your SVMs.  

You are free to choose the features you would like to use in this task.

## Evaluation
The best submission will be selected by checking the segmentation accuracy of your model, computational complexity, and classification accuracy of the system. 

Good luck & Have fun!
Be safe!","","The effects of feature extraction & segmentation in the classification tasks","","","2"
"4737","1403328","2917554","06/11/2021 18:12:42",".","","Predict the salary of an individual based on their education and salary","","","0"
"3569","1173037","2921334","02/21/2021 15:31:45","## Task Details

Comparing the rmse between Real and Prevista the rmse is 1.27. However, they have used more variables as climatologic situation, economic situation, employment situation, etc

How about study the Real variable using just the before information?



## Evaluation

_rmse_ between your model and the **Real** variable","","How close can you be using just the Real variable?","Time Series analysis to the Real variable","","0"
"2378","911576","2925793","10/08/2020 11:16:29","## Task Details
Explorar o dataset e remover os valores nulos

## Expected Submission
Submter um dataset pronto para ser utilizado em an√°lises estat√≠sticas","","Dataset cleaning","","","0"
"2385","911576","2925793","10/08/2020 19:47:03","1. Plotar histogramas das temperaturas
2. Quais s√£o as temperaturas m√°xima e m√≠nima  e seus respectivos dias
3. Historicamente, qual m√™s faz mais calor (isto √© diferente de qual m√™s √© mais quente).
4. As temperaturas m√°ximas tem algum ciclo/padr√£o?","","Estatisticas das Temperaturas","","","0"
"2824","981983","2926451","11/30/2020 13:57:28","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","First Task","First Task","12/02/2020 23:59:00","0"
"4177","1162620","6985605","04/25/2021 01:05:47","~80% accuracy","","Dandelion Image Classification (Transfer Learning)","Pretrain_model","04/27/2021 23:59:00","1"
"4563","1377020","2941522","05/30/2021 14:40:12","Dear analysis,  I am not able to find accuracy getting some ValueError.resolve my mistake.","","Find Accuracy","linear regression only","06/05/2021 23:59:00","0"
"2510","934209","2956995","10/22/2020 12:57:56","## Task Details
Most recent works in Economics, in particular Thomas Piketty's Capital In The 21st Century, argue that investment in education and specialization of workers yields more results to economic and social development than the so-called free-market and foreign investments intake in emerging countries. Can we extract some insights from this datasets that back up this argument? Even better, what ideas can we take from this dataset that increment in the discussion of better social settings in emerging economies?

## Expected Submission
Users can submit any analysis they contribute to. It's expected to address at least the relations between government expenditure on education and economic development (or social development). Users are free to increment their analysis with whatever information they find relevant.

## Evaluation
Users can contribute to each other by evaluating other's works, by including the usage of following criteria (but not limited to them):
- Good insights from data;
- Good visualization of data;
- Novel methods of analysis;
- Increment to economic and social discussions;

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Education & Development of BRICS countries","How government expenditure can improve or worsen employment, inequality and other social factors in the near-future?","","12"
"4677","1395976","2967341","06/08/2021 08:12:45","## Task Details
Task is to predict the HasDetections for Test dataset using Train dataset. 

## Expected Submission
Try to use VAEX for EDA","","EDA using VAEX","","","1"
"5312","1117292","2992577","07/24/2021 19:47:23","Explore the population data from largest to smallest administrative unit and compare within each province.","","Exploratory Data Analysis","","","0"
"5313","1117292","2992577","07/24/2021 19:49:34","Join this data with available shapefiles of Pakistani administrative boundaries and display the distribution of population at different administrative levels.","","Map the population data","","","0"
"4129","1277480","4269942","04/20/2021 02:43:31","Lorem ipsum
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Segmind","","","1"
"3539","1168231","3001434","02/18/2021 19:54:38","## Task Details
Visualize the data and discover the patterns. 

## Expected Submission
Try to visualise every variable and discover the reasons for any abnormalities.

## Evaluation
If you're proud of your creation, then it's the best.","","Data Visualization","","","1"
"2539","931339","3023333","10/25/2020 15:12:05","## Task Details


## Expected Submission
You should submit your model with the accuracy

## Evaluation
the accuracy","","Do a Classification of different drawings!","","","0"
"3243","1111924","3032581","01/22/2021 08:40:30","## Detail
Maret 2020 merupakan salah satu bulan paling buruk di sejarah pasar modal di Indonesia. Pada bulan tersebut, IHSG sempat turun ke harga 3900an sebelum kembali naik pada bulan-bulan berikutnya.

## Submisi
Pada task kali ini, Anda diminta untuk dapat mensortir saham-saham yang paling cuan mulai dari yang paling cuan sampai yang kurang cuan. Anda juga boleh menambahkan analisa-analisa kenapa saham-saham tersebut lebih cuan daripada saham-saham lainnya.

Akhir kata, selama ngulik dan salam cuan!","","Saham Paling Cuan","Cari tau saham paling cuan sejak Maret 2020","","1"
"3215","1093321","3034187","01/17/2021 18:09:59","There are three models in this data set namely desc, gen and cgan
train these further, generate better anime images.
Thanks","","Train DCGan","Train DC gan","","0"
"3174","1093321","3034187","01/12/2021 11:56:58","I am planning to add all my auto-encoder models here. 
Your task is to make something using these models.
for instance try video Processing, try image segmentation, etc.
No limit on creativity. 

Kindly use model.summary to know about the input/output tensor shapes.
Have fun","","Create some thing interesting using the models provided here. But do Enjoy Yourself.","Make something Interesting using the given models","","0"
"2872","1018159","3068011","12/06/2020 09:41:52","Nearly all research papers have at least one [DOI (Digital Object Identifier)](https://en.wikipedia.org/wiki/Digital_object_identifier) that uniquely identifies it.
  - DOIs have the following format:
    `10.DDD/...`
    where D is a digit. Beware that DOIs are case-insensitive.
      - The part before the (first) forward slash (`10.DDD`) is called the prefix.
      - The part after the (first) forward slash (`...`) is called the suffix.
      - The prefix identifies a ""publisher"", loosely speaking.
- Find the distribution of papers by publishers.","","Find the Distribution of Papers by Publishers","","","0"
"4145","1247884","3082677","04/21/2021 17:15:34","## Task Details
Use your dataset from State Farm Distraction Driver Detection and classify what the driver is doing in the video.
Take your model to a higher level.

## Expected Submission
No submission

## Evaluation
No Evaluation

### Further help
Mail me:- kunalrawat2000@gmail.com","","State Farm Distracted Driver Model Predictor","","","0"
"4325","1247950","3082677","05/11/2021 09:19:43","## Task Details
Try to build a classifier for classifying images of these actors, this dataset is for beginners to deep dive into the field of computer vision
It is a small dataset to kick start your journey","","Actor Classifier","","","0"
"4021","1261459","3084406","04/09/2021 21:55:23","## Task Details
The PNG file is further bzip2-compressed to avoid abuse by people hot-linking it in img elements. The bzip2 compression doesn‚Äôt have anything to do with the high compression ratio in the PNG.","","Learn to read File","","04/17/2021 23:59:00","0"
"2964","1040851","3141733","12/16/2020 11:18:18","## Task story
 this task essentially was given by a company Genius AI Solution. whoever is trying to get into the Machine Learning field can increase the chances of getting a job by doing these tasks.

## Task Objective :
* [ ] Perform EDA on the given [data].
* [ ] Build some new features which makes sense for the learning objective.
* [ ] Train any classifier (can be any model of your choice XGB,DNNs, etc) 
* [ ] Implement SHAP on this model to explain the decisions. (`Bonus points`) 


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict weather customer will stay or leave","","","0"
"2545","941574","3143779","10/27/2020 03:37:23","DATA DESCRIPTION:-

1- land_area : size in square miles

2-percent_city : percent of population in central city/cities

3-percent_senior : percent of population ‚â§ 65 years

4-physicians : number of professionally active physicians

5-hospital_beds : total number of hospital beds

6-graduates : percent of adults that finished high school

7-work_force : number of persons in work force in thousands

8-income : total income in 1976 in millions of dollars

9-crime_rate: Ratio of number of serious crimes by total population

10-region: geographic region according to US Census","","PROBLEM OBJECTIVE IS TO FIND THE CRIME RATE OF EACH AREA","","","1"
"3528","969830","3148426","02/17/2021 19:43:31","## Task Details
There are 9 files in the dataset each containing data from 2009 - 2019.

## Evaluation
Submit a notebook that predicts which companies will make it to the top ten in the year 2020.","","Predict Top 10 Companies in 2020","","","0"
"3731","1203832","3148426","03/11/2021 14:31:28","## Task Details
The arrival of OTT platforms has changed the way of entertainment. Your favorite TV shows and movies are one click away. There are millions of users using Amazon Prime Video. Analyze the quality of the movies streaming on the platform. 

## Expected Submission
Submit a notebook that analyzes the quality of the movies based on the rating it received.","","Quality of the movies","","","0"
"4044","1153255","3157178","04/12/2021 01:18:35","## Task Details
This is the most obvious task - predicting housing prices

## Expected Submission
Submit a notebook that builds a model to predict home prices utilizing images, and features from the dataset.","","Predict Housing Price","Utilize features and images","","3"
"4330","1334639","3160429","05/11/2021 17:33:56","Need to:
- Convert txt data to csv
- Clean data possibly, remove hyperlinks and other things which are not useful.
- Add the other columns such as keywords
- Add target column.","","Convert data into competition format","Data Cleaning and changing file format from txt to csv","","0"
"2790","999785","3161648","11/27/2020 03:35:55","Champagne Papi has dominated the Hip-Hop & R&B world for over a decade now. He's known for his infectious hits that start out as polarizing classifiers. Over the next few months or even years though, everyone will end up catching themselves at one point or another reciting the lyrics word for word without even realizing it. Lyrics are only a fraction of what makes a song but they are significant. In this task, explore what it is about the CEO of OVO's lyrics that has this effect on people.","","EDA on Drake Lyrics","Explore the lyrics of the 6 God himself","","1"
"3081","1068089","3166141","12/31/2020 12:32:56","## Task Details
Create visuals that can show population change for each community of Dubai. This visual should easily show the trend for each community. 

## Expected Submission
Users can submit notebook which can shows various options of visualization.

## Evaluation
Ease in finding population growth/de-growth for the communities of Dubai","","Visualization of population changes of each community of Dubai.","","","1"
"4443","1353321","3174210","05/19/2021 15:49:34","## Task Details
The purpose of this task is to build a computer vision model that is capable of detecting whether the person has put on his/her face mask or has the face mask put halfway.

## Expected Submission
Users are expected to submit their own notebooks, where they build their own models can perform well on identifying whether each image or video frame belongs to the following classes -&gt; ""with_mask"", ""without_mask"", ""mask_weared_incorrect"".

## Evaluation
So what makes a good submission? Having built a model, the model should be about to recognize whether the person belongs to the classes mentioned above. In other words, testing the model on unseen data.

GOOD LUCK üëç","","Build a Face Mask Classifier","","","2"
"3238","1107054","3177784","01/21/2021 15:07:09","There are lots of fun inside this kind of event-based data. Please feel free to discover...","","Transferring event-based data into frames/images/videos","Image/Video Analysis","","1"
"6853","1056391","4561355","11/16/2021 08:18:35","## Task Details
1. Clean reviews.
2. Convert reviews using TF-IDF
3. Train and predict classifiers","","Predicting PlayStore rating based on reviews","","","0"
"3047","1060065","3206435","12/26/2020 13:01:18","## Task Details
this small data set was collected from Bank that contain information about customer that have applied for a credit card the task is to build Recommendation engine that recommend credit card types for new customer 

## Expected Submission
the Recommendation engine should be build using Python

type_credit contain 3 distinct value 
0 for people who got rejected 
1 for people that got normal credit  card
2 for people that got the gold credit card","","Recommendation engine","","02/01/2021 23:59:00","0"
"5208","1416175","3238438","07/19/2021 07:43:23","## Task Details
A good benchmark metrics is the key to success for solving a machine learning task. The metric should reflect every aspect of the task objective. For instance, PR-AUC is a good metric for binary classification tasks since it covers both the precision and recall properties of a good classifier. 

Unfortunately, current popular benchmark metrics for image generative modeling are less than ideal. The inherent difficulty of evaluating the similarity between the generated distribution and unknown training distribution (with only finite training samples as proxy) poses a great challenge to metric design. To make matters worse, there have been no good method to fairly benchmark a metric so comparison between metrics are currently not possible.

In this dataset, users can benchmark a newly designed metric with the large amount of image instances sampled from diverse generative models plus human image quality annotations. Furthermore, the labels for intentional memorization technique can serve as a sanity check for whether the metric rewards training sample memorization.

## Expected Submission
Users should submit a notebook with a novel generative modeling metric and evaluate the generated instance provided in this dataset on the metric.

## Evaluation
A good generative modeling metric should produce metric scores that are at least
1. aligned with human perception
2. sensitive to training sample memorization (either ignore or penalize memorized instances)
Other properties such as distribution diversity (mode collapse detection) and evaluation efficiency are also ideal but may require external resources beyond the scope of this dataset to verify.","","Benchmarking Image Generative Modeling Metrics","A large-scale benchmark to evaluate novel image generative modeling metrics.","","0"
"2721","955555","1284499","11/18/2020 13:35:59","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

awesome","","abc nfds","abc","11/19/2020 23:59:00","0"
"3452","955555","1912150","02/09/2021 20:39:16","One thing that would be great is to also have the fips code associated with each county as this can be used for mapping geographical plotting","","Add State/County FIPS","","","1"
"3371","1132203","3244747","02/01/2021 16:03:12","Who started the GME high stock? Had some else momment like this?","","Who started the GME high stock? Had some else momment like this?","","","1"
"3927","1245169","3252339","04/01/2021 11:57:46","###Task Details
Try and find the highly demanded in the transfers

###Expected Submission
You're expected to submit a visualization of your findings

###Evaluation
If your visualization says something‚Ä¶ submit it
And also check out my notebook on the IPL auction

###Further help
If you need additional inspiration, check out these existing high-quality tasks:

https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysis of Highly Demanded players(age,position, etc.)","","","0"
"3928","1245573","3257097","04/01/2021 13:37:17","## Task Details
This dataset represents a subnetwork of public transportation in the city of Johannesburg. It contains counting of bus occupation of three significant routes as well as GPS location of Bus stations.

### Further help
https://ieee-dataport.org/open-access/bus-network-and-occupation-survey-johannesburg#files","","BUS NETWORK AND OCCUPATION SURVEY IN JOHANNESBURG","TRANSPORTATION; BUS OCCUPANCY","09/30/2021 23:59:00","0"
"3835","1226484","3257097","03/22/2021 12:56:00","## Task Details
 A dataset of steel plates faults, classified into 7 different types. The goal was to train machine learning for automatic pattern recognition.","","Steel Plates Faults","Multivariate Classification","08/25/2021 23:59:00","2"
"4437","1352355","3257097","05/19/2021 08:03:43","## Task Details
The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries.

## Expected Submission
- What is the average price of each distinct brand listed?
- Which brands have the highest prices?
- Which ones have the widest distribution of prices?
- Is there a typical price distribution (e.g., normal) across brands or within specific brands?
- Correlate specific product features with changes in price.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
- https://data.world/datafiniti/mens-shoe-prices","","Men's Shoe Prices","Shoe Prices","","1"
"4442","1353057","3257097","05/19/2021 14:04:42","## Task Details
Get to build a model to predict the doctor's consulting fee

## Expected Submission
Submission to be in notebook.

## Evaluation
1 - np.sqrt(np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean())

### Further help
- https://machinehack.com/hackathons/predict_a_doctor_s_consultation_fee_hackathon/overview","","Predict A Doctor's Consultation Fee Hackathon","Consulatation Fee","","1"
"3653","1180681","3258357","03/04/2021 02:24:53","## Task Details
The outbreak of Covid-19 is developing into a major international crisis, and it's starting to influence important aspects of daily life. For example:

Travel: Bans have been placed on hotspot countries, corporate travel has been reduced, and flight fares have dropped.
Supply chains: International manufacturing operations have often had to throttle back production and many goods solely produced in China have been halted altogether.
Grocery stores: In highly affected areas, people are starting to stock up on essential goods.
A strong model that predicts how the vaccines help in recovering from COVID-19 once it came in market. What's the trend of new cases and existing cases.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

The notebook should be well documented and contain:

Any steps you're taking to prepare the data, including references to external data sources
Training of your model
The table mentioned above
An evaluation of your table against the real data. Let's keep it simple and measure Mean Absolute Error.

## Evaluation
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).

At the end of March, we'll look at the top 5 upvoted submissions and holistically award the best solution. We‚Äôll announce and recognize March‚Äôs Kaggle Tasks Award Winner on our Twitter account at the beginning of April, sharing an official certificate of accomplishment.

Our team will be looking at:

Accuracy - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?
Data Preparation - How well was the data analyzed prior to feeding it into the model? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative, thought-provoking, and fresh all at the same time.
Documentation - Are your code and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high-quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.
This task is not strictly about getting the best submission score. Since it involves data that changes in real-time, the most accurate solution would only require uploading the real historic data on the last day of March, but that‚Äôs beside the spirit of this Task. We‚Äôre looking for genuine approaches to building models on a real problem that can serve as learning examples for our community","","Vaccinations availability","","","0"
"4313","1329112","3264446","05/09/2021 16:20:25","Answer to questions like:

- What state is vaccinated more people?
- What states is vaccinated a larger percent from its population?
- Is there any trend in numbers of vaccination?","","Investigate the progress of US COVID-19 vaccination","","","1"
"4314","1329112","3264446","05/09/2021 16:30:19","By using this [covid-19 dataset](https://covid.cdc.gov/covid-data-tracker/#cases_casesper100klast7days), forecast cases and deaths based on vaccination numbers.

- Explore the data(EDA)
- Find correlations
- Make prediction for next 30 days.","","Forecast Cases and Deaths based on Vaccination Numbers","","","2"
"4048","1266025","3264446","04/12/2021 06:08:08","## Task Details

In this three-dimensional world (store, product, date), the question is, can we find smart ways to quantify these dimensions.

## Objective

To develop a dimensionality reduction module that provides three features for three main dimensions (product_feature, store_feature, time_feature) which will can used in KNN Regression algorithm.","","Dimensionality Reduction","","","0"
"5605","1296010","3264446","08/08/2021 15:09:58","Create an EDA and show trends of cases.","","Create an EDA","","","1"
"4765","1406612","3264446","06/13/2021 16:30:58","Task is making an EDA to understand the data.

- Show and explain quarterly changes.
- Merge dataset with population and check if there is significant difference.
- Is there picks or downs?","","Exploratory Data Analysis","Make an EDA to understand the data","","0"
"3072","1066161","3265178","12/29/2020 19:18:53","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
visualizations

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA for insights into the reviews","","","0"
"3073","1066161","3265178","12/29/2020 19:19:23","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
NLP 

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NLP, Sentiment analysis","","","0"
"2950","1037063","3282305","12/15/2020 14:36:23","You could use consumption data for residential building located in Austin in combination with this dataset to predict the consumption of your HVAC system for tomorrow or for the next week.","","Energy consumption prediction","Use the weather data to predict the amount of energy consumption of your HVAC system","","1"
"3919","1242882","3283600","03/31/2021 07:22:03","Hello All,

If you worked on this dataset and decided to use in a paper, please cite the following paper:

[A histopathological image dataset for grading breast invasive ductal carcinomas](https://www.sciencedirect.com/science/article/pii/S2352914820300757)

I suggest you to run a classification algorithm on this dataset. If the precision was high it is a new contribution.

For any question, feel free to contact me:

bolhasani@databiox.com

Good luck,
Hamidreza Bolhasani","","Reference of the Dataset and a suggestion","","","1"
"2458","925632","3290492","10/17/2020 09:26:19","## Evaluation
Submissions are evaluated on micro F1 Score between the predicted and observed tags for each article in the test set","","micro F1 Score","","","0"
"3187","1095933","3294616","01/13/2021 20:33:00","1. Use the dataset for image captioning task harnessing the power of TPUs
2. See the difference in results by using only a single set and by merging all sets together.","","Image Captioning on TPU","","","1"
"4268","1317190","3298875","05/04/2021 20:45:27","## Task Details
Dataset is about the placement of class students and the task is to estimate the each student is going to be hired or not. 

## Expected Submission
User has to submit Notebooks with showing the accuracy of used ML algorithms. Any model evaluation method can be used. 

## Evaluation
Good solution is always evaluated by observing how the dataset is analyzed, how ML pipelines are used and most important is how much accuracy of your model is. Submission will be evaluated not only by the accuracy of ML model but also the way the dataset is analyzed.","","Prediction Of Students Placement","Predict Whether Student Is Going To Be Hired Or Not Using ML Algorithms","","1"
"2542","938452","3302541","10/26/2020 04:16:02","## Task Details
As of October 2020, this data set contains nearly 226029 Airbnb listings in U.S. The purpose of this task is to predict the price of U.S. Airbnb rentals based on the data provided and any external dataset(s) with relevant information.

Expected Submission
Users should submit a CSV file with each listing from the data set and the model-predicted price:

```
id, price
49091, 83
50646, 81
56334, 69
...
```

Evaluation
A solution with low root-mean-squared error (RMSE) based on cross-validation that can be reproduced and interpreted is ideal.","","Predict U.S. Airbnb Prices","","","21"
"2618","952827","3302541","11/03/2020 06:19:48","## Task Details
As of November 2020, this data set contains nearly 4500 images of different fruits and vegetables, the task here is to build a model which classifies new fruits and vegetables correctly.

## Evaluation
Evaluation can be carried out by you yourself using accuracy score, roc auc or any other metric","","Fruit and Vegetable Classification","","","4"
"2728","982742","3304759","11/19/2020 05:43:40","This task focuses on the detection of COVID19-related fake news in English. The sources of data are various social-media platforms such as Twitter, Facebook, Instagram, etc. Given a social media post, the objective of the shared task is to classify it into either fake or real news.","","Detection of COVID19-related fake news in English","","","1"
"2924","1032172","3309826","12/13/2020 08:01:13","To analyze and compare game data of different users","","Analyze and compare game data","","","1"
"4217","1302783","3309826","04/28/2021 17:22:19","Understanding market fairness and building a Fairness Aware Framework","","Building a Fairness Aware Framework","","","1"
"4363","1338117","3309826","05/13/2021 11:08:04","NLP analysis","","NLP analysis","","","0"
"4423","1346629","3309826","05/17/2021 16:32:43","Analysis of Spectrogram Images","","Analysis of Spectrogram Images","","","1"
"4431","1348548","3309826","05/18/2021 11:56:37","Data Augmentation Analysis","","Data Augmentation Analysis","","","0"
"3895","1238934","3309826","03/29/2021 08:02:23","From an analysis point of view, one can track the changes in trends over time.","","Change in trends over time","","","0"
"4084","1274125","3309826","04/15/2021 19:09:27","## Task Details
Exploratory Data Analysis of tweets and nature of reporting by different news accounts","","Exploratory Data Analysis of tweets","","","1"
"4076","1272765","3309826","04/15/2021 06:27:05","## Task Details
To capture trends in the Data Science domain over time","","Trends over time in Data Science","","","1"
"4421","1346788","3326878","05/17/2021 15:57:36","## Task Details
Predict the salary of the people.

## Evaluation
The answers of the predicted output is available in the test dataset


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict salary of people","Predict the salary of the people based on the training dataset.","","0"
"3797","1216519","3326878","03/17/2021 08:04:53","## Task Details
You can analyse the data and create custom models according to the vocals and music part of songs.","","Analyze the audio data","","","0"
"5679","1084909","4656934","08/11/2021 20:02:58","Predict whether account is fake or real","","Predict the account type","","","0"
"3112","1073446","3340197","01/03/2021 10:25:37","## Task Details
Here you can share your recommender system for online evaluation. We already know offline evolution can be different with reality.

## Expected Submission
We expect URL for your recommender system.

## Evaluation
Every user can share an application for evaluation and ask users to complete.","","Share your recommender systems","share url of your recommender","","1"
"3461","1151301","3343850","02/11/2021 00:50:12","## Task Details
How does the situation of a country affects the athlete's performance?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Economic and Social situation and athlete performance","","","2"
"3950","1184281","3350264","04/04/2021 09:58:22","## Task Details
Classify each malware's family.

## Expected Submission
Classify each malware in the family of 0-9.

## Evaluation
Evaluation is on AUC-ROC score.","","Classify families of each malware","","","0"
"4595","1381965","3352334","06/01/2021 11:42:04","**Assignment-based Subjective Questions**
1. From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable? 
2. Why is it important to use drop_first=True during dummy variable creation? 
3. Looking at the pair-plot among the numerical variables, which one has the highest correlation with the target variable?
4. How did you validate the assumptions of Linear Regression after building the model on the training set? 
5. Based on the final model, which are the top 3 features contributing significantly towards explaining the demand for the shared bikes?

**General Subjective Questions**
1. Explain the linear regression algorithm in detail. 
2. Explain Anscombe‚Äôs quartet in detail. 
3. What is Pearson‚Äôs R? 
4. What is scaling? Why is scaling performed? What is the difference between normalized scaling
and standardized scaling? 
5. You might have observed that sometimes the value of VIF is infinite. Why does this happen?
6. What is a Q-Q plot? Explain the use and importance of a Q-Q plot in linear regression.","","Assignment-based Subjective Questions","","","1"
"2913","1027924","3357224","12/11/2020 21:08:45","## Task Details  

Pancreatic cancer is rarely caught early. Could a simple urine test change that? Your task is to use this dataset to correctly identify patients with pancreatic cancer (`diagnosis` = 3).  Your **only** predictor variables should be `age`, `sex`, `creatinine`, `LYVE1`, `REG1B`, and `TFF1`. 

## Validation  

You will need to use cross-validation or a train/test split to evaluate how well your predictive model performs on new data. The original authors used a 50/50 train/test split, with a pre-specified analysis plan (their exact splits are not part of the dataset, though). 

## Expected Submission

You should submit a notebook showing how you developed your model. Clinical predictive models are most useful if they are interpretable‚Äîthe original authors of this dataset chose simple logistic regression, thanks to its easy-to-interpret coefficients. However, a less-interpretable model might be justifiable, if it performs better!  

## Evaluation

Pancreatic cancer is rare, so beyond the typical AUC - ROC, metrics like sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) are also of interest. The original paper was able to achieve an AUC of 0.936 on the validation set‚Äîcan you beat this result? 

### References 

Check out the original paper [here](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003489), plus [this paper from last year](https://www.nature.com/articles/s41416-019-0694-0) by the same group which laid the groundwork for this paper.","","Identify patients with pancreatic cancer using urine biomarkers","","","1"
"4476","1363334","3361237","05/24/2021 09:31:53","# Develop machine learning-based models to classify a given leaf image from https://www.kaggle.com/c/plant-pathology-2021-fgvc8 to detect a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.","","Apple Disease Classification","","","0"
"3171","1091321","3367050","01/11/2021 12:34:16","## Task Details
Perform sentimental analysis of customer reviews.","","Sentimental Analysis","","","0"
"2579","938833","3401050","10/30/2020 09:07:13","Every dataset needs exploratory data analysis to be performed on it to understand it in a better way.
Explore the data and perform EDA!","","Perform Exploratory Data Analysis","","","1"
"2580","938833","3401050","10/30/2020 09:09:12","In any Twitter post, there are various sentiments behind it. 
So perform sentimental analysis and find out the different sentiments in the dataset.","","Perform Sentimental Analysis","","","3"
"3579","1176727","3443584","02/23/2021 13:32:31","## Task Details
Use Advance Visualization to illustrate illustrate if the shipments have reached on-time for the customers who have the best customer rating, the best customer score, made recurring orders and high payments","","Data Analysis","Visulaizations and Feature Enginerring","","18"
"6206","1176727","8414419","09/27/2021 10:11:50","Add another column for location (cities or countries) where items shipped to. This will further enrich the dataset and enable us to do some geographic / Geospatial analysis and visualization of the data using Geepandas.","","Adding another column for location (cities or countries) where items shipped to","","","7"
"4156","1288705","3443584","04/22/2021 14:15:50","1. Use any machine learning algorithm or combination of machine learning algorithms you deem fit.
2. Prepare your model on the Train Data and you can evaluate the generalization capability of your model by using K-Fold Cross Validation, Leave One Out Cross Validation or any other validation technique that you see appropriate.","","Machine Learning","Algorithms","","0"
"2619","953012","3445273","11/03/2020 08:21:22","## Task Details
The expected solution is to classify music into different genre using machine learning/ deep learning

## Evaluation
F1 weighted average metrics would be used for evaluation","","Classify Music","","","0"
"4221","1302507","3446939","04/29/2021 03:40:09","## Task Details
Perform sentiment analysis on this data.

## Expected Submission
Submit a Notebook which outputs a model that is trained to predict sentiments.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Sentiment Analysis","","","0"
"3545","1169589","3450354","02/19/2021 14:51:05","Now that you have the data, let's find a way to make us rich! üî•üí∞

## Task Details
Cryptocurrency markets are booming! So, maybe it's worth looking at historical data and try to predict the future ...

## Expected Submission
Use the dataset to build a model that can predict market movement based on historical data (open/high/low/close) over a specific period.

## Evaluation
A good solution is a solution that can predict market moves (buy / sell) with at least 50% accuracy over a specific timeframe (so something better than playing heads or tails ...)

In any case, enjoy!

**AIOutsider**
![AIOutsider.com](https://static.wixstatic.com/media/85f433_0a80e32e33c04402ae9e39fc719ad339~mv2.png/v1/fill/w_84,h_84,al_c,q_85,usm_0.66_1.00_0.01/Logo_new.webp)
www.aioutsider.com","","Predict Market Trend","Historical data for future market prediction","","1"
"3729","1187938","3453549","03/11/2021 02:01:01","se debe analizar mayor cantidad de datos para comparar adn a nivel molecular para predecir eventos de salud","","dataset de moleculas","moleculas","03/31/2021 23:59:00","0"
"4554","1376656","3475739","05/30/2021 11:13:44","Perform a descriptive analysis of the pandemic outbreak in the country.

Make a rigorous and thorough review on the subject and we could publish the results.","","Descriptive analysis","","","0"
"4555","1376656","3475739","05/30/2021 11:14:33","Prediction of the pandemic or the vaccination rate.

Make a rigorous and thorough review on the subject and we could publish the results.","","Predict the pandemic and the vaccination rate","","","0"
"4556","1376656","3475739","05/30/2021 11:15:11","Discuss the numbers compared to other countries/world.

Make a rigorous and thorough review on the subject and we could publish the results.","","Discuss the numbers compared to other countries/world","","","0"
"4557","1376656","3475739","05/30/2021 11:15:36","Discuss the government decisions.

Make a rigorous and thorough review on the subject and we could publish the results.","","Discuss the government decisions","","","0"
"4558","1376656","3475739","05/30/2021 11:16:02","Estimate the cut-off values for step-down or step-up of the restrictions.

Make a rigorous and thorough review on the subject and we could publish the results.","","Estimate the cut-off values for step-down or step-up of the restrictions","","","0"
"4132","1283570","3475739","04/20/2021 10:50:54","1. Easy level: Preprocess Year column to make it integer (remove ‚ÄúJan‚Äù)
2. Easy level: Who had the highest ELO? In which year?
3. Easy level: What is the top 20 average ELO? What is the time trend for the average ELO? 
4. Medium level: Are the players getting stronger, weaker or there is no significant difference?
5. Easy level: What is the minimum ELO of a player who ever appeared in top 20? Who is this?
6. Easy level: What is the top 20 average age? What is the time trend for the average age? 
7. Medium level: Are the players getting older, younger or there is no significant difference?
8. Easy level: who appeared most times in the top 20?
9. Medium level: what is the average age of peak performance of top 10 players?
10. Medium level: The probability of winning for player A is P(A) = 1/(1+10^m) where m is the rating difference (rating(B)-rating(A)) divided by 400. If a chess Engine has ELO 3100, what is the chance of win for ‚Äúbest ever‚Äù Magnus Carlsen? Currently, Stockfish engine have ELO 3512, what is the chance of a win for Magnus Carlsen? What is the chance of drawing?
11. Master level: create a model which uses age as a predictor of ELO. Try to predict Magnus Carlsen‚Äôs next year rating! (If it is too difficult, try to predict his age next year, lol)","","A list for optional tasks","","","0"
"4334","1322166","3491994","05/12/2021 00:53:20","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help.
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualizing diversity in Oscars","","","0"
"2583","947376","3492127","10/30/2020 13:14:20","## Task Details
Teams participating in the competition grew tomatoes remotely. The goal in AGC 2nd was to generate the most net profits. 

Here Kaggler's are expected to merge this dataset with greenhouse information of them and visualize top features to win.

Other insights would also be interesting such as team strategy of cultivation, correlation between greenhouse and outside weather, important features for cultivation, etc.

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
A good solution would be a creative kernel that shows directly conducive information or features to growing cultivation. If you want to go beyond, you can even make a dashboard on it.","","Show me the important features to win this competition","Which items contributed the most to the net profit?","06/30/2021 23:59:00","1"
"4154","1287993","3492213","04/22/2021 11:52:00","**Task Details :**
Use this uploaded datasets to predict Good images as Good and Bad images as Bad using any of the possible machine learning technique in python language.

**Evaluation :**
A good solution accepted will be when -
Good as Good &gt;= ~92%
Bad as Bad &gt;= ~98%","","Surface Defect Detection","Grayscale Surface Defect Detection","","1"
"4174","1293628","3492504","04/24/2021 18:21:46","## Task Details
Recognise different types of images using deep learning.
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Recognise different types of rocks from images","","","0"
"2495","913603","3492504","10/20/2020 08:30:49","## Task Details
This task is created for metal tools recognition

## Expected Submission
27/11/2020","","Object Recognition","Metal Tools Classification","11/27/2020 00:00:00","3"
"2819","1005974","3492504","11/29/2020 20:23:48","## Task Details
To classify railway images - Binary classification","","Classifying Railway Track using Deep Learning","Defective and non-defective railway track classification","12/30/2020 23:59:00","0"
"2524","935035","3493897","10/23/2020 17:16:00","## Task Details
Create a methodology to determine the **REMAINING USEFUL LIFE** of a high speed bearing based on it‚Äôs acceleration measurement recorded by a vibration sensor.

With 50 records in the data set, there are two classes involved:

1. The first class called **long-life expectancy** is associated to the first 35 samples;
2. The second class called **short-life expectancy** is associated to the last 15 samples.","","Health Status of the Wind Turbine","Health Prognostic Management","","2"
"4347","1335067","3499937","05/12/2021 11:22:23","The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

**There are two datasets:**
1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010)
2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.

**Attribute Information:**

Input variables:
# bank client data:
1 - age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')
# related with the last contact of the current campaign:
8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.
# other attributes:
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
# social and economic context attributes
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)

Output variable (desired target):
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')


**Objectives:**

1. Perform a correlation analysis among all the numeric variables two at a time. Tabulate the summary of findings in a N X N matrix (Use bank-additional-full.csv)

2. Perform the sensitivity analysis and create a funnel chart with top 5 input factors that influence the ""Y"" ((Use bank-additional-full.csv)","","Exploratory Data Analysis","The data is related with direct marketing campaigns of a Portuguese banking institution.","05/16/2021 23:59:00","1"
"3280","1120491","3501914","01/26/2021 18:05:17","## Task Details
The goal of this task is to show the different visualizations of the **Pakistan Super League Datasets** like Visualization of Most match wins or Most wickets taken by a player in PSL

## Expected Submission
Submit a Notebook which contains the different visualizations of  **Pakistan Super League Datasets**

## Evaluation
Visualization with good explanation","","Visualize the Pakistan Super League Datasets","Create a Notebook which visualize the Pakistan Super League Datasets","","1"
"4451","1352205","3502800","05/20/2021 08:05:54","## Task Details
Predict whether the student is likely to become a great entrepreneur or not.

## Expected Submission
The solution must contain the target column named as `y` along with some unique IDs. 

## Evaluation
It's a binary classification. Accuracy can be used as a metric.","","Predict entrepreneurial competency","Use the 16 features to predict the entrepreneurial competency","","6"
"3772","1210385","3505796","03/14/2021 13:10:43","## Task Details
Subbmit the your own implementation of the notebook.

## Expected Submission
Clone the kernel and do it yourself.

## Evaluation
Check the performance in lines detection, smoothing, etc.","","Exercise 1","","","2"
"3062","1018569","5690863","12/28/2020 13:54:57","Consider Genre as your target and use different ML Models to predict it.","","Predict Genre","","01/31/2021 23:59:00","5"
"4123","1018569","5702221","04/19/2021 00:31:18","Build a recommendation system with given features for given Spotify songs. 
Feel free to use any features; e.g. I only used genre, mode, and duration.
Present your findings and matches with your input song name.","","Build Recommendation System For Given Spotify Songs","","","6"
"2755","991280","3511431","11/23/2020 10:33:55","Make best visualizations based on this dataset","","Make visualizations","","","4"
"4409","1344846","3514009","05/16/2021 20:19:50","## Task Details
Exploratory data analysis of voting patterns in the Kerala legislative assembly elections 2021.

## Expected Submission
Notebooks containing code and visualizations of analysis.

## Evaluation
A good solution will feature visualizations to show the voting patterns of each major competing party in the constituencies.","","District-wise exploratory data analysis of major parties","","","0"
"4561","1377186","3514009","05/30/2021 13:20:09","## Task Details
Since the member countries can not vote for them, we wish to examine if there is a pattern in allocating the votes. Do countries vote their neighbors higher? Which countries do not vote for their neighbors at all?

## Expected Submission
The submitted notebook could be an analysis of voting patterns of member countries, highlighting which countries are given more votes preferentially.

## Evaluation
A good solution would contain an EDA and visualization, along with explanation of the results.","","Do voting members vote preferentially to benefit their neighbors?","","","1"
"3356","1129245","3516768","01/31/2021 04:47:38","## Task Details
Find the word frequencies in either the abstract or the conclusion columns or both!

## Expected Submission
- Generate a wordCloud
- What is the frequencies of 'respiratory', 'lungs', 'genes'
- Is there something interesting after analyzing both the abstract and the conclusion?

## Evaluation
- Make sure to exclude the stop words 
**You might want to add more words into the list of stop words","","WordCloud","","","0"
"2517","929734","3550244","10/23/2020 00:59:08","## Task Details
I created this task to encourage students to write an ML algorithm from scratch.

## Expected Submission
Please submit your code and your r2 score of your own written ML algorithm. Explanation of the math is preferred. 

## Evaluation
A good solution is a solution that the result is comparable with a model from an existing library. For example, the result of your linear regression model is not far from scikit-learn library. 

### Further help
Please see this notebook as an example of writing the math and algorithm from scratch: https://www.kaggle.com/sinamhd9/titanic-a-complete-project-from-scratch-tutorial","","ML from scratch","Can you write your own ML algorithm from scratch? (Linear regression or any nonlinear model)","","2"
"2518","929734","3550244","10/23/2020 01:05:09","## Task Details
Perform hyperparameter tuning on at least one model and report the improvement in your score. 

## Expected Submission
Please submit your kernel with an explicit explanation of your hyperparameter tuning process and the improvement in your score. 

## Evaluation
If your hyperparameter tuned model is scoring better than the baseline, you have accomplished the task.

### Further help
Please see Sklearn documentation for hyperparameter tuning using GridSearchCV
https://scikitlearn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
You can also use these notebooks: 
https://www.kaggle.com/sinamhd9/titanic-a-complete-project-from-scratch-tutorial
https://www.kaggle.com/sinamhd9/safe-driver-prediction-a-comprehensive-project","","Hyperparameter-tuning","Can you improve your score using hyperparameter tuning?","","1"
"3940","1246377","3552593","04/02/2021 17:32:04","## Task Details
Use topic clustering to segment the titles of the posts into various categories

## Expected Submission
Categorize each post into a class. Visualize the distributions. Word clouds for words used in each category.","","Clustering by Post","","","0"
"4055","1267253","3564129","04/12/2021 16:04:53","## Task Details
Observe correlations and fluctuations between the tweet, emotional impact and Bitcoin price.","","Observe impact of Tweets in Bitcoin Price","","","1"
"3830","1225213","3565979","03/21/2021 20:40:59","## Task Details
Using the given information can we determine the price for Rent, Sell or Vacation?","","Can we predict the prices?","","","0"
"2956","1037774","3566127","12/15/2020 22:34:29","This is a Dataset downloaded from UCI Machine Learning Repository.
Description as per UCI site : The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan.","","Real Estate Valuation","","","0"
"3736","1190586","3568003","03/11/2021 17:57:53","## Task Details
Developers use Bugout.dev to save knowledge from wherever they work. One place that developers do a lot of work is in their editors/IDEs.

Bugout stores knowledge in Markdown files, and our users frequently want to store code from their editors in highlighted code blocks.

Applying the right highlighting requires us to understand which programming language a snippet was written in in the first place. We currently use file extensions to determine this.

We are curious how well machine learning models can do on the same tasks, which is why we have created this dataset.

## Expected Submission
A notebook containing:
1. Training - evaluation split of the whole data
2. Model which trains over the training partition
3. Summary of performance on model over the evaluation partition

If this dataset is too large when you're developing your model, you can start on the development version that we created: [GitHub Code Snippets - Development sample](https://www.kaggle.com/simiotic/github-code-snippets-development-sample).

## Evaluation
Beyond standard evaluation metrics, it is also important to keep model size as low as possible.","","Detect programming language from snippet","Can you determine which programming language a code snippet was written in?","","5"
"4452","1355611","2402099","05/21/2021 04:21:12","## Task Details
Create text model, generating traceback errors for specific repository, based on provided data. There is no use in it, it is just for lols.

## Evaluation
Submission will be evaluated on replies to github issues. You need to generate traceback by an input of random chosen repository name. Than open an Issue on this repo with this message and wait for the reaction.","","Create model for tracebak generation","","06/22/2021 23:59:00","0"
"5660","930469","7526686","08/10/2021 18:06:36","Please update this sir for all seasons","","Update","","","0"
"4278","1261970","3569819","05/06/2021 02:53:55","## Task Details
Character analysis of the people involved. The tv series is known for the development of its characters through the ages.

## Expected Submission
Submissions can be in the form of notebooks

## Evaluation
A good submission would be one that gives the basic keywords(so that we can practically write an essay about him/her) that can be used to describe a particular character. Each series has to be considered independently to do the character analysis of the people.","","Character analysis using NLP","","12/31/2028 23:59:00","1"
"4069","1271387","3582353","04/14/2021 13:37:53","## Task Details

Many properties have their prices missing. How accurate can they be imputed?","","Missing prices","","","0"
"4070","1271387","3582353","04/14/2021 13:40:07","## Task Details

For many listings the type was not indicated. Can the text of a listing along with other features be used to find the type of the commercial property?","","NLP for property types","","","0"
"4071","1271387","3582353","04/14/2021 13:45:00","## Task Details

One of the most important parameters for real estate agents is the price per square meter in the current postcode. A simple statistics for this parameter can be quite interesting for bargain hunting.","","To buy or not to buy?","Hunting for bargains","","0"
"4072","1271387","3582353","04/14/2021 13:52:17","## Task Details

Different types of property offer different returns. With COVID pandemic, many businesses started looking into warehouses (industrial) as they moved online. Can the property type be determined from the listing text and other features?","","Property Type Classification","","","0"
"2548","928984","3582353","10/27/2020 11:43:31","## Task Details
Negative electricity prices are damaging for electricity companies. Is it possible to predict them or determine the main factors responsible for prolonged daily periods of negative prices?","","Anomaly detection","","","0"
"2549","928984","3582353","10/27/2020 11:47:38","## Task Details
Electricity demand forecasting is vital for our daily lives as it allows energy generation to match our needs without too much waste. How accurately is it possible to forecast the demand for a day or a week ahead?","","Electricity price and/or demand forecasting","","","1"
"4287","1322121","3593672","05/06/2021 17:06:25","## Task Details
Using all the knowledge you gathered in the SMP;Can you build a superior medical image classifier that has practical application?
## Expected Submission
No submission but you are free to make your notebook public if you are proud of your results.The more we share the more we can learn from each other.Make sure to add comments to your notebooks so people benefit from reading them
### Further help
Googling is your best friend when it comes to exploring the world of Deep LearningüòÅ 
Theres a plethora of model architectures out there so feel free to experiment in any way you see fit.
Check out the notebooks [here](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database) for some inspiration.
Good Luck! And may the force be with you","","Summer Guided Project 2021","","","1"
"4494","1322121","5053007","05/26/2021 07:56:02","Submit your week 3 assignments here","","Deep Learning SGP WEEK 3 Submissions","","","1"
"3576","1175883","3595464","02/23/2021 01:34:50","Cria√ß√£o de Modelagem de T√≥picos extraindo dados do PDF ou  do arquivo txt","","Modelagem de T√≥picos","Cria√ß√£o de Modelagem de T√≥picos com o PDF/Txt","02/22/2022 23:59:00","0"
"2861","1015325","3607496","12/04/2020 12:46:58","## Task Details
In this world of big data, Trell wants you to use the data to predict the age group of their users based on their activity on social media activities.  This will help them to divide their huge userbase and cater differently to each of them. 

## Expected Submission
see sample_submission.csv file

## Evaluation
Given this huge dataset, predict the age group of the users, the evaluation metric for the competition is the Weighted F1 score.","","Predict age group","The evaluation metric is the Weighted F1 score.","","0"
"4424","1295654","6396210","05/17/2021 16:51:19","In it you have to find the peek point of the cases.","","Predict the Peek.","","05/30/2021 23:59:00","0"
"4223","1304073","3612572","04/29/2021 08:01:15","## Task Details
I want to see how well we are able to reproduce Goethe's style by training a text model on his works.

## Expected Submission
A notebook building a text model and demonstrating some interesting text generation examples.

## Evaluation
Does it sound like Goethe ?

### Further help
Check these resources on [huggingface](https://huggingface.co/) :
- https://huggingface.co/blog/how-to-train
- https://huggingface.co/transformers/model_doc/gpt.html","","Text Modeling","Write in Goethe's style !","","1"
"4736","1403243","3617295","06/11/2021 17:14:05","This is a dataset of breast cancer patient survival who went through surgery. This case study was conducted between 1958-1970. It has 4 attributes as follows","","EDA on Habermans Cancer data set","","","2"
"3537","1167174","3621590","02/18/2021 18:38:29","### Data

The data set contains the Telugu Movies released in 1930 and from 1940-2021. It basically contains names of titles, directors, producers, cast, music directors, genre and release date/year.

## Expected Submission

No timeline

## Evaluation

Explore EDA. Find who are acted in more movies?etc

### Further help

- https://www.kaggle.com/rizdelhi","","Wikipedia Telugu Movies","EDa","","1"
"3486","1155963","3621590","02/13/2021 14:57:36","A top-up loan is a facility of availing further funds on an existing loan

- LTFS provides it‚Äôs loan services to its customers and is interested in selling more of its Top-up loan services to its existing customers 

**Business Objective: Predict when to pitch a Top-up during the original loan tenure**


### Data 

1. Customer‚Äôs Demographics: The demography table along with the target variable & demographic information contains variables related to Frequency of the loan, Tenure of the loan, Disbursal Amount for a loan & LTV.

2. Bureau data:  Bureau data contains the behavioural and transactional attributes of the customers like current balance, Loan Amount, Overdue etc. for various tradelines of a given customer

I am a building a model given the Top-up loan bucket of 128655 customers along with demographic and bureau data to predict the right bucket/period for 14745 customers in the test data.

###  Evaluation
The evaluation metric is macro_f1_score.

**Problem Statement: It's multilabel classification.**

Let's explote different classification algorithms along with exploratory analysis of the data!!!","","Exploratory Data Analysis","Explore the importance of features","","0"
"2551","942508","3624895","10/27/2020 15:38:40","## Task Details
Try to come with a Architecture that recommends best phone to User based on his requirements.","","Mobile Phone recommendations","","","1"
"2658","964233","3626163","11/09/2020 12:01:28",".","","You have to analyse the data and Find the sqrt MSE","","11/24/2020 23:59:00","0"
"3225","1105807","3632349","01/20/2021 07:09:49","automatically summarize visitors‚Äô sentiments from reviews","","Build a sentiment classifier","","","3"
"4323","1105807","3632349","05/10/2021 08:57:54","## Task Details
The given dataset consists of reviews for the three Disneyland branches. Use Topic Modelling to to find out abstract topics which might serve as potential recommendations. You may also use aspect level sentiment analysis where the used aspects my hint some recommendations.

## Expected Submission
Please provide the written recommendations were it can be linked to your analysis. For example visualizing the topic models (using word clouds), showing the reviewers concerns or interests.

## Evaluation
Clean and tidy code, Text preprocessing, showing results of multiple algorithms like Topic modelling using SVD, LDA etc.","","Recommend ways to improve the three Disneyland branches.","Topic Modelling","","1"
"3567","1172951","3635877","02/21/2021 14:18:56","1. Model Building: Sentiment Analysis Predictions on 'key words' based features
* BOW word features
* TF-IDF word features
2. Predictions on 'key phrases' based features
* BOW phrase features
* TF-IDF phrase features","","Sentiment Analysis on Scrapped Tweets","","","0"
"2553","937570","3635877","10/27/2020 16:14:52","Forcasting","","Election Forecasting","","12/31/2020 23:59:00","1"
"2555","937601","3635877","10/27/2020 16:38:57","Club Soccer Prediction","","Club Soccer Prediction","","12/31/2020 23:59:00","1"
"2571","945387","3635877","10/29/2020 08:58:23","Prediction","","Natural Language Processing","","","1"
"2566","944462","3635877","10/28/2020 17:10:21","Task: Prediction","","COVID Test Prection","","12/31/2020 23:59:00","1"
"2609","950411","3635877","11/02/2020 07:09:48","Prediction and Visualization","","Crime Prediction and Visualization","","12/31/2022 23:59:00","3"
"4345","1335965","3643687","05/12/2021 09:51:48","## Task Details
This is the output dataset of Azure Kinect DK coordinate systems. where we have 4 sheets MachineOn, MachineOff, MachineFailing. And the first sheet Consist of information Machine data that captures gyro and acceleration in all axis.

## Expected Submission
I want you guys to submit a notebook on ML model Building for these three Features
MachineOn, MachineOff, MachineFailing.

## Evaluation
Based On the Accuracy of Model ?

### Further help
If you need any further info please reach out to me at 
https://www.linkedin.com/in/somesh-ghaturle-33a15a100/","","Azure Kinect DK coordinate systems","","","0"
"3512","1160245","3650686","02/15/2021 21:31:16","See the evolution of the community before and after the game lauch.","","Sentiment analysis","","","0"
"2909","1025731","3660023","12/11/2020 17:21:41","## Task Details
This dataset has been created with the sole purpose to incorporate the weights in a inference kernel to achieve the best accuracy as possible.

## Evaluation
A good solution is when you achieve as high categorical using these weights as possible.","","Create a best Deep Learning model","","","0"
"3504","1159193","3669318","02/15/2021 11:01:34","## Task Details
Use Text Generation Techniques to generate poems or words that should sync to get a meaning.

## Evaluation
There is no proper metric to evaluate, if you are native Telugu, you will know the performance of your model.","","Poem Line Completion","","","0"
"3468","1153096","3670197","02/11/2021 22:27:49","## Task Details
This first task is all about getting to know the data and preparing it for future analysis!

## Expected Submission
User submits a notebook which shows the data cleaning and his EDA.

## Evaluation
A good solution will first explore the data. Try cleaning the data without deleting the records as much as possible. Once the data is cleaned the solution should show some interesting facts about the data.","","Data Cleaning & EDA","Prepare the data!","","1"
"3469","1153096","3670197","02/11/2021 22:34:36","## Task Details
Sometimes the best way to get a story across is to create some great visuals! See this task as a great exercise of your interactive plotting skills!

## Expected Submission
The user should submit a notebook which shows an interactive plot taking into account most of the parameters from the dataset. Use the picture below as a guideline/inspiration.
![guideline](https://www.emodnet.eu/sites/emodnet.eu/files/public/image_news/Map%20of%20the%20week%20%20-%20Beach%20Litter%20-%20Cigarettes.JPG)

## Evaluation
A great solution has nice visuals consisting of a great number of parameters.","","Interactive plotting!","Let's create some great visuals!","","0"
"5433","1230616","552713","07/30/2021 17:33:55","## Task Details
Amol made a dataset available containing TSLA stock data.

## Expected Submission
Not sure what the submission is, but would like to apply different ML/DS against it.

## Evaluation
Will have to evaluate pros/cons of different approaches. Will also need to validate against more recent information.","","Check out Tesla Stock Data","","","0"
"4124","1280726","3690386","04/19/2021 04:15:12","Use your creativity","","F1 news search engine","Using the dataset, you can create a simple/complex search engine that gives the user all information based on his search request","","1"
"3271","1118067","3697699","01/25/2021 14:08:12","## Task Details
There are 5 kinds of function graphs in the dataset. Therewith, to build a classification model based on your knowledge and experience.

## Expected Submission
please leave your Notebook

## Evaluation
a confusion matrix is a good evaluation standard","","function graph classification","CNN recommend","","0"
"6264","1015762","8254172","10/06/2021 07:17:07","## Task Details
Create Visualization on tableau for analysing the data of the IPL
## Expected Submission
You can submit tableau file Or link of your tableau workbook. It should contain analysis of the given data set. 
## Evaluation
Good and attractive visualization. With some information shown from that visuals.","","Create visualization on tableau","All types of chart for analysis the data","10/09/2021 23:59:00","1"
"4244","1308278","3705733","05/01/2021 19:25:26","## Task Details
Agricultural production forecast is very important to prepare the logistics, including transportation and storage, and also to predict prices on the commodity markets.

## Expected Submission
The solution should be backtested on the historical data and should predict the production of each grain for the next couple of years","","Production Forecast","Prediction of future grains production","","0"
"3873","1234762","3717310","03/26/2021 18:26:06","## Task Details
what can we do with the data?

## Expected Submission
explain the data and data visualizations

## Evaluation
a good solution is a comprehensible one and has a meaning","","EDA(Exploratory Data Analysis)","","","0"
"6952","1324053","6646082","11/28/2021 20:21:02","## Task Details
The mall need to know what possible types of customers it has and to explore what are their features and characteristics. Create a solution that shows which possible customer groups can be found within the data.

## Expected Submission
The solution should contain the customer groupings and a clear explanation of what qualities customers in each group have.","","Find the optimal number of customer groups the company should tackle","","","0"
"2873","1018463","3724244","12/06/2020 10:19:03","Predict whether the graph of non-voter in the U.S. will increase or decrease in future.","","Future Non Voter Prediction","","","1"
"2368","910813","3724244","10/07/2020 22:34:36","## Task Details

We have to analyze Russian investigation and predict whether Russian investigation is another watergate.
Refer to this link - https://projects.fivethirtyeight.com/russia-investigation/","","Watergate Prediction","","","1"
"3246","1096724","3734498","01/22/2021 12:14:50","This dataset contains images related to 4 different weather classes. 

**Task Details**
Try to classify images into different classes namely:
- Sunrise
- Shine
- Cloudy
- Rain

**Expected Submission**
.csv files should be submitted which could be added to this dataset.

**Evaluation**
A good solution will feature different classes of weather with the image number.","","Classify the weather images","","","1"
"3856","1229864","3742025","03/24/2021 08:06:22","This is a bank account transaction data for three months to start with. I am trying to categorize the transactions based on the transaction description. And based on that I would like get to know the recurring transactions like home loan , car loan and house rent‚Ä¶ on the liabilities side. And similarly on the income source side have to check what is his salary and monthly salary row i should be able to recognize.","","Bank Account Transaction Classification","","","0"
"3327","1127252","3743102","01/30/2021 04:05:56","Use the test_data.csv file for doing machine learning models to predict price of villa in the test file.
Show the accuracy of your prediction by using accuracy score.","","Predict the price of villa based on features","Regression  task","","0"
"2852","1014691","3746000","12/04/2020 05:56:12","Problem Statement
Business Use Case
There has been a revenue decline for a Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold a better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have a higher chance to subscribe for a term deposit and focus marketing efforts on such clients.

Evaluation Metric
We will be using AUC - Probability to discriminate between subscriber and non-subscriber.","","Predict client subscription to a term deposit of bank","","","0"
"2919","1031949","3746884","12/13/2020 04:37:49","Tasks:
Identify the location and develop a kind of heat map of concentration of these organizations
Find organizations with maximum presence and impact","","Analysis","","12/31/2020 23:59:00","0"
"2920","1031991","3746884","12/13/2020 05:09:12","To find which feature to select and visualize the data, which feature correlates to the price.","","To predict the price.","","12/31/2020 23:59:00","0"
"2921","1032013","3746884","12/13/2020 05:27:09","In October of 2018, the Board of Supervisors passed and Mayor London N. Breed signed, the Ordinance 243-18 (http://bit.ly/2LSqS3U) to expand the extent to which women are represented in the public sphere, including within artwork, statues, street names, facilities, parks, and more. The representation of women in City-owned buildings includes buildings, conference rooms, clubhouses, museums, recreation centers, community rooms, auditoriums, terminals, departure halls, 7 staircases, rooms, and other places open to the public. The City Administrator's office was responsible for compiling a list of all city owned buildings named after a man or woman. You can read the accompanying report on the representation of women in city property here: http://bit.ly/2YIDCz7

Now you know the details:
Try to do EDA on the representation of men and women in various facilities
Find where woman are in bigger number and where men are in bigger number and try to find the possible reasons behind those results.","","Analysis","","","6"
"2922","1032029","3746884","12/13/2020 05:43:59","This data set contains information related to from counters of Traffic Counts and Accidents. Please contact The Department of Planning and Transportation if you have any questions: https://bloomington.in.gov/departments/planning-and-transportation , planning@bloomington.in.gov , 401 N Morton St, Suite 130 Bloomington, IN 47404 , Phone: (812) 349-3423, Fax: (812) 349-3520

Now That you have some background on this dataset
Try the following tasks:
-Find the more crash prone areas in Monroe County
-Find the most heavy pedestratia areas in city and the heat maps of that data","","Analysis","","12/31/2020 23:59:00","0"
"3427","1143662","3747871","02/07/2021 10:09:52","## Task Details
Look at the distributions of nz cities and find out which actually got quarantined enough that they had barely no covid.

## Expected Submission
Anything really.

## Evaluation
Anything with information that might help describe and help us understand why NZ  had such a low covid rate.","","Distribution difference?","Does the distribution of each city matter?","12/31/2021 23:59:00","0"
"3424","1141890","3747871","02/07/2021 08:13:53","## Task Details
Your task is to create models that predict the gender of each shooting.

## Expected Submission
Users are expected to submit a csv file with 2 columns. 1 being the index and the 2nd column being the gender of the shooter

## Evaluation
An amazing solution would be one that has an accurate model for prediction.

### Further help
Put in discussion","","Can you predict the gender of the shooters victim?","There can only be male or female","01/01/2022 23:59:00","0"
"3425","1107381","3747871","02/07/2021 09:20:36","## Task Details
I felt like this may be important for future references

## Expected Submission
anything.","","Any insightful extracts?","The task is to find any insightful extracts.","02/28/2021 23:59:00","0"
"2795","995814","3754812","11/27/2020 06:40:55","**Problem Description:**
Prepare a Machine Learning Model to predict the Persistency 13M Payment Behaviour at the New Business stage.

**Objective**:
1. Using Machine Learning techniques, provide scores for each policy at the New Business stage the likelihood to pay the 13M premium.
2. Identify the segments where maximum non payers are captured 

**Dataset:**
1. ‚ÄúTraining‚Äù & ‚ÄúTest‚Äù Dataset with the raw input attributes and the 13M actual paid/not paid flag.
2. ‚ÄúOut of Time‚Äù Datasets would be provided with just the raw input attributes.

**Expected Steps:**
1. Conduct appropriate Data Treatments for e.g. Missing Value Imputation, Outlier treatment etc.
2. Conduct required Feature Engineering for e.g. Binning, Ratio, Interaction, Polynomial  etc.
3. Use any machine learning algorithm or combination of machine learning algorithms you deem fit.
4. Prepare your model on the Train Data and you can evaluate the generalization capability of your model by using K-Fold Cross Validation, Leave One Out Cross Validation or any other validation technique that you see appropriate.
5. Score the Test and Out of Time Data and share it back to us along with the scored Train Data for evaluation. Also share all the Model Codes and Documentation.

6.	Evaluation Criteria:
1. Maximum non payers captured with minimum population. 
2. Model should hold across Out of time period
3. Mode should be explainable i.e. importance and effect of variables finally selected in the model should be clear
4. Characteristics of a good model (try to beat these min criteria, not mandatory)
                  1.  Non Payment Rate in the Bottom Most Probability to Pay Decile 
                      should be at least &lt;= 15%.
                    2. % Non Payers Capture in the Bottom Most Probability to Pay Decile 
                     should be at least &gt;= 40%.","","Machine Learning Model to predict the Persistency 13M Payment Behaviour at the New Business stage.","Persistency 13M Payment Behaviour Prediction","01/31/2021 23:59:00","0"
"4092","1276827","3754812","04/17/2021 04:13:46","## Task Details
Try to solve this using python or R.
1) The objective is to get the pin codes correctly in a 60km radius and by taking care of the border Pincode.
2) Visualize it on map.","","How to get all pincodes in 60 km radius","Map all pin codes in the given radius (50 km) by taking a any pin code of your choice","","0"
"4508","1370579","3757755","05/27/2021 09:34:07","## Task Details
This task is to create a simple visualization notebook and find relations between reasons. 
PS: Don't use title data and text data in your submission if you are a beginner.
## Expected Submission
1. Imporing dataset and preprocessing column headers
2. Preprocessing Dataset for null and string to convert all to a standard representation.

3. Some Graphs Ideas :- Reason wise ,  Number of people wise,city data.

4. Find some intuitive results like:
      (a) Is there a specific type(reason) of crime which has relation to another type(reason).
      (b) Is there a specific type(reason) of crime which leads to more number of people getting victimized per case?
      Please provide the graphs in support of (a) and (b).
      (c) Can you think of some ways of using text data in your analysis. Dont code this just write the ideas. 

## Evaluation Criteria
Preprocessing, Graph and Reasoning Quality.","","Data Visualization and Relation Finding","Create a iPython notebook to Visualize and find relations between Reasons across categories.","","34"
"4868","1402100","3757755","06/21/2021 04:37:41","## Task Details
1. Go throught the dataset and perform preprocessing and then perform a 90:10 split and for train and test pruposes. 
2. Firstly label encode the columns which are required. 
3. Your target or y variable is TAXI-OUT time. Use all 8 algorithms above on the dataset with loss score as RMSE (Root mean Square Error).
4. Now, One-Hot encode all the data points and preform the 3rd Step again.

Keep in mind that you will be using the same splitted dataframe for all the training and testing and should not split again.

Some models to consider:-
Linear Models:-
1. Linear Regression 
2. Ridge Regression(Popularily L1)
3. Lasso Regression(Popularily L2)

Non linear Models:-
1. KNN model
2. SVR
3. Naive Bayes
4. Random Forest
5. LightGBM(Tree Based Model)

## Expected Submission
Open Submission till end of July.
Submit a Notebook: End-Goal: We will see how label encoding or one hot encoding is better for the model and which out of the 8 algorithms which is the best. Just a cojparative study and plots to understand the results on bigger datasets.

## Evaluation
Having a complete report along with comparative graphs.","","Predicting the Taxi-Out Delay.","GIven the dataset predict the runway time of the flight","","19"
"4121","1279979","3758223","04/18/2021 16:34:53","## Task Details
Introduce a multi-class classification method with the highest training speed.

## Expected Submission
You may submit a multi-class classification method that has the fastest training speed.

## Evaluation
The method with the highest accuracy and lowest training time.

### Further help
It is better to show your training speed on the plot and also compare it with different metrics.","","Training speed","multi-class classification method","","0"
"3869","1234174","3758223","03/26/2021 12:13:54","## Task Details
It has been a few years since the introduction of the  BoostedTreesClassifier by TensorFlow. You can participate in this task to develop boosting methods.

## Expected Submission
- Submit your version of TensorFlow BoostedTreesClassifier implementation.
- As this method include DataFrame and its features, you may develop your DataFrame as well.
- The submission should be using Notebook.
- Use a method like gridsearch to optimize the hyper-parameters.


## Evaluation
1. Introducing a new method to optimize the hyper-parameters
2. Evaluation using accuracy metric (given the output class).
3. The simple implementation using different libraries for building Dataframe.


### Further help
If you need additional inspiration, check out these libraries:
1. https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933
2. https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403","","Implement TF BoostedTreesClassifier","Multi-class Classifier","","1"
"2968","1041811","3758223","12/16/2020 22:22:29","## Task Details
Statistical analysis of financial data.

## Expected Submission
The solution should include a statistical analysis of attributes based on tables and graphs.","","Data analysis","Review and manage data","","0"
"3783","1213397","3776780","03/15/2021 21:14:57","## Task Details
Split the data in a 75:25 ratio and try building a classifier model that classifies the images into subsequent classes.

## Expected Submission
The expected solution should contain an image followed by its label.

## Evaluation
Other than accuracy, try using metrics like f1-score and AUC Score too.

### Further help
If you need additional help feel free to reach out on my email: stan0azhan@gmail.com","","Classification","Create a face classification model","","0"
"2734","984876","3778112","11/20/2020 07:07:51","Determine a coloring of Gujarat state in such a way that only 4 colors are used and neighboring districts do not have same colour.","","Four color problem","Four color problem on Gujarat state","","0"
"3641","1151237","3781883","03/02/2021 15:06:37","## Task Details
Create you models . Check your rank on the leaderboard and have have fun during learning . 

## Expected Submission

you can submit your results at the below link 

https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/submissions/

## Evaluation
PRIMARY EVALUATION METRIC
Classification Rate =1N‚àëNi=0I(yi=yi^)
The metric used for this competition is the classification rate, which calculates the percentage of rows where the predicted class y^ in the submission matches the actual class, y in the test set. The maximum is 1 and the minimum is 0. The goal is to maximize the classification rate.","","Build your pump working condition model","","11/21/2021 23:59:00","0"
"2401","914887","3786946","10/10/2020 20:45:54","## Task Details
Real estate, like stocks, is a very easy place to speculate.

Abnormal transactions are common in such markets.

## Expected Submission
Detect abnormal transactions and submit relevant datasets.","","Real Estate Abnormal Transaction Analysis","","","3"
"2402","914887","3786946","10/10/2020 20:50:18","## Task Details
The price of an apartment in the future is something so many people are wondering.

In particular, in Korea, apartments are more than just living spaces, they also serve as investment products.

## Expected Submission
Predict future Korean apartment prices by region.

Or predict with a control variable.","","Predicting future apartment prices","","","0"
"3819","1222183","3800337","03/20/2021 04:35:07","## Task Details


The goal of this assignment is to examine your skills in implementing NLP and machine learning algorithm
Extract the meaningful information from 'Text' & classify the **Service type




## Evaluation?

We want interpretability of models , Its multiclass classification problem","","Clinical  notes","Healthcare has lots of data  our goal is to use that data & make there work simple using AI","","0"
"4247","1311349","3820779","05/02/2021 15:06:21","## Task Details
Create a hand gesture recognition model using any method you wish.

## Expected Submission
The Notebook should contain preprocessing of the data and that it works on a user given input image which is masked as well.","","Hand Gesture Detection","Classify Different Types Of Hand Gestures","","1"
"2686","969864","3827709","11/12/2020 17:28:43","## Task Details
It would nice to know the sentiment of every briefing. Are they negative or are they positive? How confident is your model in determining the sentiment?

Use a pre-trained sentiment classification model to determine the sentiment of the White House Briefings and Statements

## Evaluation
Does the sentiment classification hold up to your own judgment of the statements and briefings tone?","","Sentiment Classification","Determine the sentiment of the briefings and statements","","1"
"2846","1013130","3827709","12/03/2020 10:28:55","## Task Details
There is often a focus on negative news in news organizations. Is this also the case with the articles of the largest news organizations in the Netherlands?

## Expected Submission
You should submit the sentiment of every article in the dataset.

## Evaluation
It will be difficult to determine whether the solution is good because we do not have a target sentiment label. 

However, if you can read Dutch, you can view several articles yourself and assess whether your sentiment classification was good.","","Sentiment Analysis","Determine the sentiment of dutch news articles.","","1"
"2893","1013130","3827709","12/09/2020 08:55:15","## Task Details
It would be interesting to know which organization or persons are most mentioned in these articles. Does the presence of these organizations change over time?

## Expected Submission
You should submit an overview of the most mentioned entities per article title per name entity group over time. Either in a graph or in a table.","","Named Entity Recognition","Discover which entities are most often mentioned in the articles.","","1"
"2578","946669","3828601","10/30/2020 02:53:56","COVID Impact Analysis","","COVID Impact Analysis","","","0"
"2411","916740","3828601","10/12/2020 03:19:14","Predict the difficulty (I_Zscore) of a word using various models & compare the result.","","Predict the difficulty (I_Zscore) of a word","","","0"
"2372","911013","3828601","10/08/2020 02:25:11","EDA of the Flora population of Queensland","","EDA of the Flora population of Queensland","","","0"
"2373","911025","3828601","10/08/2020 02:38:00","EDA of the Air Quality of Brisbane CBD","","EDA of the Air Quality of Brisbane CBD","","","0"
"2374","911053","3828601","10/08/2020 03:31:36","Analysis of the extent of Australian Bush Fires","","Analysis of the extent of Australian Bush Fires","Analysis of the extent of Australian Bush Fires","","0"
"4100","1277818","3830632","04/17/2021 15:32:48","## Task Details
This task will focus on gaining more insight into the esports industry. Datasets about esports are often fairly limited but this taks focuses on showing some unique aspects that are often overlooked. 

Exploratory Data Analysis (EDA) refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations. ([Definition Source](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15))

## Expected Submission
Submit a kotebook that performs a full EDA on this dataset or other datasets that you have available. This dataset can be combined with other data retrieved from [esportsearnings.com](https://www.esportsearnings.com/). 

Important:
1. The notebook should be well documented and contain references to the used data sources.
2. Please make the notebook as visually attractive as possible and include atleast one data visualization.

## Evaluation
This is not a formal competition but enables us to gain new insights into the booming esports industry. Results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory data analysis with insightful conclusions.

### Further help
Feel free to use the following questions as inspiration for your EDA:
- Which three countries/cities are the most popular location for large prize pool esports tournaments?
- What percentage of large prize pool esports tournaments is played online?
- Which year was the combined total USD prize pool the highest?
- Are prize pools for team based esports tournaments higher that individual tournaments? (Hint: Gameplay boolean)
- What percentage of tournaments puts the year in the tournament title?
- How long does the average high prize pool tournament last?

Thank you for participating!","","Exploratory Data Analysis (EDA)","","","0"
"3118","1074776","3837716","01/03/2021 13:24:26","Its a regression data. So you can start by implementing various regression algorithms. For example:-
1. LINEAR MODEL
2. LIGHTGBMREGRESSOR
3. XGBREGRESSOR
4. XGBREGRESSOR WITH GRIDSEARCH CV USING KFOLD
5. SVM
6. XGBREGRESSOR WITH GRIDSEARCH CV USING KFOLD ==&gt; HANDTUNING","","PREDICTING OPEN PRICE FOR EACH COMPANY","","03/31/2021 23:59:00","0"
"4397","1342852","3841073","05/15/2021 19:25:52","## Task Details
Get insights from data 

## Expected Submission
Only R Notebooks are acceptable
## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exprplatory Data Anaysis Using R","","","0"
"2427","918140","3847685","10/13/2020 04:32:32","This dataset can be used for power Analytics, Forecasting or several other regression models. It has been used for one ahead step Forecasting using ARIMA.","","Power Analytics","","","1"
"4599","1379264","3862996","06/01/2021 16:29:40","Explore the variables available within the netCDF files and visualize NO2 concentrations over a map of Ireland.","","EDA and Visualization","","","0"
"4891","1426765","3867359","06/23/2021 06:47:39","## Task Details

Hi, Recently I have encountered an interesting project from a startup windmill company. The core of the project is the application of a secondary braking system that is purely based on predictive analytics. We have stumbled upon a situation when we are asked to predict the possibility of an increase/decrease in speed of rotation at a given time. We know that it gets affected by external factors like wind speed, wind direction, and rotor size, etc. But they're willing to find it only through the speed variation data because the primary braking system senses environmental conditions and applies to the brake to make the blades rotate in a controlled/safe optimized range. I personally believe with their R&D department that we don't need external sensors to get more environmental data because there is a chance that the speed variation data alone have innate required features to predict the decrease or increase in speed in that controlled range.

Data Description:
*Industry Safety range 40m/s to 60m/s
*The safety range is handled by a primary braking system that maintains the speed at optimal 50-55m/s.
*Data given is the normalized speed variation in this primary optimal range at a time interval of 10 minutes.

Help me whether the speed variation is predictable or not? or can we predict drastic changes with an optimized model?

## Expected Submission
Probability of increase/decrease in speed for at least 10% of the dataset.

## Evaluation
70% Train 30% Test.","","Need to know the predictability of Increase or decrease in Windmill Rotor Speed.","","","1"
"2379","911634","3885917","10/08/2020 11:55:53","## Task Details
Perform sentiment analysis of reviews given to the products using reviews and ratings.","","Sentiment Analysis","","","8"
"2419","917800","3890590","10/12/2020 17:54:12","I have uploaded Datasets from all the different categories available on Udemy. With each category having around 20 thousand courses. This data can be used to get various insights regarding all the available courses on Udemy, price insights, popularity and lots of other details.","","Exploratory Data Analysis for Beginners","","","0"
"2606","951249","3890590","11/02/2020 05:56:51","I have uploaded Datasets from all the different categories available on Udemy. With each category having around 39 thousand courses. This data can be used to get various insights regarding all the available courses on Udemy, price insights, popularity and lots of other details.

Also, using the other datasets of all the categories that I have uploaded, comparative analysis can be performed to get the different metrics of all the categories","","Exploratory Data Analysis for Beginners","","","0"
"2383","912002","3902810","10/08/2020 17:14:15","## Task Details
Your client is a large MNC and they have 9 broad verticals across the organization. One of the problems your client is facing is around identifying the right people for promotion (only for the manager position and below) and prepare them in time.

Now, The task is to predict whether a potential promotee at a checkpoint in the test set will be promoted or not after the evaluation process.

## Expected Submission
The solution file should contain two columns ""employee_id"" and ""is_promoted"".

## Evaluation
The evaluation metric for this competition is the F1 Score.","","HR Analytics challenge","","","0"
"3325","1126527","3913463","01/29/2021 18:48:00","Analyze the text of Trump's tweets.","","Sentiment Analysis / Natural Language Processing","","","2"
"4073","1271473","3919261","04/14/2021 14:00:06","it is a good way to measure all the things with data. this one 1 phase","","meterflow","","04/30/2021 23:59:00","0"
"4065","1267155","3919261","04/13/2021 14:32:06","find the way to get the 2 phase result","","nnFlowmeter","","04/30/2021 23:59:00","0"
"4162","1219348","3925987","04/23/2021 07:19:10","## Task Details

While the previous FridayDataSetTask episodes were about ""identification"", this one is actually a step within a project, and focused on ""association"", i.e. using the link within the dataset to ""seed"" a NLP mini-project.

In this case, I am actually sharing the concept, but already did both this step and some of the ensuing NLP steps.

So, each FridayDataSetTask episode in this ""string"" will actually share some steps and some concepts.

The concept? Whenever preparing a workshop for a brainstorming, it happens often to have to collect contributions, and then prepare a synopsis to share with all the participants, checking after the discussions if all the contributions have been considered.

Doing that manually is extenuating- also when you have few thousand pages less than those covered by this set of documents.

This first step is apparently simple (retrieve documents using the link within the dataset), but, as there are hundreds of documents for thousands of pages across a dozen of sessions:
1. retrieving and storing documents is the simplest part of the task
2. before writing a single line of code, you should explore the documents and dataset
3. after exploring, you should first define a ""storing and classification approach"" that makes possible to access information
4. access to information should be both to documents as they are, and to their content
5. this all should enable also cross-document analysis.

Obviously, as stated above, I already did all the steps in this task.

But, before sharing later on this year my own ""solution"", I would like to share the approach and see if others can come with other solutions.

As I wrote above: this approach on this documentation is actually derived from prior experience on other similar yet different activities that involved collecting documents- be it to define a new initiative, define a budget, change the organizational structure (and culture) or a company, design a new (software and service) product, carry out vendor evaluations and audit, or presenting/discussing a roadmap or the results of an initiative.

For more details on the documentation and the context of this dataset, please visit its documentation.

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask04 : PNRR proposals - society and politics - part 1","retrieving documents as per links, and save each as a new file","","0"
"4290","1219348","3925987","05/07/2021 06:32:13","## Task Details

Continuing on the PNRR series started with the previous [FridayDataSetTask04 : PNRR proposals - society and politics - part 1](http://robertolofaro.com/DataSetTask_20210423), this is another step within the same project, and focused on ""extracting information"", i.e. extracting and storing contents from hundreds of Acrobat documents linked within the dataset this task is attached to.

Each FridayDataSetTask episode in this ""string"" will actually share some steps and some concepts.

In this case, the key issue is simple: each Acrobat was produced by a different source using different software packages and following their own design guidelines.

The sources ranged from text files that had been converted into Acrobat files, to the usual Microsoft Office or GoogleDoc, to more exoteric applications (some resulted in Acrobat files that are anything but seen as ""standard"").

This task is therefore simple: find a Python library (or set of libraries) that can extract the text from the Acrobat files, and [store it in a way that is consistent with the ""storing and classification approach"" that you identified in the previous task](http://robertolofaro.com/DataSetTask_20210423).

Obviously, as stated above, I already did all the steps in this task.

For more details on the documentation and the context of this dataset, please visit its documentation.

To follow-up on the evolution of the PNRR and associated documentation, you can also [follow the associated GitHub repository](https://github.com/robertolofaro/pnrr).

## Expected Submission

I plan to release some material by June 2021, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask05 : PNRR proposals - society and politics - part 2","extracting and storing contents from documents","","0"
"4633","1018662","3925987","06/04/2021 06:54:01","## Task Details

While the previous task was on text visualization, this one is on data visualization using maps (and geopandas).

This dataset was created as a support for future publications (and webapps) based on data.

Since the 1980s, when I designed and delivered my first Decision Support System model, one of the first requests from the business side was to be able to interact with data.

And, as my first models were for senior management and other ""systemic view"" (e.g. financial/operations controller) managers, more often than not this meant: territories.

In this case, could be political territories, i.e. countries, as represented within this sample conversion, containing the map of Italy (as a conversion using open source tools from SHP files into CSV).

This dataset is part of a series that contains three other datasets:

* [UN SDG - EU 27 sample datamart (subset for 7 KPIs)](https://www.kaggle.com/robertolofaro/sdgeu-datamart-sample)
* [Selected Indicators from World Bank 2000-2019: 33 indicators from World Bank website for a data project](https://www.kaggle.com/robertolofaro/selected-indicators-from-world-bank-20002019)
* [EU 27 Energy sources - consumption 1990-2018 / energy production, import, export by source - consumption by sector](https://www.kaggle.com/robertolofaro/eu-27-energy-sources-consumption-19902018)

While you can extract other maps from the sources (see the description of the dataset for the links), you could also ""go creative"" on the concept of ""territory"" and ""patterns"".

Example: creating new maps based on specific patterns within the KPIs, a kind of [Gerrymandering](https://en.wikipedia.org/wiki/Gerrymandering) that could identify territories where, e.g. specific initiatives could have more chances of success than others.

Tailoring public incentives and business initiatives based on data is still a matter of art- not because we lack the means, but we lack motivation.

In both cases, political choices are involved- hence, a purely data-based approach might be useful to identify patterns, but not to make choices.

## Expected Submission

This is the last FridayDataSetTask of this series- more will follow in the future.

Until the beginning of the next series, I plan to release, and then identify if I should create other datasets, or implement the current dataset (as an example, I store locally also the press releases, but are not shown on the online database)

Anyway- if you post Notebooks suggesting other areas of development, I will see to include / amend further requirements, KPIs, etc

## Evaluation

I am interested (and working on) what can be repeatable, not linked to a single, specific event (albeit once in a while I use the database also to support my publications focused on specific events or ideas)

### Further help

As I am on a learning track since March 2020, I will routinely revise this section of this task, to add links that I found useful for the purposes stated above","","#FridayDataSetTask07 : Visualize datasets in maps","use the KPIs datasets to ""map"" patterns","","0"
"3131","1078015","3944866","01/04/2021 23:50:21","Task Details
Add visuals for geographical location and spread prediction, with RNA and protein mutation hits by timeline. Build models that predict spread by region, by strength or emphatic protein expression and RNA nucleotide host adaptation and / or mutation.","","Data exploration, EDA","","","0"
"5881","930779","3952964","08/26/2021 06:28:03","## Task Details
Develop emotion Recognition model using the data","","Emotion Recognition","Develop emotion Recognition model using the data","","1"
"3966","1251960","3955584","04/05/2021 08:46:51","## Task Details
Since the data is cleaned already, Apply ML models on the dataset and make sure to find some better accuracy without being biased","","Applyin ML models on Reasons of Drug Addiction In Bangladesh","","","0"
"2537","938711","3969026","10/25/2020 10:58:28","## Task Details
This task is aimed at improving your skill","","Use this dataset to create a ml model to classify between cats and non-cats","","","0"
"4644","1389920","3974712","06/05/2021 07:48:21","## Task Details
The problem here is the need of recommendation or prediction of ' job type ' and ' category ' from the job's description available to us in form of text.","","Multi Class Text Classification","Prediction Job Type and Category using Job Description","","0"
"4859","1421008","3974712","06/20/2021 13:34:30","The Task

To produce a computational framework capable of performing fully automatic segmentation of the LA cavity from 3D GE-MRIs without any manual assistance. The participants will receive 30 data+masks to develop their approach and be evaluated on 54 different test data for evaluation. The participants will submit the masks of the LA cavity for the 54 patients. The evaluation will be done by comparing the submitted masks for the test data the with their manually segmented masks (not open to the public). The test data (data only) will be released 2 weeks prior to the deadline of the challenge so participants can submit their predicted masks. This challenge provides a chance for participants to carefully study and experiment on a large GE-MRI dataset, and further push the state-of-the-art performance for atria segmentation.","","Produce a computational framework !!","Automatic segmentation dev","","1"
"4207","1227243","3982608","04/27/2021 23:03:27","# Age prediction
Create an image classification model for age prediction within the classes in the dataset. Should be classification, NOT regression. Any model architectures are ok.","","Age Prediction with high accuracy.","Create an image classification model for age prediction within the classes in the dataset.","","0"
"3480","1145431","3984132","02/13/2021 08:59:06","Create a content based Recommendation System where headlines will be recommended based on the similarity of the Article's Content.","","Recommendation System","","","0"
"3481","1145431","3984132","02/13/2021 09:01:10","Create an Abstractive Summarizer which summarizes the articles and compare that summarization against the summary present.","","Abstractive Summarizer","","","0"
"2898","1025342","3988302","12/09/2020 16:04:44","## Task Details
Create a fantasy XI for the BBL.","","Create a Best BBL XI","","","0"
"2821","990900","3988302","11/30/2020 07:28:51","## Task Details
Cricket contains some really good batting and bowling statistics which indicate a player's performance in a match, season or just his whole career. Tweak these statistics using the dataset and create a fantasy dream team for the whole tournament and one for just the 2020 season.","","Create a Fantasy IPL Dream Team","","","21"
"2752","990900","3988302","11/23/2020 07:35:57","## Task Details
In cricket, a batsman's batting position is an indicator of how good a batsman is. In every inning, a batsman has his own unique batting position. The task here is to assign this batting position to each batsman in an inning accordingly.

## Expected Submission
The submission must be a dataset created with the help of Python/R with columns indicating the Match ID, Inning, and all batsmen with their own batting posiiton.","","Assign Batting Position to each batsman in a match","","","29"
"2990","990900","6382270","12/20/2020 08:01:15","Help teams decide what should they opt for upon winning toss on a specific venue given the historic data of the venue.","","Help Opting Decision","","","2"
"3595","990900","5556999","02/25/2021 18:27:50","## Task Details
IPL is the game of 4's & 6's. It becomes more curious to know on which balls batsman hit four or six in an inning and resulted score at those balls.

## Expected Submission
A solution is a notebook with a scatter plot showing all the fours & sixes (with two different colors) in a chosen inning (match id & inning number as input). 
Note- x-axis labels the balls and y-axis resulted score.

## Evaluation
On hovering four & six balls on scatter plot shows the tuple (ball number, score) & type of shot hit.","","Four and Six hit balls in an inning","","","1"
"4144","990900","5378011","04/21/2021 14:58:28","## Task Details
Create a condition depicting the runs scored and wickets lost in powerplay.

## Evaluation
Result most closed to result showed on Google","","Runs Scored and wickets lost in Powerplay from Season 2008-2020","","","2"
"4103","1278055","3988302","04/17/2021 16:59:43","## Task Details
Using the historical data, predict who can become the player of this season.","","Predicting the Player of Tournament","","","1"
"5400","1424328","3991483","07/28/2021 18:39:12","Perform data analysis.","","Analyze the data","","","0"
"5401","1424328","3991483","07/28/2021 18:39:58","Predict HCV infection from blood parameters
Use any model of your choice.","","Predict HCV infection from blood parameters","","","0"
"4105","1278174","4002632","04/17/2021 20:09:35","Creation of the most autonomous and efficient AutoML algorithm","","Improvement of AutoML algorithm","","","1"
"3839","1226967","4002632","03/22/2021 19:24:24","Analysis of the content of posts about Data Science on Reddit to study trends and their changes over time. 
Post similarity analysis.","","Reddit Data Science posts analysis","Analysis of the content of posts about Data Science on Reddit.","","1"
"3840","1226967","4002632","03/22/2021 19:33:56","Who doesn't love pretty graphics? Let's create a lot of beautiful and informative charts that illustrate the Reddit Data Science Posts data well.","","Let's create many beautiful visualizations!","Let's create many beautiful visualizations!","","1"
"3850","1226967","4002632","03/23/2021 16:20:44","We have the classic task of natural language processing - predicting the potential popularity of posts on Reddit by its title and text. The predicted variable is the ""Score"" column - the ratio of likes to dislikes for a post. The columns ""title"" and ""post"" can be used as predictors for RNN.","","Reddit Data Science posts popularity prediction","Predicting the potential popularity of posts on Reddit by its title and text","","1"
"3125","1077025","4002632","01/04/2021 15:19:47","Creation of the most understandable and detailed analysis.","","Analysis and Visualization","","","1"
"3743","1206972","4010658","03/12/2021 13:29:27","## Task Details
Natural Langiage Understanding Engine and Chatbot

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create an NLU system","Intent Recognition System","09/12/2025 23:59:00","0"
"3752","1206972","4010658","03/13/2021 18:32:36","## Task Details
Let's Chat up
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Chatbot Engine","Replicate a Chatbot","","0"
"3796","1207011","4010658","03/17/2021 05:16:59","## Task Details
Fake news

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Identify Fake News","","","0"
"2751","990948","4010658","11/23/2020 07:32:17","## Task Details
NLU Engine

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Intent Recognition NLU","","","0"
"2753","990983","4010658","11/23/2020 07:49:12","## Task Details
NLU Engine
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Intent Recognition NLU Engine","","","0"
"2754","990994","4010658","11/23/2020 07:51:06","## Task Details
NLU Engine
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NLU Engine","","","0"
"2519","935560","4010658","10/23/2020 08:38:03","## Task Details

NLU Engine for a Chatbot
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NLU Engine","Create an NLU Engine","","3"
"3211","1102323","4022661","01/17/2021 15:17:47","## Task Details
1. Acquire data regarding the availability of Built-to-Order (BTO) flats in Singapore for each year. This will likely influence the prices of the resale flats.
2. Compute **travelling time** instead of manhattan distance between flats and amenities/city center as this gives a better approximation of how convenient the flat location is.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Explore the Dataset and Predict the Resale Prices!","Remember to enjoy and learn while doing it!","","0"
"4098","1277252","4036875","04/17/2021 10:01:01","## Task Details
Basic I/O for dataset listed","","Starter","","","0"
"3104","1067643","4042723","01/02/2021 12:58:28","The goal would be to determine which Policies are a cause of a reduction in propagation, and perhaps mortality.","","What government policy leads to a decrease in propagation ?","Determine leading Policy indicators of reduction in disease propagation.","","0"
"5087","1460811","4049504","07/11/2021 09:46:54","## Task Details
The small bowel constitutes the gastrointestinal (GI) tract‚Äôs mid-part, situated between the stomach and the large bowel. It is three to four meters long and has a surface of about 30 m^2, including the surface of the villi, and plays a crucial role in absorbing nutrients. Therefore, disorders in the small bowel may cause severe growth retardation in children and nutrient deÔ¨Åciencies in children and adults. This organ may be affected by chronic diseases, like Crohn‚Äôs disease, coeliac disease, and angiectasis,or malignant diseases like lymphoma and adenocarcinoma. These diseases may represent a substantial health challenge for both patients and society, and a thorough examination of the lumen is frequently necessary to diagnose and treat them. However, the small bowel, due to its anatomical location, is less accessible for inspection by Ô¨Çexible endoscopes commonly used for the upper GI tract and the large bowel. Since early 2000, video capsule endoscopy (VCE) has been used, usually as a complementary test for patients with GI bleeding. A VCE consists of a small capsule containing a wide-angle camera, lightsources, batteries, and other electronics. The patient swallows the capsule, which then captures a video as it moves passively through the GI tract. A recorder, carried by the patient or included in the capsule, stores the video before a medical expert assesses it after the procedure. [Source of information: Reasearch paper: Kvasir-Capsule, a video capsule endoscopy dataset]

Artificial intelligence (AI) is predicted to have profound effects on the future of video capsule endoscopy (VCE) technology.The potential lies in improving anomaly detection while reducing manual labour. However, medical data is often sparse and unavailable to the research community, and qualified medical personnel rarely have time for the tedious labelling work.

## Expected Submission
You are required to build a machine learning model to recognize the disease label of the respective images.

## Evaluation
Submissions are evaluated using Weighted F1 Score.

## What Skills would you gain?
- CNN
- Dealing with Class Imbalance in case of Image data","","Medical Image Classification","Working with class imbalance on Image Data","09/30/2021 23:59:00","0"
"4619","1386893","4050035","06/03/2021 18:27:13","what is the frequency of ""total_cases"", ""new_death_smoothed"", ""new__tests"",  ""new__death_smoothed__per__thoudsand""?","","Visualize the Frequency in Description","","","1"
"4620","1386893","4050035","06/03/2021 18:29:20","what Co relation Between field ""new_cases"" and ""new_cases_permillion"" through graph ?","","Co relation Between field ""new_cases"" and ""new_cases_permillion"" through graph.","","","0"
"4621","1386893","4050035","06/03/2021 18:31:27","What is the Frequency of ""total_cases_per_million""   ? visualize it through graph.","","Co relation between ""total caes per million"" and ""new_tests_per_thousands""""","","","0"
"4623","1386893","4050035","06/03/2021 21:20:03","How many Outlier appear when we co-relate the ""new_test_per_thousand"" with ""new_tests_smoothed."" Hint : there are 5 outliers","","How many Outlier appear when we co-relate the ""new_test_per_thousand"" with ""new_tests_smoothed.""","","","0"
"4624","1386893","4050035","06/03/2021 21:29:41","Dig deep in the dataset and do EDA at your own","","Explore the dataset at your own find some new insights.","","","0"
"3955","1250869","4078789","04/04/2021 15:39:03","Features in data:-
**Area name:** Name of the area that data is collected.
**Latitude and longitude:** Coordinates of the area.  	
**Number of houses:** number of houses constructed in a row and its opposite row.
**Total plots: **total number of plots empty as well as constructed.
**Area density:** area density is given by ratio of number of houses to total number of plots.
 **Area density** :number of houses/total number of plots
This will give a value in range of 0 to 1. 
1 means there is no empty space, 0 means no houses are constructed.
Average age: it is average age of number of houses feature
Material used: what kind building blocks are used to construct a house(Stones or Bricks).
**No. of old homes(above&gt;25 years):** number of old houses which are above 25 year old. By this we can tell that, if old houses
are more then construction chances are high.
**House Type:** type of houses in the area or construction type(normal type, modern type or simple)
**Type: **new&nbsp;home&nbsp;means&nbsp;construction&nbsp;in&nbsp;open&nbsp;plots,&nbsp;Renovation&nbsp;means&nbsp;renovation&nbsp;or&nbsp;complete&nbsp;reconstruction&nbsp;of&nbsp;house
**Construction chances:** the variables are High, moderate and low. 
**Categories:**- Constriction chances categorized as High, moderate and low.
High:- Area density must be less than 0.5 & Average age must 0 ‚Äì 10 or 20 above .(on avg 3 houses a year).
**Moderate:-** Area density must be between 0.51 to 0.8 & Average age must be about 10 to 20.(on avg 1 to 2 houses a year).
Low:- Area density must be above 0.8 & Average age must be about 10 to 20.(on avg less than 1 houses a year).","","predict construction chances in particular area in next 5 years.","","","2"
"3383","1135529","4085264","02/03/2021 06:54:45","## Task Details
you will identify the modulation classes in the given data sets. This is a supervised learning problem. There will be 8 modulation classes sent over an impulsive noise environment. The challenge is to design the best classifier.

## Expected Submission
The code must be submitted via accuracy of 99% and will be carefully investigated.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","you will identify the modulation classes","","","0"
"4436","1351139","4088703","05/19/2021 03:59:23","The data set is an augmentation of the Stanford Cars dataset, with data split into train, validation and test.

A csv file is to be created, containing image name, subfolder name (whether train, validation or test). All images sizes to be mentioned.","","Metadata Creation","Folder image counts, image sizes","","0"
"3889","1237036","4095338","03/28/2021 07:58:07","Extend dataset by adding  alphabets like a b c d x y z for solving linear and polynomial equation.","","Add alphabet","Extend dataset","","1"
"4670","1394927","4130755","06/07/2021 17:03:03","This dataset is about the grinding machines and the factor affecting the part condition","","Develop Classification model with target as ""State"".","Find Factors affecting the 'State'. Remove Outliers from the data set","","0"
"4698","1394927","4130755","06/09/2021 10:26:25","Analyse the reason that influence the Grind_CT","","Deploy regression model","Analyse the reasons (feature importance) for 'Grind_CT'","","0"
"3691","1198571","4165774","03/08/2021 02:37:35","## Task Details
Given the 2019-2020 and the 2020-2021 stats, who do you have for most improved player?

## Expected Submission
I expect users to load in the datasets in a notebook and perform some data analysis and visualizations to see if they can find outlier from the masses.

## Evaluation
A good solution looks at several metrics and provides visualizations to support their findings. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Who is your most improved player and why?","","","1"
"2610","951506","4166890","11/02/2020 09:28:20","Is there a pattern to the numbers and when they appear in the data, can we predict if value N (some unknown integer) will have it's position swapped if it is multiplied by 0.75","","Is there a pattern to the numbers?","","","0"
"2475","926947","4181516","10/18/2020 06:08:05","**Task 1:** The Marketing Team has launched a campaign recently and they want to understand how well their campaigns are able to attract new customers.
Show the New Customer Acquisition on a trend line. During which period did new customers grow the most. During which period did it flatten?

**Task 2:** The business wants to calculate profitability per business day.
They want to know how many days each month has been Profitable and Unprofitable.

**Task 3:** The Customer Loyalty team wants to run some special campaigns for customers. But they want to look at Cohorts. For example - Cohort 2011 are people who purchased the first time in 2011, Cohort 2011 are people who purchased the first time in 2012 and so on. Which Cohort should they run the campaign with, and why?

**Task 4: **Create a Sales Forecast for the next 2 years.

**Task 5:** Which are the Top 5 countries that have shown the best Sales CAGR?
The Strategy team feels that if we remove the Bottom 10 Percentile transactions -it will change the Top 5 countries ranking.
Validate this. 

**Task 6:** The company wants to determine quantitatively which customers are best by determining their recency, frequency and monetary values. (Hint: Run an RFM Analysis, show all the steps).

**Task 7:** From the analysis done on the dataset, provide recommendations for the company to increase their sales and market share.","","7 crazy task to become market analyst","","10/21/2020 23:59:00","0"
"2531","927444","4205660","10/24/2020 13:36:44","Explore how people feel towards Facebook","","Sentiment Analysis","","","1"
"3908","1241617","4213037","03/30/2021 13:54:38","## Task Details
Try to understand if it's possible to make predictions on how much time will I probably spend on X machine while making Y part.

Which is the best sequence of machines to make a part in the minimum time possible?

## Expected Submission

## Evaluation


### Further help","","Predict the time that will it take to make a specific part","","","0"
"3887","1236593","4223639","03/27/2021 20:44:42","## Task Details
What makes jewelry expensive? Is it the gemstones, the material the carats? Which types of jewelry are the most expensive?

## Expected Submission
Use a Kaggle notebook to submit you analysis. If you go in the trouble of writing actual code I will most definitely be interested to read it.

## Evaluation
A scientific analysis with the most quantifiable results wins. Show me correlations, show me p-values and visualization. Show it to me ALL.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","What makes a piece of jewelry expensive?","Which factors influence the price of a piece of jewelry and how strongly are they correlated to it?","","0"
"4035","1262940","4223639","04/10/2021 15:58:05","## Task Details
A known phenomenon in Warframe is Power Creep. The phenomenon where upon all the newer weapons outshine the older ones in terms of power.
It feels like every new weapon and especially the Prime ones, is the next best weapon in the game. Power Creep is primarily measured by the damage per second (dps) of the newer weapons.
The task is to quantify and measure the Power Creep and to track its change over time.

## Expected Submission
The solution needs to provide a definition of how the Power Creep is calculated as well as it's change over time. Time can be inferred by the Update that any weapon was introduced in.

## Evaluation
The most scientifically sound calculation of the Power Creep effect wins. Props for a cool visualization of it's change over time.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Measure Power Creep","Can we scientifically quantify the Power Creep of the weapons of Warframe?","","0"
"3640","1188717","4223639","03/02/2021 13:11:46","## Task Details
Bikes are cool right? Let's figure out some interesting insights about them.

## Expected Submission
If anyone is interested in the task, they should post a notebook with their analysis.

## Evaluation
A good solution will be one that scientifically proves the insights discovered through the data and visualizes them in an appealing way.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","How much have bikes improved over time and in what aspects?","Analyze the data to measure the change in the cabilities of bikes over the years. Which aspect of bikes has changed more spectacularly?","","0"
"2492","929301","4224735","10/19/2020 15:14:23","Describe the crime severity by use of the given penalties and categories. You can connect this data set with another ones available in the Kaggle environment.","","Visualisation","","","0"
"3097","1072553","4228362","01/02/2021 09:18:45","Websites have Search Engine Optimization (SEO). YouTube videos should also have an algorithm optimization too, designed to maximize the click-through rate of a video. This task hopes to help YouTuber identifies what makes a video with as many views as possible, except for its content.

## Expected Submission
The solution should contain information about what contributes to a higher view / subscribers rate, and how should those features be formatted for YouTubers.","","What contributes to video clicks?","","","0"
"3547","1169977","4238900","02/19/2021 19:57:17","## Task Details

Investigate the patterns or the factors that favour a player to get sold.","","Factors VS Player Sell","Relation Between Factors and Player Sell","","0"
"3560","1169977","4238900","02/20/2021 20:05:59","## Task Details
There are missing values or blanks in the **Bought_By column**, fill it with the correct values i.e., Unsold or name of the team that bought the player.

## Expected Submission
Share the updated data.

## Evaluation
I will manually check a certain number of values, whether they are correctly filled.

### Further help
The data is available on https://www.espncricinfo.com/ and cricbuzz.com.","","Fill Missing Values in Bought_By Column","","","0"
"3561","1169977","4238900","02/20/2021 20:11:02","## Task Details
There are only 292 players that went into the 2021 IPL Auction, but currently, in the data, there are 294 entries.

## Expected Submission
Share the two names by any means, simply comment here or email me.

## Evaluation
I will validate if the suggestion is correct.","","Remove 2 Wrong Entries","","","0"
"3562","1169977","4238900","02/20/2021 20:33:43","## Task Details
Create a new column ""Playing_Role2"" which will have another level of playing role. For example, in the current column (Playing_Role) many are just batsmen but they could also be wicketkeepers. SImilarly for bowlers, they could also be bowling allrounder or batting allrounder.

An example:
KL Rahul
Playing_Role: Batsman
Playing_Role2: Wicketkeeper","","Create a new column ""Playing_Role2""","","","0"
"3409","1138568","4238900","02/04/2021 21:07:22","## Task Details
Analyse the tweets to find useful patterns.","","Exploratory Data Analysis","","","0"
"2661","961023","4265030","11/09/2020 16:34:35","## Task Details
It would give an example to other users who wants work on this data set in future.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Country details analysis","","11/16/2020 23:59:00","1"
"3918","1242901","4273652","03/31/2021 07:09:10","## Task Details
This task could help us understand what makes a Youtube channel more popular when compared to others.
Do there exists any underlying factors which affect the popularity of a channel on Youtube?


x","","Predict Number of Comments and Views for a channel","","","0"
"2407","914894","4281175","10/11/2020 12:53:35","## Task Details
1) To find the most significant and weighted parameter affecting the ranks of Universities 
2) To find the trend of rankings of past 8 years based on the parameters provided as columns in dataset.
3) To visualise the ranking rise and fall of a particular university with rankings as y- axis and years as x-axis. [Line Graphs]

## Expected Submission
1) The most correlating factor affecting the university ranks.
2) Reporting the trends of the rankings with explanation.
3) University with maximum and minimum gradients in their rankings and also the parameters affecting them.

## Evaluation
1) A good correlation with proof.
2) Best explaining report.
3) Gradient calculations and line graphs.","","Correlation between Rankings and Parameters","To find correlation between the rankings and the parameters like publications, citations, quality of staff,etc","10/31/2020 23:59:00","0"
"2780","997793","4285820","11/26/2020 07:00:48","## Task Details
In theory, as you grow older, you have more chances to try new foods. Can this hypothesis be confirmed using the data in this dataset? Can it be graphed?

## Expected Submission
This isn't a formal competition, so please use a Notebook to show all your thoughts and work used to reach your conclusion.","","Trying foods as you grow older","","","11"
"2807","1003997","4285820","11/29/2020 00:56:21","## Task Details
Do different demographics have different favorite colors?

## Expected Submission
Show the average favorite color of various demographic groups. Are any similar?","","Average favorite colors by demographics","","","1"
"2403","914946","4287302","10/10/2020 22:46:34","## Task Details
Currently there is a significant increase in confirmed COVID-19 cases numbers (as of today 10th October 2020) in the UK, can these numbers be accounted for by increased testing? Or is the situation actually deteriorating? If so hows does the current second wave compare to the first wave that started in March 2020?

## Expected Submission
Publish a Notebook of your findings. 

## Evaluation
There is no official evaluation, this task has been setup to see what the community can ascertain from officially available information and to allow Kaggles to show case their skill in Data Science!

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","COVID-19 Current UK Situation","","","0"
"2556","935914","4287302","10/27/2020 21:31:07","## Task Details
Perform sentiment analysis on the tweets that is available for the two presidency candidates.

## Expected Submission
Submit a notebook with the insights you found on the sentiment of the general population for each candidate.","","Perform Sentiment Analysis on the US Election Candidates","","","9"
"3446","1147479","4291460","02/09/2021 06:55:42","Create an image classification model to classify between 20 Classes","","Food Classification","","","1"
"3750","1208818","4312629","03/13/2021 15:12:44","Submission should contain a csv file with details of the selected employees","","Recommend best employees for a project on machine learning domain.","","10/01/2021 23:59:00","0"
"2871","1018513","4315881","12/06/2020 09:34:45","## Task Details
Which district is the best place for disabled tourists to travel?
Which district is the worst place for disabled toursits to travel?
Can we figure out which elements needs to be complemented to help disabled travel well?","","Evaluation of districts in respect of disabled-friendly tourism","","","0"
"2779","917769","4319258","11/26/2020 04:57:42","## Task Details
Determine the following facts:
- What mistakes I do mostly? 
- What move should I rather play?

## Expected Submission
How can I reach higher rating with wiser moves. Preferably by determining from the information given in the dataset.

## Evaluation
A solution that describes about the mistake I've been making constantly will be considered as best.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Getting better at Blitz first!!","","","0"
"3844","1227674","4325590","03/23/2021 05:45:19","## Task Details
This task is focusing on cleaning data to make it useful for upcoming tasks.

## Evaluation
* No duplicates
* No missing/invalid values
* Feature engineering","","Data Wrangling","Cleaning, Pre-processing, Transformations","05/01/2021 23:59:00","0"
"3384","1135661","4326262","02/03/2021 08:31:26","1. Mammography Dataset
2. Explore the Dataset
3. Model Test and Baseline Result
4. Evaluate Models
5. Make Predictions on New Data

There are two classes and the goal is to distinguish between microcalcifications and non-
microcalcifications using the features for a given segmented object. 
Ùè∞Ä Non-microcalcifications: negative case, or majority class.
Ùè∞Ä Microcalcifications: positive case, or minority class.","","Breast Cancer Classification","","","0"
"3369","1131585","4326262","02/01/2021 08:22:45","The assumption is that the task involves predicting whether a customer will pay back a loan or credit.

A total of 70 percent of the examples are good customers, whereas the remaining 30 percent of examples are bad customers.
Ùè∞Ä Good Customers: Negative or majority class (70%). 
Ùè∞Ä Bad Customers: Positive or minority class (30%).


A cost matrix is provided with the dataset that gives a different penalty to each misclas- sification error for the positive class. Specifically, a cost of five is applied to a false negative (marking a bad customer as good) and a cost of one is assigned for a false positive (marking agood customer as bad).
Ùè∞Ä Cost for False Negative: 5
Ùè∞Ä Cost for False Positive: 1
This suggests that the positive class is the focus of the prediction task and that it is more costly to the bank or financial institution to give money to a bad customer than to not give money to a good customer.","","Classification of Good & Bad Customers","","","0"
"5014","1167113","5377216","07/06/2021 02:19:25","## Task Details
Search engines exist. Why not create a custom search engine for your personal purposes?

## Expected Submission
- A website allowing you to effectively search for articles

## Evaluation
- Effective search mechanism (is it easy for me to find relevant articles?)
- Naive solutions (e. g. simple keyword lookup) are fine too","","Create an interactive Knowledge Base using the dataset","","","1"
"4172","1175444","4331575","04/24/2021 14:18:58","Evaluate various kinds of models on this dataset and find out which one performs the best and worst","","Evaluate various models","","","0"
"4190","1297126","4331575","04/26/2021 09:34:14","Performing a time series analysis and forecasting of various currencies and analyzing the trends among them","","Time Series Analysis and comparing various currencies","","","3"
"4458","1358431","4331575","05/21/2021 19:56:42","Doing a time series analysis and forecasting to figure out the trends in the data","","Time Series Analysis","","","1"
"4137","1127745","4333519","04/20/2021 13:41:09","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Alzheimer's Disease","Alzheimer's Classification","04/20/2022 23:59:00","2"
"4138","1025756","4333519","04/20/2021 13:50:38","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Breast Cancer Coimbra","Breast Cancer Coimbra Classification","09/10/2022 23:59:00","2"
"3410","1139394","4350332","02/05/2021 06:43:00","It can be difficult to navigate the logistics when it comes to buying art. These include, but are not limited to, the following:

Effective collection management Shipping the paintings, antiques, sculptures, and other collectibles to their respective destinations after purchase Though many companies have made shipping consumer goods a relatively quick and painless procedure, the same rules do not always apply while shipping paintings or transporting antiques and collectibles.

**Task**

You work for a company that sells sculptures that are acquired from various artists around the world. Your task is to predict the cost required to ship these sculptures to customers based on the information provided in the dataset.

**Data description**

The dataset folder contains the following files:

train.csv: 6500 x 20
test.csv: 3500 x 19
sample_submission.csv: 5 x 2

The columns provided in the dataset are as follows:

**Column name  	Description**
*Customer Id:*	Represents the unique identification number of the customers
*Artist Name:*	Represents the name of the artist
*Artist Reputation:* Represents the reputation of an artist in the market (the greater the reputation value, the higher the reputation of the artist in the market)

*Height:*	Represents the height of the sculpture
*Width:*	Represents the width of the sculpture
*Weight:*	Represents the weight of the sculpture
*Material:*	Represents the material that the sculpture is made of Price  Of Sculpture Represents the price of the sculpture Base Shipping Price Represents the base price for shipping a sculpture
*International:*	Represents whether the shipping is international
*Express Shipment:*	Represents whether the shipping was in the express (fast) mode
*Installation Included:*	Represents whether the order had installation included in the purchase of the sculpture
*Transport:*	Represents the mode of transport of the order
*Fragile:*	Represents whether the order is fragile
*Customer Information:*	Represents details about a customer
*Remote Location:*	Represents whether the customer resides in a remote location
*Scheduled Date:*	Represents the date when the order was placed
*Delivery Date:*	Represents the date of delivery of the order
*Customer Location:*	Represents the location of the customer
*Cost:*	Represents the cost of the order

**Evaluation metric
**score = 100*max(0, 1-metrics.mean_squared_log_error(actual, predicted))

**Result submission guidelines**

The index is Customer Id and the target is the Cost column. 
The result file must be submitted in .csv format only.
The size of this result file must be 3500 x 2.

Note: Ensure that your submission file contains the following:

Correct index values as per the test file
Correct names of columns as provided in the sample_submission.csv file","","Predict the cost to ship the sculptures","","03/06/2021 23:59:00","1"
"2951","1037070","4356057","12/15/2020 14:43:11","## Task Details
Train a model and Try to Generate Text based on the Style of Robert Frost. Believe Me you will be amazed the outcome.

## Expected Submission
Create a simple model. Try to understand the level of depth and automatically you will see how good the outcome and the results are.



### Further help
You can check out the Official Tensorflow and PyTorch documentation on Text generation.","","Text Generation","Creating Text Similar to the Style of Frost-ian scripts","","1"
"2952","1037070","4356057","12/15/2020 14:50:08","## Task Details
Modelling a simple mask model that can substantiate masks based on Frost-ian flow of words.","","Masking","Substantiate The Text Masking Based on the Works of Robert frost","","0"
"2953","1037070","4356057","12/15/2020 14:52:41","## Task Details
We all know that Robert Frost's Work Vary from being cold to joyful. Try to create a model that can enunciate how a certain work is depicting the mood.","","Sentiment Analysis","Predicting the Mood of a Certain Poem/Short Story","","0"
"2494","930122","4357049","10/20/2020 06:22:20","Do some analysis and visualize the dataset.
Thanks","","Analysis of dataset","","","0"
"2451","923298","4357049","10/16/2020 17:33:22","Using this dataset we should recommend post id to the user based on which approach you will use.

DO Share and upvote please if you like.
and make notebook of that too.","","Make Recommender System of recommending Post Using this Dataset","","","0"
"4095","1276684","4363905","04/17/2021 04:38:19","## Task Details
Detect if the image contains bread and if so, draw a bounding box around it!","","Bread-Tection!","Detect Bread and Draw Bounding Boxes","","0"
"4292","970311","7123760","05/07/2021 09:00:46","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
COVIDx CT-1","","COVIDx CT-1","COVIDx CT-1","05/31/2021 23:59:00","0"
"3948","1233246","4377569","04/04/2021 00:50:35","Create a CNN model to classify the dataset images.","","Image classification","","","0"
"3617","1185811","4377983","02/28/2021 18:50:53","Health Insurance Lead Prediction
==================================
Your Client FinMan is a financial services company that provides various financial services like loan, investment funds, insurance etc. to its customers. FinMan wishes to cross-sell health insurance to the existing customers who may or may not hold insurance policies with the company. The company recommend health insurance to it's customers based on their profile once these customers land on the website. Customers might browse the recommended health insurance policy and consequently fill up a form to apply. When these customers fill-up the form, their Response towards the policy is considered positive and they are classified as a lead.

Once these leads are acquired, the sales advisors approach them to convert and thus the company can sell proposed health insurance to these leads in a more efficient manner.

Now the company needs your help in building a model to predict whether the person will be interested in their proposed Health plan/policy given the information about:

Demographics (city, age, region etc.)
Information regarding holding policies of the customer
Recommended Policy Information


Data Dictionary
Train Data
Variable	Definition
ID	Unique Identifier for a row
City_Code	Code for the City of the customers
Region_Code	Code for the Region of the customers
Accomodation_Type	Customer Owns or Rents the house
Reco_Insurance_Type	Joint or Individual type for the recommended insurance  
Upper_Age	Maximum age of the customer 
Lower _Age	Minimum age of the customer
Is_Spouse	If the customers are married to each other
(in case of joint insurance) 
Health_Indicator
Encoded values for health of the customer
Holding_Policy_Duration	Duration (in years) of holding policy (a policy that customer has already subscribed to with the company)
Holding_Policy_Type
Type of holding policy
Reco_Policy_Cat	Encoded value for recommended health insurance
Reco_Policy_Premium	Annual Premium (INR) for the recommended health insurance
Response (Target)	0 : Customer did not show interest in the recommended policy,
1 : Customer showed interest in the recommended policy

Test Data
Variable	Definition
ID	Unique Identifier for a row
City_Code	Code for the City of the customers
Region_Code	Code for the Region of the customers
Accomodation_Type	Customer Owns or Rents the house
Reco_Insurance_Type	Joint or Individual type for the recommended insurance
Upper_Age	Maximum age of the customer 
Lower _Age	Minimum age of the customer
Is_Spouse	If the customers are married to each other
(in case of joint insurance) 
Health_Indicator
Encoded values for health of the customer
Holding_Policy_Duration	Duration (in years) of holding policy (a policy that customer has already subscribed to with the company)
Holding_Policy_Type
Type of holding policy
Reco_Policy_Cat	Encoded value for recommended health insurance
Reco_Policy_Premium	Annual Premium (INR) for the recommended health insurance

Sample Submission
This file contains the exact submission format for the predictions. Please submit CSV file only.

Variable	Definition
ID	Unique Identifier for a row
Response	(Target) Probability of Customer showing interest (class 1)


Link: https://datahack.analyticsvidhya.com/contest/job-a-thon/","","Help me to Improve this(presented) Solution","Improving the Accuracy of Solution","","0"
"4469","1323362","4378523","05/23/2021 14:28:43","![Nuclei image to segment](https://github.com/espSiyam/Deep-Learning/blob/master/mask/2.PNG?raw=true)

![Segmented Example](https://github.com/espSiyam/Deep-Learning/blob/master/mask/1.PNG?raw=true)","","Segmentation","Predict mask of nuclei using unet","12/23/2022 23:59:00","0"
"4340","1330744","4382914","05/12/2021 06:13:40","## Task Details

The user should pick a task (classification, entity-extraction, ...) and then prepare a study comparing the different variants in the dataset. 

## Expected Submission

Submission can be done either as a Notebook, Dashboard or a interactive report ( such as a [**Weights and Biases Report**](https://wandb.ai/site/reports) )

## Evaluation

The Submissions will be assessed on the basis of their completeness and presentation.","","Ablation Study to compare performance among the different Variants","","","0"
"4358","1335671","4382914","05/13/2021 05:48:49","## Task Details
The user should pick a task (classification, entity-extraction, ‚Ä¶) and then prepare a study comparing the different variants in the dataset.

## Expected Submission
Submission can be done either as a Notebook, Dashboard or a interactive report ( such as a [Weights and Biases Report](https://wandb.ai/site/reports) )

## Evaluation
The Submissions will be assessed on the basis of their completeness and presentation.","","Ablation Study to compare performance among the different Variants","","","0"
"4506","1369875","4382914","05/27/2021 03:03:28","## Task Details
The user should pick a task (classification, entity-extraction, ‚Ä¶) and then prepare a study comparing the different variants in the dataset.

## Expected Submission
Submission can be done either as a Notebook, Dashboard or a interactive report ( such as a [**Weights and Biases**](https://wandb.ai/site/reports) Report )

## Evaluation
The Submissions will be assessed on the basis of their completeness and presentation.","","Ablation Study to compare performance among the different Variants","","","0"
"2663","953803","4383161","11/09/2020 18:31:07","## Task Details

This task aims to detect speech activity in a mixed audio signal, containing speech, noise
and silence. The basic idea is to discriminate speech and other types of sounds (noise, silence, music, etc) by measuring audio signal parameters, such as short-time ZCR (zero-crossing rate), short-time amplitude, F0, etc. The first method consists in choosing some threshold values for such parameters, whereas the second methods consists in using neural networks to classify frames into speech/non-speech, using the speech parameters as input features. The final system should be able to display an audio signal and tag each frame as speech/non-speech. 


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
The submission should be a Notebook containing a pipeline to tag frames.","","Speech activity detection","","","2"
"3861","1232267","4396359","03/25/2021 13:00:53","## Task
You can classify tags based on the question asked. You are free to use any other resources for this dataset.","","Multi-label tags classification","","","0"
"3909","1241641","4399067","03/30/2021 14:05:59","As you see the data is tidy and useful.
You can use all of variables except price to predict price value.

We wonder which algorithm you will use and what will you get as a result. 

Don't forget share your result values, please. 

What makes a good solution? How do you evaluate which submission is better than another?

All of us can help you, don't worry about tiny issues.","","Price prediction","","","1"
"4613","1384726","4399136","06/02/2021 17:00:15","## Task Details
The given dataset has more than 150 columns. Extract useful information crossing data.

## Expected Submission
Submit a Notebook presenting the process of conversion of the Dataset.

## Evaluation
The fewer columns and more data crossed to get more useful data, the better.","","Data Cleaning","Select data and create new columns with crossover information to simplify the dataset.","","0"
"4614","1384726","4399136","06/02/2021 18:00:33","## Task Details
The present Dataset has a lot of useful information about the matches, teams, and players. Among them, would there be crucial data to identify which team would win?

## Expected Submission
Submit a Notebook with the process of creating a Machine Learning model to predict the winning team

## Evaluation
To evaluate your submission. I recommend the questions:
 - What is your model accuracy?
 - The data used in your model can be obtained before the match?","","Machine Learning","Creating a model to predict winners in possible matches","","0"
"2829","1009629","4403646","12/01/2020 12:23:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
This data will help you understand concepts of Feature Selection and How to handle an Imbalanced Dataset.","","Detect Anomalies In water for your projects","","12/27/2040 23:59:00","0"
"5847","1033829","5276714","08/22/2021 17:04:14","## Task Details
For this task you have to predict the price a diamond will sell for.

## Expected Submission
To complete this task you must use the train/test sets provided in the example notebook to train your model and predict test set values.

## Evaluation
The evaluation metric is RMSE","","[REGRESSION] Predict the price a diamond will sell for","Regression Task","","1"
"4253","1309565","6010122","05/03/2021 09:29:00","## Task Details
Visually showing the impact of forest area changes across the different countries or continents from 1990 to 2015.","","How does the net forest area change across the world from 1990 to 2015?","Exploratory Data Analysis","","1"
"2672","966538","4419900","11/10/2020 20:06:29","How to load and data exploration","","Create starter notebook","","","1"
"3300","1124722","4422790","01/28/2021 17:07:52","1) Predict the most profitable product that the company must keep manufacturing.
2) Use Logistic regression to determine whether Hamburg is a good destination for exporting more than 100 units of any products.
3) Implement linear regression for ‚ÄòQuantity‚Äô vs ‚ÄòUnit rate in FC‚Äò.Analyze this model
for overfitting and implement a solution using Ridge regression.
4) Classify the data given into 5clusters. Analyze and determine the best attribute
that can be used for clustering in the given dataset.","","The export data for a company has been provided","","01/31/2021 23:59:00","1"
"3160","1087748","4428441","01/09/2021 12:39:26","The file contains lot of comments  from reddit(A forum based App), the main goal here is EDA and create a predictive model to predict Upvotes. Which makes this a regression Problem.
Word2Vec approach to this problem is quite intersting.

Any new ideas are welcome in the Discussion Thread.

**Best Of Luck!**","","Predict Upvotes on a Given Reddit Comment","","01/30/2021 23:59:00","0"
"4849","1416633","7526686","06/19/2021 18:41:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Can you scrap the complete data innings by innings of players like this?","","Complete data","","","0"
"3399","1137308","4435858","02/04/2021 05:26:30","## Task Details
In this task,## Task Details
In this task, I want to find out how mindfulness affects the mental state of person especially his/her stress level through correlation analysis or topic modeling or semantic analysis

## Expected Submission
To find the important research topics related to mindfulness
How mindfulness affect depression/stress on people gender-wise/location-wise
how to evaluate?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Association between mindfulness and stress or depression","Insights on mindfulness and depression in people","03/06/2021 23:59:00","0"
"3448","1147827","4435864","02/09/2021 11:55:23","## Task Details
Apply style transfer to convert any given image of real face to ""Riga face""","","Riga Face Style Transfer","","","0"
"3449","1147827","4435864","02/09/2021 11:57:11","## Task Details
Train a model capable of generating new ""Riga Faces"".

### Further help
You might try few-shot transfer learning on existing face GANs.","","Riga Face GAN","","","0"
"3870","1233562","4435864","03/26/2021 12:17:04","## Task Details
Riga Data Science Club is building online presence using LinkedIn company page. We want to make our efforts more intelligent and effective:
* What are the most successful posts? 
* What do they have in common? 
* What makes them different from the rest? 

## Expected Submission
We are open for any effort that gives the answer to the questions stated above - could it be a simple EDA or some NLP topic model.

## Evaluation
Ideally we would like to have a model capable of predicting engagement given post body. Target metrics: Impressions and Engagement rate.

### Further help
Topic modelling [notebook](https://github.com/algerza/neobanks-topic-modelling/blob/master/b_topic_modeling.ipynb) by our club member, [Alvaro Ager](https://www.linkedin.com/in/alvaroager/)","","How to post on LinkedIn for success?","","","0"
"3871","1233562","4435864","03/26/2021 12:31:44","## Task Details
The home of Riga Data Science Club is our Slack workspace. We are having a strong influx of new members, but not so many have become really active. We want to find out:
* What are the TOP active members?
* What is the mean activity lifespan of a member before becoming inactive?
* Are there any returning members (active after being idle &gt; 30 days)?


## Expected Submission
* EDA notebook with some visualisations
* A single python method returning a descending list of TOP active members out of slack stats.","","How members are retained and when they are lost?","","","0"
"3475","1153536","4440537","02/12/2021 14:06:46","## Task Details
To build effective machine learning algorithms that are able to predict the probability of a client subscribing to the bank's product. 

## Expected Submission
We should note that, even though we are talking about calculating probabilites, we will create classification algorithms - meaning that the final output of our models will be a binary result indicating if the client subscribed ('yes') to the product or not ('no'). Of course applying EDA on the dataset is highly welcomed.

## Evaluation
 The prediction probability of a client subscribing to the bank's product. Highly preferable to calculate the precision and recall score and besides the ROC AUC score.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
https://www.kaggle.com/pankajbhowmik/bank-campaign-subscriptions-eda-xgboost","","Predict the probability of a client subscribing to the bank's product","EDA & Evaluation Metrics (precision, recall, ROC AUC score etc.)","","0"
"5661","1118552","7526686","08/10/2021 18:11:22","Hi sir...can you please update this for batting & bowling data. I mean innings by innings data with singles, doubles etc.....","","Innings by innings batting and bowling data","","","0"
"4819","1416308","4455233","06/17/2021 23:21:42","**Task:** A binary classification to detect when users are asleep or awake.","","Sleep-wake classification","Detect when users are sleeping and awake in bed.","","0"
"4373","1339332","4455569","05/14/2021 04:09:56","## Task Details

Every legal proceeding in Brazil is one of three possible classes of status: (i) archived proceedings, (ii) active proceedings, and (iii) suspended proceedings. The three possible classes are given in a specific instant in time, which may be temporary or permanent. In this datasets, each proceeding is made up of a sequence of short texts called ‚Äúmotions‚Äù written in Portuguese by the courts‚Äô administrative staff. The motions relate to the proceedings, but not necessarily to their legal status.

Can you develop good machine learning classifiers for text sequences? :)","","Classify legal proceedings according to their status","","","0"
"2479","926901","4478650","10/18/2020 11:05:05","You have to make such CNN classifier using the dataset given with a high validation accuracy. The validation dataset will be given afterwards for testing purpose.","","Create Classifier","Make a classifier with accuracy  &gt; 95 %","","1"
"2799","1001023","4482199","11/27/2020 16:18:31","##Detalles

Cada a√±o se realiza un examen Teorico para la admision a las residencias medicas, se tiene la creencia popular que este examen esta corrompido, existen mitos de que el examen es vendido.
El objetivo de este Reto (Task) es obtener pruebas no paramtericas que permitan demostrar la hipotesis de que este examen efectivamente cuenta con un grupo de personas que han adquirido el examen antes de la evauluacion. 

## Resultados Esperados
De ser cierto lo anterior , entonces se espera qencontrar dos grupos de personas: 
1.- Las facultades que no Hicieron trampa, formaran parte de una distribucion normal.
2.- Las facultades que hiceron trampa formaran parte de otra distribucion normal, con una media y distribucion normal diferente a la anterior.

##Evaluacion
La solucion debera contener el listado de las facultades que probablemente han hecho trampa, mediante al analisis de atipicos.

Es decir si una facultad en la mayoria de los a√±os de evaluacion se encontraba en un ranking inferior y en algun a√±o se encontraba en un ranking superior, esto podria resultar como una anomalia.

## Details

Every year a Theoretical exam is carried out for admission to medical residences, there is a popular belief that this exam is corrupted, there are myths that the exam is sold.
The objective of this Challenge (Task) is to obtain non-parametric tests that allow to demonstrate the hypothesis that this test actually has a group of people who have acquired the test before the evaluation.

## Expected results
If the above is true, then you are expected to find two groups of people:
1.- Faculties that did not Cheat will be part of a normal distribution.
2.- The faculties that cheated will be part of another normal distribution, with a mean and normal distribution different from the previous one.

##Evaluation
The solution should contain a list of the faculties that have probably cheated, by means of the atypical analysis.

In other words, if a faculty in most of the evaluation years was in a lower ranking and in some year it was in a higher ranking, this could result as an anomaly.","","Detection of Atypical, how index of corruption","Deteccion de atipicos como indice de corrupcion","","1"
"2800","1001023","4482199","11/27/2020 16:24:54","##Detalles
La columna de Entidad esta a un 44% vacia, el objetivo del task es rellenar esos espacios vacios , partiendo de nombre de la facultad.

##Resultados Esperados
 Se espera obtener un catalogo de facultades homologadas, y su respectiva entidad federativa.","","Clear the data","Limpiar los datos","","0"
"2801","1001023","4482199","11/27/2020 16:31:28","##Detalles
 Obtener la estadistica descriptiva del concurso ENARM.

##Resultados esperados
Obtener por a√±o el promedio, maximo , minimos, desviacion estandar, graficas y la ruta cronologica de los Top 10 de cada a√±o.

## Details
 Obtain the descriptive statistics of the ENARM contest.

##Expected results
Obtain per year the average, maximum, minimum, standard deviation, graphs and the chronological route of the Top 10 of each year.","","Descriptive statistics of the ENARM.","Estadistica descriptiva del ENARM","","0"
"2878","1019338","4482199","12/06/2020 18:27:13","## Objetivos

- crear estadisticas acerca del mejor atleta para cada categoria
- los paises con mejores atletas 
- analizar la desviacion estandar de los resultados entre hombres y mujeres por lugar obtenido.","","Estadisticas del Campeonato IRONMAN","estadisticas descriptivas","","0"
"2879","1019437","4482199","12/06/2020 19:38:13","## Objetivos
Estadistica Descriptiva

- Observar la desviacion estandar por pais, y por genero.
- Comprobar mediante el analisis de los datos que pais pose la mejor genetica para esta prueba.

- Determinar si el lugar donde se realiza la competnecia afecta al resultado de la prueba , de ser asi cual es el mejor lugar para correr y cual es el peor.


## Tasks
Descriptive statistics

- Observe the standard deviation by country, and by gender.
- Check by analyzing the data that country has the best genetics for this test.

- Determine if the place where the competition takes place affects the result of the test, if so, which is the best place to run and which is the worst.","","World Marathon Winners","Ganadores de los Maratones mundiales","","0"
"2880","1019499","4482199","12/06/2020 20:35:07","## Objetivo

Graficar la mejora del salario que se gana en mexico por a√±o","","Grafica de tiempo de salarios en Mexico","Grafica de tiempo de salarios en Mexico","","0"
"2881","1019631","4482199","12/06/2020 22:28:59","## Obejtivo

Graficar la tendencia de ascensos por grado y sexo 
Realizar una prediccion de ascensos para el 2021","","Generar la Estadistica Descriptiva","","","0"
"2525","936468","4496228","10/23/2020 18:19:46","## Task Details
This online retail company is looking for some valuable suggestion from your end.
‚Ä¢	Which are the segments that you would like the online retail company to promote its products and why?
‚Ä¢	What is the expected Revenue for 2019 ? What are the factors that you prescribe for the online retail company to indicate correction in this place would increase revenue in 2019?
‚Ä¢	Sales growth was exponential between 2016 and 2017. In your opinion what could have been the reason for this growth?
‚Ä¢	They have decided to launch a targeted campaign to attract its customers. Can you help identify the low hanging fruits for this targeted campaign.

1.	Some customers didn't write a review. But why are they happy or dissatisfied? (Discriminant)
2.	Prediction: With purchase date information predict the future sales information (Forecasting)
3.	Performance: Identify delivery performance and find ways to optimize delivery times.

4.	Product Quality: Discover the product categories that are more prone to customer dissatisfaction. (Decision tree/CHIAD)
5.	There are 256 product categories. However, the ecommerce platform wants to optimize their number of product categories and want to reduce by 75%. How will they do that and which all will be the categories that they should concentrate? (TURF)


## Expected Submission
Submission of python file only.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Make Data More Informative","","11/08/2020 23:59:00","1"
"3303","1125228","4499176","01/29/2021 01:22:52","## Task Details
This is a very basic data, but finding course categories for certain industries and understanding what content is **Popular** as of 12-30-20","","Understanding Corporate Trainings and Online Learning","","","0"
"4438","1346041","4513873","05/19/2021 08:55:15","Houses price prediction using regression models","","House _prediction","","06/19/2021 23:59:00","0"
"6845","1008497","3486296","11/15/2021 13:00:47","Hi. Can you provide more information about features, columns etc?","","More information","","","0"
"2972","1021622","4520458","12/17/2020 15:44:46","## Task Details
As I uploaded this dataset by scraping MyAnimeList.net, I wanted to create a notebook in order to cluster the many mangas here. However my current skills doesn't allow me to do this! 

## Expected Submission
Just create a notebook for manga lovers all around the world so that they can explore your in-depth analysis!

## Evaluation
Every notebook produced is the right solution!","","Basic EDA - Mangas","Just make a basic EDA to train and show some insights","","0"
"2973","985856","4520458","12/17/2020 15:46:37","## Task Details
As I uploaded this dataset by scraping MyAnimeList.net, I wanted to create a notebook in order to cluster the many animes here. However my current skills doesn't allow me to do this! 

## Expected Submission
Just create a notebook for animes lovers all around the world so that they can explore your in-depth analysis!

## Evaluation
Every notebook produced is the right solution!","","Basic EDA - Animes","Just make an EDA and if possible a clustering algorithm and viz","","0"
"3602","1182508","4520458","02/26/2021 17:10:44","## Task Details
- Create a exploratory data analysis notebook
- Mine interesting insights
- Create beautiful visualizations

## Expected Submission
A notebook with the details given above....

## Evaluation
Based on the notebook quality.

Good luck!","","EDA - Timeline","Analyze the events in temporal dimension","","0"
"3603","1182508","4520458","02/26/2021 17:13:03","## Task Details
-  Create a model that can predict the magnitude based on the available variables
- Create data visualization for the model evaluation

## Expected Submission
A notebook

## Evaluation
On the notebook's quality","","Predict Magnitude","Predict magnitude based on the other variables","","0"
"2719","981383","4522850","11/18/2020 11:44:19","## Task Details
Here you have to analysis what is the price of the vehicle ? Just explore the data.

## Expected Submission
There is not any specific date for submission

## Evaluation
What makes a good solution?
Here, you have to use all basics and beginner's code","","Vehicle price prediction","EDA,FEATURE ENGINEERING,DATA ANALYSIS AND VISUALIZATION","","0"
"3538","1162974","4523418","02/18/2021 18:49:29","Use this dataset to create a project that solves some Vision problems.","","Use it to create any project","","","0"
"3535","1167471","4523418","02/18/2021 17:28:22","Use this dataset for a project- small or big.

No deadlines.","","Use it for a Notebook","","","1"
"3799","1214864","4523418","03/17/2021 12:58:33","## Task Details
Generate GAN-generated handwritten Bengali digits.

## Expected Submission
Submit full Notebooks. Use Shallow-GANs, Deep Convolutional GANs or something else of your choice.

## Evaluation
I am not providing any particular metrics for evaluation. Feel free to use one you deem helpful.","","Bangla_MNIST GAN","Generate Handwritten Bengali Digits with Generative Adverserial Network","","0"
"3800","1214864","4523418","03/17/2021 13:02:54","## Task Details
Go through the data, and do Exploratory Data Analysis.

## Expected Submission
Use a framework like PyTorch (preferred) or Tensorflow to do exploratory data analysis. This will not only serve as a starter Notebook but will also demonstrate the framework to be used for similar data.

## Evaluation
The produced Notebooks should be clearly readable, elaborate, and helpful to anyone willing to get started with the Dataset.","","Bangla-MNIST EDA","Do EDA on Data","","0"
"5025","1451728","4523418","07/06/2021 17:50:46","## Task Details
Generate Bengali poetry with the data provided in any way you see fit.

## Expected Submission
Generate poetry with the knowledge, and provide easy-to-use and replicate Notebook/Colab demo that can generate poems given a prompt.

## Evaluation
* The generated poem should be good enough to undetectable to a human.
* The whole project should be properly documented and reproducible. 

### Suggested Reading

* [ Bengali Poem Generation Using Deep Learning Approach](https://www.researchgate.net/publication/338349844_Bengali_Poem_Generation_Using_Deep_Learning_Approach)
* [Poetic Machine: Computational Creativity for Automatic Poetry Generation in Bengali](https://computationalcreativity.net/iccc2014/wp-content/uploads/2014/06/11.3_Das.pdf)
* [Sequence-to-sequence Bangla Sentence Generation with LSTM Recurrent Neural Networks](https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050919X00083/1-s2.0-S1877050919306775/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEM%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIDnF537%2FEsZl7KraPqdEsabqdlUMNWEBrfI9M8QUiAS7AiB9NwHrUvF962KfPgeAXs6IRk6LFLNQ6p7XqV7uR%2FizeyqDBAio%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDA1OTAwMzU0Njg2NSIMpN5t8M%2FK5Hmv2PDAKtcDJxqIi10qkoYU7vLgvMFcBVX2AsHAyh31ZoI2A1Wm7AHkfcXevpwO9Cl%2FULuoatp%2BaBQaSdCvawWd%2FEPoBws%2FsRuN%2BicsCOazkOID4X2JbL1iiWPD%2FZvVGG96pCVZ3uOeJv%2B376NRvj1RakLtGR5K1htYp%2BGcIsyE5bFYdbwXsR%2Bg8rFRRhRE722ckwtwzc4wWDXb79NKlhDoe%2F2xV5R6UTd7NMhFmPoKo5lDCMz4z1rSkrruGIjfoI7Wsk0pur3xqP4EK%2B%2FmjbpHTMJ88AIXBIuSnke8ayoba8ylzMC%2B%2FJ11C5PtV8L8%2FxjQ7j2LdMn8IwOOx%2BvfLFnj%2FWHehpi66FiYIG6Pm0ef4b0oZysww%2FIxKdpwrEI8wDyMHG7M7xu3eXQr4YYRDvtWsMomVaQO2lCb9z64VYnRjfnHo2Y3opuhyTItataO4UDIoAqbNNedqRGpiS34KfjiXEwrsdMRAfsivDfRfK22LdI5Ik%2FRcs39glP3iZJiPYkmc0mS4wuTWdF866F5x%2FC7Qe3rV%2F%2B66chqcW6LLW%2F2jfIzc76p4MFf%2BLHiXaN9XP0IBeuRDo55SbEBv9ygU50JG5mJq%2FfG7W4O4R8mFMzA3XQzehSnfBnmWK0%2BF%2BpwML%2F%2F8YYGOqYBa2UyGLonr3NQnu9K4grb%2FOSkxmFphXZ7upPioZQ0sKiB%2Fteu3htV8LQFrrFTfYsgEYvGUkmyIP54eKgX8klD6hoUiyFMnSXVxgBDbJ9dRbca1pivFIHYf4jh84gWNG2VOAbeI5a%2FaZj%2BZjAx3%2FK20dOFImTbCsA%2BGiaLyI4ivRK8YznNlT8ircGH9slTQr%2BtE8VOXzw%2BpxlXRMmey6JcsKmyONYnqQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210630T151034Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRF52SBWG%2F20210630%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=54d2f4e3f4879b8c5ce06c17dd3b41b7dcd90e1bafce79cf1488d0977b9ac342&hash=b352e96d63520e897b51beae76897a2f656e24c01ed99b6d351b9aced0afc677&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050919306775&tid=spdf-85e03079-55e0-47b0-860d-246038f57864&sid=a39ac8145fb57543059b0ec6b15ca24f93e6gxrqb&type=client)
* [ Context-driven Bengali Text Generation using Conditional Language Model](http://www.iapress.org/index.php/soic/article/view/1061)
* [ Training an AI to create poetry (NLP Zero to Hero - Part 6)](https://www.youtube.com/watch?v=ZMudJXhsUpY)
* [ NLP with Bangla: Generating Text and Sentiment Analysis](https://www.datasciencecentral.com/profiles/blogs/nlp-with-bangla-word2vec-generating-bangla-text-amp-sentiment)

### Note

I expect that you will use Transformer models.","","Generate Novel Bengali Poetry","Generate Poetry Using Transformer Models","","0"
"4853","1420635","4524191","06/20/2021 09:19:07","Does the age and gender define the risk of diabetes? If yes, which age and gender are at high risk?

Submissions receive upvote from the community.","","Age and Gender","","","1"
"4854","1420635","4524191","06/20/2021 09:24:11","does all features are important in our classification problem if not what are the features to remove and what is the solution to use ??","","dimension reduction","","","1"
"6049","1444338","4526353","09/09/2021 17:45:01","## Task Details
Tell a story based on data","","data visualization","","","0"
"6050","1444338","4526353","09/09/2021 17:46:09","## Task Details
Classify the emotion/s of a tweet (if possible)","","sentiment analysis","","","0"
"5046","1422136","4527731","07/08/2021 09:07:11","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.","","Predict the loan sanction amount","","","0"
"4723","1401039","4528870","06/10/2021 14:48:32","## Task Details
https://youtu.be/h9wxtqoa1jY Inspired me to create a data set, where the hypothetical example from the lecturer can be tackled with the graph coloring algorithm.

## Expected Submission
Write your own coloring algorithm and show how many colors (or dates you need).","","Use Graph Coloring Algorithm to create an exam schedule","","","0"
"3293","1123487","4532197","01/28/2021 05:24:39","## Task Details
These are digits of house numbers taken from google street view. A self-driving vehicle would see the house number and identify the address (e.g. for delivery or taxi drop off/pick up)

## Expected Submission
Write a function that tells what digit is in each image: f(image) = digit.
Accuracy and speed are the key factors in its quality.

## Evaluation
Accuracy and speed are the key factors.

##Further thinking
Some house numbers align their digits in diagonal cascade or zigzag. While recognition of one digit is a simpler problem, we still need to think how to tell machines about the aesthetics  of house numbers.  

## Further help
This is my first task, roast with care and constructively.

Check out some high-quality tasks that kaggle suggested:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Recognition of digits","","01/31/3310 23:59:00","0"
"4936","1436618","4533005","06/28/2021 15:24:01","## Task Details
For years we tried to create a model able to predict blood lactate concentration [La] during exercise. We did not succeeded. Our attempts have been summarised in a recent (review on the topic)[https://link.springer.com/article/10.1007/s11332-019-00557-x]. Info about oxygen consumption (VO2) prediction  during exercise are also give in the review, but predicting cycling VO2 seems to be easier (see this regression model (here)[https://journals.plos.org/plosone/article/comments?id=10.1371/journal.pone.0229466]). 

## Expected Submission
A model that can take as input easy-to-obtain inputs should be prioritised. Power and cadence and respiratory frequency as well as heart rate all constitute easy-to-collect input. Oxygen consumption requires a metabolic cart. 

## Evaluation
Goodness of the solution will be evaluated based on the accuracy of the predictions in [La] for an entire exercise not used to train the model (i.e. to calibrate the parameters of the model).","","Predicting blood lactate during cycling exercise","","","0"
"2949","1032096","4534017","12/15/2020 10:55:22","## Task Details
Find the most used words and create a word cloud.","","LEXICON data twitter","Stock market tweets that mentioned #stocks","","0"
"2948","1036434","4534017","12/15/2020 09:28:53","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (aPredict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and Viz, explore !","Try eda using the data provided.","","7"
"4160","1289627","4534539","04/23/2021 00:53:02","## Task Details

You have the data from 2017 to 2019 from 25 Stations in Seoul. Use this data to predict the 2020 gas emissions and compare them with the actual results

In this task  we want to check how well our model predict  gas data for an uncommon year.","","Predict 2020 Gases Level","","","0"
"4152","1244012","4540909","04/22/2021 09:55:47","## Task Details
You are required to use the training dataset to identify patterns that could predict default. Apply the patterns on the test dataset to identify ‚Äúpotential‚Äù defaulters.","","Predicting Potential Defaulters","Risk Analysis","","0"
"4734","1372662","4543173","06/11/2021 16:19:29","For Increase Skills in Data Science","","Practice For Hackathon In Data Science","","","0"
"4248","1311525","4545940","05/02/2021 16:09:14","In this task, you are expected the find the algorithm that works the best for the attached classification dataset. 

You can only use the following algorithms: 
1. Logistic Regression
2. Support Vector Machines
3.Naive Bayes
4.Decision Trees

You are expected to try several parameters for each of those algorithms first (using grid search with cross validation), and then perform a comparison among them. Again, first optimize each algorithm, and then compare. This week, you will only use ""accuracy"" metric for comparison. 

At the end, you should make a decision on the model that works best for this dataset; more importantly you should point out the model that have the best generalization ability. You should write something like this at the end:
 **""I believe X algorithm with Y,Z,T parameters have the best generalization ability""**","","Parameter Optimization with Grid Search","","","0"
"4327","1334013","4554236","05/11/2021 11:44:09","you have to predict final score of each innings for every match with 90% above accuracy","","final score","predict final score in each innings","","0"
"4328","1334013","4554236","05/11/2021 11:45:31","What will be the score at the end of each over in every innings when wickets at the innings is given.","","over by over score","","","0"
"4273","1318738","4557384","05/05/2021 12:40:06","## Task Details
Create a Machine Learning model that can detect scratch noises in music playback.

## Expected Submission
Submit a notebook of the model with the training data you used. See my work as an example. Try to beat it!","","Create Scratch detection model","","","0"
"4922","1433228","4559219","06/26/2021 19:13:07","Perform exploratory data analysis of the tweets and the sentiment score and build insights. The EDA process may include:
1. Null value analysis 
2.Univariate analysis of sentiment columns
3. Distribution of polarity score
4. Letter distribution
5. word distribution

The preprocessing steps may include:
1. Managing null values
2. Tokenization
3. Building vocabulary
4. Removing punctuations and stop words
5. Lemmatization","","EDA of the Tweets and Sentiment","","","1"
"4731","1402504","4574701","06/11/2021 10:30:37","Predict the Canada Income-Per-Capita in 2025","","Canada-Income-Per-Capita","","","1"
"4692","1395777","4574701","06/09/2021 02:15:23","Identifying where the various flights are going and what is the busiest and most lengthy routes from the airport","","Analysis Most Busy and Most Lenghty routes from the Airport","","","1"
"4694","1397660","4574701","06/09/2021 03:12:59","Analyze it from the perspective that your company wants to see in which sector it will be a beneficial idea to start the work so that your company can collaborate with FDI player and become successful.","","FDI-Case-Study","Analysis of FDI (Foreign Direct Investment)","","1"
"4691","1393473","4574701","06/09/2021 01:40:01","Analyse the data of England School workforce. Analysis salary comparison between male and female staff also analysis how to grade effect on the salary of stafff.","","Analysis the Salary Comaprison Between Male and Female Staff","","","1"
"4759","1405690","4574701","06/13/2021 09:19:18","Analyse the dataset of Traffic accidents occurs in Pakistan from 2008 to 2019.","","Analyse Pakistan Traffic Accident Data","","","1"
"4770","1407313","4574701","06/14/2021 05:13:51","Analysis On Crimes in Pakistan","","Pakistan Crimes Analysis","","","0"
"4754","1405584","4574701","06/13/2021 05:17:21","Analysis On Newyork Air Quality
How Ozone, Solar Radiation and Wind affect the temperature of Newyork.","","Newyork Air Quality Data Analysis","","","1"
"4802","1414663","4574701","06/17/2021 05:58:44","1. How is the overall situation of total parking spaces by (a)Owned/Leased
(b) Property type (c) (Within each property  type how is owned and leased)
(d) By Building Status (e) By Building State
2. In Which Building state parking situation is in excess-put it in order?
3. Building cities that have been decommissioned along with total parking.
4. In Which type of property parking space is in excess?
5. Where is more active parking i.e either in owned or leased?
6. States With more leased parking similarly with more owned parking.
7. Which property type has more active parking?
8. How's the distribution of parking in the city by property type?
9. How's the distribution of parking in the city by building status and building state?
10. Building states which have 0 parking space?","","Analysis US Parking Data","","","3"
"4779","1409252","4574701","06/15/2021 03:59:23","1- Do Exploratory Data analysis to figure out which variables have a direct and clear impact on employee retention (i.e. whether they leave the company or continue to work)
2- Plot bar charts showing the impact of employee salaries on retention
3- Plot bar charts showing a correlation between department and employee retention
4- Now build lologistic regression model using variables that were narrowed down in step 1
5- Measure the accuracy of the model","","Hr Dataset Analysis","","","4"
"2514","934685","4576966","10/22/2020 16:59:48","Task Details :
A recommendation system is required in subscription-based OTG platforms. Recommended engine generally in three types 1.content Based recommended engine 2.collaborative recommender engine and 3.hybrid recommended engine

Expected Submission
With the help of this particular data set, you have to build a recommended engine. And your recommended engine will return a maximum of 10 tv shows name if a user searches for particular tv shows.Movie Recommendation for OOT Platforms


Evaluation
The recommended engine must return 5 tv show names and maximum it can return 10 movie names if a user searches for particular tv shows. This recommender engine should not give suggestions in between 1 to 4 and 6 to 10 it has to return 5 movie names for 10 movie tv shows.","","Tv Shows Recommendation for OOT Platforms","","","5"
"2448","924360","4576966","10/16/2020 09:57:48","Figure out the best players in overall rating per position in Fifa 2019.

**Expected Submission**
A notebook showing the results of the analysis.

**Evaluation**
Clear representation of the results of the analysis. A comparison can also be made with Fifa 2018.","","Best players","","","0"
"2444","922168","4835558","10/15/2020 07:14:29","## Task Details : 
Recommendation system is required in subscription-based OTG platforms. Recommended engine generally in three types 1.content Based recommended engine 2.collaborative recommender engine and 3.hybrid recommended engine

## Expected Submission
With the help of this particular data set you have to build a recommended engine. And your recommended engine will return maximum 10 movies name if an user search for a particular movie.

## Evaluation
Recommended engine must return 5 movie names and maximum it can return 10 movie names if an user search for a particular movie. This recommender engine should not give  suggestion in between 1 to 4 and 6 to 10 it have to return 5 movie names for 10 movie names.","","Movie Recommendation for OOT Platforms","","","2"
"4158","1288972","4579543","04/22/2021 16:27:22","Predict the movement of Dogecoin price according to the tweets of people","","Sentiment Analytics","","","0"
"6005","1168337","7924458","09/04/2021 16:02:11","## Task Details
Need to predict physical activity according sensor data

## Expected Submission
Need to submit python code that use mhealth_raw_data.csv dataset","","Physical Activity Prediction","Given ECG data from subjects performing physical activities, let's try to predict which activity is being performed by a given subject.","","0"
"3593","1178754","4582542","02/25/2021 08:17:50","Create a real-time object detection model trained on the given Data which is able to precisely detect the three classes.","","Object Detection","","","0"
"3742","1206775","4583637","03/12/2021 11:22:33","## Task Details
Split data for training and testing and create a efficient model for predicting the future price

## Expected Submission
User should submit the predictions of next 30 days forecasted values

## Evaluation
RMSE is cosidered for the evaluation

### Further help
Need any help , Let me know in Discussions","","Create Predictive Models","","07/10/2025 23:59:00","1"
"2389","912792","4585998","10/09/2020 08:58:28","## Task Details
Train the data in a way that explains the variance in the Reviews. 

## Recommendations
- Dummy variables
- Does the description sells the product?
- Does the logo sells the product?","","Why some EAs have more reviews than others?","","","0"
"2942","1035283","4585998","12/14/2020 18:25:05","## Task Details

Is this data sufficient to predict the match outcome in any direction?","","Can you predict the winner or match rounds using this data?","","","0"
"3896","1239194","4596006","03/29/2021 10:13:17","## Task Details
Develop a model to recognize CAPTCHAs

## Expected Submission
Submit .ipynb file

## Evaluation
Low Loss and High Accuracy","","CAPTCHA Recognition","","","0"
"3633","1187749","4605598","03/01/2021 21:29:59","## Task Details
Did you ever wandered in which month you will get more likes if you post something? Now you can find out.

## Expected Submission
Upload an exploration notebook!

## Evaluation
No evaluation just hope to gather a lot of Notebooks and have a conclusion","","Determine which months do better","","","0"
"3634","1187749","4605598","03/01/2021 21:31:31","## Task Details
Find out if previous posts have a correlation with the likes of upcoming post. Maybe you can add nice plots too &lt;3","","Determine if previous like have correlation with upcoming post","Correlation with previous posts","","0"
"3635","1187749","4605598","03/01/2021 21:32:26","## Task Details
Date time exploration and an insight in which time or day is better to post","","Determine what time each day you will end with more likes","","","1"
"4255","1312443","4631534","05/03/2021 13:15:51","## Task Details
Perform a Sentimental Analysis on the tweets database.

While you are free to choose your method, Natural Language Processing might be of help!","","Perform a Sentimental Analysis on the tweets","","","4"
"4233","1307126","4631534","04/30/2021 14:48:52","## Task Details
Predict an affair based on the factors given.

## Expected Submission
The final answer should be a Boolean value in 1 or 0","","Predict an Affair!","Sounds odd, but isn't!","","0"
"4232","1307136","4631534","04/30/2021 14:30:55","## Task Details
Modelling a relation between age of the target and response","","Response to social media ads","","","1"
"4240","1307591","4631534","05/01/2021 11:25:05","## Task Details
Based on the age of the male and female partners, can you predict if they have a chance to match?","","Let's Make a Match","","","0"
"4236","1307679","4631534","05/01/2021 03:04:52","## Task Details
Visualize and interactively analyze Pima-Indians-diabetes and discover valuable insights using an interactive visualization platform. Compare with hundreds of other data across many different collections and types.

## Objective
Use Machine Learning to process and transform Pima Indian Diabetes data to create a prediction model. This model must predict which people are likely to develop diabetes with &gt; 70% accuracy (i.e. accuracy in the confusion matrix).","","The Pima Indians Diabetes","Let's crack it!","","3"
"4230","1304147","4631534","04/30/2021 11:04:41","## Task Details
Parkinson‚Äôs disease is a progressive disorder of the central nervous system affecting movement and inducing tremors and stiffness. It has 5 stages to it and affects more than 1 million individuals every year in India. This is chronic and has no cure yet. It is a neurodegenerative disorder affecting dopamine-producing neurons in the brain.

## Expected Task to be done
To build a model to accurately detect the presence of Parkinson‚Äôs disease in an individual.

## Evaluation
The higher the accuracy the better the solution. Use 'accuracy_score' to check the score.","","Solve It!","","","1"
"4225","1304411","4631534","04/29/2021 10:18:26","## Task Details
Here we need Two driver features: mean distance driven per day (Distance_feature) 
and the mean percentage of time a driver was &gt;5 mph over the speed limit (speeding_feature).

## Expected Submission
Perform K-means clustering to distinguish urban drivers and rural drivers.
Perform K-means clustering again to further distinguish speeding drivers from those who follow speed limits, in addition to the rural vs. urban division.
Label accordingly for the 4 groups.","","Delivery Fleet Code Challenge","Unsupervised Machine Learning Competition","","0"
"4362","1338063","4633533","05/13/2021 09:48:58","The manager wants a midfielder who will give the team a boost in the midfield. Suggest the player to buy with stats to back it","","Suggest a midfielder","","","0"
"3405","1138040","4634248","02/04/2021 14:29:31","Choose appropriate columns and find the model with maximum accuracy.","","Multi Class Classification","","","0"
"2810","1004995","4636312","11/29/2020 11:58:56","# Objective 

Our goal in this problem is to identify fake news. Fake News represents a dangerous risk in our whole life, it makes It Harder For People To See the Truth,  
and trusting these false stories could lead you to make decisions that may be harmful to your life.

# Content
This dataset consists of one CSV file. These files contain 3 columns: the title, the text labeled by fake or real.

# Evaluation
A good solution should explore the news data, classify the different news and make a good model, and then look at when they are fake and when not.

# References
[The DataSet Source](https://drive.google.com/file/d/1er9NJTLUA3qnRuyhfzuN0XUsoIC4a-_q/view)","","Predicting fake news.","Is everything on the Internet true?  How to detect false news? What are the indications that might help us discover false information?","01/28/2022 23:59:00","0"
"3860","1221796","4657764","03/25/2021 11:26:14","## Task Details
How can apply deep hit in the R language for the given dataset or other suggested one?

## Expected Submission
Dataset is provided but feel free to advise.

## Evaluation
What is the optimum model performance with deep learning compared to other survival analysis models?

### Further help
For more info please don't hesitate to contact me on: 
alsinglawi@gmail.com","","Deep Hit with R","Deep Learning in Survival Analysis","","0"
"4951","1434956","4661574","06/30/2021 04:45:05","## **Task Details**
In this dataset , there are two classes , people who wearing glasses and not wearing glasses, All pictures are of high resolution. It is difficult for classifier when all features are same except some feature (glasses) .  can you classify it using self supervise learning???

## **Expected Submission**
submission contain  ***Image_id***   (from 1)* **labels*** (0-no glasses ,1- glasses)
## **Evaluation**
Expected accuracy is more than 70%","","Classify the data in respective classes","","01/30/2022 23:59:00","1"
"3597","1181838","4663387","02/26/2021 09:53:45","This is a beginner-friendly task for beginners who wish to practice working with large datasets and performing data cleaning as well as basic EDA.

## Task Details
Perform basic exploratory data analysis (EDA) for aviation accidents and incidents data available in the dataset. Provide your inferences and conclusion. Suggest if any type of modelling (machine learning, classification, etc.) can be applied on the available dataset.

## Expected Submission
Submit a notebook where you have performed EDA using any of your preferred methods. Explain your approach and ideas. Draw inferences from the analysis and present them in the notebook.

## Evaluation
Submissions will be evaluated based on quality of analysis done and inferences obtained. Make sure to properly explain your approach, ideas and inferences.","","Exploratory Data Analysis (EDA) for aviation accidents and incidents","","","4"
"2649","955207","4663601","11/08/2020 17:49:25","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiratin, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","predict coupon code","","11/11/2021 23:59:00","0"
"6810","1045743","4561355","11/10/2021 13:08:18","## Task Details

1. Perform EDA on dataset and analyze reasons for the high insurance charges.
2. Select features that will affect our model in prediction.

## Expected Submission

Submit a notebook with EDA, data preparation, model creation and evaluation.
NOTE :- Documentation is optional.","","Perform EDA and predict charges","Perform EDA and find if Linear Regression is best suitable model?","","1"
"7033","1375808","8111464","12/09/2021 07:57:44","## Task Details
Make AGE detection From  voice on pycharm or spider  by using any api or dataset , Accuracy must be 50% or above 50% 

## Expected Submission
*) Code or python files or your whole project files","","age recognition by voice","Age Detection From Voice","12/12/2021 23:59:00","0"
"3032","1056403","4683527","12/24/2020 08:42:35","## Task Details
Perform EDA and give suggestions on whom to target and how can they improve their match rate

## Expected Submission
1. EDA
2. Perform statistical tests on match rate","","How Tinder can improve it's match rate ?","Performance in Tinder","","0"
"2641","961263","4683527","11/07/2020 14:53:17","## Target variable
Output

Use any classification model and predict whether the consumer of these attributes will most likely to purchase or not ?","","Classification modelling","Predict whether a consumer will buy next time or not?","","3"
"2642","961263","4683527","11/07/2020 14:54:35","## Variable-latitude and longitude

Use latitude and longitude and perform geographical analysis and check is there any location based demand and figure out the reason why","","Geospatial Analysis","Which region demands heavily ?","","3"
"2643","961263","4683527","11/07/2020 14:55:45","## Variable-Reviews

Check the last column which has reviews of respondents. Remove NIL and perform text analysis and check whether you can predict using reviews","","Text analysis","Analyse reviews of consumers","","1"
"3507","1160011","4687113","02/15/2021 18:48:16","## Task Details
Perform an EDA on satellite dataset, particularly on Starlink satellite constallation.

## Expected Submission
Submit notebook containing interesting conclusions drawn from data analysis.","","Perform EDA on Satellite data","","","0"
"3582","1177049","4688022","02/23/2021 15:52:36","## Task Details
Was there any change in the user behavior over time?
Were people excited or angry? Explore and find an interesting point that can help summarize people's reaction to the lending of the rover.","","Track the Viewer Behavior Over The Video Timestamp","","","1"
"3676","1177049","6345165","03/06/2021 15:38:16","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further helphy
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","boredom","","","0"
"3684","1198299","4688022","03/07/2021 18:57:26","## Task Details
Try and understand are there any patterns that emerge which can indicate a recipe for a good catchprhase?","","Explore And Understand The Text","What makes a good catch phrase?","","4"
"3898","1198299","6750947","03/29/2021 15:23:46","## Task Details

- use NLP to tokenize words for the context of each catchphrase
-a list of these words will be connected to the catchphrase
-a dictionair storing {catchphrase: list of countvevtorized tokenized words}
-loop through dict and based on what you say, will predict the catchphrase that should be used

## Evaluation
works.","","project","choosing a catchphrase based on context","","0"
"3146","1082416","4688022","01/06/2021 19:39:46","## Task Details
It can be interesting to explore different medications and drugs and how their prices change over the years.","","Explore And Analyze The Data","","","3"
"3128","1077651","4688022","01/04/2021 18:12:46","## Task Details
Explore the text data and visualize the different aspects of the horror stories. Can we uncover the recipe for a really spooky story?","","What makes a good creepypasta ?","","","1"
"2601","950770","4688022","11/01/2020 19:50:53","## Task Details
Using NLP models/techniques categorize the costumes by the groups of costumers the costume is best suited for (kids/ adults)
using the item description/reviews etc.
There are some costumes in the dataset which are strictly for adults and strictly for kids, can your model properly distinguish between them?","","Categorize the costumes by the target costumers","","","2"
"2384","912120","4688022","10/08/2020 18:34:39","Can we uncover any reasons for an injury or any common injury","","Explore the injuries","","","0"
"2465","926157","4688022","10/17/2020 14:59:51","## Task Details
Can we understand what makes these handguns stand out? what are the common features between all handguns in the dataset.","","What's common to all high rated guns","","","2"
"2530","937330","4688022","10/24/2020 12:16:39","## Task Details
Is there any hidden pattern behind the description of the books and the target age ranges?","","Behind The Description","","","2"
"2620","937330","4688022","11/03/2020 12:57:06","## Task Details
Create a pipeline which will generate new descriptions for children books, 
(On basis of there description maybe new books can be written) 

*Possible starting point is using a simple N-grams model but this just a tip :)","","Describe A New Book!","","","0"
"3077","1068213","4688022","12/30/2020 19:10:48","Graph theory is a fundamental subject in computer science, and many real-world problems can be reduced to graph problems.

Many graph theory problems are considered very computationally expansive; I wonder to what extent we can bring computer vision algorithms to what insight can be extracted from an image of a graph.","","Research Graphs","","","0"
"3014","1053480","4688022","12/22/2020 20:09:27","## Task Details
Explore the data and find out are there any words or features which are shared across different brands.
Can we break down the slogans and answer the question is there a formula for a perfect slogan?","","Are there any similarities between slogans?","Exploratory data analysis","","0"
"2954","1037291","4688022","12/15/2020 16:19:47","## Task Details
Can you use lstm models or any architecture for that matter to generate new interesting scenarios based on the scripts of the last 3 movies?","","New scenarios for a terminator movie!","","","1"
"2895","1025129","4688022","12/09/2020 14:04:19","Can you uncover any interesting connections between personal information and garment details?","","Explore The Data!","","","0"
"4411","1345636","4688022","05/17/2021 07:00:38","## Task Details
Use GAN to generate new and interesting book covers","","Generate New Book Covers","","","1"
"4040","1264013","4688022","04/11/2021 07:04:11","## Task Details
Generate New Descriptions For an Anime Plot.","","Use LSTM's to Generate New Anime Synopsis","Can We Create The Next Anime Pitch?","","1"
"6794","962431","4695459","11/09/2021 14:12:35","Predict the Predict Number of People who donate blood","","Blood Donation Prediction","","","0"
"3220","1087067","4695459","01/18/2021 13:20:34","Use the dataset to make the classification model and get accuracy over 85%","","COVID-19 Classification model","","","1"
"4441","1351984","4697476","05/19/2021 14:00:09","## Task Details
Discover Insights about Fate/Grand Order Universe.

## Expected Submission
A kernel with your discoveries through visualization and storytelling.

## Evaluation
Any kernel that presents its insights through visualization is evaluated.","","Fate/Grand Order EDA","","","0"
"3145","1082326","4698299","01/06/2021 18:44:34","Human rights are one of the key conditions for an individual or a community to thrive. All citizens should be allowed to exercise their basic rights irrespective of race, sex, national or social origin, religion, political or other views, and more, to live with dignity, freedom, equality, justice, and peace.   

You work for a non-governmental organization that helps people voice their grievances. With your expertise in Machine Learning, you plan to develop a predictive model that will help the government prioritize grievances of complainants for resolution. Build a sophisticated Machine Learning model that predicts the importance of a citizen‚Äôs grievance while considering various articles, constitutional declarations, enforcement, resources, and so on.","","Resolving citizen grievance","","","0"
"3894","1222432","4698936","03/29/2021 07:19:46","##Task Details
Every task has a story. Tell users what this task is all about and why you created it.

##Expected Submission
Visualization","","Visualization World Happiness Report","","","80"
"5150","1222432","7754191","07/16/2021 16:00:19","## Task Details
Show the relationship between Life Expectancy and GBP for all the countries in South America.  I'm curious to how economics can correlate to life expectancy within that area of the world.

## Expected Submission
I'm looking for Excel charts and/or Tableau charts



## Evaluation
A good solution will show a strong relationship between GDP and Life expectancy or if there is really a strong relationship between the two variable.","","Life Expectancy by GDP in South America","","07/31/2021 23:59:00","3"
"4030","1261929","4698936","04/10/2021 10:15:16","##Task Details
Every task has a story. Tell users what this task is all about and why you created it.

##Expected Submission
Visualization","","Visualization World population working in different sectors","","","2"
"2738","986174","4701203","11/20/2020 19:16:17","## Task Details
Got the data from a bike vendor website where it has a huge collection of different brands and models around the world manufactured from 2010 to 2020. How do the factors or features affect an increase or decrease in the price?  It has mainly focussed on the bike engine, engine type, CC , and engine warranty. It is good to work on the regression model to predict the price after feature engineering.


## Expected Submission
Take time to read more about the data. How the features are affecting. Find out the correlation. Analyze the data. Apply a few statistical concepts. 


## Evaluation
It depends upon your RMSE, MSE, MAE, and r2_score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Bike price prediction","Predict the bike price using regression models. Its an easy way to learn more about the data","02/28/2021 23:59:00","1"
"3084","1070091","4701203","12/31/2020 19:22:22","## Task Details
Implement the appropriate algorithm which will recommend the medicine

## Expected Submission
Take time to explore the data and submit it within 2 months

## Evaluation
Similarities of formula would help to work.
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Medicine recommendation system","","02/28/2021 23:59:00","1"
"4184","1294654","4708826","04/25/2021 07:46:00","### Task Details
Predict student performance in secondary education (high school).","","Predict student performance in secondary education","","","4"
"3972","1252700","4708826","04/05/2021 15:47:04","Voltage signal fault detection model which can spot any faults in phases while transmitting signal.","","Voltage signal fault detection","","","1"
"2380","911618","4718356","10/08/2020 12:09:03","## Task Details
The data is web scrapped from the web which consist of of the reviews about the movie and rating out of 10.

## Expected Submission
Test your Text_Analytics skills and NLP skills and separate the positive reviews and negatives reviews out them, so next time when somone  enters a review we may know either it is positive or negative","","Text Analytics","NLP","","0"
"3057","1062019","4718701","12/27/2020 16:53:45","## Task Details

Were the lockdowns effective? Did we experience spikes after the Holidays? These are the questions that we will seek to answer.

One can combine this dataset with other news and key points in time (lockdowns, new UK mutation, Holidays, etc.) in order to study the effects of those events on the progression of Covid-19 infections in a multitude of countries.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Key-points in the pandemic and their effects on Covid-19 spreading.","Were the lockdowns effective? Did we experience spikes after the Holidays?","","4"
"3058","1062019","4718701","12/27/2020 16:56:32","## Task Details

Getting a deep learning algorithm to learn from this sea of data and try to predict the future turn of events could be quite interesting! Could render catastrophic events avoidable? Could this model help us better plan our actions in the fight versus Covid?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Deep learning versus Covid-19: How to utilize time series prediction to fight Covid-19 effeciently?","","","3"
"4130","1275481","4719461","04/20/2021 06:45:27","## Task Details
Here's an Interesting Dataset about a PVC Manufacturing company getting enquiries about orders.
## Expected Submission
This task stays as long as I stay in this world. Feel free to submit yours

## Evaluation
 As we all know proper visualisation using Business intelligence could yield us better insights to improve a business. Bring your own insights and observation out of this datasets.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Visualization - Retail Analytics","","","1"
"4226","957775","5185638","04/29/2021 14:23:52","## Task Details
Michale wants a new laptop but he has not much knowledge of recent laptops. So he goes to a shop but he is not able to choose the right one, and he does not want to get involved in any marketing scheme. So he got an idea for a website where you provide basic details of your need and get suggestions of laptop.
He is finding one Machine Learning Engineer to do this. So what you think, are you able to make this program.

## Expected Submission
The output should be a list of 15-20 laptops.* In GUI Format(optional).*

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Suggest Me a Laptop","Learn Machine Learning Level: Easy","","3"
"3793","1216066","4721133","03/17/2021 02:45:30","I tried myself to predict the player's latest market value but I didn't have much success, especially because I think that to predict everything that surrounds a player's price there would have to be more specific match data in this dataset. Having that said, I challenge you to predict the player's price (""latest_market_value"").","","Predict player's latest market value.","","","0"
"3771","1210349","4730101","03/14/2021 13:04:32","The Studio has produced more than 600 films since its debut film, Snow White and the Seven Dwarfs in 1937. While many of its films were big hits, some of them were not. In this notebook, so explore the dataset of Disney movies and analyze what contributes to the success of Disney movies.","","Analyze the dataset","","","5"
"4497","1210349","4730101","05/26/2021 09:45:11","This task caters to the needs of all those who find data interesting, amusing, and excel in telling stories using data visualization! 


Your task is to go through the data, understand the underlying story from the data. Provide logical insights, EDA, and a story from the data. Finally, conclude how your EDA/ insights can help someone understand the hidden meaning/story/trend of the data.

**You can use the cleaned version of the data**","","Data Analysis Challenge","Ready to some interesting Data analysis tasks?","","2"
"3709","1201000","4730101","03/09/2021 09:27:36","Do EDA as you please with the cleaned version of the data. I will surely give an upvote.","","Explanatory Data Analysis","","","1"
"3710","1201000","4730101","03/09/2021 09:28:32","Clean the uncleaned version of the data as you please. I will surely give an upvote.","","Cleaning scrapped data","","","3"
"4498","1201000","4730101","05/26/2021 09:45:13","This task caters to the needs of all those who find data interesting, amusing, and excel in telling stories using data visualization! 


Your task is to go through the data, understand the underlying story from the data. Provide logical insights, EDA, and a story from the data. Finally, conclude how your EDA/ insights can help someone understand the hidden meaning/story/trend of the data.

**You can use the cleaned version of the data**","","Data Analysis Challenge","Ready to some interesting Data analysis tasks?","","1"
"3039","1057171","4730101","12/24/2020 21:49:25","You can use any method you like to develop a predictor that will predict if a given x-ray is of a covid patient who doesn't have pneumonia or of a covid patient who has pneumonia","","Covid classification","","","1"
"2884","1021737","4730101","12/07/2020 22:58:14","Task Details
This is a very simple task. I scraped the data myself. You'll have to clean and process the data and then just need to build a content-based recommendation system.

Expected Output
Given the preferences of a user give a top 10 recommended movie list for the specific user.","","Make Bollywood movie recommendation system","","","0"
"3943","1248313","4730101","04/03/2021 04:06:17","You can implement MBA using the apriori algorithm. Learn more about [apriori algorithm](https://www.geeksforgeeks.org/apriori-algorithm/amp/).","","Implement MBA","","","0"
"3827","1224135","4730101","03/21/2021 07:12:35","Our goal is to look through this dataset and classify songs as being either 'Hip-Hop' or 'Rock' - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.","","Binary Classification","","","1"
"4981","1445863","4730101","07/03/2021 13:51:39","Create a dashboard or do EDA in notebook to show some analytical results.","","EDA on covid cases in bangladesh","","","0"
"5070","1445953","4730101","07/10/2021 06:28:59","There are four age groups so you have to predict for 4 groups","","Predict age","","","0"
"4976","1412028","4730101","07/02/2021 16:19:23","Use your skill to present a better accuracy model for gender prediction either using mainstream ML or DL. Provide sufficient evaluation metrics to prove your model's accuracy. Then submit your notebook.","","Gender Prediction","Build a gender recognition model","","0"
"2446","923114","4730925","10/16/2020 05:51:13","## Task Details
Extract spectrograms from the wav files provided.

## Expected Submission
Submit the notebook where you explored the data 

## Evaluation
The notebook which shows the exact difference between different kinds of musics and their attributes will be considered perfect solution","","Exploratory Data Analysis","Analyze the difference between the different kinds of music and why they sound like a particular kind to you","","0"
"3665","1193856","4753861","03/05/2021 12:55:36","Lethal outcome (cause) (LET_IS) is the 124 column of the dataset. 
The classes of it along with their distribution is listed below: 
class : description: count: fraction 
0: unknown (alive) 1429 84.06%
1: cardiogenic shock 110 6.47%
2: pulmonary edema 18 1.06%
3: myocardial rupture 54 3.18%
4: progress of congestive heart failure 23 1.35%
5: thromboembolism 12 0.71%
6: asystole 27 1.59%
7: ventricular fibrillation 27 1.59%","","Predict Lethal Outcome from the input features.","","","1"
"4439","1352627","4754773","05/19/2021 10:51:05","## Task Details
Since the UNET Segmentation Model is highly accurate for Lung Mask Detetctio, Apply it to your own Biomedical imaging datasets.

## Expected Submission
Datasets with Lung Segmented Images!

## Evaluation
There are segmentation metrics, such as IoU, F1 Score, and Pixel Accuracy. try to evaluate models using such tasks.


### Further help
If you need additional inspiration, check out these existing high-quality datasets:
- [Chest Xray Masks and Labels](https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels) - (Predict the Lung Masks in CXRs)","","Create your own Datasets with this Novel Image Segmentation Model!","Try Segmenting your Own Datasets!","","0"
"4444","1353721","4754773","05/19/2021 19:23:40","## Task Details
Reach the heights of the LeaderBoard using this Dataset!","","Perform Classification and Localization Tasks","Perform Classification and Localization on Segmented Data!","","0"
"4456","1357388","4754773","05/21/2021 12:21:53","## Answer Why a journal has a particular Impact Factor?","","Obtain Insights for High Impact Factor Journals","Try to answer as to why a journal has a particular Impact Factor.","","0"
"3596","1135581","6812427","02/26/2021 06:38:29","Because of my final year project I have created this task. I have to predict the corrosion behavior of steel family through machine learning","","Corrosive properties of steel family","Corrosion properties","","0"
"2605","948942","4789522","11/02/2020 05:35:02","Use classification models to build a web page phishing detector","","Build a web page phishing detector","","","3"
"3006","1046860","4790330","12/21/2020 23:46:59","## Task Details
Market basket analysis is such a common application in market nowadays 
we can use apriori in the seeing usage patterns of the millennials 

this data set is created to study the behaviour pattern of the millenials apps usage 

its just a practice data set to practice apriori data analysis 

### market basket analysis being so common 

the data contains only one column having all the choices of 30 individual millennials , seperated by commas .

just try to make the  best out of it","","To find Apps used by users the most","","","1"
"4249","1311461","4793295","05/02/2021 16:12:41","## Task Details
Develop a model for detecting and counting pistachios in videos.","","Pistachio detection and counting in videos","","","0"
"2376","911312","4795937","10/08/2020 07:35:51","This task relates to the Sales of videos games  the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Exploratory Data Analysis","","","6"
"2367","910727","4795937","10/07/2020 20:45:51","This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Background Data analysis","","","4"
"2836","1011340","4796387","12/02/2020 11:39:24","Write binary classifier to predict direction of current conditional branch","","Predict target Branch Taken/Not Taken","Binary classification prolem","","0"
"2723","970658","4800378","11/18/2020 16:55:20","### Task
Find out the frequency of rumors related to COVID Vaccines","","Find out the frequency of rumors related to COVID Vaccines","","","0"
"3329","1121599","4801377","01/30/2021 07:00:55","## Task Details
Emulate the IMDb Top 250 leaderboard

## Expected Submission
IMDb reassigns TV Series/Movie Ratings while assessing the Top Shows & Movies for their Leaderboard. This is he reason why a few highly rated entries don't make it on the list. It would be great if folks could try and emulate the algo for creating the list.

## Evaluation
The closer you can get to the actual list, the better. Without any manual hijinks of course.","","IMDb Top 250","","","0"
"3917","1226624","4813487","03/31/2021 04:30:58","## Task Details
Prepare a model to classify the skin into wrinkled or Non-wrinkled. Use the computer vision to detect and analyze the skin. Use of saliency maps or attention to detect abnormal locations are recommended but not required.

## Expected Submission
Everyone is required to submit the work in the Jupyter notebook. 

## Evaluation
This is for learning purposes.

### Further help","","Computer Vision Analysis of Skin Wrinkles vs Non Wrinkles","","","0"
"3228","1075326","4823627","01/20/2021 09:44:02","## Task Details
Since the dataset was collected using PushShift API the dataset contains some posts that were not related to suicide and depression. The majority of these kinds of posts are made by the moderators of respective Subreddits for announcements and maintenance. The task is to clean the dataset by identifying and removing such posts to further improve the dataset.


## Expected Submission
The expected submission is a notebook displaying the methods used to clean the dataset.

## Evaluation
The evaluation is done by comparing with the original dataset by training a model on both the datasets and verify whether the newly curated dataset performs well compared to the original one","","Data Cleaning","Remove posts by moderators","","0"
"5161","1206061","4828068","07/17/2021 08:31:44","## Task Details
I was able to create an interesting visualization with Tableau, i'd like to create something similar with Python

## Expected Submission
Submit how you'd like to but any glacier visualised with the change over time.

## Evaluation
Any chosen glacier to be visualised with the change over time to see the change in the glaciers. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create one Visulisations of the data per area","","","0"
"4047","1262467","4835558","04/12/2021 04:59:56","This is a very easy task as match summaries are stored into a seperated file.","","Highest Man of The Match Award","Find the player having highest player of the match award","05/31/2021 23:59:00","1"
"4997","1262467","7526686","07/04/2021 13:30:35","Can you do this data for all seasons???","","Dataset","","","0"
"2992","1046735","4837224","12/20/2020 11:25:00","## Task Details
Republic of Korea has dramatically developted its economy after Korea war in 50's. Now its world economic figure ranks around 11th. Apart from the economic development any problem behind this? 

## Expected Submission
Any insights from the data such as income inquality among gender, age and job occupation etc? 

Can we expect its income trends based on modeling? 

Whatever insights behind data can improve the society. Please do not hesitate to try to work on. 

## Evaluation
A good model can give prediction with insight which can allow Korean government to apply.","","How can improve policy?",": in case of income inquality","","1"
"3031","1056247","4837224","12/24/2020 07:35:11","## Task Details
Korea now faces 3rd wave of COVID-19. People and self-owned workers are getting difficult these day. By when this pandemic will end? 

## Expected Submission
Please predict the confirmed cases trends and by when the pandemic will go down. 

## Evaluation
A good solution will include how the epidemic will end in seeking the behaviour on time of infection rate or transmission rate, recovery rate and deaths rate or mortality rate.
Actually no limits in this task so that please feel free to participate. Thanks.","","Forecasting the end of COVID-19","Predict by when the epidemic will end in Korea","","2"
"4683","1396581","4842681","06/08/2021 13:16:07","## Task Details
Create and train a model to map pupil centers to screen positions

## Expected Submission
Model should be exported into gaze-cursor:: https://github.com/chethan-hebbar/gaze-cursor

## Evaluation
Model should give fairly good results 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Gaze-Cursor-Model","A model to map pupil centers to screen pixel locations","06/15/2021 23:59:00","0"
"2644","960023","4848480","11/07/2020 15:35:05","This dataset is original from Denver Gov website without any change.
But @gatandubuc noticed the 'Geo_x' and 'Geo_y' looks weird in the dataset.
I thought they are just longitude and latitude at the beginning, but it will be in middle of nowhere in the ocean if so.
I already emailed the gov website about this question.
Meanwhile, if anyone have seen this kind of geo measure before?","","The type of the data","","","0"
"2622","952137","4848480","11/03/2020 17:25:14","## Task Details
When I was trying to make plot, I think I should put the proportion on each columns on the count plot, but I didn't figure out how to do this.

## Expected Submission
Just notebook about this","","Seaborn countplot probelm","How to add number on the countplot","","0"
"3587","946751","4848480","02/24/2021 03:21:55","Please analysis whatever you think is useful:)
Even data visualization!","","Analysis the rental in nyc","","","0"
"2671","966562","3907139","11/10/2020 18:56:22","weights","","Weight","","","0"
"3113","1074447","4852445","01/03/2021 10:57:05","## Task Details
Your task is to discover useful insights about the dataset using any tools that you wish

## Expected Submission
A notebook with your visualisations and findings

## Evaluation
You should try answer questions such as

- Is the data skewed? how will this impact my ability to model?
- How is the data distributed?
- What are the correlations of fraud transaction?
- What feature best helps define whether a transaction is fraud or not?","","Ethereum Fraud Detection EDA","","","0"
"3114","1074447","4852445","01/03/2021 11:01:38","## Task Details
In this task, you must do your best to predict the probability of a transaction being fraud or valid

## Expected Submission
A notebook with your appropriate findings

## Evaluation
You should use appropriate CV as the dataset is **imbalanced**","","Prediction of Fraud/Valid Transaction","","","1"
"3067","1065169","4869449","12/29/2020 09:51:10","Testing the process","","Testing the process","","12/30/2020 23:59:00","0"
"3868","1234063","4869710","03/26/2021 11:07:19","Correct face rectangle","","Image faces align celebra","http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html","","0"
"3489","1156519","4870161","02/14/2021 01:39:57","## Task Details
Forecasting the solar radiation is an important research field .

## Expected Submission
Forecasting a day  of 2019 

## Evaluation
I have made my evaluation with RMSE so i think RMSE seems reasonable evaluation and comparison is cool as well.","","Time Series Forecasting","","","0"
"3663","1189488","4874474","03/05/2021 10:04:15","The given data is about cubes which are used for advanced vertical farming. You can insert seeds and they will be grown automatically, the cube controls e.g. watering, ventilation, lighting etc. The cubes are connected via wifi to our backend where multiple signals from the cubes (e.g. temperature) are monitored. The cubes have two separate layers where plants can be grown (A and B). Usually the data is dumped in a json format but we converted them for you into a .csv format.

Tasks:
Your overall task is to analyse and understand the given data by preparing a data journey.
Undertake necessary data cleaning steps. How do you explain the missing values? How do you handle them?
Visualize and describe the data. Give insight into your interpretation and thoughts.
Some of the cubes do not behave like the others. Your task is to find out which ones. Visualize your results.






My Take on dataset 
Not much attributes are provided here, so

Task 1
According to dataset cube id is used for mark of identification for many tasks.
here, they have provided 2 different layers so i can compare between both of them.
with this comparison i will choose a layer for my farming.

Task 2

In Data cleaning remove the unwanted data
Visualize them according to layerA and layerB like pie chart comparison
explain the findings
so, according to the result (probably apply clustering algorithm, it may provide u  (Some of the cubes do not behave like the others) the required result. and visualize them again.","","vertical farming","cubes which are used for advanced vertical farming","07/05/2021 23:59:00","0"
"4157","1004524","4877125","04/22/2021 15:11:24","Contains 3 csv files , 2 for the train and 1 for the test . Each file contains approx. 1000000 samples . Cleanse the data if required and Build a best possible ML model for the dataset.","","Predict Salaries","","","0"
"3071","1060121","4878232","12/29/2020 19:10:12","## Task Details
Build a machine learning model with the data provided to achieve the best AUC-ROC Score.

## Evaluation
AUC-ROC Score","","Multilabel Classification of text","","","0"
"3184","1094173","3564129","01/13/2021 17:00:15","## Task Details
Which Highschools in Romania have the biggest chances of a pupil to pass this exam?

In other words: which Highschools have the biggest chance of having 100% of pupils passing the Baccalaureate exam?

## Expected Submission
A notebook with EDA.","","Which Highschools have the biggest pass rates?","","","1"
"4364","1338481","4894234","05/13/2021 14:02:03","## Task Details
Perform an HR analysis by visualizing the data set.","","Data Visualization","","","0"
"4365","1338481","4894234","05/13/2021 14:03:10","## Task Details
Do the employee segmentation.","","Clustering","","","1"
"3234","1109790","4901421","01/21/2021 06:58:25","ÔÉò	–ù–∞–ø–∏—à–∏—Ç–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.

ÔÉò	–û—Ü–µ–Ω–∏—Ç–µ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. 

ÔÉò	–ß–µ–º—É —Ä–∞–≤–µ–Ω –ø—Ä–æ—Ü–µ–Ω—Ç –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –æ–±—ä—è—Å–Ω—ë–Ω–Ω—ã–π –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏? 

ÔÉò	–ß–µ–º—É —Ä–∞–≤–µ–Ω –ø—Ä–æ—Ü–µ–Ω—Ç –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –æ–±—ä—è—Å–Ω—ë–Ω–Ω—ã–π –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏? 

ÔÉò	–ö–∞–∫–∞—è –∏–∑ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–∞ —Å –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π?

ÔÉò	–ü–æ–¥—Ä–æ–±–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π—Ç–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –æ–¥–Ω–∏–º –∏–∑ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–≤ –∏ –∑–∞–≤–∏—Å–∏–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.","","Training task according to YouTube","Perform multiple linear regression analysis","","0"
"3682","1170633","4908082","03/07/2021 11:50:12","This dataset can be used publicly for the purpose of exploratory data/predictive analysis.","","Exploratory Data Analysis","","","1"
"3136","1080784","4932016","01/06/2021 05:16:19","DO EDA and Viz","","Data Analysis","","","1"
"4942","1437628","4932840","06/29/2021 07:13:54","For each user, predict the top 3 probable product categories that they may purchase from, in the future.","","Predict the top 3 probable product categories","","","0"
"3207","1099653","4946340","01/16/2021 01:23:24","Volatility is a variable that represents the frequency and intensity of fluctuations in the price of an asset over a given period of time. The greater the volatility, the greater the change in the price of the active asset.
But the volatility goes beyond that. It shows it is also related to the market itself. Financial market indices can be influenced by a number of external factors, such as the political scenario, investor optimism and pessimism, or a **pandemic**.","","Analyse the volatility","Use pct.changes() in PANDAS library","","0"
"3208","1099653","4946340","01/16/2021 01:40:36","!!!","","Visualize the stock exchange","Use lineplots and your creativity","","0"
"3216","1099653","4946340","01/17/2021 20:20:59","In statistics, a moving average (rolling average or running average) is a calculation to analyze data points by creating a series of averages of different subsets of the full data set. It is also called a moving mean (MM) or rolling mean and is a type of finite impulse response filter.","","The power of the moving average","","","0"
"3210","1099653","4946340","01/16/2021 15:45:38","Use corr() and heatmap","","Correlations, correlations and correlations","Correlation between companies","","0"
"3893","1236764","4946340","03/28/2021 19:24:15","Be creative","","Neural style transfer","","","0"
"3845","1228387","4946340","03/23/2021 12:05:10","Clusterizing the books","","Clusterization","","","0"
"3849","1228637","4946340","03/23/2021 14:41:19","Classification by features","","Classification algorithm","","","0"
"3842","1227452","4946340","03/23/2021 02:35:23","Clean and organize the data","","Data Engineering","","","0"
"3864","1232842","4946340","03/25/2021 19:41:37","Tutorials are very importants in Kaggle","","The basic visualization (barplots)","","","0"
"3854","1229253","4946340","03/23/2021 22:02:00","Be crative","","Correlation between price and weight","","","0"
"3855","1229356","4946340","03/24/2021 02:50:46","Are you a martian?","","Visualize the red planet","","","0"
"3859","1231264","4946340","03/25/2021 00:06:30","Data cleaning is a very important step","","Data cleaning","","","0"
"3262","1114985","4948370","01/24/2021 02:04:23","## Task Details
Take the noise data and mash's it up with speech and creates a DNN/RNN model that can remove fan noise from future speech recordings. 

## Expected Submission
A notebook

## Evaluation
No noise or reduced noise","","Denoise audio","DL Model to reduce/remove Noise","","0"
"2690","962637","4948370","11/13/2020 10:43:17","## Task Details
Every labelled data needs to have more than 1 labeller go throw it. It helps with bias and human error.

## Expected Submission
The helpers go throw the spreadsheet and note down the tweets whose labelling they disagree with. How they share this is left to them.

## Evaluation
A before and after should be included. If applicable a reason should be given why they think the label should be different.


### Further help
You can contact me on LinkedIn.","","Second Opinion on labels","Verify the existing labels given to tweets as appropriate","","0"
"2691","962637","4948370","11/13/2020 10:51:27","## Task Details
Labelling 340 is not enough. We need more labelled tweets to get better classification results. Contributing 50 should not be too hard. Your name will be added if you consider helping out.
Before starting, please contact me so that two people do not submit the same subset. I will assign you numbers.


## Expected Submission
Submit another spreadsheet. Follow the tweet order given in the dataset. Only add the labels. I will update it on my end.

## Evaluation
When in doubt about the label, go with your gut feeling. Getting a negative tweet right is crucial.  

### Further help
My own examples should give you an idea.","","Label more tweets","50 labels per contributor","","0"
"3520","962637","4948370","02/17/2021 04:40:11","Deploy the use of Glove Embeddings and solve the sentiment analysis problem","","Use Word2vector for sentiment analysis","","","0"
"2967","1041732","4948370","12/16/2020 21:30:05","## Task Details
Classify Snape's mood and create interesting visualizations.

## Expected Submission
Notebooks

## Evaluation
Unique insights","","Mood Map","","","0"
"4564","1377394","4954907","05/30/2021 15:02:05","Perform an end-to-end EDA on the dataset linked above.

What expecting?

1.Detailed univariate and bivariate analysis with proper observations.
2. Identify some research questions and perform hypothesis testing to test your hypothesis.
3. A proper conclusion containing the best observations.
4. Your Jupyter Notebook should look like a storytelling book.","","Perform an end-to-end EDA on the dataset","","","0"
"3080","1067633","4959048","12/31/2020 03:32:33","catatan üíØ 

uji coba kernel cnn 1D 3x3 conv acc 89.9
uji coba kernel cnn 1D 5x5 conv acc 88.9
uji coba kernel cnn 1D 7x7 conv acc 82.4
uji coba kernel cnn 1D 8x8 conv acc 85.5
uji coba kernel cnn 2D 5x5 conv acc 92.5
uji coba kernel cnn 2D 6x6 conv acc 91
uji coba kernel cnn 2D 7x7 conv acc 88.4
uji coba kernel cnn 2D 8x8 CNN acc 98.0 LSTM 1 D acc 98.5

uji coba data 90/10 cnn 2D 8x8 conv acc 97.0 LSTM 1 D acc 100

uji coba kernel LSTM 1D acc 98
uji coba kernel GRU 1D acc 99
uji coba kernel BiLSTM 1D acc 100

uji coba kernel cnn 1D 8x8 conv CNN+LSTM acc 88.9

uji coba 9 machine learning acc tetinggi KNN 0.91 RF 0.885
uji MLP acc 0.9
uji Restricted Boltzmann Machine acc 0.90
uji ELM acc 0.765

uji pca+RF acc 0.825
uji pca+KNN acc 0.76
uji pca+DT acc 0.71

uji GA+KNN acc 0.95
uji GA +RF acc 0.90
uji GA+LR acc 0.66

uji FastICa+KNN+GA acc 0.98
uji FastICa+RF+GA acc 0.90
uji FastICa+DT+GA acc 0.80




windowing 1 s time series perkatoda
filter notch 60 HZ
filter bandpass 7=13 hz
fft 60 hz
gyro off


- dengan menggerakan tangan dan kaki secara berulang selama satu menit
- windowing 1 detik pada open bci dengan rentang fiter nitch 60 hz dan band pass filter 7-13 hZ
- peletakan katoda dengan lima point pada pengiriman lsl 8 chanel

<p><img src=""https://www.opensourceimaging.org/wp-content/uploads/Open-Source-Imaging-OpenBCI-headsetcomponents.jpg""></p>
<p><a href=""https://youtu.be/vVa7jKWX0eY""> <img src=""https://firebasestorage.googleapis.com/v0/b/unejpcd.appspot.com/o/tesmodel.jpg?alt=media&token=e4d25000-6af3-4476-b33a-393abe9dc71b"" style=""width: 400px; height: 200px""></a></p>


<p><a href=""https://youtu.be/fizB0YE4Olw""> <img src=""https://firebasestorage.googleapis.com/v0/b/unejpcd.appspot.com/o/ujimodel.jpg?alt=media&token=803028bd-37a0-4aa4-8e94-94492887273f"" style=""width: 400px; height: 200px""></a></p>","","rancangan thesis","sensor eeg","01/30/2021 00:00:00","1"
"3202","1097412","6376555","01/15/2021 15:13:11","Take several pictures of a small object (e.g. tea cup, small chair) from different agles and short distance. (e.g. as far as to reach by jand)
    For better assessment, describe the object in question (what are you taking pictures of).
    Obtain the collage of the images that includes the object in question.","","Step 1","","","1"
"3355","1129032","4969123","01/31/2021 00:33:58","Extract keywords with respect to types/topics and recognize patterns","","Keyword Extraction","","","1"
"3382","1134893","4969123","02/02/2021 21:54:54","Build a Computer Vision model to predict whether a car image belongs to a luxury or mass-market car brand","","Luxury/Mass-Market Prediction","Determine whether a car is luxury or mass-market car","","0"
"3397","1136809","4969123","02/03/2021 21:01:37","Determine the most successful brands and agencies according to the given metrics.","","Most Successful Brand & Agencies","","","0"
"3374","1132863","4969123","02/01/2021 21:40:12","The main purpose is to find a way to get the actual ranking. Before you take action, try to get insights from Exploratory Data Analysis.","","Determine the Global Ranking","","","0"
"3362","1130536","4969123","01/31/2021 22:19:27","Using statistical analysis and data visualization techniques, try to extract insights from country distributions","","Explore the Global Distribution","","","0"
"3363","1130536","4969123","01/31/2021 22:22:42","Perform NLP and Classification operations on types.csv and build a model which takes a text as input, extracts characteristics and predicts MBTI-Type of given person","","Classify Types","","","1"
"3431","1144608","4969123","02/07/2021 21:48:28","Analyze the trends and predict the next years","","Future Predictions","","","0"
"4090","1275510","4969123","04/16/2021 19:07:13","Compare the audio features of each market. You might also group regional areas and compare these regions (e.g. Europe vs. Asia), or you can cluster directly by similar audio features.","","Global Trends","Analyze global music trends","","0"
"4126","1278758","4969123","04/19/2021 10:21:06","Analyze the global distribution of followers and popularity with respect to artists and markets. Perform statistical analysis and visualize your findings.","","Global Trends","Analyze Global Trends","","0"
"5640","1351962","4971832","08/09/2021 17:15:07","The opening, closing and lowest and highest price has been mentioned. Design a model to predict each of the feature using time series analysis. 
Using other parameters to predict the continuous value is not called time series analysis","","Create a model to perform time series analysis","Using LSTM based models for time series forecasting","","0"
"4355","1337268","4979413","05/12/2021 23:26:12","## Task Details
I created this to see how submitting an article to Medium Partner Program could potentially change how successful the article is

## Expected Submission
A notebook with the analysis

## Evaluation
How complete the analysis is","","Analyze the differences in data when partner program is enabled or disabled on an article","","","0"
"4780","993957","7445759","06/15/2021 04:16:25","## Task Details
i want to start my researh thesis in Voice recognition through machine learning algorithm and want to implement in Matlab software. i need guidance regarding this data set. 

Regards","","This data is suitable for Voice recognition through ANN / MACHINE LEARNING in MatLab software?","","","0"
"4992","1447285","4997746","07/04/2021 11:09:08","This dataset has some columns whose names have been replaced with character for privacy issues. You can use these for training your data  to predict the target class - 1 or 0 using any classification algorithm","","Predict the target class for the dataset","","","1"
"4993","1447382","4997746","07/04/2021 11:18:00","You can perform EDA to see what demographics define the customer who will purchase the products and find what exactly our customer looks alike","","Exploratory Data Analysis of purchased customer","Check which demographics best define purchased customer","","6"
"2966","1040102","4998079","12/16/2020 18:50:00","## Task Details
You can build a recommendation system using this dataset.

## Evaluation
You can check the accuracy score as the accuracy of the model. Because this will be the main data of the model after training of the model.

### Further help
If you need additional inspiration:
I have built a movie recommender using this dataset. Which you can check out:
[live demo](https://mov-recommender.herokuapp.com/) or the repository on [GitHub](https://github.com/divyanshugit/Recommendation-System-based-on-Sentiment-Analysis)","","Recommendation System","","","0"
"3741","1206582","5007041","03/12/2021 09:14:20","You can find more info about dataset at:
https://www.moneycontrol.com/financials/jubilantfoodworks/balance-sheetVI/JF04

The task in to predict how healthy the stock is and whether we should invest in it short/long term.

The notebook should contain exploratory data analysis of the stock. You have 3 key datasets: balance sheet, cash flow ratio and income statement. Other than this, you also have share prices monthly and daily.","","Predict if the stock is investment worthy","","","0"
"3459","1074096","5007851","02/10/2021 19:11:11","Find out which country won the most gold medals in the Olympics between 1896 and 2018.","","Analyze which country has the first place in getting gold medal","Find out which country won the most gold medals in the Olympics between 1896 and 2018","","1"
"3447","1117941","5007851","02/09/2021 11:13:01","## Task Details
This dataset contains 20 years of various MNC's stocks now you need to predict the stock price for the next 30 days.

You can play with this dataset as you wish and create a notebook to better understand your performance to others and if you are wrong you can correct yourself","","Predict the price of stocks for the next 30 days","","12/31/2021 00:00:00","1"
"4359","1337761","5011242","05/13/2021 07:56:17","Use the Machine learning model to predict the stress level of the company .","","Predict the final degree of stress of a company .","","","0"
"3230","1108393","5011373","01/20/2021 13:41:39","## Task Details
Perform some basic exploratory data analysis using plotly or any other visualization library.","","Exploratory Data Analysis on Dataset","","","0"
"3138","1081153","5011373","01/06/2021 08:53:44","## Task Details
Create and Analyze India Flood Inventory.","","Exploratory Data Analysis on India Flood Inventory.","","","0"
"3101","1072696","5011373","01/02/2021 11:10:16","Perform some Exploratory Data Analysis on Eminem's Lyrics or you could also do views prediction.","","EDA on Eminem's Lyrics","","","0"
"2371","910671","5017794","10/08/2020 01:46:39","## Task Details
Exploratory Data Analysis - Analyze flight delay and/or cancellation data to determine the airline(s) most affected by the pandemic. You can utilize older data from other airline datasets as your control data.

## Expected Submission
The submission should be in the form of a Notebook ideally including visual elements.","","Analysis of Most Affected Airline(s)","Determine the Airline(s) most affected by the COVID-19 Pandemic","","4"
"4735","1403213","5024569","06/11/2021 16:58:56","## Task Details
The dataset contains data of news title , Text snippet and their respective group.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
Accuracy
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Text Classification","News_group Classification","","0"
"4739","1403569","5033862","06/11/2021 23:11:52","Using the genres and other features given in the dataset try to predict suggestions for similar anime.","","Recommend Similar Anime","","","0"
"4797","1405292","5033862","06/16/2021 22:18:35","## Task Details
Given all the genre and plot points can a recommender system be built that suggest similar shows.","","Recommendation System","","","1"
"2876","1018871","5040080","12/06/2020 13:21:14","Perform EDA to find out how much one of these features influence the others and also to find out parts of history that affected the climate.","","Exploratory Data Analysis","","","0"
"4513","1299503","5041979","05/28/2021 03:15:44","Just add more data if available or just use existing and classify wheteher an image is terror related or not","","Classification Of Terrorism","","","0"
"3920","1243037","5042976","03/31/2021 08:31:21","**Goals**
1. Get an overview of the Starbucks population
- What is gender distribution?
- What is the income distribution?
- When do people typically become a member?
- What is the average purchase distribution?
2. How do people react to different promotions?
- Do people react to different promotions differently?
- Does the reward of the promotion make people react differently?
- Does it make sense to offer certain rewards?
- Would Starbucks save money overall if they offer a certain reward?
3. How well does a machine learning algorithm cluster the groups of users together?
- How many clusters should Starbucks use
- 2 Clusters? (BOGO and Discount customers?)","","Starbucks Customer Clusters","","","1"
"3989","1255948","5042976","04/07/2021 08:32:38","## Task Details
Draw some simple plots.","","Quick Charts","","","0"
"3936","1246823","5043139","04/02/2021 07:48:26","## Task Details
Visualized the registered citizen data daily. An interactive map is highly encouraged.

## Expected Submission
An interactive maps, using Plotly, for example, to visualize the data.

## Evaluation
The best one stands out!

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualizing Daily Registered Data","","","1"
"3540","1168829","5043171","02/19/2021 06:47:51","Try to generate titles given their images.","","Title generation","","","0"
"3818","1222086","5043444","03/20/2021 03:26:38","## Task Details
The entity extraction from the images is a two step process. First reading the data from images (using an OCR) and extracting the relevant information from the data. This task consists of the second step.  The information from images is given in XML files and the task for the users is to extract entities (NER) from these XML files. The entities to be extracted are: invoice no, invoice data, company name,  address of the company, telephone number, invoice amount and particulars of the invoice.   

## Expected Submission
An Ideal solution should recognize different entities with maximum accuracy.","","Entity extraction using text data (XML Files)","Extract entities like invoice no./date from text data of XML files","","1"
"4544","1374851","5045408","05/29/2021 10:18:47","## Task Details
The Dataset has been fetched from National Stock Exchange. It has 5 years of various Stock components in the Banking Industry. Your task would be to forecast prices for the next 1 week to a month.


## Evaluation
Try out different forecast metrics and compare your performance.","","Stock Price Forecasting","Forecast Prices for the next 1 week to a month","","1"
"4818","1416017","5045408","06/17/2021 17:11:35","## Task Details
The Dataset has been fetched from National Stock Exchange. It has 5 years of various Stock components in the Banking Industry. Your task would be to forecast prices for the next 1 week to a month.


## Evaluation
Try out different forecast metrics and compare your performance.","","Stock Price Forecasting","Forecast Prices for the next 1 week to a month","","0"
"4703","1398992","5045408","06/09/2021 15:16:29","## Task Details
How the Air Quality Index would vary in the upcoming month.

## Evaluation
Use any metric of your choice.","","Forecast Air Quality","Can you produce accurate forecast for the next month","","0"
"3650","1190309","5047229","03/03/2021 09:13:35","check what is the growth of one stock with respecting to another stock.","","Analyze of different stocks price","","","0"
"3655","1190309","5047229","03/04/2021 05:03:21","Based on historical EOD stock price data try to predict the what would be the stock price on next day.","","predict the stock opening price","","","0"
"3656","1190309","5047229","03/04/2021 05:04:21","Do the analysis of different stock volume data of every day and try to find the different trends and pattern in the everyday stock volume data.","","Analyze stock volume","","","1"
"3657","1191907","5047229","03/04/2021 06:29:31","You have last 2 years data of each stock. You can analyze the price growth of each stock in last two year and what kind of returns it is giving. Then try to analyze one stock growth with another stock to see which stock is performed better in last two years span.","","Stock  price growth Analysis","","","0"
"4133","1281393","5058161","04/20/2021 11:18:04","## Task Details

Among all the cars of top brands find which one out performs others 

## Expected Submission

Any kind of visualization using any library and any language

### Further help

If you need additional inspiration, check out these existing high-quality tasks:

https://www.kaggle.com/soumyadipghorai/covid-19-vaccination-drive-india

https://www.kaggle.com/soumyadipghorai/indian-food-visualization-3d-west-bengal","","Find the supercar with best features","Among all the cars of top brands find which one out performs others","","0"
"5052","1456381","5058161","07/08/2021 20:54:58","## Task Details
Try to find which states has the highest number of top institutions 

## Expected Submission
Any kind of visualization using any library and any language


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/soumyadipghorai/covid-19-vaccination-drive-india

- https://www.kaggle.com/soumyadipghorai/indian-food-visualization-3d-west-bengal","","Plot location of the top colleges on map","Try to find which states has the highest number of top institutions","","1"
"3338","1128175","5067577","01/30/2021 14:28:45","Using the data, you can try multiple methods to either predict the price of the land by using KNN, linear regression, or a decision tree model. The data can also show demand on lands based on location. There are a lot of potentials that can be used to get intel from this dataset!

Some challenges to face would be dealing with the null values in purpose, as they are real null values. Something else to consider is working with sublocation to be within mainlocation, and/or combining different main location based on the province or the government it is on.","","Things to consider","","","0"
"4478","1364652","5069500","05/24/2021 18:55:15","...","","Bellabeat Case Study","","","0"
"4088","1275753","5073182","04/16/2021 13:31:57","This is the data of a social media platform of an organization. You have been hired by the organization & given their social media data to analyze, visualize and prepare a report on it. 

You are required to prepare a neat notebook on it using Jupyter Notebook/Jupyter Lab or Google Colab. Then, zip everything including the notebook file (.ipynb file) and the dataset. Finally, upload through the google forms link stated below.
The notebook should be neat, containing codes with details regarding your code, visualizations, and description of your purpose of doing each task.

You are suggested but not limited to go through the general steps like -&gt;
Data Cleaning,
Data preparation,
Exploratory Data Analysis(EDA),
Correlations finding,
Feature extraction, 
and more. (There is no limit to your skills and ideas)

After doing what needs to be done, you are to give your organization insights and facts. For example, are they reaching more audiences on weekends? Is posting content on the weekdays turn out to be more effective? Is posting many contents on the same day make more sense? Or, should they post content regularly and keep day-to-day consistency? Did you find any trend patterns in the data? What are your advises after completing the analysis? Mention them clearly at the end of the Notebook. (These are just a few examples, your findings may be entirely different and that is totally acceptable. )

Note that, we will value clear documentation which states clear insights from analysis of data & visualizations, more than anything else.
It will not matter how complex methods are you applying, if it eventually does not find anything useful.","","Social Media Data Analysis","","","0"
"2678","968249","5076215","11/11/2020 17:57:13","This dataset contains information about used cars listed on www.cardekho.com This data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.","","Prediction","This dataset contains information about used cars listed on www.cardekho.com This data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.","","0"
"4477","1363746","5079094","05/24/2021 11:54:00","Predict the power that is generated (in KW/h) based on the various features in the dataset.

score -&gt; max(0, 100*metrics.r2_score(actual, predicted))","","A Fine Windy Day- HackerEarth Challenge","","","0"
"2900","1026116","5100459","12/10/2020 03:55:53","Greyhound racing is very prosperous, if you pick the winner..
Dense Neural Net gets me around 60% accuracy check the notebook.

can better be achieved? or is there a reason they're willing to wager against you selecting the winner ^.^","","beat 60% accuracy on the validation set!","","01/05/2021 23:59:00","1"
"2398","914403","5107963","10/10/2020 17:41:53","Task Details
The task is to build a machine learning model which can predict the AQI based on the given features. Also find a way if you can to fill out the null values.

Evaluation
Try to keep the MSE(mean squared error), MAE(mean absolute error) low as possible.

Submission
You can submit your notebook named as ""submission"".","","Pune AQI prediction","","","3"
"4237","1308417","5108943","05/01/2021 06:24:34","which country people are  more affected to residential changes??","","Residential changes","","","0"
"4227","1304979","5108943","04/29/2021 16:18:36","How many number of billionaires who are in top and what and all the companies related to sector whether it belongs to software, finance etc...","","Top Billionaires","","","0"
"4218","1303023","5108943","04/28/2021 18:34:59","How many people are died who are in politics,doctors,scientists compared to age","","Doctors died due to corona","","","2"
"2626","924084","5121559","11/04/2020 07:46:51","## Task Details
What's more interesting than analyzing the best team composition with this dataset. Find out which champion contributes to a win the most at which length of the game. Or maybe find out what are the worst champions at their early-late game.

## Expected Submission
Anything you found interesting : )

## Evaluation
Up to you as well : )","","Predict Win vs Loss","","","0"
"3079","1068533","5121559","12/31/2020 01:01:43","## Task Details
Optimize team draft.

## Expected Submission
Best team comp with evidence.

## Evaluation
Up to you","","What is the optimized team composition to win?","","","0"
"3570","1173299","5130232","02/21/2021 17:47:15","## Task Details
Use data cleaning/feature engineering techniques to process the data with any regression algorithm. 

## Expected Submission
No submission is needed, just stay calm and practice you cleaning/wrangling techniques and regression analysis. 

## Evaluation
If you learned smth while doing this task you already a winner:)
P.S. also you can try to reach 99.99% accuracy.","","Predict the price of the flat","","","0"
"4051","1266932","5130232","04/12/2021 13:26:56","Use Data Augmentation and other techniques to boost the accuracy of your classification","","Use Data Augmentation and other techniques to boost the accuracy of your classification","","","0"
"5020","1450817","5130382","07/06/2021 09:49:38","## Task Details

Vaccine Reactions of Pfizer/BionTech, Moderna and Johnson & Johnson (Janssen) vaccines","","EDA and Visualization of Vaccine Reactions","","","0"
"4790","1411746","5135796","06/16/2021 07:18:04","On the basis of the amount of the electricity bill, make a Bar Chart. Then, on the basis of the Descending, accomplish the ""Sorting Process"".","","Electricity Bill","Unit based expense.","","0"
"2716","980894","5135877","11/18/2020 05:56:54","In this you will perform some predictive analytics on the loans data. Predictive analytics problems can be classified into two types: classification problems and regression problems. You will encounter both types of problems this week. First, you will use predictive analytics to classify the loans into good and bad loans. The predictive analytics task is a classification problem. You are expected to consider several different classification tools. You need the strongest predictive performance, you are expected to try different ways to improve the model performance.
Second, you will use predictive analytics to predict the loss from bad loans, which is a regression problem. Again, you should consider different tools available and try to improve predictive performance.","","Predictive modelling","","","0"
"3565","1172880","5138057","02/21/2021 14:09:03","## Task Details
Do some data analysis and show them with the help of seaborn for better understanding.

## Expected Submission
Data Analysis and Visualization on data","","Data Analysis & Visualization","","","2"
"3683","1172880","3121857","03/07/2021 16:30:14","## Task Details
Build a recommender system to recommend Top 10 TV series similar to any Title provided","","Recommender System","","","1"
"3626","1186840","5138057","03/01/2021 11:01:49","Make a recommendation model based on genre.","","Which should I watch next according to genre??","","","3"
"3521","1131493","5138057","02/17/2021 11:25:32","## Task Details
Mention the name of the movies that are getting the good rating and also able to earn good amount of money according to different genre. 

## Expected Submission
Do Exploratory Data Analysis.

## Evaluation
Mention the top 5 movies names according to different genre.
like:- 
for comedy:- 1,2,3,4,5 names 
for action:- 1,2,3,4,5 names
etc...
but you must follow the condition mention in task details.","","What type of movies are getting good rating and gross amount?","Exploratory Data Analysis","","10"
"3522","1131493","5138057","02/17/2021 11:34:50","## Task Details
Tell the best 5 name of the movies according to the entered name of the movie.
So, it must give me best 5 recommendation.

## Expected Submission
Use Content Based Filtering.

## Evaluation
With the name of the movie you must print its details like name of movie, director name, name of all 4 stars, genre, imdb rating, and gross. Only these things not more than these.","","Which movie should I watch next?","","","1"
"3109","990927","5144058","01/03/2021 05:40:09","Every task has a story. Should we insert 1 language columns to the table. The value will be language. 

Expected Submission

you will submitt csv. should you use the dataset and Notebook
Evaluation

Every graphs is solution.
If the csv include the Language new column it is good task result.

I would like to suggest kaggle page. so I say that you can search here on the dataset.","","make new columns","new colums","01/05/2021 23:59:00","0"
"3108","947899","5144058","01/03/2021 05:39:40","## Task Details
Every task has a story. Should we insert 1 language columns to the table. The value will be language. 

## Expected Submission
you will submitt csv. should you use the dataset and Notebook

## Evaluation
If the csv include the Language new column it is good task result.

### Further help
I would like to suggest kaggle page. so I say that you can search here on the dataset.","","make a graphs about the film mumber.","make  a graphs","01/04/2021 23:59:00","0"
"3142","1082186","5145658","01/06/2021 17:53:09","## Task Details
What is the breakdown of tweets using various sentiment analysis tools?","","Tweet Sentiment Analysis","","","0"
"3143","1082186","5145658","01/06/2021 17:54:46","## Task Details
There are two CSVs in this dataset, one of tweets that contained ""#beandad"" and one which contained the more general ""bean dad"". What are the differences between these kinds of tweets?","","Compare the Hashtag Tweets to Normal Tweets","","","0"
"3144","1082186","5145658","01/06/2021 17:57:47","## Task Details
How quickly did the bean dad controversy take off? 
How did the unearthing of older, controversial tweets change the discussion?
Did the public apology change anything?","","Track the Evolution of Tweets About Bean Dad","","","0"
"4283","1321317","5150501","05/06/2021 11:45:20","## Task Details
Visualize data 

## Ideas for visualization

- Visualize top 10 countries with the highest GDP
- Group countries by the location (mainland), summarize GDP and visualize it","","Ideas for data visualization","","","0"
"4081","1273964","5152786","04/15/2021 16:30:56","Try to make a correlation if the team with the highest possesion rate have a high ranking score.","","Make a correlation between the best ranking teams with possession","","","0"
"4553","1376894","5157521","05/30/2021 09:58:11","## Task Details
You are required to perform the following tasks:
**Condition:** Predict if the vehicle provided in the image is damaged or not
**Amount:** Based on the condition of a vehicle, predict the insurance amount of the cars that are provided in
the dataset

## Evaluation

* The evaluation will be made based on the following scores:
#### For predictions of the Condition column:

`SCORE1 = max(0, 100*F1_Score(actualConditions, predictedConditions, average=""micro""))`

#### For predictions of the Amount column:
`SCORE2 = max(0, 100*R2_Score(actualAmount, predictedAmount))`

* The final score will be calculated in the following way:
`FINAL_SCORE = 0.5(SCORE1 + SCORE2)`

## Further help
If you need additional information, check out the official site:
- https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-vehicle-insurance-claim/","","Predict the condition and insurance amount","","","2"
"3237","1110430","5157614","01/21/2021 13:43:02","**The aim of the task is to build a simple predictive model that helps users understand, how many people their post reaches out to an Instagram depending on certain factors.**","","Post Reach","","","3"
"4630","1387482","5014805","06/04/2021 03:11:09","Cleaning and exploring data","","Data Cleaning and EDA","","","9"
"2883","997223","5167350","12/07/2020 20:19:14","## Task Details
The dataset is riddled with various data value abnormalities. The error codes are near impossible values that are simple to detect and address. But some sensor malfunctions are not so easy to detect, and potentially more difficult to address.

## Expected Submission
Solutions can be notebooks containing cellblocks that address the various issues presented in the data

## Evaluation
The solution will detect and clean erroneous data values, however, the evaluation will be based upon the complete detection of all abnormalities/other issues, efficiency, and proper cleaning of data.","","Error codes/Sensor Malfunctions","Identify and Isolate error codes/malfunctions from valid representative values","","0"
"3297","1124220","5168235","01/28/2021 12:47:08","Through various factors mentioned in the dataset try to reach a conclusion by training your machine learning model so that it could predict how the life of a life of a test individual might have got affected by covid-19 (positively, negatively or not affected at all)","","Analyze how the lives of majority of the people have been affected due to covid-19","","","0"
"2632","955047","5170084","11/05/2020 09:08:44","Temperature data of Sounding rocket and MSIS empiric prediction model","","Sounding rocket and MSIS temperature dataset","","","0"
"2748","990062","5177548","11/22/2020 19:06:40","## Mission
The Zombie Apocalypse has begun. A polar bear contracted a dangerous disease, Zombolavirus, and has spread this disease to Patient 0. Patient 0 contacted many others, and we now have 103 known cases of Zombolavirus. In order to find a cure, we need to first identify Patient 0.

## Known Facts
Patient 0, in order to become infected, must have recently traveled outside of the United States. We also know that excessive screen time ( &gt; 20 hours a week) and unhealthy eating and sleeping habits makes initial contraction more likely. Patient 0 has likely taken some college courses, although taking a CS course (along with its many other benefits) makes contraction less likely.

### Symptoms:
In the Zombolavirus infection process, symptoms progress from
cold/cough/fever 
‚Üí eating raw meat/biting/coughing blood
‚Üí zombie

Beware: although the above statements are assumptions, real life is messy, and Patient 0 may not possess all of these attributes.

## Software
### Google Users
Open zombies.xlsx with Google Sheets.
Click Data-&gt;‚ÄúFilter Views‚Äù-&gt;Create New Filter View
To create SQL queries, and sort the data, follow these steps.
To filter based on any of the traits/columns, click the arrow next to the column name. 
Click ‚ÄúFilter by Condition‚Äù
Ex. to filter by age, click the arrow next to the Age column. Click ‚ÄúFilter by Condition‚Äù and then select the ‚ÄúLess than‚Äù filter. Enter 40. Your table will now only show the data where  the age of the person is less than 40.
You can also click ‚ÄúFilter by Value‚Äù to only show people with certain categorical traits.
Create new queries with different filters to complete the challenge.

### Non-Google Users
Open ‚ÄúAccess 2016‚Äù on your computer. If you do not have ‚ÄúAccess 2016,‚Äù then any other version of Access will work fine.
Open the zombies.accdb file in Access.
To create SQL queries, and sort the data, follow these steps.
Click the ‚ÄúCreate‚Äù tab
Click ‚ÄúQuery Wizard‚Äù
Click ‚ÄúSimple Query Wizard‚Äù
Click the ‚Äú&gt;&gt;‚Äù button to see all of the columns in your new query
Click ‚ÄúNext‚Äù twice
Give your new Query a title
Click ‚ÄúFinish‚Äù
You will now see the datatable with all of the zombies data.
To filter based on any of the columns, click the arrow next to the column name. Ex. to filter by age, click the arrow next to the Age column. Click ‚ÄúNumber Filters‚Äù and then ‚ÄúLess than.‚Äù Enter 40. Your table will now only show the data where  the age of the person is less than 40.
Create new queries with different filters to complete the challenge.

## Concluding Questions
Once you have identified Patient 0, please answer the questions below.

What is/are the name(s) of the potential Patient 0? It is okay to have more than 1 possible patient 0.
What is the name of the most likely Patient 0. From the list of above patients, choose the patient most likely to be patient 0. Use this candidate to answer the remaining questions.
Does P0 have more than 20 hrs screentime?			
Has P0 left the country lately?			
Does P0 sleep less than 8 hours a night?				
Does P0 eat healthy foods?			
What is P0's medical history? Has any prior illnesses?			
Where does P0 live?			
Is P0 old? (Over 60)			
Is P0 young? (Under 20)			
What are P0's earliest symptoms?		
What are P0‚Äôs current symptoms?	
Has P0 gone to college?			
Has P0 taken CS courses?	
What traits of P0 do not seem to fit the profile(e.g. Screentime, educational background, age)?
Overall, why did you decide on the identity of P0? What seems to make them P0 (food? screen time?)	
		
## Bonus Questions
These questions should be answered if students have extra time.

Some patients at first showed symptoms of Zombolavirus, but seemed to get better by Day 14 (with only mild symptoms, such as colds). What factor(s) did these recovered patients have in common?
How many patients are less than 5 years old?
How many patients are at least 80 years old?
Of those who turned into zombies, what percent have traveled internationally recently?

## End of Event Discussion
Each group that completes the challenge should choose their P0, and explain to the class the factors that led to their decision. If different groups have different P0‚Äôs, the class should vote after each group has presented. Once a consensus has been reached, the teacher should submit the class decision online.
Hurray!","","Indiana Computes! Data Science Challenge","Grades 7-12","","1"
"2749","990066","5177548","11/22/2020 19:10:33","## Mission
Several students took their puppies for a walk in a dog park, got distracted, and lost their puppies! Now, they are asking you for help in finding their lost puppies. We can see 200 dogs in the dog park, and we need to find the lost puppies.
Each puppy has a microchip that contains their puppy ID. For each missing puppy, determine that puppy‚Äôs ID.

## All the Puppies We Need to Recover
Find the ID‚Äôs for each of these puppies.

Luna
     Gray fur, wearing a collar, small and short (less than 20 inches tall), and very           
     friendly. Luna‚Äôs owner, Abby, is so upset that she can‚Äôt remember whether Luna           
     has a short tail or a long tail, although she thinks that Luna probably has a long 
     tail.
Halo
     Trained, golden fur, slobbery, wearing a collar, lightweight
Geode
     Spotted fur, slobbery, mean attitude, mixed breed, more than 45 inches tall
Charlie
     Black fur, not wearing a collar, less than 10 inches tall
Frosty
     Large and heavy purebred puppy, who is very shy. Has a short tail but does not      have sharp teeth. 
Shadow
     Trained, medium sized puppy who is very lightweight. Mixed breed, wearing a collar, has sharp teeth, but is very playful.

Students must answer extra questions about Luna. For the other 5 puppies, however, students will only need to determine the puppies' ID.

## Known Facts
You have a list of the 200 dogs in the dog park. Try to filter the rows of your spreadsheet until you have found the ID‚Äôs of each puppy!

Once you have found Luna in particular, please answer the following questions. You, as a veterinarian in training, have also been asked to answer a few questions about Luna‚Äôs health, development, and personality.

For the rest of the puppies, simply find their ID.

## Software Instructions
### Google Users
Open puppies.xlsx with Google Sheets.
Click Data-&gt;‚ÄúFilter Views‚Äù-&gt;Create New Filter View
To create SQL queries, and sort the data, follow these steps.
To filter based on any of the traits/columns, click the arrow next to the column name. 
Click ‚ÄúFilter by Condition‚Äù
Ex. to filter by height, click the arrow next to the Height (in) column. Click ‚ÄúFilter by Condition‚Äù and then select the ‚ÄúLess than‚Äù filter. Enter 20. Your table will now only show the data where the height of the puppy is less than 20.
You can also click ‚ÄúFilter by Value‚Äù to only show people with certain categorical traits.
Create new queries with different filters to complete the challenge.

### Non-Google Users
Open ‚ÄúAccess 2016‚Äù on your computer. If you do not have ‚ÄúAccess 2016,‚Äù then any other version of Access will work fine.
Open the puppies.accdb file in Access.
To create SQL queries, and sort the data, follow these steps.
Click the ‚ÄúCreate‚Äù tab
Click ‚ÄúQuery Wizard‚Äù
Click ‚ÄúSimple Query Wizard‚Äù
Click the ‚Äú&gt;&gt;‚Äù button to see all of the columns in your new query
Click ‚ÄúNext‚Äù twice
Give your new Query a title
Click ‚ÄúFinish‚Äù

You will now see the datatable with all of the puppies data.
To filter based on any of the columns, click the arrow next to the column name. Ex. to filter by height, click the arrow next to the Height column. Click ‚ÄúNumber Filters‚Äù and then ‚ÄúLess than.‚Äù Enter 20. Your table will now only show the data where  the height of the puppy is less than 20 inches.
Create new queries with different filters to complete the challenge.
To remove filters and see all of the puppies, click ‚ÄúFiltered‚Äù at the bottom of the screen.

## Concluding Questions

Once you have identified Luna, please answer the questions below.
Is/are there more than one puppies that could be Luna? What is/are their id(s)?
If there are more than one puppies  that could be Luna, which puppy is most likely to be Luna. What is their id? Why are they most likely to be Luna? Use this puppy to answer the remaining questions.
Is Luna slobbery? 
How tall is Luna?
Does Luna have sharp teeth? 
Is Luna trained? 
Does Luna have a long tail? 
Is Luna a purebred? 
Should Luna go on a puppy diet? Is Luna heavy? 
Overall, why did you decide on the identity of Luna? What seems to make them Luna (tail length? Fur color?)

## End of Event Discussion
Each group that completes the challenge should choose their Luna, and explain to the class the factors that led to their decision. If different groups have identified different Luna‚Äôs, the class should vote after each group has presented. Once a consensus has been reached, the teacher should submit the class decision online.

Each group should also choose the ID‚Äôs of every other puppy, although for these puppies, there should be no ambiguity (only 1 possible ID for each).

Hurray! Each puppy is very excited to be reunited with the owners.","","Indiana Computes! Data Science Challenge","Grades K-6","","0"
"2823","990066","2926451","11/30/2020 13:56:17","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

First Task","","First Task","First Task","12/01/2020 23:59:00","0"
"2621","953480","5196267","11/03/2020 14:13:50","## Task Details
Build models to forecast the prices of products to be sold for the test data

## Expected Submission
Submit the best performing model and its results.

## Evaluation
The best model performing on the test data will be chosen for the final evaluation.","","Time Series Forecasting","Build models to forecast the prices of products to be sold for the test data","12/31/2021 23:59:00","1"
"2627","949779","5196267","11/04/2020 13:23:07","## Task Details
You are given a task to understand and observe the mental health of all the employees in your company. Therefore, you are required to predict the burn-out rate of employees based on the provided features thus helping the company to take appropriate measures for their employees.

## Expected Submission
You are required to write your predictions in a .csv file that contains the following columns, as provided in the sample submission file.
1. Employee ID
2. Burn Rate

## Evaluation criteria
The evaluation metric that is used for this problem is the r2_score. The formula is as follows:
score = 100 * r2_score(actual_values, predicted_values)

### Further help
If you need additional inspiration, check out these existing notebooks:
- https://www.kaggle.com/blurredmachine/employee-burn-rate-analysis-eda-viz-pred

And rightly so! Happy and healthy employees are indisputably more productive at work, and in turn, help the business flourish profoundly.","","Predict Burn Out Rate","","12/31/2021 23:59:00","3"
"2984","949779","6360570","12/19/2020 03:51:19","Creating a test task","","creating task","","","0"
"2760","992945","5198908","11/24/2020 02:48:23","Same task as per [Predict Sales Future (Kaggle Competition)](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview)

Task: To predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.","","Predict Future Sales","","","1"
"2588","948111","5202641","10/31/2020 00:09:28","## Task Details
1. Compare the % of total accidents by month. Do you notice any seasonal patterns?

2. Break down accident frequency by day of week and hour of day. Based on this data, when do accidents occur most frequently?

3. On which particular street were the most accidents reported? What does that represent as a % of all reported accidents?

4. What was the most common contributing factor for the accidents reported in this sample (based on Vehicle 1)? What about for fatal accidents specifically?","","Recommended Analysis","","","0"
"2589","948114","5202641","10/31/2020 00:19:31","## Task Details
1. Which date in the sample saw the largest overall trading volume? On that date, which two stocks were traded most?

2. On which day of the week does volume tend to be highest? Lowest?

3. On which date did Amazon (AMZN) see the most volatility, measured by the difference between the high and low price?

4. If you could go back in time and invest in one stock from 1/2/2014 - 12/29/2017, which would you choose? What % gain would you realize?","","Recommended Analysis","","","2"
"2616","952694","5202641","11/03/2020 02:44:56","How does the overall flight volume vary by month? By day of week?

- What percentage of flights in experienced a departure delay in 2015? Among those flights, what was the average delay time, in minutes?

- How does the % of delayed flights vary throughout the year? What about for flights leaving from Boston (BOS) specifically?

- How many flights were cancelled in 2015? What % of cancellations were due to weather? What % were due to the Airline/Carrier?

- Which airlines seem to be most and least reliable, in terms of on-time departure?","","Recommended Analysis","","","0"
"4730","1401125","5205793","06/11/2021 10:18:33","## Task Details
Predict cryptocurrency prices as accurately as possible. 

## Expected Submission
Please submit the Notebook, it will help me and other members of the community in their work.

### Note
Please do not use highs, lows, and other columns as features in the model. It might give a better result, but in practical situations, we might not have these pieces of information, which will render your model useless.","","Cryptocurrency Price Prediction","","","5"
"3053","1061112","5211682","12/27/2020 06:20:12","## Task Details
Death due to heart attack is on the increase, this dataset is gathered to build a model that will not only predict if an individual with surviving a heart attack but factors that the health practitioner will put in place to ensure that the patient will survive a heart attack after a heart problem appointment.  The Death event feature is the feature of interest. Statistical analysis is of high importance.","","Death due to heart attack","","","1"
"3052","1061176","5211682","12/27/2020 06:15:18","## Task Details
Alzheimer‚Äôs disease. Dementia is the term applied to a group of symptoms that negatively impact memory, but Alzheimer‚Äôs is a progressive disease of the brain that slowly causes impairment in memory and cognitive function. The exact cause is unknown and no cure is available. 

## Expected Submission
This dataset presents some key features that have helped shaped the diagnosis of Alzheimer‚Äôs disease. It is a clean dataset however users can still validate this.
There is a target feature ""y"" in the diagnostic column to be predicted, however, users are free to build any kind of algorithm that they see fit to make this prediction.","","Alzheimer‚Äôs disease Diagnosis","Early Detection of Alzheimer‚Äôs disease","","0"
"4427","1346780","4088703","05/18/2021 04:14:17","The data set is an augmentation of the Stanford Cars dataset, with training and validation images cropped to provided bounding boxes. A csv file is to be created, containing image name, subfolder name ( whether train, validation, test_cropped or test_uncropped). All images sizes to be mentioned.","","Metadata Creation","Folder image counts, image sizes","","0"
"2907","1018618","5223111","12/11/2020 06:19:05","How would you like to make a rant Generator?

Use LSTMs?
Use RNNs?
Use Transformers?

Go ahead and try everything!
But make it sound like Donald Trump.","","DONALD TRUMP RANT GENERATOR","Generate Rants that sound like Donald Trump Blabbering.","12/16/2020 23:59:00","0"
"2864","1016444","5223111","12/05/2020 15:53:28","Well, Sentiment Analysis boys.","","Sentiment Analysis","Train a model that can accurately tell what emotion is the person writing the tweet feeling.","","0"
"5818","1358964","3485219","08/20/2021 13:23:29","Dear Kagglers, hi.

Firstly, I  would like to thank all the people who participated in the creation of these awesome datasets for providing them for us.

Authors of the [paper](https://springerplus.springeropen.com/articles/10.1186/s40064-015-1080-x) that these datasets are originally used had achieved 78.1 percent in the accuracy of Fault Classification, using an ANN, consisting of one input layer (with 6 neurons), one hidden layer (with 68 neurons), and an output layer with (4 neurons). Let's check if higher levels of accuracy can be achieved.","","Fault Classification","","","0"
"3455","1149132","5228744","02/09/2021 22:57:24","Exploratory Data Analysis
Natural Language Processing
Data Visualization","","Possible Tasks","","","0"
"2592","948042","5232622","10/31/2020 06:16:35","Make a model to translate Hindi-English and vica-versa.","","Translate","","","0"
"4282","1321069","5238268","05/06/2021 09:49:03","## Task Details
**Train a model and use it to predict who will be the champion in 2020-2021 session.**

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict New Champion","","05/28/2021 23:59:00","0"
"4617","1386479","5240195","06/03/2021 15:15:41","Make a decisive solution that will predict if a person has a laptop or not 
Hope this simple practice will boost your skillset","","Predict if a person has laptop or not!","","07/01/2021 23:59:00","1"
"4724","1401068","5240195","06/10/2021 15:14:57","Here in this dataset, some are affected by X-Linked Disorder and some are not,
you have to identify the condition of the child by analyzing their parents.

Make a model that will perfectly predict the condition of the child","","Dataset of X - Linked Disorder","Predict the condition of the child","","0"
"4640","1388817","5240195","06/04/2021 15:42:11","This dataset for beginners for practice
You have to make a model that will predict the pizza price üçï
and visualize this dataset

Hope this dataset will help you to boost your skillset üí™","","Predict Pizza Price !","Make a model to predict a pizza price !","","1"
"3670","1162557","5250822","03/06/2021 07:15:25","## Task Details
You are provided with a training file (.csv) with first names and the respective genders. Your aim is to write code to train and serve a machine learning model to assign a gender (male, female) to first names. The machine learning model must be able to determine the gender of names that have not been used to train the model.

Requirements
‚óè The codebase must be in python (.py) or Jupyter Notebook.
‚óè Using TensorFlow or Keras would be preferred.
‚óè Aim to achieve the highest accuracy model possible without overfitting.","","Gender Prediction by using names","","","1"
"3361","1115942","5254254","01/31/2021 20:43:52","## Task Details
Classify the Garbage Images in this data set with the highest Macro avg F1 score.","","Garbage Classification","","","0"
"3722","1202939","5256312","03/10/2021 10:22:34","Anomaly detection deals with the problem of finding data items that do not follow the patterns of the majority of data. The task is to distinguish good items from anomalous items. This can be defined as a binary classification problem and as such solved with supervised learning techniques. However, classes can be highly imbalanced.

Imagine an industrial manufacturing processes, where millions of parts are produced every day, but 1 percent of the production may be defective. A supervised learning approach would clearly suffer from this imbalance. Auto-encoders however are perfect for this situation, because they can be trained on normal parts and don't require annotated data. Once trained, we can give it a feature representation for a part and compare autoencoder output with input. The larger the difference, the more likely the input contains an anomaly.

Auto-encoders consist of two parts: an encoder that encodes the input data using a reduced representation and a decoder that attempts to reconstruct the original input data from the reduced representation. The network is subject to constraints that force the auto-encoder to learn a compressed representation of the training set. It does this in an unsupervised manner and is therefore most suitable for problems related to anomaly detection.","","Detect Anomalies in Images","","","1"
"3703","1200659","5256312","03/09/2021 05:53:40","The participants are called to address this task by using the provided clinically-acquired training data to develop their method and produce segmentation labels of the different glioma sub-regions. The sub-regions considered for evaluation are: 1) the ""enhancing tumor"" (ET), 2) the ""tumor core"" (TC), and 3) the ""whole tumor"" (WT) [see figure below]. The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to ‚Äúhealthy‚Äù white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (fluid-filled) and the non-enhancing (solid) parts of the tumor. The appearance of the necrotic (NCR) and the non-enhancing (NET) tumor core is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edema (ED), which is typically depicted by hyper-intense signal in FLAIR.

The provided segmentation labels have values of 1 for NCR & NET, 2 for ED, 4 for ET, and 0 for everything else.
The participants are called to upload their segmentation labels as a single multi-label file in nifti (.nii.gz) format, into CBICA's Image Processing Portal for evaluation.","","Segmentation of gliomas in pre-operative MRI scans.","","","1"
"3704","1200659","5256312","03/09/2021 05:57:43","Once the participants produce their segmentation labels in the pre-operative scans, they will be called to use these labels in combination with the provided multimodal MRI data to extract imaging/radiomic features that they consider appropriate, and analyze them through machine learning algorithms, in an attempt to predict patient OS. The participants do not need to be limited to volumetric parameters, but can also consider intensity, morphologic, histogram-based, and textural features, as well as spatial information, and glioma diffusion properties extracted from glioma growth models.

Note that participants will be evaluated for the predicted survival status of subjects with resection status of GTR (i.e., Gross Total Resection).
The participants are called to upload a .csv file with the subject ids and the predicted survival values (survival in days), into CBICA's Image Processing Portal for evaluation.","","Prediction of patient overall survival (OS) from pre-operative scans","","","1"
"3705","1200659","5256312","03/09/2021 05:58:11","In BraTS 2019 we decided to experimentally include this complementary research task, which is mainly run by Raghav Mehta, Angelos Filos, Tal Arbel, and Yarin Gal.

This new task focuses on exploring uncertainty measures in the context of glioma region segmentation, with the objective of rewarding participating methods with resulting predictions that are: (a) confident when correct and (b) uncertain when incorrect. Participants willing to participate in this new task are asked to upload (in addition to their segmentation results of Task 1) 3 generated uncertainty maps associated with the resulting labels at every voxel.

The uncertainty maps should be associated with 1) ""enhancing tumor"" (ET), 2) ""tumor core"" (TC), and 3) ""whole tumor"" (WT) regions. In this manner, the uncertainties will be associated with the traditional BraTS Dice metrics. The participants should normalize their uncertainty values between 0 - 100 across the entire dataset, such that ""0"" represents the most certain prediction and ""100"" represents the most uncertain. Note that, in any one single patient case, the values of the uncertainties do not need to take on the full range from [0 100] (i.e. The algorithm may be confident for predictions at all voxels for a single patient). To keep storage requirements to a minimum, participants are expected to submit uncertainties in ‚Äòuint8‚Äô type.

The participants are called to upload 4 nifti (.nii.gz) volumes (3 uncertainty maps and 1 multi-class segmentation volume from Task 1) onto CBICA's Image Processing Portal format. For example, for each ID in the dataset, participants are expected to upload following 4 volumes:
1. {ID}.nii.gz (multi-class label map)
2. {ID}_unc_whole.nii.gz (Uncertainty map associated with whole tumor)
3. {ID}_unc_core.nii.gz (Uncertainty map associated with tumor core)
4. {ID}_unc_enhance.nii.gz (Uncertainty map associated with enhancing tumor)","","Quantification of Uncertainty in Segmentation.","","","1"
"3763","1209633","5256312","03/14/2021 04:37:58","Breast ultrasound images can produce great results in classification when combined with machine learning.","","Classification of Breast Ultrasound Images","","","3"
"3764","1209633","5256312","03/14/2021 04:39:09","Breast ultrasound images can produce great results in detection of breast cancer when combined with machine learning.","","Detection Of Breast Cancer","","","16"
"3765","1209633","5256312","03/14/2021 04:39:49","Breast ultrasound images can produce great results in segmentation of breast cancer when combined with machine learning.","","Segmentation Of Breast Cancer","","","5"
"4394","1341834","5256312","05/15/2021 09:04:26","Few Task Suggestions:
- Perform Data-Cleaning,Preprocessing,Visualizing and Feature Engineering on the Dataset.
- Implement Heirarchical Clustering, K-Means Clustering models.
- Create RFM (Recency,Frequency,Monetary) Matrix to identify Loyal Customers.","","Perform Customer Segmentation For The Given Dataset","Customer Segmentation Using Unsupervised Learning Methods Like Clustering","","3"
"4131","1283741","5256312","04/20/2021 10:29:14","Model Relating Viscosity to moisture, protein, and
ash contents in flour used in baking ice cream cones.
Model: V = b0 + b1M + b2P + b3A","","Perform OLS Regression","Make use of VIF values to check multicollinearity in the dataset, Perform OLS Regression, Predict Viscosity","","2"
"5062","1457589","5256312","07/09/2021 11:59:42","Current traffic monitoring systems all rely on spatio-temporal features. These features are typically used to segment and track vehicles to compute traffic flow. For it to work, they need high frame rate videos with stable environmental conditions.cameras often send to the server one frame every 2 or 3 second.The limited bandwidth makes it hard to have more than 2 frames per second. So using this method they can analyse traffic on limited number of unregistered images.","","Perform Highway Traffic Analysis","","","1"
"5063","1457589","5256312","07/09/2021 12:00:19","An AI traffic counter is implemented in this project to detect and track vehicles on a video stream and count those going through a defined line on each direction of a highway","","Build an AI Traffic Counter","","","1"
"5017","1450579","5256312","07/06/2021 05:47:44","The challenging and realistic setup of the ‚ÄòWILDTRACK‚Äò dataset brings multi-camera detection
and tracking methods into the wild.
It meets the need for deep learning methods of a large-scale multi-camera dataset of walking
pedestrians, where the cameras‚Äô fields of view in large part overlap. Being acquired by current high
tech hardware it provides HD resolution data. Further, its high precision joint calibration and
synchronization shall allow for development of new algorithms that go beyond what is possible
with currently available data-sets.

Positions file
The ‚Äòpositions file‚Äô allows for omitting the work with calibration files and focusing for instance on
classification, while making use of the fact that the cameras are static. It consists of information
about where exactly a given set of particular volumes of space project to in all of the views. The
height of each volume space corresponds to the one of an average person‚Äôs height.
We discretize the ground surface as a regular grid. The 3D space occupied if a person is standing at
a particular position is modelled by a cylinder positioned centrally on the grid point. Each cylinder
projects into each of the separate 2D views as a rectangle whose position in the view is given in
pixel coordinates.
Using a 480x1440 grid ‚Äì totalling into 691200 positions ‚Äì and the provided camera calibration files,
we yield such file which is available for download. Each position is assigned an ID using 0-based
enumeration ([0, 691199]). The views‚Äô ordering numbers in this file also follow such enumeration,
i.e. they range between 0 and 6 inclusively. The positions which are not visible in a given view are
assigned coordinates of -1.
Annotations
Full ground truth annotations are provided for 400 frames using a frame rate of 2fps. On average,
there are 20 persons on each frame. Thus, our dataset provides approximately 400x20x7=56,000
single-view bounding boxes. By interpolating, the annotations‚Äô size can be further increased. This
annotations were generated through workers hired on Amazon Mechanical Turk.
Note that the annotations roughly correspond to the coordinates of the above-elaborated position
file and thus include the ID of the annotated position which is estimated to be occupied by the
specific target. These position IDs are in accordance with the provided positions file.","","Perform Multi-Camera Detection","","","0"
"5018","1450579","5256312","07/06/2021 05:49:28","The challenging and realistic setup of the ‚ÄòWILDTRACK‚Äò dataset brings multi-camera detection
and tracking methods into the wild.
It meets the need for deep learning methods of a large-scale multi-camera dataset of walking
pedestrians, where the cameras‚Äô fields of view in large part overlap. Being acquired by current high
tech hardware it provides HD resolution data. Further, its high precision joint calibration and
synchronization shall allow for development of new algorithms that go beyond what is possible
with currently available data-sets.

Positions file
The ‚Äòpositions file‚Äô allows for omitting the work with calibration files and focusing for instance on
classification, while making use of the fact that the cameras are static. It consists of information
about where exactly a given set of particular volumes of space project to in all of the views. The
height of each volume space corresponds to the one of an average person‚Äôs height.
We discretize the ground surface as a regular grid. The 3D space occupied if a person is standing at
a particular position is modelled by a cylinder positioned centrally on the grid point. Each cylinder
projects into each of the separate 2D views as a rectangle whose position in the view is given in
pixel coordinates.
Using a 480x1440 grid ‚Äì totalling into 691200 positions ‚Äì and the provided camera calibration files,
we yield such file which is available for download. Each position is assigned an ID using 0-based
enumeration ([0, 691199]). The views‚Äô ordering numbers in this file also follow such enumeration,
i.e. they range between 0 and 6 inclusively. The positions which are not visible in a given view are
assigned coordinates of -1.
Annotations
Full ground truth annotations are provided for 400 frames using a frame rate of 2fps. On average,
there are 20 persons on each frame. Thus, our dataset provides approximately 400x20x7=56,000
single-view bounding boxes. By interpolating, the annotations‚Äô size can be further increased. This
annotations were generated through workers hired on Amazon Mechanical Turk.
Note that the annotations roughly correspond to the coordinates of the above-elaborated position
file and thus include the ID of the annotated position which is estimated to be occupied by the
specific target. These position IDs are in accordance with the provided positions file.","","Tracking Methods in the Wild","","","0"
"5019","1450579","5256312","07/06/2021 05:50:18","The challenging and realistic setup of the ‚ÄòWILDTRACK‚Äò dataset brings multi-camera detection
and tracking methods into the wild.
It meets the need for deep learning methods of a large-scale multi-camera dataset of walking
pedestrians, where the cameras‚Äô fields of view in large part overlap. Being acquired by current high
tech hardware it provides HD resolution data. Further, its high precision joint calibration and
synchronization shall allow for development of new algorithms that go beyond what is possible
with currently available data-sets.

Positions file
The ‚Äòpositions file‚Äô allows for omitting the work with calibration files and focusing for instance on
classification, while making use of the fact that the cameras are static. It consists of information
about where exactly a given set of particular volumes of space project to in all of the views. The
height of each volume space corresponds to the one of an average person‚Äôs height.
We discretize the ground surface as a regular grid. The 3D space occupied if a person is standing at
a particular position is modelled by a cylinder positioned centrally on the grid point. Each cylinder
projects into each of the separate 2D views as a rectangle whose position in the view is given in
pixel coordinates.
Using a 480x1440 grid ‚Äì totalling into 691200 positions ‚Äì and the provided camera calibration files,
we yield such file which is available for download. Each position is assigned an ID using 0-based
enumeration ([0, 691199]). The views‚Äô ordering numbers in this file also follow such enumeration,
i.e. they range between 0 and 6 inclusively. The positions which are not visible in a given view are
assigned coordinates of -1.
Annotations
Full ground truth annotations are provided for 400 frames using a frame rate of 2fps. On average,
there are 20 persons on each frame. Thus, our dataset provides approximately 400x20x7=56,000
single-view bounding boxes. By interpolating, the annotations‚Äô size can be further increased. This
annotations were generated through workers hired on Amazon Mechanical Turk.
Note that the annotations roughly correspond to the coordinates of the above-elaborated position
file and thus include the ID of the annotated position which is estimated to be occupied by the
specific target. These position IDs are in accordance with the provided positions file.","","Deep Learning Based Pedestrian Tracking","","","0"
"5085","1460724","5256312","07/11/2021 09:17:28","We will start by looking at the games in the 2018 NFL season. Then, we will move onto look at the player statistics in order to understand the players in the season. We will also look at the plays of the NFL season and finally, building a data visualization project where we will be visualizing the American Football Field and players on top of it.","","Analyze NFL Games","","","1"
"5086","1460724","5256312","07/11/2021 09:18:41","We will start by looking at the games in the 2018 NFL season. Then, we will move onto look at the player statistics in order to understand the players in the season. We will also look at the plays of the NFL season and finally, building a data visualization project where we will be visualizing the American Football Field and players on top of it.","","Perform Data Visualization in terms of Sports Analytics","","","1"
"4924","1433833","5256312","06/27/2021 09:53:41","When humans navigate a crowed space such as a university campus or the sidewalks of a busy street, they follow common sense rules based on social etiquette. In order to enable the design of new algorithms that can fully take advantage of these rules to better solve tasks such as target tracking or trajectory forecasting, we need to have access to better data. To that end, we contribute the very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus. In the above images, pedestrians are labeled in pink, bicyclists in red, skateboarders in orange, and cars in green.","","Perform Target Tracking","","","2"
"4925","1433833","5256312","06/27/2021 09:54:21","When humans navigate a crowed space such as a university campus or the sidewalks of a busy street, they follow common sense rules based on social etiquette. In order to enable the design of new algorithms that can fully take advantage of these rules to better solve tasks such as target tracking or trajectory forecasting, we need to have access to better data. To that end, we contribute the very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus. In the above images, pedestrians are labeled in pink, bicyclists in red, skateboarders in orange, and cars in green.","","Perform Trajectory Forecasting","","","2"
"4911","1429830","5261065","06/25/2021 10:12:47","perform exploratory data analysis on this dataset.","","perform EDA","","","0"
"4752","1405198","5261065","06/12/2021 20:36:51","build a classification model which will predict whether a customer will buy the product or not based on the given features.","","classification model","","","0"
"4398","1342999","5261065","05/15/2021 21:51:00","Try to predict the median house value. I hope you know this is a regression task.","","Make a prediction model that predicts median_house_value?","","","1"
"4400","1342999","5261065","05/15/2021 21:54:41","Choose the benchmark according to your convenience and explain it why?","","Classify the median_house_value into Low, Average and High priced.","","","1"
"5648","1382745","7557758","08/10/2021 05:00:18","1) Which hospital is better for low income individuals for Pulmonary embolism? (Give
support with the above analysis).
2) Which hospital has the most discharges and the least discharges in Alabama, California,
Florida, Georgia, Illinois and Indiana.
3) List down the most costly hospitals in Alabama, California, Florida, Georgia, Illinois and
Indiana.","","Data Analysis","","","0"
"3546","1165253","5263458","02/19/2021 16:54:16","## Task Details
COVID affected many small businesses in the U.S. Can you use the available data to explore features that led to some businesses taking out larger loans than other businesses?","","What factors determine loan size?","","03/31/2021 23:59:00","0"
"3365","1130943","5263458","02/01/2021 00:38:34","In May 2020, the United States experienced some of the worst single-month job losses in its history as state and local governments implemented public policies aimed at reducing the spread of the COVID-19 virus. 

## Task Details
Using private sector employment concentrations (locations quotients) averages from 2019, how well can this information predict the year-over-year percentage change in total employment in May 2020?

What sectors, or group of sectors, were the most important predictors of employment change?

## Expected Submission
Post your data analysis and modeling descriptions using Notebooks? Concentrate not only on producing the most accurate model but also explain what the model means in practical terms.

## Evaluation
Use the Root Mean Square Error (RMSE) metric to evaluate the data.","","How closely can you predict job losses?","","","2"
"4575","1377458","5263719","05/31/2021 05:01:17","## problem description:
Credit Card Lead Prediction Happy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings.

The bank also cross-sells products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc.

In this case, the Happy Customer Bank wants to cross sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards.

Now, the bank is looking for your help in identifying customers that could show higher intent towards a recommended credit card, given:
* Customer details (gender, age, region etc.)
* Details of his/her relationship with the bank 
     (Channel_Code,Vintage,'Avg_Asset_Value etc.)","","Credit Card Lead Prediction","","","0"
"4052","1266890","5265307","04/12/2021 13:40:48","Beginner friendly task","","Predict the open and close value for the next 10 days","","05/01/2021 23:59:00","1"
"4009","1248072","5730096","04/08/2021 19:24:21","## Task Details
EDA

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and classifiers","","","0"
"4393","1341414","7310251","05/15/2021 08:32:20","Trace the movement of UFO","","UFO Tracing","","","1"
"3009","1047364","6042913","12/22/2020 14:37:50","**## Task Details**
Easy data to work on with an important subject, required only a few tweaks and changes in the raw data and data prep. 

**## Expected Submission**
Divide and shuffle the data into 75% train data and 25% test data (do not touch the label of the test data) . You can use whatever data or notebook. The solution contains your code ending with the MSE. 

**## Evaluation**
Compare y_pred to y_test and submit the lowest MSE, use log on the 'Total Costs' label.","","Predict costs for cancer treatment","","","1"
"3514","1160310","5276714","02/15/2021 22:01:20","The aim is to predict the Critical Temperature of Superconductors  (82nd column)","","Predict Critical Temperature of Superconductors","","","0"
"4596","1381803","5280881","06/01/2021 11:44:22","## Task Details
This task will test your analytical and visualizations skills.

## Expected Submission
One can submit notebooks for this task.

## Evaluation
Evaluation is based on how well the notebook is presented and the skills required to create it.

## Additional Information
If you want some inspiration for Visualizations, you can check this notebook out [here](https://www.kaggle.com/jaykumar1607/heart-attack-advanced-visualizations-modelling)","","Creating an analytical report","","","0"
"2452","925036","5281883","10/16/2020 18:26:14","Take 75% data as training and 25% as test set
Now take the XGBoost classifier
Finetune the hyper parameters in this search domain
Hyperparameter name	Search domain
- Max depth	(3,10)
- Gamma	(0,1)
- Learning rate	(0,1)
- N estimators	(100,120)

Use the following methods for hyperparameter tuning and optimization
a. Genetic Algorithm
b. Differential Evolution
c. Particle Swarm Optimization","","Methods for hyperparameter and optimization","","","0"
"4372","1339266","5286162","05/14/2021 01:37:29","This is a dataset containing over 1,200,000 images of twitch real twitch emotes. Most emotes (99.99%) are 28 by 28 Could be used to create a GAN or for other applications.","","Create A Gan Using This Dataset","","","0"
"3604","960301","5291112","02/26/2021 18:46:06","test","","test_test","test","03/06/2021 23:59:00","0"
"5090","1462294","5309530","07/12/2021 05:23:11","[Article1:
{a:description1a,
b:description1b}]","","Create JSON","Create json file like Article and their sub articles","","0"
"5096","1462560","5309530","07/12/2021 11:53:28","Create a Model that Predicts the next Prime number!","","Create a Model that Predicts the next Prime number!","","","0"
"4863","1400814","5314291","06/20/2021 16:43:31","## Task Details
This is a medium search results query dataset, with searches related to data science. This was made to help Medium users

## Expected Submission
A EDA notebook showing the relation between the various posts and their respective rankings
A text generator note book which generates titles with respect to the text given

## Evaluation
The text generator should be able to generate grammatically correct and sensible text","","Medium Top Results Analysis","Test Generation","","0"
"2603","951101","5315484","11/02/2020 02:32:13","Build a model to best predict respiratory diseases. Use F1, ROC, and AUC for accuracy metrics.","","Build a model to best predict respiratory diseases","","","0"
"4056","1265689","5319702","04/12/2021 16:56:09","Do an Exploratory Data Analysis in Game-Meta.csv and the text files","","Do an Exploratory Data Analysis","","","0"
"4057","1265689","5319702","04/12/2021 16:57:06","Use Topic Modeling to generate topics for the documents in Games folder, use the meta-data to validate the model","","Use Topic Modeling to generate topics","","","0"
"4058","1265689","5319702","04/12/2021 16:58:50","Use Document Clustering to cluster the documents in Games folder, use the meta-data to validate the model.","","Use Document Clustering to cluster documents","","","0"
"4059","1265689","5319702","04/12/2021 16:59:42","Create a text generation model via LSTM to generate game analysis.","","Create a text generation model","","","0"
"3499","1157793","5337747","02/14/2021 17:51:38","Based on other existing information, can you predict a person's Death Reason?","","Predict a person's Death Reason based on other existing information","","","5"
"3935","1246069","5337747","04/02/2021 06:09:38","Birds of a feather flock together. Thankfully, this makes it easier to hear them! There are over 10,000 bird species around the world. Identifying the red-winged blackbirds or Bewick‚Äôs wrens in an area, for example, can provide important information about the habitat. As birds are high up in the food chain, they are excellent indicators of deteriorating environmental quality and pollution. Monitoring the status and trends of biodiversity in ecosystems is no small task. With proper sound detection and classification‚Äîaided by machine learning‚Äîresearchers can improve their ability to track the status and trends of biodiversity in important ecosystems, enabling them to better support global conservation efforts.","","Identify bird calls in soundscape recordings","","","0"
"3932","1245939","5337747","04/01/2021 17:38:23","Two scenarios will be evaluated: (i) the recognition of all specimens singing in a long sequence (up to one hour) of raw soundscapes that can contain tens of birds singing simultaneously, and (ii) chorus source separation in complex soundscapes that were recorded in stereo at very high sampling rate (250 kHz SR).","","Detect and Classify Bird Sounds","","","1"
"2791","999923","5341005","11/27/2020 03:42:26","Create a field in the dataset called `genres` that include the main genre of the song. Field might also include any subgenres the track is highly associated with.","","Add genres","","","0"
"2787","999923","5341005","11/27/2020 03:15:44","Create a new field called `lyrics` that has the lyrics of each song in the dataset.","","Add lyrics","","","0"
"3161","1087810","5342977","01/09/2021 14:13:34","EDA, Feature engineering, NLP technique, and making a model to predict Reddit upvotes using a Regression technique.","","Predicting Reddit upvotes through comments","","02/14/2021 23:59:00","0"
"3534","1163477","5342986","02/18/2021 02:27:55","## Task Details
Show the relation between BRL and USD, since 1994 (when the Real Plan take place) until feb 2021.

## Expected Submission
- Interactive Charts
- Timeline, stats and etc
- Solve the task primarily using Notebook","","BRL/USD Interactive Chart","","","0"
"2987","1046807","5345489","12/19/2020 12:41:05","## Task Details
Explore the questions 1970s to 2020s. What questions are still used , what changed over the years ?? Try to identify 

## Expected Submission
The submission should be clear and concise . While perhaps it can't be used directly for eda , try to check it out and find out the answers 

## Evaluation
Every solution is a good one . The first step is most important , the exploring , try to identify and understand the data . Perhaps you might find something extraordinary.","","Exploring Questions","Check out the question from 1970s to 2020s!","","0"
"2988","1046851","5345489","12/19/2020 13:42:06","## Task Details
On an average , there are at least 5 children which are taken for checking everyday. Try to identify them , their ethnicity , how it has evolved over  the recent years 

## Expected Submission
The submission should contain an EDA with respect to the immigrants , on different basis . Try to find any great insights if possible . 

## Evaluation
Every submission is a good submission. However ,as a rule of thumb try to keep it clear and concise , with respect to code , try to keep it clean .


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Checking the Immigrants","Learn about the Immigrants !","01/19/2021 23:59:00","0"
"3185","1095672","5345984","01/13/2021 17:15:29","## Task Details
Evaluate the impact on Hauwei of recent interactions between the current US Presidential Administration and Chinese companies.","","Evaluate US politics on Huawei","","","0"
"4493","1365968","7489491","05/26/2021 05:55:41","## Task List

- 50 most used words in the title and content of the news
- Create a crawler to add a new column about the author of the news and URL","","Descriptive analysis","","05/31/2021 23:59:00","0"
"2386","910808","5362626","10/08/2020 22:54:34","## Task Details
Additional files should be added to this dataset. Feel free of choose between csv, excel or parquet files. Do not use images or pdf. API's are allowed, like Spotify's APPI for the music sells or other stores.

## Expected Submission
At least, you should have this graphs on your dashboard:
1.	Time: (‚Äú%Y-%m-%d %H-%M‚Äù).
2.	Rank position (int or float).
3.	Product Names: (String).
4.	Stars: (Float).
5.	Reviews: (Int) Total number of reviews since the product is on sell at Amazon.
8.	Price_std_or_min (Float).

## Evaluation
Use this task to earn likes for your profile. Is not a competition but remember that is rated by the people.","","Dashboard","","","1"
"3973","1252721","5364905","04/05/2021 16:11:04","## Task Details
Based on the historical data of some members, predict who in another group of members will buy back products.
User data in the retail industry, including personal information, transaction records, etc., can help operations or marketing strategies","","predict who in another group of members will buy back products","","","1"
"5021","1450980","5366144","07/06/2021 10:02:29","## Task Details
**This dataset in particular is a challenge for data cleaning and preparation. If you want to practice these skills, this task is for you!**

To increase the challenge you can start from the scratch, i.e the excel file where all the data is present in different sheets and work your way up from there!

Once your dataset is all cleaned up and prepared, you can start with visualisation!

ALL THE BEST!","","Data Handling","Data Cleaning, Preparation & Visualisation","","0"
"4164","1288985","5367681","04/23/2021 16:42:00","## What's a name worth?
Is there any correlation between first name and pay? What is the name / what are the names that receive the highest pay, and which receive the lowest?

## Expectation
This is meant to be practice in looking for correlation when it might exist without any causation - it is not expected that a name would actually influence pay.

## Disclaimer
Names are made public due to the Freedom of Information Act.","","What's in a name?","","","0"
"3749","1207834","5375801","03/13/2021 05:15:10","Analysis:
1. Analyse the sentiment before, during and after the interview such as positive/neutral/negative, emoticons used, number of retweets etc

2. Which users and users' tweets are influential i.e. most retweets

3. The topic that is most discussed about the interview such as racism, mental health etc
You can have a look at the main topics discussed during the interview and classify each tweet:
insider.com/biggest-bombshells-revealed-prince-harry-meghan-markle-oprah-interview-2021-3

Cleansing:
1. Removing stopwords
2. tokenization","","Sentiment Analysis","Analysing Tweets","","2"
"4231","1306889","5378328","04/30/2021 12:07:10","A .csv file containing data due to fire focus in Brazil, during the year of 2010.


I strongly suggest you to analyze this dataset without any political bias, because of the specific year...","","An√°lise de Focos de Inc√™ndio em 2010","Queimadas no Brasil em 2010","","1"
"2891","1023105","5378717","12/08/2020 15:11:58","Predict the rent of the properties with respect to the different features","","Predict the total rent of the houses","","","0"
"3781","1212347","5389167","03/15/2021 15:21:42","## Task Details
Identify the best configurations that have a good impact on the popularity of your applications.

Recommendation
Firstly please define how you will measure popularity? What is it and how do you represent it in numbers?

Task
Process the data and show with plots the impact of features on popularity.
Please, use any means for this explanation (correlations, general EDA, model weights, SHAP, Partial plots, etc)","","Explore GameDev Trends","Find the best options for modern game development","","1"
"3385","1135677","5390466","02/03/2021 08:37:54","The target attribute for classification is Category (blood donors vs. Hepatitis C (including its progress ('just' Hepatitis C, Fibrosis, Cirrhosis).","","Category blood donors vs. Hepatitis C.","","","0"
"2582","947250","5401349","10/30/2020 11:17:17","## Task Details
Swarm behavior analysis","","Swarm behavior analysis","","","0"
"4324","1332595","5401349","05/10/2021 21:01:13","## Task Details
Eda of bitcoin vs all meme coins","","Bitcoin vs all meme coin","Eda of bitcoin vs Memecoin","","1"
"4395","1342490","5401349","05/15/2021 15:33:33","## Task Details
Visualizations","","Visualization","","","2"
"4425","1346844","5403380","05/17/2021 16:57:14","Evaluate all the Regression models and pick the model with higher accuracy.","","Make a prediction model that predicts estimated price of Computer Hardware","","","0"
"4426","1346844","5403380","05/17/2021 17:05:35","Choose the best suited Pathway (with EDA and then ML algorithm ) according to your convenience and explain it how it beat the other models?","","classify the Estimated Price of Hardware into low, average and highly priced","","","1"
"4941","1429751","5406442","06/29/2021 06:23:06","Data Visualization

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis","","","0"
"4654","1391699","5406442","06/06/2021 07:55:07","## Task Details
This dataset is for providing young data scientists hands-on experience on EDA. I have purposefully kept the data dirty, there are missing values, duplicates, etc. This will provide you a hands-on experience on preprocessing data and also EDA!","","Exploratory Data Analysis","","","1"
"4693","1394941","5406442","06/09/2021 02:26:39","## Task Details
Exploratory Data Analysis
Prediction of House Prices

## Expected Submission
This data is purely for practicing the EDA and ML algorithm.


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","House price prediction","Predicting the house price using ML algorithm","","0"
"3019","1052877","5423203","12/23/2020 13:57:49","## Task Details
Make a recommendation system using this data

## Expected Submission
generate random ids from 1-10 and loop them over the length of the data

## Evaluation
Predict 10 similar animes

### Further help
If you need additional inspiration, check out these existing notebooks:
https://www.kaggle.com/namankohliml/anime-recommendation-for-inspiration","","Recommendation system","","","0"
"3020","1052877","5423203","12/23/2020 14:27:09","## Task Details
Every dataset has something to hide. Your task is to bring out hidden information and tell a story around it, other datasets may be used.

## Expected Submission
Submit a notebook, which represents graphically the data and tell a story around it (Use seaborn for beginners or matplotlib if you prefer that)

## Evaluation
The notebook with the best story backed by data makes a good submission

### Further help
Check out this excellent course on data visualization to get started -
https://www.kaggle.com/learn/data-visualization","","Spin a yarn","","","0"
"5569","1430484","5431518","08/06/2021 05:49:24","## Task Details
Recommend places of Interest for a visiter according to user inputs which may be:
- Budget
- Duration of vacation
- Type of Vacation (Adventure, Historical,..etc.)
- Available Timings","","Recommend Places","Recommend places of Interest for a visiter according to user inputs","","0"
"5570","1430484","5431518","08/06/2021 05:55:26","## Task Details
Analyse potential places of high congestion that might help to analyse the red zones relating to crowded areas considering the COVID-19 situation (as of August 2021)","","Geospatial Analysis","Analyse location of high tourism","","0"
"5571","1430484","5431518","08/06/2021 05:57:26","## Task Details
Recommend substantial routes for visitors within the city. This can be done considering the following:
- Location and minimum distance
- Available Timings
- Budget
- Type of Place","","Recommend Routes","Recommend routes for visitors within the city","","0"
"2974","1042597","5444680","12/17/2020 15:48:46","## Task Details
Please feel free to anticipate and have fun with the task. The primary goal is to share our knowledge and make a better world

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classification group of people in Sweden based on their income","","","0"
"2917","1031111","5445078","12/12/2020 15:00:48","Visualize the Data which country did PM of India Travels the most and which state of india PM travels the most.","","Data Visualization","Feature Engineering , Data Visualization , Machine Learning","","0"
"2391","913292","5445078","10/09/2020 14:55:08","Visualize the Dataset and Find the main Cause of suicide.","","Data Visualization","main cause of suicide","","1"
"3711","1183824","5445917","03/09/2021 10:13:00","## Task Details
Utilize both the datasets provided.
1.[Crops](https://www.kaggle.com/raghavramasamy/crop-statistics-fao-all-countries?select=Crops_AllData_Normalized.csv)
2.[Population](https://www.kaggle.com/raghavramasamy/crop-statistics-fao-all-countries?select=Total_Population_All_Countries.csv)

Aside from finding the most popular crops in your country and comparing the crop production level of your country to the rest of the world, you should also compare the info on crop production levels to the population of your country over the given time range.

## Expected Submission
Use Jupyter Notebooks or R Markdown reports with graphics to showcase your insights
Should have great visuals and should have a conclusion section where you document your findings from the given data


Most importantly, remember to have fun!

### Further help
If you want you can visit the following [dataset](https://www.kaggle.com/nitishabharathi/gdp-per-capita-all-countries) for data on the GDP level of your country per year and involve that in your analysis.","","How much crops does your country produce?","","","0"
"3048","1060406","5446560","12/26/2020 22:06:28","## Task Details
The SPY is considered by many to be the market and beating it has been a challenge for decades. Hundreds of indicators and algorithms have been used and whilst some have been successful most have failed. The question is, can you beat the SPY?

## Expected Submission
You can try to play around with the data. Use your machine learning prowess to find an edge!

## Evaluation
Beating the market is simple. If you algorithm after, taxes, slippage, and other fees can outperform the market then it is considered to be excellent. 

### Further help
To get started check out the field of algorithmic trading which focuses on automating trading systems. Have fun on your journey!","","Beat The Market","(No peeking into the future)","","1"
"4053","1267139","5447087","04/12/2021 15:00:07","Use this dataset and share the techniques and ideas you find. This will help others learn and enhance the level of the competition.","","Share your accuracy and techniques used on this dataset","","","0"
"5146","1357914","4656934","07/16/2021 12:21:59","Predict whether a person will test covid positive or negative.","","Predict the covid Result.","","","1"
"4033","1262545","5454565","04/10/2021 11:20:49","We all know how amazing Netflix's recommendation engine is. It has a really short window for recommending a new show before you leave the platform to do something else. The quality of content does matter but what also matters is what the platform is doing to retain its customers, to keep them hooked to their platform.
Our task is to create a popularity based or collaborative filtering based recommendation system.","","Recommend at least 5 titles for any 5 users","","","0"
"4186","1292407","5454565","04/25/2021 10:44:50","Task Details

Create a model to determine if the sample tested from the water body is fit for human consumption or not.
This dataset may require you to treat missing value if any and check for data imbalance.","","Predict if water is safe for Human consumption","","","98"
"4773","1292407","7618590","06/14/2021 14:33:55","EDA for water potability","","EDA for water potability","","06/15/2021 23:59:00","25"
"5548","1379078","4656934","08/04/2021 14:07:20","## Perform EDA 
- **List out the companies whose change is &lt; 0%, =0% and &gt;0%**
- **Company Value(in cr) &gt;=1000 and their Change**
- **CompanyValue(in cr) in range(500,1000) and their Change**
**and so on**
- **CompanyValue(in cr) less than 1 and their Change**
#### Do more analysis with your creativity","","Perform EDA","","","0"
"5654","1361621","4820339","08/10/2021 10:37:34","## Task Details
What makes a movie popular? Generally, it's to do with the plot! It's difficult to come up with new movie plots on the spot so try to make a model that generates it for you.

Use any of the attributes given to you but the main one would be the ""timeline"" attribute.

You can extend this and allow your model to make new movie titles too!

## Expected Submission
Examples of movie plots generated by your model. Display your best ones.

## Evaluation
A good solution will make semantic sense. This can be done through:
- Manual GAN Generator Evaluation
- Qualitative GAN Generator Evaluation
- Quantitative GAN Generator Evaluation e.g. Average Log-likelihood

There are many but choose whichever one you think is best.

### Further help
These sources might help:
- https://machinelearningmastery.com/how-to-evaluate-generative-adversarial-networks/","","Generate New Movie Plots","","","0"
"4407","1343913","5456766","05/16/2021 13:08:56","## Task Details
Scale all the images to a specific dimension as not all are of the same dimension.

## Expected Submission
Try submitting the the model in HD5 format.

## Evaluation
A better architecture will count for the better results.

### Further help
Contact me.","","Image Classifier","Create a model based on the provided images","","1"
"5072","1458934","5457851","07/10/2021 08:40:13","Build a deep learning model that can classify hand signs.","","Bangla Sign Language Detector","","","0"
"2683","969376","5459251","11/12/2020 11:16:31","This is a sample datasets for Electric Vehicle","","Electric Vehicle Population","","","1"
"2830","969376","5459251","12/01/2020 14:11:51","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)","","sample Task","","","0"
"3600","1182336","5459251","02/26/2021 14:26:48","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Testing of Usability","","","0"
"3792","1215950","5459862","03/16/2021 23:17:39","Data Preprocessing
- Data Transformation / Feature scaling / Feature engineering
- Classification problem / interpretation / evaluation / optimization","","Examples of Tasks","","","0"
"3739","1206038","5459862","03/12/2021 00:29:44","You can perform:
- Data preprocessing
- Impute missing data
- Train and test data
- Optimize accuracy
- Interpret data
- Visualize data
- Provide strategic marketing recommendations","","Some of the things that can be done","Several business analytics tasks","10/02/2021 00:00:00","1"
"3863","1232781","5459862","03/25/2021 17:42:29","Perform Classification with e.g. Hierarchical Clustering or KMeans","","Perform Classification with e.g. Hierarchical Clustering or KMeans","Perform Classification with e.g. Hierarchical Clustering or KMeans","","1"
"4360","1337759","5461070","05/13/2021 09:30:34","## Task Details
Creating an efficient model that can predict the future price of the crypto token.","","Predict the price of Ethereum","","","0"
"5060","1457162","5472192","07/09/2021 08:36:51","looking Forward To Your Submissions.","","Predict Whether A Customer Will Buy Travel Insurance Or Not","Use ML Or DL, Any Other Method Will Also Work.","","2"
"5061","1457162","5472192","07/09/2021 08:39:13","## Task Details
Perform Indepth EDA","","Perform EDA And Help Find Insights.","EDA Is Something That Can Solve Problems Without Even Using Models. Let's See What You Can Offer","","2"
"3824","1223650","5472692","03/20/2021 23:23:47","There are different counties in Istanbul. Make an analysis based on differences between these counties.","","Comparison of each county","","","0"
"3454","1148703","5480136","02/09/2021 22:49:26","The schools of philosophy used in the dataset are those traditionally assigned to the texts by historians of philosophy. But any classification like this could be debated. One potentially interesting analysis of this data would be to cluster it with an unsupervised machine learning model and see if the clusters such a model produces correspond to the traditionally assigned schools of thought.","","Unsupervised Clustering of the Data","Check if the traditional school labels are accurate","","0"
"3091","1071415","5488820","01/01/2021 16:53:21","## Dataset

You can find 130 000 images of bitmoji faces with different expressions

## Task

In this task, you have to generate the most realistic and diversified bitmoji faces with the algorithm you want (GAN, WGAN, Autoencoder?, ...).","","Generate bitmoji faces","Generate bitmoji faces from 130000 images","","1"
"3090","1030028","5488820","01/01/2021 16:27:49","## Dataset

You can find 10 categories of bitmoji expression (angel, smile, heart, ...), for each category you have the same characters with the corresponding expression

## Task

Try to classify those 10 categories with every algorithms you want (CNN, RAN, Dense?, ...). You can use transfer learning, regularization, ...","","Bitmoji expressions classification","","","0"
"4495","1367988","7511299","05/26/2021 09:34:40","#I love football so being able to back up team performances with facts is a entertaining for me.","","bundesliga","","","3"
"4642","1386885","5014805","06/05/2021 03:18:57","Exploratory Data Analysis for the dataset","","Exploratory Data Analysis","","","0"
"4900","1019046","7632306","06/23/2021 13:31:24","Performed an exploratory data analysis on store data set .
In this codes we can understand about EDA which is very use full in machine learning.#datascience","","Store Data Set","","","0"
"3401","1137520","5509551","02/04/2021 08:21:03","Deal with missing data, explore and visualize data, to find any useful insights and patterns.
Waiting for your submissions!","","Exploratory Data Analysis","","","1"
"3402","1137520","5509551","02/04/2021 08:22:32","Choose and build appropriate ML model to predict the prices of mobile phones.
Waiting for your submissions!","","Predict the prices","","","2"
"5003","1381603","6979629","07/05/2021 09:41:22","simply you have to predict bike prices using their features","","predict bikes prices using their features","","07/10/2021 23:59:00","2"
"3415","1140725","5522624","02/05/2021 20:30:15","There are multiple scopes of Data cleaning on the Dataset like in Price","","Do Data Cleaning on the Dataset","","","1"
"3925","1244048","5524512","03/31/2021 18:28:16","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.","","test dataset","plz test and give feedback","","0"
"2562","943728","5528379","10/28/2020 12:57:40","## Task Details
So far 47 matches have been played out of 60 (59th or 60th match could be the final). Can we predict which teams will pay into finals and, off course, which would win based on the data from 2008- till 27th Oct 2020?","","Which teams will play into finals on 11th Nov 20?","","11/05/2020 23:59:00","0"
"4590","922457","5530860","06/01/2021 08:28:37","predict the house price by giving some own required input data","","Pune House price prediction","predict price for new hosue","","2"
"3186","1095816","5531429","01/13/2021 18:37:58","Can you predict the team that will win the 2021 season using the data provided?","","Predict the 2021 season winner","","","0"
"3450","1145370","5543650","02/09/2021 13:12:30","make text preprosessing with Case Folding, Filtering, Stopword, stemming, and Tokenize...","","Text Preprosessing","","","1"
"3661","1193321","5543691","03/05/2021 02:13:25","## Task Details
Offshore Oil and Gas industry generates thousands of hours of subsea inspection per day. There also millions of hours of legacy videos.
Most of these videos are have an ""burned"" overlay showing real time data like coordinates, water depth and heading.

## Expected Submission
Submit the inpainted frames and also animated gifs comparing the original clips versus the inpainted ones

## Evaluation
There is no ground truth solution. The evaluation is purely subjective.
A good solution should look like nothing were inpainted at all","","Remove the overlay of each clip using inpainting techniques","","","0"
"2664","963535","5544705","11/09/2020 23:15:24","Exploratory Data Analysis","","EDA and Preliminary EDA","","","1"
"3018","1054729","5547013","12/23/2020 11:23:50","This task is to help users in expanding their data analysis skills.

The solution should contain the actor who acted in most films, that is, a string

The solution must be crisp and without errors","","Actors who acted in most number of films","","","0"
"4810","1407827","5547273","06/17/2021 13:33:39","There are 3 steps to complete this task -

In the first step, the annotators mark the tweet as being offensive or not offensive.
If the tweet is offensive then the annotators need to tell if the offense is targeted towards somebody or something or it is not targeted.
If the offense is targeted then the annotators also need to tell who it is targeted against.

Rules & Tips
Please rate the tweet according to whether it is offensive generally or to the target, not whether you are personally offended by it.

Sub-task A: Offensive or not

In this sub-task we are interested in the identification of offensive posts and posts containing any form of (untargeted) profanity. In this sub-task there are 2 categories in which the tweet could be classified -

Not Offensive - This post does not contain offense or profanity.  Non-offensive posts do not include any form of offense or profanity.
Offensive - This post contains offensive language or a targeted (veiled or direct) offense.  In our annotation, we label a post as offensive if it  contains any form of non-acceptable language (profanity) or a targeted offense which can be veiled or direct. To sum up this category includes insults, threats, and posts containing profane language and swear words.
Sub-task B: Offense types

In this sub-task we are interested in categorizing offenses. Only posts containing offenses are included in sub-task B. In this sub-task, annotators need to label from one of the following categories -

Targeted Insult - A post containing an insult or a threat to an individual, group, or others;
Untargeted - A post containing non-targeted profanity and swearing. 
Posts containing general profanity are not targeted but they contain non-acceptable language. On the other hand, insults and threats are targeted at an individual or group.

Sub-task C: Offense target

Finally, in sub-task C we are interested in the target of offenses. Only posts which are either insults or threats are included in this sub-task. The three categories included in sub-task C are the following:

Individual - The target of the offensive post is an individual: a famous person, named individual or an unnamed person interacting in the conversation.
Group - The target of the offensive post is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or something else.
Other ‚Äì The target of the offensive post does not belong to any of the previous two categories (e.g. an organization, a situation, an event, or an issue).

All possible label combinations across all the sub-tasks are as below:

Not offensive
Offensive, Untargeted
Offensive, Targeted Insult, (Individual | Group | Other)

2) Examples

Sub-task A: Offensive language identification

Some of the examples include:

@thecomeback @JABItalia Fuck @APrecourt - Offensive
Hey @LIRR , you are disgusting. Offensive
A true American literary icon. #PhilipRoth will be missed. - Not offensive

Sub-task B: Automatic categorization of offense types

Some of the examples include:

@thecomeback @JABItalia Fuck @APrecourt - Offensive Untargeted
I mean I'm dating to get fucking attention - Offensive Untargeted
Hey @LIRR , you are disgusting. Offensive, Targeted Insult
@BreFields1 @jonesebonee18 fuck you lol - Offensive, Targeted Insult
@karlsantix You are a complete knob! It's ppl like you who are messing up this country - Offensive, Targeted Insult
If I pull up to yo crib and you offer me cockroach milk you getting yo ass beaten - Offensive, Targeted Insult
@Top_Sergeant Assuming liberals are unarmed would be a grave mistake by the deplorables. - Offensive, Targeted Insult

Sub-task C: Offense target identification

Some of the examples include:

Hey @LIRR , you are disgusting. - Offensive, Targeted Insult, Other
@BreFields1 @jonesebonee18 fuck you lol - Offensive, Targeted Insult, Individual
@karlsantix You are a complete knob! It's ppl like you who are messing up this country - Offensive, Targeted Insult, Individual
If I pull up to yo crib and you offer me cockroach milk you getting yo ass beaten - Offensive, Targeted Threat, Individual
@Top_Sergeant Assuming liberals are unarmed would be a grave mistake by the deplorables. - Offensive, Targeted Insult, Group","","Predicting the Type and Target of Offensive Posts in Social Media","OLID (Offensive Language Identification Dataset)","","0"
"2599","949701","5560036","11/01/2020 04:52:50","## Task Details
Make a predictive model that classifies reddit users as either extraverts or introverts based on the data recording their interaction with various subreddits. 

## Expected Submission
Submit a notebook that details how you cleaned the data (you will have to create a binary variable for extraversion based on the mbti column), what methods of dimensionality reduction you pursued, and the learning model implemented. 

## Evaluation
The aim of the task is to make a classifier that is as accurate as possible. As such, the higher your average accuracy score over a 5-fold cross validation of your model, the better.","","Predict Extraversion","Predict if a reddit user is an extravert or not based on the subreddits they use","","1"
"4388","1337544","5560753","05/14/2021 15:32:46","## Task Details
- In which country people are getting vaccinated at faster rate?
- In which country people are getting vaccinated at slower rate?","","Faster/Slower vaccination happening in?","","","1"
"3853","943374","5563010","03/23/2021 18:37:19","[Github Repository link](https://github.com/San411/Predicting-Poverty-Replication)","","Contribution to open source","","","0"
"4046","1265888","5563010","04/12/2021 04:46:37","https://github.com/San411/Time-Series-Analysis","","Github Project","Contribute to open source","","0"
"2985","1046198","5564592","12/19/2020 05:58:17","## Task Details
This task is for beginners who want to explore data or play wit the data.
You are provided with lots of amount of data which you can use for practicing visualizations 

## Expected Submission
Its just for practice based so you can play around with this data.

## Evaluation
A good solution will be that, which can be easily understood to a noob in basketball stats and make a newcomer understand about basketball stats.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/maithiltandel/titanic-disaster-top-1 (Predict the 
  survivors of on the basis of name, age, ticket)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Data Visualization","","","1"
"3087","1070664","5564592","01/01/2021 09:37:29","## Task Details
Create a data visuals of the data given.

## Expected Submission
Submission should be a kaggle notebook or any .ipynb file format.

## Evaluation
If the data visuals is understood by the beginner very appropriately will be considered as best notebook","","Data Visualization","","","0"
"3203","1099174","5570735","01/15/2021 16:51:13","## Task Details
After trying out the [Im a Painter Myself](https://www.kaggle.com/c/gan-getting-started) getting started competition, try making a GAN with a different dataset.

## Evaluation
This is meant to be the next step up in making GAN networks, and the better the images your generators produce the better you have done!


### Further help
If you have not built a GAN before check out this notebook [here](https://www.kaggle.com/amyjang/monet-cyclegan-tutorial), and others in the [Im a Painter Myself](https://www.kaggle.com/c/gan-getting-started) competition.","","GAN Model","Create a cycleGAN that creates kaokore-esque faces!","","0"
"4915","1431707","5571210","06/25/2021 19:50:39","Marketplace Feature Table 
Your Client ComZ is an ecommerce company. The company wants to focus on targeting the right customers  with the right products to increase overall revenue and conversion rate.

To target the right customers with the right products, they need to build an ML model for marketing based on user interaction with products in the past like number of views,  most viewed product, number of activities of user, vintage of user and others. 

ComZ has contacted the Data Science and Engineering team to use this information to fuel the personalized advertisements, email marketing campaigns, or special offers on the landing and category pages of the company's website.

You, being a part of the data engineering team, are expected to ‚ÄúDevelop input features‚Äù  for the efficient marketing model given the Visitor log data and User Data.

1. Visitor Log Data ‚Äì It is a browsing log data of all the visitors and the users. This table contains the following information:

WebClientID
Unique ID of browser for every system. (If a visitor is using multiple browsers on a system like Chrome, Safari, then there would be a different web clientid for each browser). The ID remains consistent unless the user clears their cookie.

VisitDateTime
Date and time of visit. There are two different formats for DateTime. 

One is in datetime format ‚Äú2018-05-07 04:28:45.970‚Äù
Another one is in unix datetime format ‚Äú1527051855673000000‚Äù
ProductID
Unique ID of product browsed/ clicked by the visitor

UserID      
Unique ID of the registered user. As expected, this is available for registered users only, not for all visitors. 

Activity
Type of activity can be browsing (pageload) or clicking a product

Browser
Browser used by the visitor

OS
Operating System of the system used by the visitor

City
City of the visitor

Country
Country of the visitor



2. User Data ‚Äì It has registered user information like signup date and segment.

UserID
Unique ID of the registered user.

Signup Date
Date of registration for the user

User Segment
User Segment (A/B/C) created based on historical engagement

Now based on the above two tables, you need to create an input feature set for the Marketing Model.



3. Input Feature table:

UserID
Unique ID of the registered user

No_of_days_Visited_7_Days
How many days a user was active on platform in the last 7 days.

No_Of_Products_Viewed_15_Days
Number of Products viewed by the user in the last 15 days

User_Vintage
Vintage (In Days) of the user as of today

Most_Viewed_product_15_Days
Most frequently viewed (page loads) product by the user in the last 15 days. If there are multiple products that have a similar number of page loads then , consider the recent one. If a user has not viewed any product in the last 15 days then put it as Product101. 

Most_Active_OS
Most Frequently used OS by user. 

Recently_Viewed_Product
Most recently viewed (page loads) product by the user.

If a user has not viewed any product then put it as Product101.

Pageloads_last_7_days
Count of Page loads in the last 7 days by the user

Clicks_last_7_days
Count of Clicks in the last 7 days  by the user



Process to create Input Feature:

When ComZ does a targeting campaign, It follows the below process. 



In the current case, you are supposed to generate an input feature set as on 28-May-2018. So, the visitor table is from 07-May-2018 to 27-May-2018.

As a Data Engineer Creating ETL Pipeline would definitely be appreciated and provide you the added advantage in interviews, Your effort should be to build ETL Pipeline such that passing the information of user data and log data, It can generate the input feature table automatically

Things you should take into consideration:

You are supposed to smartly clean and pre-process data like 

Imputing missing values effectively
Handle different format of date time features
Values stored in different case for the text information


Evaluation Metric:

For continuous features, we will first calculate Mean Absolute Percentage Error (MAPE) for each continuous feature.
MAPE = Absolute Value (Derived Value - Actual Value)/ Actual Value

Then, we will calculate accuracy of Derived Value which is Mean Performance (MP) 

MP = (1 - MAPE)

For categorical features, we will calculate Accuracy for each categorical feature.
Accuracy = Percentage of value same in both derived feature and actual feature.
Finally, we will take the weighted sum of MP and Accuracy for all features.
Score = 1/8 ( MP(No_of_days_Visited_7_Days)  + MP(No_Of_Products_Viewed_15_Days) + MP(User_Vintage)+    Accuracy(Most_Viewed_product_15_Days) + Accuracy(Most_Active_OS) + Accuracy(Recently_Viewed_Product) + MP(Pageloads_last_7_days) + MP(Clicks_last_7_days))

Please note that scoring is going to be done using an automated script and difference in between the field names or order from the submission file format may result in zero scoring/error message due to the failure of the scoring script.
Participants may do multiple submissions. They would have to select on the platform which one to be treated as the final submission. If not selected, the submission with the highest score would be considered as final.
Only 5 submissions per day are allowed
Final winners would be announced only after the submitted code reviews and the analysis of the rest of the document submissions made by the participants.
Quality of code would be judged on the following parameters ‚Äì functionality, reusability, modularity, documentation, testing and validation.
Should be scalable to be executed on 10 GB data as well.","","Target the right Customer","","","0"
"3802","1217489","5574718","03/17/2021 17:50:13","Using this dataset, the following tasks and inferences can be derived:

1. Predict the price of the cars
2. Look out for Luxury cars and economic cars
3. Visualize the impact made by brands, year of make on price. 

There is a lot more to explore... Good luck !","","Price Prediction of the used cars","","","1"
"3945","1249022","5574718","04/03/2021 13:16:26","## Task Details
1. Predict prices for the rental listings of AirBnB using various features that affects the price such as Locality, No. of bedrooms, bathrooms, beds and so on...","","Price prediction","Predict the prices for the rental listings in AirBnB","","1"
"3423","1143386","5578183","02/07/2021 07:08:49","## Task Details
Create a image classifier to detect different types of skin cancers disease

## Expected Submission
model weights

## Evaluation
Performance on new data. (Accuracy)

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Skin-Cancer-Classification","Classification using CNN","","1"
"3575","1175688","5581490","02/22/2021 20:31:55","Try to detect the right amount of peaks!

Create an algorithm and perform peak prediction on the data as it is. Afterwards add some noise on each value in the dataset(e.g. By numpy.random). How much noise can you add without affecting your algorithms performance too much?","","Peak Detection","Find the number of peaks","","0"
"4416","1345963","5583620","05/17/2021 14:03:06","## Task Details
Your task is to discover useful insights about the dataset.

## Expected Submission
A notebook with your visualisations and findings","","Car offer EDA","","","0"
"4417","1345963","5583620","05/17/2021 14:10:49","## Task Details
Create a model to predict car price based on provided variables.

## Expected Submission
A notebook with your model.","","Predict car price","","","0"
"3380","1133702","5590238","02/02/2021 13:18:49","## Problem Statement:
Find the number of job opening in Bangalore, IN and in Seattle, US?
Print the number of job opening in Bangalore and Seattle as integer value.

## Output Format:
CountBangalore CountSeattle


## Problem Statement:
What are the total number of job opening related to computer vision?

Note: For finding the job related to computer vision check the job title column.
print the count as the integer value.

## Output Format:
Count


## Problem Statement:
Find the number of jobs opening in Canada?
Print the count as integer value

Note: Here you should analyse the country code in location feature.(you can use dictionary for analyse part).

## Output Format:
Count


## Problem Statement:
Find the month having most job opening in year 2018?
Print the month(month name i.e January, February, March) and number of job opening as integer value.

## Output Format:
MonthName Count


## Problem Statement:
Find the number of job openings are present if applicant have Bachelor Degree?
Print the count as integer value.

Note: Here we will use the basic qualifications feature to find out whether bachelor degree for job is required or not. Keywords that can be used are 'Bachelor', 'BS', 'BA'.

## Output Format:
count


## Problem Statement:
Among Java, C++ and Python, which of the language has more job openings in India for Bachelor Degree Holder?
Print the Language(i.e Java,C++,Python) and number of job opening as integer value.

Note : Here we will use the BASIC QUALIFICATIONS feature to find out whether bachelor degree for Job is required or not. Keywords that can be used are 'Bachelor', 'BS' and 'BA' and we will use the BASIC QUALIFICATIONS feature to find out whether Language is required for the job or not.Keywords that is used for language searching are 'Java','C++' or 'Python'.(there case should not be changed).

## Output Format:
Language Count.


## Problem Statement:
Find the country does Amazon need the most number of Java Developer?
Print the Country(Country Shortcut as given in Dataset) and number of job opening as integer value.

Note :Here we will use the BASIC QUALIFICATIONS feature to find out whether Java is required for the job or not.Keyword is used is 'Java'.(here case should not be changed).

## Output Format:
Country Count","","Problems to solve","","","1"
"3770","1210330","5596867","03/14/2021 12:41:26","## Task Details
You have to predict the house prices in the specific area of Pune using ML Algorithm.","","Predict house prices in pune","","","1"
"5774","1276105","5601216","08/17/2021 15:25:33","Using Siamese network and few shot learning try to create a face recognition network.","","Face Recognition using siamese networks","You have to create a siamese network using contrastive loss, pair wise ranking loss and any other such losses to perform facial recognition","","0"
"3417","1141501","5602038","02/06/2021 07:47:39","Beginner friendly notebook for headstart","","Beginners Notebook","","","0"
"3247","1112344","5602038","01/22/2021 13:03:38","Starter Notebook for Beginner's","","Beginner Notebook","","","0"
"3075","1067807","5602038","12/30/2020 14:39:19","Advanced notebook","","Advanced Notebook","","","0"
"2991","1048457","5602038","12/20/2020 10:20:14","Advanced Regressor Model","","Advanced Prediction Model","Advanced Regressor with EDA","","0"
"2617","952740","5602038","11/03/2020 03:33:44","Detailed Starter NoteBook.","","Starter NoteBook","","","0"
"4502","1294793","5602674","05/26/2021 14:05:51","An organization wants to predict who possible defaulters are for the consumer loans product. They have data about historic customer behavior based on what they have observed. Hence when they acquire new customers they want to predict who is more risky and who is not.","","Predict Loan Defaulter","","","0"
"4474","1354376","7375531","05/24/2021 05:40:21","## Task Details
There are multiple attributes for each hero ranging from physical attributes and their powers, etc. Let's try out unsupervised learning algorithms to group heroes that are similar to one another.","","Find Clusters of Heroes","","","0"
"4450","1354376","5617013","05/20/2021 06:46:32","## Task Details
Overall Performance of the Super Hero by there stats.

## Expected Submission
Predict the overall performance of Super Hero by there related stats.

## Evaluation
Try to predict the overall performance with the good accuracy.","","Overall Performance","Overall Performance of Superheros.","","0"
"4507","1370146","5617013","05/27/2021 05:58:05","## Task Details
This dataset is perfect for using classification so let's do it.

## Expected Submission
A submission with good classification and accuracy is the best submission.

## Evaluation
Evaluation of submission will be done by keeping in mind the classification and accuracy.","","Supervised Classification on Bank Marketing","","","0"
"5273","1370146","4656934","07/21/2021 18:30:25","EDA on Bank Marketing.","","EDA on Bank Marketing","","","1"
"4344","1335572","5617013","05/12/2021 07:43:15","## Task Details
The Open and Close Time columns of all minutes are in Unix Timestamp.

## Expected Submission
So, convert them into normal time.

## Evaluation
When both the columns are converted into human readable time.","","Convert Time Columns into normal time.","","","0"
"4357","1335572","5617013","05/13/2021 05:09:26","## Task Details
As we all know that the Bitcoin's price is a very dynamically changing crypto price.

## Expected Submission
So, the expected submission is something from that a person can predict the price change with a good accuracy.

## Evaluation
Because of this my dynamic changes. Accuracy is the key for a perfect submission.","","Predict the Price","","","1"
"2729","982921","5618523","11/19/2020 08:24:25","Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as Non-churning will do. So recall (TP/TP+FN) need to be higher.

Till now, I have managed to get a recall of 62%.  Need better.","","Improve Performance of predicting churned customers.","","","126"
"2896","982921","5618523","12/09/2020 15:20:02","One thing which is clear to our businessman is that the traditional approach of choosing a credit card for a customer needs to change. He has decided to study all the other features of a user and not just income to help them choose a more suitable card for each user. 

This way it will help both customers and our business

Here, we need your help to determine some of the most influential factors that can lead to a customer's decision of leaving our business.

## Expected Submission
We are expecting a notebook having in-depth Exploratory Data Analysis that can help us visualize where the difference lies between churning and non-churning customers.  

## Evaluation
An easy to understand approach is what appeals to a client.","","Most Influential Factors","","","40"
"4988","982921","7100054","07/04/2021 04:35:55","The dataset has many unknowns that need to be imputed and different imputation techniques can be used for replacing the unknowns. Furthermore, this is an imbalanced dataset with only 16% of the dataset churned. Using different techniques we can balance the dataset for building more representative predictive models. Descriptive, predictive, and prescriptive analysis can all be done to help the bank manager.","","Imputing Unknowns | Balancing Data Set | Descriptive | Predictive | Prescriptive","","07/01/2022 23:59:00","4"
"3403","1137858","5621788","02/04/2021 12:08:23","As a Logistics company, it is necessary to provide an effective trip experience to the customer in an optimized cost. 
1. To provide an effective service we need to identify the parameters that impact the on-time arrival of the truck. 
2. With the pattern formed we need to formulate the data points that would help reduce the trip cost","","Truck delivery optimization","","","3"
"4203","1299683","5625251","04/27/2021 13:51:04","## Possible questions
- Did the Covid-19 pandemic affect the City Bike usage?
- Does temperature affect City Bike usage?","","Covid Effect","Did Covid-19 affect the City bike usage?","","1"
"4663","1392101","5630044","06/07/2021 06:56:38","## Task Details
Develop a deep learning model to model the semantic segmentation task. There is no submission deadline. No monetary rewards. This is task is meant exclusively for improving Computer Vision skills of kagglers. 

Please use PNG files as they are lossless. Arrange the dataset images in the alphanumerical order of their names. Split the dataset into Train and Validation using `sklearn.model_selection.train_test_split` with a `test_size` of 0.2 and a `random_state` of 0 for comparison among notebooks.

## Expected Submission
No specific submission is expected. Model should generate masks and compare it with ground-truth masks provided as part of the dataset.

## Evaluation
Accuracy can be used as the evaluation measure, though around two-third of the pixels belong to the class 0, that is the background (referred to as 'null')

### Further help
Please post your queries and suggestions in the Discussion field.","","Semantic Segmentation","pixel-wise multiclass classification into 59 segmentation classes","","0"
"3288","1122720","5631849","01/27/2021 18:27:03","## Task Details
Obviously, all languages are important; equally obviously, reliable NLP processing can only be done one language at a time. In previous, smaller versions of this dataset, I have successfully used Python 3's langdetect library to remove all but a tiny fraction of non-English user reviews, and repeating this would be ideal.

## Expected Submission
Ideally this would involve the submission of new versions of a given title's csv file, with as many non-English reviews as possible removed.

## Evaluation
Obviously, removing as many as possible of the non-English reviews while having as small as possible of a false negative for English language would preserve the most data for meaningful analysis.","","Filter out non-English reviews","Since the reviews are overwhelmingly in English, ensuring that data consistency for NLP is maintained will require removing other languages as part of preprocessing","","0"
"3289","1122720","5631849","01/27/2021 18:29:10","## Task Details
To get a base impression of what's going on with this dataset, comparing unsupervised models from Gensim, Tensor-based, or possibly Stanford models will hopefully allow us to see the commonalities and differences between review bombing targets and others.","","Perform an unsupervised topic model on one or more of the titles' reviews","","","0"
"3910","1240905","5636250","03/30/2021 17:37:12","## Task Details
Learn the Apriori algorithm.

## Expected Submission
Users should submit the association rules they find.","","Learning the Apriori algorithm","","05/01/2021 23:59:00","0"
"3394","1131458","5637976","02/03/2021 16:20:42","## Task Details
The diesel price fluctuates a lot over the years, and to understand: What causes peaks and valleys in the prices of gasoline and diesel? first we need to start with a data visualization.

## Expected Submission
Create a data visualization or time series chart on the Diesel prices.

## Evaluation
Is it possible to identify the year with the highest price on Diesel with your Data Visualization?

### Further help
If you need additional inspiration, check out this notebook:
https://www.kaggle.com/mruanova/us-gasoline-and-diesel-retail-prices-1995-2021","","Predict the retail price of Diesel in 2022 based on historic data","Linear Regression","","2"
"3395","1131458","5637976","02/03/2021 16:37:30","## Task Details
Gasoline can be either regular or midgrade or premium. How do they compare in price?

## Expected Submission
Create a data visualization that compares regular, midgrade and premium prices.

## Evaluation
Can you use a new type of chart that you have never used before? how about a ""bar chart race""?

### Further help
If you need additional inspiration, check out this notebook:
https://www.kaggle.com/mruanova/us-gasoline-and-diesel-retail-prices-1995-2021","","How does Regular gasoline compares with Midgrade or Premium?","Data Visualization on Gasoline","","4"
"3584","1177020","5637976","02/23/2021 16:08:12","## Task Details
Exploratory Data Analysis of the prices of shrimp.

## Expected Submission
Exploratory Data Analysis of the prices of shrimp

## Evaluation
Exploratory Data Analysis of the prices of shrimp

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis of the prices of shrimp","","","0"
"3406","919872","5637976","02/04/2021 15:34:09","## Task Details
Gross domestic product GDP is a monetary measure of the market value of all the final goods and services produced in a specific time period. GDP (nominal) per capita does not, however, reflect differences in the cost of living and the inflation rates of the countries; therefore, using a basis of GDP per capita at purchasing power parity (PPP) is arguably more useful when comparing living standards between nations, while nominal GDP is more useful comparing national economies on the international market.

## Expected Submission
Use linear regression to predict the GDP of your country in 2018 based on historic data.

## Evaluation
Did you learn something new with this exercise?

### Further help
If you need additional inspiration, check out this existing notebooks:
- https://www.kaggle.com/mruanova/gdp-forecasting","","Predict the GDP of your country in 2018 based on historic data","","","0"
"3181","1095282","5642357","01/13/2021 12:57:29","Use Tensorflow or Pytorch to generate plain C codes.","","Generate Some C Code","","","0"
"3304","1122059","5645472","01/29/2021 04:31:12","## Task Details
Output the top 5 countries which have done the most number of vaccinations. 




### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Top 5 countries","","","0"
"2665","965244","5645472","11/10/2020 02:08:16","## Task Details
A poll was attended by around 54,000 students where they were asked ""Are you getting a good education?""

## Expected Submission
Evaluate the percentage of children who said ""YES"" and who said ""NO"".

## Evaluation
An accurate percentage is required.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find the number.","","","0"
"2666","965244","5645472","11/10/2020 02:10:37","## Task Details
Over 54,000 students attended a poll on ""Are you getting a good education?""

## Expected Submission
Find the percentage of students who were girls and find the percentage of students who were boys.","","Gender Classification","","","0"
"3283","1108010","5646773","01/27/2021 06:24:40","Determine the whether the users have a positive or negative reaction to the change in Star Wars rights associated with EA","","Reaction to the News","","","0"
"4041","1264559","5654110","04/11/2021 13:59:08","You can use data analysis and try different models on the dataset.","","Good Prediction","","","1"
"3151","1084791","5658374","01/07/2021 18:35:37","### Task Details: You are required to write an AI systems to read tweet and classify it to negative or positive using  ‚àöRNN or ‚àöLSTM  or ‚àöCNN","","Twitte Analysis","","06/08/2021 23:59:00","0"
"3969","1249902","5658773","04/05/2021 09:58:50",".","","how to distinguish between rough and nice places","like if you don't know the town and wanna know which is the most peacful places to live into , how you are gonna do that !?","","0"
"3654","1191769","5659407","03/04/2021 04:32:44","## Task Details
You are passionate about data & animals and are looking for a way to make meaningful impact. To do that, you'll explore and analyze an animal shelter's data and give them **1-3 actionable insights**.

## Expected Submission

Submit a well documented notebook with these two sections:

### Section 01: Exploratory Data Analysis

Explore the data and answer the below questions using both markdown write-ups & visualizations:

- Are there any null values or outliers? How will you wrangle/handle them?
- Are there any useful variables that you can engineer with the given data?
- Are there any relationships between the variables? Can you prove any of these relationships with statistical tests?

Please don't feel limited to these questions. Explore the data as you please! The most important thing is to be like Sherlock Holmes and look in between the lines (data) to find something that isn't apparent!

### Section 02: Actionable Insights

Provide 1 to 3 actionable insights that are supported by your EDA. Please make sure that each insight is accompanied by a visualization. 

Some guiding questions:

- Which locations are booming with stray animals? Can we manage a stray neutering campaign in that area?
- Are we frequently intaking strays that are sick? Is there a hidden pattern for those strays?
- Which animals receive positive outcomes types? Can we capitalize on that?


## Evaluation

This is not a formal competition, so results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory  with well supported and visualized actionable insights

### Data Exploration

- Was the data wrangled properly? How well was the data analyzed? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative and thought provoking. 
- Were the right statistical tests used? How well was the statistical output interpreted? A great entry will interpret results without the use of any statistical jargon.

### Actionable Insights 

- Were the insights tied to your EDA in section 01? Are they data-driven and focused on helping the animal shelter optimize operations or improve outcomes/intakes of strays?

### Documentation 

- Are your code, and notebook well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Exploratory Data Analysis","Help an animal shelter with your EDA powers!","","2"
"2986","1046184","5659407","12/19/2020 06:40:44","## Task Details

You're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.

## Expected Submission

Submit a well documented notebook with these **four sections**:

### Section 01: Exploratory Data Analysis

- Are there any null values or outliers? How will you wrangle/handle them?
- Are there any variables that warrant transformations?
- Are there any useful variables that you can engineer with the given data?
- Do you notice any patterns or anomalies in the data? Can you plot them?

### Section 02: Statistical Analysis

Please run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. *Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.*

- What factors are significantly related to the number of store purchases?
- Does US fare significantly better than the Rest of the World in terms of total purchases?
- Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent *an above average amount* on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test
- Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do ""Married PhD candidates"" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? *(Hint: use your knowledge of interaction variables/effects)*
- Is there a significant relationship between geographical regional and success of a campaign?

### Section 03: Data Visualization

Please plot and visualize the answers to the below questions.

- Which marketing campaign is most successful?
- What does the average customer look like for this company?
- Which products are performing best?
- Which channels are underperforming?

### Section 04: CMO Recommendations

Bring together everything from Sections 01 to 03 and provide data-driven recommendations/suggestions to your CMO.

## Evaluation

This is not a formal competition, so results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory and statistical analysis with insightful conclusions.

1. Data Exploration - Was the data wrangled properly? How well was the data analyzed? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative and thought provoking.
2. Statistical Analysis - Were the right statistical tests used? How well was the statistical output interpreted? A great entry will interpret results without the use of any statistical jargon.
3. Business Recommendation - Were the recommendations tied to your analysis in Sections 1-3? Are they data-driven and focused on marketing concepts such as targets, channels, or products?
4. Documentation - Are your code, and notebook well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Business Analysis with EDA & Statistics","","","45"
"3373","1046184","4846884","02/01/2021 18:22:43","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019).","","Market_data_EDA","","","25"
"2969","1040318","5659407","12/17/2020 00:06:31","## Task Details

eSports is a booming industry and there's not much publicly available data. With what's available here, conduct an exploratory data analysis (EDA) and make some rational conclusions based off your findings.

**Exploratory Data Analysis (EDA)** refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations. ([Definition Source](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15))

## Expected Submission

Submit a notebook that implements the full lifecycle of EDA mentioned above. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, please focus on doing your absolute best and don't compare yourself with others.

The notebook should be well documented and contain:

- Any steps you're taking to prepare the data, including references to external data sources
- Visualizations of explanations and vice versa

## Evaluation

This is not a formal competition, so results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory data analysis with insightful conclusions.

1. **Data Exploration** - Was the data wrangled properly? How well was the data analyzed? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative and thought provoking.

3. **Documentation** - Are your code, and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.

## Guidance

Some questions that can help guide your EDA:

- Who are the top 10 players? Where are they form?
- Which country or continent has most player earnings? (Hint: join data)
- What is the proportion of earnings per genre for the top 10 games in terms of both teams and players?
- Which game is most popular in terms of number of tournaments?","","Exploratory Data Analysis (EDA)","","","5"
"3065","1064629","5659407","12/29/2020 04:26:51","# Task Details
You're curious about whether or not COVID-19 impacted Boston's BlueBikes bike sharing service. You need to analyze the data sets in order to identify and contrast patterns between 2019 & 2020 data.

# Expected Submission

Submit a well documented notebook with these two sections:

## Section 01: Exploratory Data Analysis

- Are there any null values or outliers? How will you wrangle/handle them?
- Are there any useful variables that you can engineer with the given data?
- Do you notice any patterns or anomalies in the data? Can you plot them?

## Section 02: 2019 & 2020 Comparison

- Do you notice any differences between 2019 & 2020? Can you plot them?
- Do you think COVID-19 had any influence? Explain the rationale.

## Guiding Questions:
- Which stations were most popular and vice versa?
- Which time of day was the busiest and vice versa?
- Can we segment/cluster our users into categories? What is their frequent destinations?
- Do you have a suggestion for redistribution of bikes in the short run? (for e.g. during a specific time of day, and specific neighborhood, which possible stations would you choose for bike redistribution?)
- Can you suggest a new location for new stations?

## Bonus Points
Plot an animation that displays trips from start to end over a month or year of your choosing.

Example:

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F5659407%2F4905061b801b7f1b417acb72cc92ded8%2Fbluebikes_animation.gif?generation=1609215704982298&alt=media)

# Evaluation

This is not a formal competition, so results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory analysis with insightful conclusions.

1. Data Exploration - Was the data wrangled properly? How well was the data analyzed? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative and thought provoking.

2. Documentation - Are your code, and notebook well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Exploratory Data Analysis (2019 vs 2020)","Has COVID-19 impacted BlueBikes in 2020?","","6"
"3780","1211680","5662396","03/15/2021 08:41:12","Just make your analytic and visualization guys. we can share each other üòÅ 
go go go!üî•","","Data Analytic and Visualization","make nice Analytic and Visualization","","0"
"3559","1166074","5663726","02/20/2021 19:48:39","## Task Details
Aquatic invasions is a problem. It's necessary to monitor the spatial and temporal trends and spread in order to guide prevention and control efforts and to develop effective policy aimed at mitigating impacts.

## Expected Submission
So, given the data, define some insights that help visualize the occurrence of a new specie. Examples include:
- First apparition of each specie
- What foreign area it comes from?
- Neigborhood spread by year
- Interactive map by year
- Time series by state
- Growth rate per year and state

Optional: Propose some user-friendly interactive dashboard. You would have three filters and/or segmenters: specie(animal), family, group, state and year.

### Further help
If you need additional inspiration, check out these:

**Notebooks:**
* https://www.kaggle.com/andreshg/timeseries-analysis-a-complete-guide
* https://www.kaggle.com/umerkk12/geo-animation-of-vaccination
* https://www.kaggle.com/learn/geospatial-analysis
* https://www.kaggle.com/dbennett/test-map

**Papers:**
* http://www.aquaticinvasions.net/2018/issue3.html
* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6209226/","","Monitor the spatiotemporal trend","Define insights and make plots","","1"
"3997","1235075","5663726","04/08/2021 03:02:03","## Task Details
The saga of The Last of Us was a great sucess in the industry of video-games. I'd like to see all the reasons. üôå üôå 
## Expected Submission
A notebook that shows diverse plots, reports and insights. Some ideas are the following:
- Strong and weak points of each game
- Differences between the games
- Why did this game is better than other ? 
- Differences between critic's and user's scores
- Differences betwen the reviews of a specific user in each game
- What about with ellie's reviews? What about with joel's reviews? In general, for each character
- What's the most repeateds word? the most repeated bigram, trigram?
- What emotions you can detect in a review?
- Is possible find categories? Ex: zombies, action, drama, gameplay, interactive, story, etc.
- Exist a temporal pattern? A spatial pattern (use language)? 
- What's up with the good and bad reviews (score) ? Exist any association with the other variables?
- With all above info define some segments, or realize a dashboard (Very optional)

## Evaluation
You choose you want to analyze and send it. Here all submissions have 100/100. This type of works are always well received üòä 

### Further help
If you need additional inspiration, check out these existing high-quality resources:
**Text preprocessing**
- https://www.kaggle.com/lazaro97/the-last-of-us-ii-a-sentiment-analysis
- https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained
- https://www.kaggle.com/ruchi798/sentiment-analysis-the-simpsons
- https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts#3.-Dialogues-:-Who-talks-the-most?-%F0%9F%92%AC
**Visualizations**
- https://www.kaggle.com/thebrownviking20/intermediate-visualization-tutorial-using-plotly
- https://www.kaggle.com/ahmedterry/disneyland-reviews-nlp-sentiment-analysis
- https://www.kaggle.com/erikbruin/text-mining-the-clinton-and-trump-election-tweets
- https://www.kaggle.com/jaseziv83/clustering-to-help-club-managers/report
**Books**
- https://www.tidytextmining.com/sentiment.html#:~:text=The%20nrc%20lexicon%20categorizes%20words%20in%20a%20binary,a%20binary%20fashion%20into%20positive%20and%20negative%20categories.","","Make awesome visualizations","Define and plot insights","","1"
"3998","1235075","5663726","04/08/2021 03:26:23","## Task Details
The Last of Us II is the most discussed video-game the last year. Some users says the game was awesome, but others are disagree and says the game was horrible.. Define a binary model that predicts if a review is excellent or bad.

## Expected Submission
With The Last of Us II reviews (**user-reviews-g2u**) take only the 0 and 10 scores and define a binary model.
The notebook should be clear and show the score in the test set.

## Evaluation
Only get the better test auc, here i think a neural network is better in this cases ü§îü§î. 
Another point: i define the index of train and test in the update, please check out.

### Further help
If you need additional inspiration, check out these existing high-quality resources:
**Text preprocessing**
- https://www.kaggle.com/lazaro97/the-last-of-us-ii-a-sentiment-analysis
- https://www.kaggle.com/ruchi798/sentiment-analysis-the-simpsons
- https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts#3.-Dialogues-:-Who-talks-the-most?-%F0%9F%92%AC
**Text classification**
- https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained
- https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d
- https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn
- https://www.kaggle.com/immvab/transformers-covid-19-tweets-sentiment-analysis
- https://www.kaggle.com/arunrk7/nlp-beginner-text-classification-using-lstm
- https://www.kaggle.com/abhinavkrjha/one-hot-enc-lstm-91-67-acc
**Books**
- https://www.tidytextmining.com/sentiment.html#:~:text=The%20nrc%20lexicon%20categorizes%20words%20in%20a%20binary,a%20binary%20fashion%20into%20positive%20and%20negative%20categories.","","Why The Last of Us II has very opposite reviews?","Realize a Binary Text Classification","","1"
"3999","1235075","5663726","04/08/2021 04:23:28","## Task Details
The Last of Us is an excellent game. Many experts, web-pages and critics are agree. The main topic of apocalyptic world, the stories for each character, the interaction with the user: all is great.. So, define a model with those reviews.
## Expected Submission
With The Last of Us reviews (**user-reviews-g1u**) take all scores and define a multi-classification model.
The notebook should be clear and show the score in the test set.

## Evaluation
Only get the better test auc, here i think a neural network is better in this cases ü§îü§î. 
Another point: i define the index of train and test in the update, please check out.

### Further help
If you need additional inspiration, check out these existing high-quality resources:
**Text preprocessing**
- https://www.kaggle.com/lazaro97/the-last-of-us-ii-a-sentiment-analysis
- https://www.kaggle.com/ruchi798/sentiment-analysis-the-simpsons
- https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts#3.-Dialogues-:-Who-talks-the-most?-%F0%9F%92%AC
**Text classification**
- https://www.kaggle.com/andreshg/nlp-glove-bert-tf-idf-lstm-explained
- https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d
- https://www.kaggle.com/eliotbarr/text-mining-with-sklearn-keras-mlp-lstm-cnn
- https://www.kaggle.com/immvab/transformers-covid-19-tweets-sentiment-analysis
- https://www.kaggle.com/arunrk7/nlp-beginner-text-classification-using-lstm
- https://www.kaggle.com/abhinavkrjha/one-hot-enc-lstm-91-67-acc
**Books**
- https://www.tidytextmining.com/sentiment.html#:~:text=The%20nrc%20lexicon%20categorizes%20words%20in%20a%20binary,a%20binary%20fashion%20into%20positive%20and%20negative%20categories.","","Analyze one of the best video-games of all time: The Last of Us","Realize a Multi-Class Text Classification","","1"
"4756","1405631","5663726","06/13/2021 07:10:54","## Task Details
The 2021 World Chess Championship match has been announced for November 24-December 16, 2021, and will be held alongside the World Expo in Dubai, UAE. The championship will be played between reigning World Champion Magnus Carlsen and GM Ian Nepomniachtchi, who won the 2020 Candidates Tournament. So, who will win?

## Expected Submission
Analyze the evolution of both players. You can see their favorites openings and their weak points for example. In general, i hope to see an excellent exploratory analysis.
## Evaluation
This type of works is always well received üòä.","","Who will win the Chess Championship?","Magnus Carlsen or Ian Nepomniachtchi","","1"
"2918","1025498","5664962","12/12/2020 21:03:59","## Task Details
You have some experience with Python and machine learning basics. This is a perfect competition for Data Science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition. 

## Expected Submission
It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. 

## Evaluation
Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)


## File descriptions
‚Ä¢	train.csv - The training set
‚Ä¢	test.csv - The test set
‚Ä¢	data_description.txt - A full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here
‚Ä¢	sample_submission.csv - A benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms



## Data fields
Here's a brief version of what you'll find in the data description file.

‚Ä¢	SalePrice - The property's sale price in dollars. This is the target variable that you're trying to predict.
‚Ä¢	MSSubClass: The building class
‚Ä¢	MSZoning: The general zoning classification
‚Ä¢	LotFrontage: Linear feet of street connected to property
‚Ä¢	LotArea: Lot size in square feet
‚Ä¢	Street: Type of road access
‚Ä¢	Alley: Type of alley access
‚Ä¢	LotShape: General shape of property
‚Ä¢	LandContour: Flatness of the property
‚Ä¢	Utilities: Type of utilities available
‚Ä¢	LotConfig: Lot configuration
‚Ä¢	LandSlope: Slope of property
‚Ä¢	Neighborhood: Physical locations within Ames city limits
‚Ä¢	Condition1: Proximity to main road or railroad
‚Ä¢	Condition2: Proximity to main road or railroad (if a second is present)
‚Ä¢	BldgType: Type of dwelling
‚Ä¢	HouseStyle: Style of dwelling
‚Ä¢	OverallQual: Overall material and finish quality
‚Ä¢	OverallCond: Overall condition rating
‚Ä¢	YearBuilt: Original construction date
‚Ä¢	YearRemodAdd: Remodel date
‚Ä¢	RoofStyle: Type of roof
‚Ä¢	RoofMatl: Roof material
‚Ä¢	Exterior1st: Exterior covering on house
‚Ä¢	Exterior2nd: Exterior covering on house (if more than one material)
‚Ä¢	MasVnrType: Masonry veneer type
‚Ä¢	MasVnrArea: Masonry veneer area in square feet
‚Ä¢	ExterQual: Exterior material quality
‚Ä¢	ExterCond: Present condition of the material on the exterior
‚Ä¢	Foundation: Type of foundation
‚Ä¢	BsmtQual: Height of the basement
‚Ä¢	BsmtCond: General condition of the basement
‚Ä¢	BsmtExposure: Walkout or garden level basement walls
‚Ä¢	BsmtFinType1: Quality of basement finished area
‚Ä¢	BsmtFinSF1: Type 1 finished square feet
‚Ä¢	BsmtFinType2: Quality of second finished area (if present)
‚Ä¢	BsmtFinSF2: Type 2 finished square feet
‚Ä¢	BsmtUnfSF: Unfinished square feet of basement area
‚Ä¢	TotalBsmtSF: Total square feet of basement area
‚Ä¢	Heating: Type of heating
‚Ä¢	HeatingQC: Heating quality and condition
‚Ä¢	CentralAir: Central air conditioning
‚Ä¢	Electrical: Electrical system
‚Ä¢	1stFlrSF: First Floor square feet
‚Ä¢	2ndFlrSF: Second floor square feet
‚Ä¢	LowQualFinSF: Low quality finished square feet (all floors)
‚Ä¢	GrLivArea: Above grade (ground) living area square feet
‚Ä¢	BsmtFullBath: Basement full bathrooms
‚Ä¢	BsmtHalfBath: Basement half bathrooms
‚Ä¢	FullBath: Full bathrooms above grade
‚Ä¢	HalfBath: Half baths above grade
‚Ä¢	Bedroom: Number of bedrooms above basement level
‚Ä¢	Kitchen: Number of kitchens
‚Ä¢	KitchenQual: Kitchen quality
‚Ä¢	TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
‚Ä¢	Functional: Home functionality rating
‚Ä¢	Fireplaces: Number of fireplaces
‚Ä¢	FireplaceQu: Fireplace quality
‚Ä¢	GarageType: Garage location
‚Ä¢	GarageYrBlt: Year garage was built
‚Ä¢	GarageFinish: Interior finish of the garage
‚Ä¢	GarageCars: Size of garage in car capacity
‚Ä¢	GarageArea: Size of garage in square feet
‚Ä¢	GarageQual: Garage quality
‚Ä¢	GarageCond: Garage condition
‚Ä¢	PavedDrive: Paved driveway
‚Ä¢	WoodDeckSF: Wood deck area in square feet
‚Ä¢	OpenPorchSF: Open porch area in square feet
‚Ä¢	EnclosedPorch: Enclosed porch area in square feet
‚Ä¢	3SsnPorch: Three season porch area in square feet
‚Ä¢	ScreenPorch: Screen porch area in square feet
‚Ä¢	PoolArea: Pool area in square feet
‚Ä¢	PoolQC: Pool quality
‚Ä¢	Fence: Fence quality
‚Ä¢	MiscFeature: Miscellaneous feature not covered in other categories
‚Ä¢	MiscVal: $Value of miscellaneous feature
‚Ä¢	MoSold: Month Sold
‚Ä¢	YrSold: Year Sold
‚Ä¢	SaleType: Type of sale
‚Ä¢	SaleCondition: Condition of sale","","IMDb Movie Dataset","Let's Play With The Data","01/01/2022 23:59:00","0"
"4712","927555","5230987","06/10/2021 07:15:31","Label dogs and cats and check the model accuracy on predictions","","Classify Cats and Dogs","","","1"
"2773","971001","5675624","11/25/2020 06:39:38","# Details : 

This is screening task for the participants to be shortlisted for closed group discussion with the core team till the end of the project. Below are the list of activities to be completed under this task:

üìå Initial Statistical analysis of data
üìå Feature selection
üìå Approach to be followed for modeling

# Submission Details:

‚úîÔ∏è Word document to be submitted which lists down details of all the above points. Maximum 2 pages write up is allowed. Send email to colearninglounge@gmail.com with subject as ‚ÄúIPL Kaggle Task #1 ‚Äì (Your Name)‚Äù. Any other subject line will not be entertained and would not be considered for evaluation.
‚úîÔ∏è Notebook to be submitted with all details.


## Important Note :

This task is to mainly understand the approach of solving a problem, the use of domain knowledge, etc. Prediction is the last but final thing. And yeah all points mentioned above will result in better prediction.
 
As the solution here is a statistical or Machine learning-based model we need to give importance to EDA, Feature engineering, Algorithm, etc.
No Black box here. Meaning no deep learning would help as much as ML can help.
 
Here final prediction is on the player list-making to the DT team for a specific match. Player point prediction is not in the scope of this competition.","","Model Approach","Statistical Analysis, Feature Selection and Model Approach","12/02/2020 23:59:00","1"
"3155","1086050","5678898","01/08/2021 16:38:54","## Task Details
Bring out more insights/visualizations from the movies dataset


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and Visualization","","","0"
"3970","1250635","5680654","04/05/2021 10:50:11","## Task Details
You should label all the symbols placed in `glyphs/` folder using some classifier.

## Expected Submission
Your result should be a single CSV file with the following columns:
* _string_ `filename` - address to the glyph you are classifying.
* _string_ `name` - unique name of the symbol as used in `glyphs.csv`.
* (optionally) _bool_ `is_uppercase` - should be `True`, if the symbol is classified as uppercase.

## Evaluation
Precision of classification can be measured using precision:

```
precision(feature) = 
    true_positive(feature) /
    (true_positive(feature) + false_positive(feature))
```

Solution can be evaluated with the following metric:

```
score =
    (number of predicted symbols) / (number of all symbols in glyphs.csv) +
    precision(name) + 0.5 * precision(is_uppercase)
```","","Create symbol classifier","","","0"
"3954","1250635","5680654","04/04/2021 15:25:34","## Task Details
Imagine we have divided the letter with a straight vertical line. With that line we can clearly see whether the letter leans to right or to the left. Can you find how much the letter is slanted and direction of it?

## Expected Submission
Your submission should create two features for each letter:
* _string_ `filename`: address of the file you are labeling. Use `glyphs/[number]-[name].png` format.
* _float_ `slant`: normalized coefficient in -1 to 1 range,
* _category_ `slant_direction`: one of `left`, `right` or `straight`.
See detailed explanation on the picture below.

![Example of slants](https://i.ibb.co/h2QgBxz/slants.png)

## Evaluation
A good solution should done with simple analysis and be well-documented.","","Find writing slant","","","0"
"3681","1197352","5680845","03/07/2021 09:27:25","Where does each region stand in terms of sanitation access?","","Unraveling disparities and inequalities  in sanitation facilities","where does each region stand in terms of sanitation access?","","0"
"2472","926506","5681768","10/17/2020 21:04:37","Problem statement: In this Dataset how many people prefer going out and eat depending on the rating?

I would love to find the most amazing food by the top 10 restaurant","","Zomato Data Pending Analysis","","","4"
"3242","1094380","5683124","01/22/2021 03:35:48","## Task Details
predict the vegetable price according to the situation.

## Expected Submission
please submit the predicted value of test.csv

## Evaluation
good solution will be the best predicted value depending on Mean Absolute Error . 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Vegetable_market EDA","","","0"
"2731","979081","5685793","11/19/2020 19:16:29","## Task Details
Perform regression analysis on the ads and develop a general model for predicting house prices  in Tbilisi. Use `housing_clean_2.csv` or your own clean CSV file. Avoid using the JSON file for this task.

## Expected Submission
Submit a notebook with a model that can predict previously unseen data with least **mean squared error**. Try to isolate all the data cleaning to a separate notebook, and use  the resulting CSV in your analysis notebook. That way it will be easier to see the results of your analysis. 
 
## Evaluation
The model will be trained on a fixed-size random subset of data and asked to predict the rest. The best predictor of unseen data wins.","","Perform regression analysis on house price","","","1"
"2732","979081","5685793","11/19/2020 19:20:58","## Task Details
Given house_3.json file, sharpen your pandas skills by extracting a well formatted CSV file from it.  
 
## Expected Submission
A notebook that generates and explains the process of data cleaning. Reasoning is most important here.

## Evaluation
A good solution will contain as much, or even more data as `housing_clean_2.csv`. The best solution will find a way to represent most of the useful data in CSV.","","Perform data extraction and cleaning","","","0"
"3255","1103863","5689514","01/23/2021 08:30:28","Use RNN architecture to generate meaningful poems","","Generating Meaningful Poems using RNN's","","","1"
"3296","1122251","5689764","01/28/2021 11:04:04","## Task Details
This task is about data visualization of the total number of cases, deaths, actif and recovered cases.
This datasets help us to find the distribution of cases between states and the average age of deaths.","","Tunisian_Covid_Dashboard","","03/01/2021 23:59:00","0"
"4113","1250515","5932338","04/18/2021 12:36:53","## Task Details
The task is to classify the power signals according to the disturbances they represent.

## Expected Submission
The submission should contain a notebook which classifies the signals according to their respective class. You are free to choose any feature extraction technique other than FFT.

## Evaluation
Criteria
1) Proper organization of the code.
2) An associated EDA.
3) Accuracies, model parameters and the model size will contribute towards the first point of difference.","","Classify the signals according to power quality issue that they represent.","","05/31/2021 23:59:00","2"
"2793","946378","5918497","11/27/2020 05:05:43","## Task Details
usa election result data that will help you to understand total vote distribution among candidates, parties since 1976.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","US election 1976-2020","Usa election result data that will help you to understand total vote distribution among candidates, parties since 1976.","","0"
"4208","1301765","5709158","04/28/2021 06:50:33","## Task Details
### Preprocessing the dataset by the step
#### 1.interpolation
#### 2.denoising
#### 3.outlier handling
#### 4.treament of missing points","","Data preprocess","Preprocess the dataset","04/30/2021 23:59:00","0"
"4209","1301807","5709158","04/28/2021 07:11:42","## Task Details
## Preprocessing the dataset by the step
### 1.interpolation
### 2.denoising
### 3.outlier handling
### 4.treament of missing points","","Data preprocess","Preprocess the dataset","04/30/2021 23:59:00","0"
"4471","1362138","5716182","05/23/2021 17:13:58","## Task Details
During several home searches, phones were confiscated. After examining these phones, they were found to contain location information. It is believed that these two individuals met. Can you find out where?

## Expected Submission
The submission shall contain the longitude and latitude coordinates of the place where the two suspects met.","","Meeting Point Suspects","Find Out Where The Two Suspects Met","","0"
"4472","1362138","5716182","05/23/2021 18:05:07","## Solution
The two suspects met at: 52.3605,4.8745","","Solution - Meeting Point Suspects","","","0"
"4486","1362138","7489491","05/25/2021 13:45:28","We must bear in mind that if the two people are criminals and have been in the same place at different times, it is possible that they have shared a written message or drugs, weapons, etc.

# Description 

Find points in common at the same time and different time.Investigate collection points of possible criminals","","Investigate collection points of possible criminals","","05/26/2021 23:59:00","0"
"3672","1193002","5717512","03/06/2021 08:29:35","Recognize characters of Captcha images.","","Optical Character Recognition","","","0"
"5045","1455080","5724750","07/08/2021 08:24:52","## Task Details
The task is to explore the data and build and test regression models to predict Target values.

## Expected Submission
Expected submission is a notebook with EDA and model evaluation with an RMSE of the model 

## Evaluation
The evaluation metric is RMSE.

### Further help
If you need additional inspiration, check out some of these notebook that are built for regression:
- https://www.kaggle.com/bhaveshjain1612/posttestprediction-mae-2-35/notebook","","Predict Target variable","","09/08/2021 23:59:00","0"
"4219","1303133","5730096","04/28/2021 19:18:26","## Task Details
Classify best restaurants","","Which restaurants serve healthy food and which do not?","","","0"
"4215","1302856","5730096","04/28/2021 16:38:54","## Task Details
EDA","","Exploratory Data Analysis","","","0"
"4216","1302856","5730096","04/28/2021 16:45:22","Determination can be found by measurement of height, diameter by height, survival, and other contributing factors.","","Which seed types are best for soil conservation","","","0"
"4285","1321966","5730096","05/06/2021 15:31:12","EDA","","Exploratory Data Analysis","","","0"
"4284","1321723","5730096","05/06/2021 14:31:34","EDA","","Exploratory Data Analysis","","","1"
"4276","1319819","5730096","05/05/2021 19:03:49","## Task Details
Does seniority and reviewer expertise have an influence on review quality

https://www.researchgate.net/publication/340465725_Distributed_peer_review_enhanced_with_natural_language_processing_and_machine_learning","","Predict expertise of reviewers.","","","0"
"4275","1319696","5730096","05/05/2021 18:07:36","## Task Details
Health","","explore the univariate and multiple regression correlations of cardiovascular early deaths","and other health outcomes with risk factors","","0"
"4353","1337192","5730096","05/12/2021 21:42:54","EDA","","Exploratory data analysis","","","1"
"3986","1255154","5730096","04/06/2021 22:55:36","## Task Details
It is highly desirable among the decision makers in the garments industry to track, analyse and predict the productivity performance of the working teams in their factories. 

## Expected Submission
This dataset can be used for regression purpose by predicting the productivity range (0-1) or for classification purpose by transforming the productivity range (0-1) into different classes.

## Evaluation
.

### Further help
[1] Imran, A. A., Amin, M. N., Islam Rifat, M. R., & Mehreen, S. (2019). Deep Neural Network Approach for Predicting the Productivity of Garment Employees. 2019 6th International Conference on Control, Decision and Information Technologies (CoDIT). [Web Link]

[2] Rahim, M. S., Imran, A. A., & Ahmed, T. (2021). Mining the Productivity Data of Garment Industry. International Journal of Business Intelligence and Data Mining, 1(1), 1. [Web Link]","","predict the productivity range","","","5"
"3996","1255154","5730096","04/07/2021 20:40:31","## Task Details
Visualize the productivity #

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis","","","6"
"4147","1287749","5732687","04/22/2021 05:28:52","## Task Details
Try to use the dataset and train a model that could classify if a given patient has pneumonia or not.","","Classify if the patient has pneumonia or not.","","","0"
"4148","1287749","5732687","04/22/2021 05:30:23","## Task Details
Try to use the dataset and train a model that could classify the type of the pneumonia.","","Classify the type of the pneumonia.","","","1"
"3805","1218411","5735521","03/18/2021 06:13:54","## The History Behind
After being a Minecraft Youtube Fan, creating this dataset for image generation with GANs and failing, the hope still remains with transfer learning and state-of-the-art models (StyleGAN...). Getting good results with transfer learning is a great part of generative networks, and the main goal behind is to get good results with small datasets, which normally is an impediment for most of the datasets.

## Submission
A good submission will have:
- 50 to 100 generated images
- A Kaggle notebook with the code and the generative algorithm

## Evaluation
The generated images will be passed through a minecraft front face skin binary classifier. The more images are passed as real, the better the score will be.

## Help resources
You can create your code with some of these notebooks that implement generative algorithms:
- [Keras DCGAN implementation](https://keras.io/examples/generative/dcgan_overriding_train_step/)
- [StyleGAN2 PyTorch implementation for transfer learning](https://github.com/NVlabs/stylegan2-ada-pytorch)

# Remember, the main task is to have fun!","","Generate New Front Faces","Can you generate faces with this small dataset?","","1"
"2798","1000904","5741975","11/27/2020 13:35:01","Analysis of trends and patterns of data. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","JournalArticles","","12/07/2020 23:59:00","0"
"3992","1256229","5758131","04/07/2021 11:15:14","Predicting winners can be useful for making profit on betting sites.","","Predicting match winner","Predict the winner of an upcoming match based on previous results.","","0"
"4004","1256229","5758131","04/08/2021 11:47:36","Create a ELO based ranking for the players to possibly replace the current BWF ranking based on points. This could allow fairer ranking for players not being able to travel due the ongoing pandemic.","","Ranking","Create a new ELO based ranking to replace the current BWF ranking","","0"
"2640","960672","5759082","11/07/2020 05:46:06","Merge all files and consolidate all the addresses in one column plus retains the other columns as shown below:

Address	
EPSG:31370_x	
EPSG:31370_y	
EPSG:4326_lat	
EPSG:4326_lon	
region_code

No missing value or 0 value.","","Data Cleaning","","","0"
"2870","1018340","5762985","12/06/2020 07:25:38","## Task Details
It was my course project in the Kuban State University, I have to recognize Ishihara blind test cards. I hadn't found any datasets on the internet so I created it by myself. It's generated by list of google fonts. 

During my study I hadn't heard about Alexnet and so on, so I just hardcode filtering to b&w pictures.

## Expected Submission
It's really interesting for me how small can be DL model to solve such problem.","","Digit recognition in conditions of color proximity","","","2"
"4082","1265066","5768653","04/15/2021 17:41:50","## Task Details

Questions like:
- Which state has the highest vaccination?
- On which day was the vaccination highest/lowest?


## Expected Submission
- Analysis of the given data","","Track the progress of Covid-19 vaccination in India","Analysis of vaccination in the country","","0"
"4083","1265066","5768653","04/15/2021 17:47:34","## Task Details
- Prediction of the progress of vaccination in India based on the data for up till now
- When will 50% of the total population be completely vaccinated based on the current pace?
- When will the complete population be completely vaccinated?

## Expected Submission
A simple model based on the daily data of covid vaccinations that will be able to predict the vaccinations in the future.","","Predict future vaccination progress","Predict vaccinations in upcoming months based on the daily vaccinations","","1"
"4414","1277517","5769741","05/17/2021 13:56:22","## Task Details
You can experiment around and come up with a conclusion about what activities affect most and how these data are related. you can also come up with amazing stats that I don't even know.

## Evaluation
The task will be evaluated on Explanation and data visualization. Make it simple and explain your process.","","Data Analysis","Mood vs Activities","07/17/2021 23:59:00","0"
"4415","1277517","5769741","05/17/2021 14:02:06","## Task Details
You can divide data into train and test and then use any machine learning model to predict future mood.

## Evaluation
lower loss and better accuracy than 60 percent will make it.","","Future Mood Prediction","Can you predict my mood based on my past data.","07/17/2021 23:59:00","0"
"4060","1267512","5769741","04/12/2021 19:07:35","## Task Details
Use what ever method is required to produce trtranscript for the test dataset","","Fine Tune Wav2Vec2 Model","","","1"
"4418","1346575","5769741","05/17/2021 14:22:02","## Task Details
You can experiment around and come up with a conclusion about what activities affect most and how these data are related. you can also come up with amazing stats that I don't even know.

## Evaluation
The task will be evaluated on Explanation and data visualization. Make it simple and explain your process.","","Data Analysis","Mood vs Activities","07/17/2021 23:59:00","0"
"4419","1346575","5769741","05/17/2021 14:22:40","## Task Details
You can divide data into train and test and then use any machine learning model to predict future mood.

## Evaluation
lower loss and better accuracy than 60 percent will make it.","","Future Mood Prediction","Can you predict my mood based on my past data.","07/17/2021 23:59:00","0"
"3068","1065362","5769741","12/29/2020 11:32:35","## Task Details
Create a four cluster  of the province in Argentina based on economic indicators. You can visualize your cluster and determine the most diverse cluster out of 4.


## Expected Submission
Your notebook should contain detain explanation of preprocessing and visualization of your data. You can use any method and come up with a solution. 

## Evaluation
What makes a good solution? It should contain a detailed explanation of how you arrived at coconclusi and what was the reason behind your chosen methodology.","","Create Cluster of province","","","1"
"3102","1072705","5769741","01/02/2021 11:38:20","## Task Details
In this project, you will use a form of regression called hierarchical modeling to capture and explore crime statistics collected by the State of Maryland to see if there is a linear trend in violent crime across the state between 1975 and 2016. These data come from the Maryland Statistical Analysis Center. 

## Expected Submission
Try using a map of Maryland plot the data. Predict violent crime for the next 10 years. The best submission contains a detailed description of the methodologies used. 


## Evaluation
Using visualization and using appropriate regression machine learning models to predict the data. The more accurate score will get top position.","","Violent Crime prediction in Maryland","","01/30/2021 23:59:00","1"
"2963","1040820","5769741","12/16/2020 11:00:59","## Task Details
Use dataset to find which ingredients and products are good for Dry skin

## Expected Submission
submit predictive products and ingredients that are dry skin-friendly, based on ingredients in a product.","","Machine Learning for products","","","0"
"2928","1032846","5769741","12/13/2020 14:49:15","## Task Details
Use a common password dataset and compare it with your existing password user database","","you can compare common passwords","","","0"
"3810","1215636","5769741","03/19/2021 09:16:01","## Task Details
You can use any method to tokenized and train your language model, for help look at this repo [English_Yoruba_Transformer](https://github.com/Olabiyisam/English_Yoruba_Transformer)


## Expected Submission
You can use sample submission format and translate test.csv to English?

## Evaluation
For Evaluation better score BLEU score will determine the top notebook?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://zindi.africa/competitions/ai4d-yoruba-machine-translation-challenge (AI4D Yor√πb√° Machine Translation Challenge)","","Train and Evaluated ML Translation Model","Using Transformers and Google Seq2Seq","","0"
"4917","1169552","5769741","06/26/2021 07:17:06","## Task Details
You can use any pre-trained model to come up with the solution of classification of arabizi tweets.

## Expected Submission
notebook showing your f1 score.

## Evaluation
The best notebook would have a higher f1 score.","","Text Classification","Use Transformers to predict sentiments","","0"
"4908","1429009","5769741","06/24/2021 17:55:25","## Task Details
Use feature engineering, explore the data, visualize and come up with patterns and trends. 


## Evaluation
all the notebooks with good explanations are approved.","","Exploratory Data Analysis","Find patterns and trends in this simple datasets.","","2"
"4909","1429009","5769741","06/24/2021 17:59:39","## Task Details
Use a simple ML model to predict the date of the last vaccination of the world. 

## Expected Submission
You need to give Data on when the entire world will be vaccinated, and you have to show your work in form of a notebook. 

## Evaluation
the best possible model with accurate prediction will get accepted. We will be comparing the results with real data.","","Predict the date of final vaccination","use machine learning model to predict when entire world will be vaccinated.","11/24/2021 23:59:00","2"
"4906","1429009","5769741","06/24/2021 12:25:52","## Task Details
You can use any python library to create interactive graphs and explain your thought process. Also, e explain what trends are common in vaccination progress. 

## Expected Submission
The solution should contain a notebook to explaining the interactive graph such us, World Dose Administered.


## Evaluation
A good notebook with clean code and a clear explanation of interactive graphs get accepted.","","Create Dashboard","On Vaccine progression using plotly or bokhe.","08/18/2021 23:59:00","2"
"3089","1071350","5771810","01/01/2021 15:50:08","## Task Details
we want to know in which month have best sale and which product sale more on that 
for example in jun we have best sale and our product is laptop and pen

## Expected Submission
if you jave any idea or other task takes comment below","","best sale base on month","","","1"
"4271","1317934","5779931","05/05/2021 05:46:22","## Task Details
Classify unseen URLs into categories.

## Evaluation
A model with high accuracy that is able to classify unseen website URLs is a good solution.","","Classification of websites","","","0"
"4626","1386457","5014805","06/04/2021 00:48:53","This task is to perform some data cleaning of the data set prior to our exploration and analysis.","","Data Cleaning","","03/04/2023 23:59:00","0"
"2740","986204","5780391","11/20/2020 20:32:05","## Task Details
Make the same animation graph in the kernel in the link below.
https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Animation Graph For Unemployment Rate 2000-2020","Animation Graph For Unemployment Rate","11/20/2021 23:59:00","1"
"2739","986205","5780391","11/20/2020 20:27:54","## Task Details
Make the same animation graph in the kernel in the link below.
https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Animation Graph For Unemployment Rate 2000-2019","Animation Graph For Unemployment Rate","11/19/2021 23:59:00","1"
"2866","1017752","5783021","12/05/2020 20:56:39","## Task Details

Make the same animation graph in the kernel in the link below.
https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Animation Graph For Long-term Unemployment Rate","Animation Graph For Long-term Unemployment Rate","12/05/2021 23:59:00","0"
"3478","1127949","5785977","02/12/2021 18:35:22","## Task Details
Imagine a library needs a software to help them classify and order science fiction books according to their subgenres.

## Expected Submission
Try to develop a multilabel text classification model.

## Evaluation
A good solution would be a solution with a good accuracy, f1 score and roc auc.

### Further help
If you need inspiration, check out my notebook:
- https://www.kaggle.com/tanguypledel/sf-books-eda-genre-classification","","Predict subgenres from book descriptions","","","1"
"2597","948000","5797807","10/31/2020 14:15:31","## Task Details
An prediction always starts with an **Exploratory Data Analysis (EDA)**. This approach is a necessary step, since it's fundamental to tell us more about the data beyond the formal modeling and experimenting steps.

You'll be asked to state and answer interesting questions about the data by using a wide range of tools and techniques.

Some interesting questions about the data may include (although you can state more questions):
- What are the most acclaimed albums of each of the year in the millenium (2000-2020) by the users?
- What are the most hyped records year by year by the users.
- What artists generate the most hype?
- What are the most critically acclaimed artists of the millennium?
- Which genre generates the most feedback either from the critics and from the users?
- What are albums with the most negative reception, either from the critics and from the users?
- Which EP album was the most hyped?
- What release months are the most well received by the users and critics?
- Which records not well received by the users are most critically acclaimed?
- Which records not well received by the critic are the most acclaimed by the users?

## Expected Submission
A notebook that accomplishes the task.

## Evaluation
A good solution must perform better in these aspects:
- Documentation: of course an EDA must be well documented by including a description of each each step or conclusion and sources for your work. Every graph must be interpreted and questions must be clearly stated.
- Preparation: the current data is not entirely clean, so data preparation is a necessary step and it must be well documented.

### Further help
These high-quality tasks must provide you with some ideas for your analysis:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA Analysis","Answer interesting questions about the data","","1"
"4281","1320895","5797884","05/06/2021 08:37:08","You can check instant noodle price history and visualize.","","Visualize","","","0"
"4602","1372354","5797884","06/02/2021 00:45:44","Visualize & Wine Recommendation.","","Visualize & Wine Recommendation.","","","0"
"2867","1017753","5814948","12/05/2020 21:11:44","## Task Details
Make the same animation graph in the kernel in the link below. https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Air Quality of Europe Countries 2002-2018","","12/22/2021 23:59:00","0"
"2438","921545","5815839","10/15/2020 00:30:20","## Task Details
This is a dataset of type effectiveness data of all Pokemon currently in the Galar Pokedex (post-Isle of Armor, pre-Crowned Tundra). Analyze type effectiveness to create a compelling story of the best typed moves to have on hand for general usage. 

## Expected Submission
This will be done best in a notebook rather than a script. Remember, readability will really help you in the future, so use variable names that give you more information about what it is doing and utilize the notebook's features to describe what you are doing.

## Evaluation
Use the data however you see fit to describe the information you are seeing. Just be sure to keep in mind that lower scores here does not necessarily mean the type is bad.","","Analyze Type Effectiveness Data","","","0"
"2741","918976","3632349","11/21/2020 08:38:15","Explain your insights through visualizations","","Exploratory Data Analysis (EDA)","","","75"
"3013","918976","653883","12/22/2020 20:07:25","## Task Details
Predict user rating.

## Evaluation
rmse","","Predict user rating of a book","","","25"
"5412","918976","4656934","07/29/2021 21:36:26","Build a Recommendation system using euclidean or minkowski or manhattan  distance and cosine similarity.
- Should Recommend atleast 5 books based on given (chosen) book.

Example Demo:
Input:
**getRecommendation_books_for(""Breaking Dawn (The Twilight Saga, Book 4)"", recommend_books = 5)**

output:
**Similar Books for Breaking Dawn (The Twilight Saga, Book 4):**

1. New Moon (The Twilight Saga) 	
2. Twilight (The Twilight Saga, Book 1)
3. Eclipse (Twilight) 
4. Eclipse (Twilight Sagas)
5. The Short Second Life of Bree Tanner: An Eclip....

Note:
I have used manhattan distance metric.
you can choose whatever distance metric you want.","","Recommendation","","","1"
"3036","1057209","5821522","12/24/2020 16:51:23","Explore the dataset- Its distiribution, outliers?. If you see an easy way to extract the manufacturer and model into two columns. Buzz if any questions/problems.
Final goal is prediction of price!","","Exploration, Cleansing or Prediction","","","1"
"2882","1010647","5824113","12/07/2020 12:27:55","visualize this dataset uniquely","","visualize this dataset uniquely","","","0"
"2874","1018635","5824113","12/06/2020 10:53:14","Just enjoy the data
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.


## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualize and analyse The dataset","I know that  the data set is very small. But find out what makes these all movies best!!","","0"
"4010","1246102","5828601","04/08/2021 22:10:38","## Task Details
Use the train set to train a Recurrent Neural Network that is able to predict the next character that follows a sequence of text. Then use the model to generate new text from scratch.

## Evaluation
A good model is able to correctly predict the next character after a sequence of text in the validation and test sets. 82% accuracy is realistic.","","Create a character-based RNN model","Use the model to generate fake regulatory text in the style of the US civil aviation regulations","","0"
"4011","1246102","5828601","04/08/2021 22:13:10","Apply techniques to create word embeddings to the full corpus of US civil aviation regulations.","","Create a word embedding from the full corpus of regulations","","","0"
"4616","1385478","5833379","06/03/2021 06:15:02","##Aim
To differentiate between covid patient lungs and normal ones.

##CONTENT
The dataset contains 162 images of lung x-ray of covid people and normal ones. It's an image classification problem.","","Identify the covid patient","","","1"
"3990","1256015","5836792","04/07/2021 09:49:00","## Task Details
This data set supports anomaly detection on time series data with accurate labels.
Feel free to use the whole observation or just the **x**, **y** and **z** values.

## Expected Submission
***Notebooks are fun!***

## Evaluation
Interesting evaluation criteria are:
- Accuracy
- Balanced Accuracy
- False Negatives
- False Positives
- **F1 Score**
- Precision
- True Negatives
- Selectivity
- True Positives
- Recall","","Detect Anomalous Events in a Smart Environment","Detect the anomalous events by training and testing","","0"
"3720","1199497","5841875","03/10/2021 06:09:11","Create a recommendation program which helps user identify what to watch on Amazon Prime video using different techniques","","What to Watch?","What to watch on Amazon Prime Video","","0"
"3147","1082831","5842243","01/06/2021 23:51:45","Trying to figure out who is the best in the vision of Data Analysis & Data Science","","Cristian Ronaldo Vs Lionel Messi EDA","Cristian Ronaldo Vs Lionel Messi EDA","","0"
"2809","972430","5842720","11/29/2020 10:02:21","## Task Details

Proper Motion measures the change in right ascension and declination of star denoted in milliarcseconds per year ( mas/yr ) .

These values are provided within fields 15 and 16 .

## Expected Submission

The output of the notebook should be the same dataset , but with updated values for fields 4 , 5 and 9 , 10 .

This will be a change in the epoch of the dataset from 1991.25 to the date on which the new file will be created .

## Evaluation

We will only verify the algorithm used to update the RA and DE fields for every year since the epoch date .

Missing values for every field need to be handled appropriately .

Preference given to solutions that use a high precision library for the calculations .","","Written by the Stars Challenge","Account for the Proper Motion of stellar objects since 1991","","0"
"4956","1295752","7726014","06/30/2021 22:05:28","axonemes","","axoneme tree","","","0"
"5295","1295752","5842720","07/23/2021 16:50:03","Write code to find the nearest common ancestor for any two given species . 

This requires you to follow the links between nodes until the first common match is found . 

Here we evaluate the ancestry of your species :

```
16421 Homo sapiens
16418 Homo
16416 none
16414 none
16412 none
16411 none
16299 Hominidae
16298 none
16293 Catarrhini
16291 none
16290 none
15963 Primates
15962 none
15961 none
15955 none
15997 Eutheria
15995 none
15993 none
15990 none
15040 Mammalia
15030 Cynodontia
15028 none
15026 Theriodontia
14973 Therapsida
14971 Sphenacodontoidea
14969 none
14967 none
14965 none
14963 Sphenacodontia
14961 none
14959 none
14957 Eupelycosauria
14845 Synapsida
14990 Amniota
14989 none
14988 Reptiliomorpha
14987 Tetrapoda
14986 none
14985 none
14984 none
14983 none
14982 none
14981 none
14980 none
14979 none
14978 none
14977 none
14976 none
14975 none
14952 Terrestrial Vertebrates
14950 none
14948 none
14944 none
14922 Sarcopterygii
14921 Osteichthyes
14920 Teleostomi
14919 Node 1
14843 Gnathostomata
14840 Node 3
14838 Node 2
14836 none
14833 Node 1
14829 Vertebrata
14826 Craniata
14822 none
14820 none
2499 Chordata
2466 Deuterostomia
2459 Bilateria
2458 none
2374 Animals
2373 none
2372 Opisthokonts
3 Eukaryotes
1 Life on Earth
```","","Nearest Common Ancestor","","","0"
"3730","1197108","5845181","03/11/2021 08:31:55","simplify the data system by reducing the number of variables to be studied, without sacrificing too much accuracy. such a system would help Subsidy Inc. in planning subsidy outlay, monitoring, and preventing misuse","","Develop an income classifier system for individuals.","","","1"
"2877","1019029","5845534","12/06/2020 15:19:26","## Task Details
The main motive for cybercrime across all the cities for both the years? The crime rate analysis for both the years of major cities.","","Why Do It ?","","","0"
"2399","914788","5850041","10/10/2020 19:19:57","# Task includes:
1. analyze the dataset to summarize the main characteristics
2. 'Visual Representation' is aesthetically pleasing
3. seeing what the data can tell us before the modeling","","Exploring House Prices In Bengaluru","","","0"
"2400","914788","5850041","10/10/2020 19:20:57","1. Predict prices for the test dataset
2. Method of evaluation: RMSE","","Predicting House Prices In Bengaluru","","","1"
"4228","1305507","5858511","04/29/2021 20:42:03","I primarily used this data set in STATA, however, I am curious if it can be used in other places.

## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","STATA Usage","Great for work in STATA","01/29/2025 23:59:00","0"
"3878","1090726","5870027","03/27/2021 09:27:36","## Task Details
This dataset is generated using various augmentation techniques and there is a chance that there might be duplicate image files because skewering may produce identical results.

## Expected Submission
If anyone finds a duplicate image, a file with the duplicate image pairs can be submitted as well as code or any other tools used to find the duplicates. This can all be done in a notebook.

## Evaluation
- It would help a lot if code could be thoroughly documented or otherwise clear. (i.e., code style)
Usability:
- The submission accomplishes the task specified.","","Check if dataset has duplicate images","","","0"
"3170","1090054","5873425","01/11/2021 00:25:30","Create an efficient supervised model that classifies pulsars.","","Create a model based on this dataset that classfies pulsars","","","0"
"3611","1183135","5878399","02/27/2021 05:46:21","Suggested method:
Use the Open/Close price of the day, and value each trade.","","Compare Trades","Provide a suitable metric to compare/normalise trades","","0"
"3471","1148270","5879992","02/12/2021 01:04:12","## Task Details
Simulate the game's process of choosing a random game by choosing a random row from the games.csv file.

## Expected Submission
The submission should print out a random game every time it is run.

## Evaluation
Does every game have an equal chance of being picked, or is it biased?","","Random game","Choose a random game from all of the games in the game.","","0"
"3470","1147387","5879992","02/12/2021 00:02:41","## Task Details
Simulate the game's process of choosing a random map by choosing a random row from the maps.csv file.

## Expected Submission
The submission should print out a random map every time it is run.

## Evaluation
Does every map have an equal chance of being picked, or is it biased?

## Extra Credit
Not all realms contain the same amount of maps, so realms with more maps will be chosen more often than realms with less maps. How could you alter your code to even the odds and give the realms with less maps better odds?","","Random map","Choose a random map from all of the maps in the game.","","0"
"4726","1342347","7618590","06/10/2021 20:15:05","Predict defaulters","","Predict defaulters","","","0"
"4727","1342347","7643228","06/10/2021 20:50:13","EDA for defaulters","","EDA for defaulters","","","1"
"4688","1395012","7622857","06/08/2021 17:48:05","Can any tell me how to do association?","","Association Rule Learning","","","0"
"4673","1395012","5882083","06/07/2021 17:56:17","You have to build a item recommendation system using this dataset.
Find out which item pairs go well which each other","","Item Recommendation","","","0"
"4674","1395030","5882083","06/07/2021 18:10:56","Using the dataset predict is the food being liked or not?","","Predict food is being liked or not?","","","1"
"4675","1395030","5014805","06/08/2021 02:54:02","Using NLP classification and Tf-idf","","NLP Classification and Tf-idf","","","0"
"4678","1395030","7618590","06/08/2021 10:31:04","create bag of words model","","Bag of words","","","0"
"4666","1394179","5882083","06/07/2021 12:36:00","You have to predict single_prediciton folder's images","","Single Test Prediction","","","1"
"4686","1394179","7618590","06/08/2021 17:17:26","Predict the images in the dataset","","Image Predictions","","","1"
"4679","1394586","7618590","06/08/2021 10:37:36","Predict the item has been purchased or not!","","Predict the item has been purchased or not!","","","0"
"4671","1394586","5882083","06/07/2021 17:29:00","Predict whether the item has been purchased or not?","","Predict whether the item has been purchased or not??","","","0"
"4782","1410082","7683220","06/15/2021 11:39:26","Tutorial for turicreate","","Tutorial","","06/16/2021 23:59:00","0"
"2688","970997","5892483","11/13/2020 09:26:03","Try to explore the relation between on-field performance with stock trends","","Predicting the effect of matchday results on stock trends","","","0"
"2684","969065","5900387","11/12/2020 14:51:40","## Task Details
After course we just have fun playing with different pictures as a masks to visualize word cloud. Not all of them are looking good, especcially without frames. So, fill free to play with me and try to find the stile and picture that will look good. For what reason? Just because it's interesting I think (especially if you are beginner as me) and good way to learn wordcloud ;)

## Expected Submission
Notebook with code and finall picture

## Evaluation
What makes a good solution? Solution from which you will be satisfied","","Find your Simpson photo and style as better mask for WordCloud","","","2"
"4447","1253419","5904009","05/20/2021 03:37:45","## **Task Details**
As i told you in description, we could use Computer Vision to identify, track, sort, and determine what kind of process should do with the waste. Let's start with a simple thing. Please make neural network model to classify waste's material.

## **Expected Submission**
Submit your notebooks here. You could use any language, framework, technique, or strategy in your notebook. Don't forget about documentation, make it as clear as possible. So, everyone in the community can understand it. 

## **Evaluation**
Just imagine that it's a playground. I just want to know how far you can do with this dataset. No deadline. Evaluate yourself.","","Main Task","Binary Classification to Identify the Type of Materials","","1"
"3157","1086633","5905558","01/08/2021 19:33:08","## Task Details
Display and inspect the summaries of the TV and halftime musician DataFrames for issues.

Plot a histogram of combined points then display the rows with the most extreme combined point outcomes.

Modify and display the histogram of point differences, then display the rows with the most extreme point difference outcomes.","","Explore the notebook","","","0"
"3513","1159749","5907678","02/15/2021 21:34:21","## Task Details
Since it's a huge dataset the best way to understand the data will be with an EDA that visually explains  it.

## Expected Submission
A Notebook with graphs and figures is expected.

## Evaluation
The notebook submited should be easy to understand and the information given with the graphs must be relevant.","","Exploratory Data Analysis (EDA)","Visual Exploratory Data Analysis.","","1"
"5580","1159749","5194950","08/06/2021 12:50:06","## Task Details
For centuries chess players have debated which openings provide the most advantage against stronger, equal, and weaker opposition; for slow and fast time controls; for White or Black; etc.

## Expected Submission
Solutions should contain chess openings (ECO codes) in the dataset along with either qualitative or quantitative information.

## Evaluation
Good solutions could apply well to predicting results of games either via cross-validation or for games not included in the dataset.  Use your imagination.","","Measure opening strength","","","0"
"3441","1146894","5913692","02/08/2021 22:16:10","## Task Details
- What can you tell about ""data workers"" in the developers sanctuary?
- How is the annual income of ""data workers"" affected by several factors?
- Could you visualize some interesting infographics?","","Exploratory Data Analysis","","","0"
"3865","1233044","5913692","03/25/2021 22:02:21","## Task Details
What is the impact of Region on salary?
What is the impact of education major on salary?
What is the impact of Years of Experience? 
What is the impact of the organization‚Äôs size?
What is the impact of the Job Title?","","Are there any significant factors affect the salary?","Exploratory data analysis, Regression","","0"
"2590","948299","5913986","10/31/2020 04:58:01","## Task Details
The usage of clickbaits seems to emerge because of the advertising business model commonly used in online media, but when was the trend started? at what specific topics? 


## Expected Submission
Users can submit trend plots and topic clusters to help describe this dataset. Also, users can write additional argument to why the online news site agree to use the petty trick of clickbaiting","","Usage of Clickbait Trend Analysis","","","0"
"2591","948299","5913986","10/31/2020 05:17:21","## Task Details
Every media entity has an agenda. By clustering the topics used in multiple time periods, analyzing its trend, and other methods, we can identify the agenda-setting and frames used by this specific media company.  Further analysis and argument may give light to why this agenda-setting needs to be set, and maybe can be correlated to predict public behavior.

## Expected Submission
Notebooks to detect clusters, topic models, and visualization graphics to tell the story about the data and its possible implication to the society","","Topic Clustering to Identify Agenda Setting and Frames","","","0"
"2765","994468","5915164","11/24/2020 15:59:06","**Total births by GENDER and YEAR**","","Visualize the proportion of babies given a particular name over time","","","0"
"2766","994468","5915164","11/24/2020 16:02:13","Vowels ,Consonants ,Length ,Overall diversity ,changes in spelling First and Last letter.","","Analyze trends in names","","","0"
"4893","1426927","5916217","06/23/2021 08:06:36","Compute the service level by SKU (Item) and by client","","Service Level","The Service Level is the ratio delivered quantity over ordered quantity","","0"
"4894","1426927","5916217","06/23/2021 08:07:57","A category is less than 80%
B category from 80 - 90%
C category is above 95%","","ABC classification","Sort by volumne and class by sum in percentage of total","","0"
"4895","1426927","5916217","06/23/2021 08:10:01","Example: using time, volumne, ...","","Recommendations for further investigations","Other insights from the dataset","","0"
"2442","922253","5921597","10/15/2020 06:35:25","****You have To predict spending Score of mall customers using all other attributes.****","","What TO DO?","Prediction Of Spending Score Of Customers","10/16/2023 23:59:00","0"
"3334","1127993","5921597","01/30/2021 12:12:06","## Task Details
Data contains data of corona virus cases
and state wise testing details.

## Expected Submission
NA

## Evaluation
Only for visualisation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","COVID 19 India","COVID DATA","","1"
"2576","936723","5926459","10/29/2020 17:53:09","## Task Details
Creating a script for Data Extraction

## Expected Submission
Automate script submission

## Evaluation
Commit the Data File","","Create a Script for Daily Update of Data","Generating script for daily data update","11/29/2020 23:59:00","0"
"4432","1351057","5933479","05/18/2021 16:26:55","## Task Details
Through the analysis I hope you will try to answer the following questions:
1. What are the differences in the preferences and behaviors of people of different genders, different ages and different regions when buying fruit?
2. Which fruit brand and supermarket have the highest popularity?
3. Try to use clustering method to classify volunteers according to different indicators and explore the relationship between them.
4. Are there hidden attributes and rules of fruit retail consumers?","","Analysis based on questionnaire data","","","0"
"3049","1060594","5939391","12/26/2020 22:07:46","One use of this dataset is to predict the price of the PV System based on all other features. If you look in my notebook, I got an r2_score of 21.1%. Try to aim towards getting a better score.","","Predict the total_installed_price of the PV System","","","0"
"3366","1131056","5939391","02/01/2021 02:18:01","Using this data and LSTM or Transformer models, make a program that can generate Trump tweets","","Generate Trump Tweets","","","0"
"3217","1103215","5939391","01/17/2021 22:54:02","Using neural networks such as LSTM, GRU, Conv-Nets, or Gpt-2 to use the data to generate Trump tweets.","","Generate Trump speeches","","","1"
"3261","1114689","5939391","01/23/2021 20:06:21","Using the data from this csv file, prove that as vaccine rates go up, disease rates go down","","Study correlation between vaccine rates and disease rates","","","0"
"2441","917051","5939547","10/15/2020 05:54:29","## Task Details
The dataset was inspired by the ""Learning to See in the Dark"" work. We continued the research as a final project of a course from our Master's degree. 
The images were taken with a PROSILICA GT 1930 camera of AlliedVision. We trained a UNet and get pretty good results. We challenge you to try to get better results. 
We took the 85 first images as train set and the 15 last images as validation. 

## Evaluation
The evaluation should be the SSIM and MSE of the images in validation set.

### Further help
For questions, contact us:
yakirhadad@mail.tau.ac.il
tzvilederer@mail.tau.ac.il","","Learning to See in Greater Dark","","","0"
"2704","974974","5944036","11/16/2020 01:26:26","## Task Details
The data is collected among married individuals in ƒ∞stanbul in Turkey in 2020. It is asked to reveal impact of the demographic variables on depression scores. Aim of the study is to find out whether there is a difference or not.

## Expected Submission
Notebook submission is required.

## Evaluation
Answer of the question below will be considered in evaluation.
1. Do you think the Beck Depression Inventory items are consistent to calculate a score with them? How would you test it?
2. Which type of A/B Testing method (parametric or non-parametric) must be used for each categorical question?
3. How would you decided to choose the type of A/B Testing method (parametric or non-parametric) must be used for each categorical question?
4. What are the hypotheses for each test method used, how it is interpreted? 
5. Check out the ANOVA and Kruskal Wallis Test compare the results with the results of Independent Samples t Test and Mann Whitney U Test? Which way it better?
6. How would you evaluate the significance of difference when the scores are categorised based on the cut-off point?

### Further help
If you need additional inspiration, check out the scoring information of Beck Depression Inventory and its cut-off points.","","Demographical Differences in Depression Among Married People","A/B Testing Examples","","1"
"2938","1034839","5944036","12/14/2020 13:59:52","## Task Details
The aim of the study is to examine whether ""Magnetic Resonance Imaging (MRI) Units"", ""Computed Tomography (CT) Scanners"" and ""Hospital Beds"" investments of the governments have an impact on Length of Hospital Stay of citizens in OECD countries.

1. Are the time series stationary? (Durbin Watson Test)

2. How would you make a serie stationary if it is not? (Log Transformation, Difference Taking)

3. Apply Fixed Effects and Random Effects in panel data, which model would you prefer? (Hausman Test)

4. Is there heteroskedasticity? (Breusch-Pagan Test or White Test)","","Panel Data Analysis in Healthcare Investments and Length of Hospital Stay","Panel Data Analysis in Healthcare Investments and Length of Hospital Stay","01/14/2022 23:59:00","2"
"2859","996759","5944036","12/04/2020 11:48:34","## Task Details
Make the same animation graph in the kernel in the link below.
https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Animated Graphs","","12/04/2021 23:59:00","0"
"2742","987042","5944036","11/21/2020 09:05:49","## Task Details
The aim of the study is to examine whether the 2015 Women's Entrepreneurship Index and Global Entrepreneurship Index show a significant difference between OECD countries that are members of the European Union and not, as well as to determine the significance of the relationship between the indexes.
1. Are European Union membership variable and development variable independent from each other? (Method Chi-Square Test and Crosstab)

2. Do the Women Entrepreneurship Index and Global Entrepreneurship Index values show a statistically significant difference between the countries that are members of the European Union and not? (Method Mann-Whitney U)

3. Is there a statistically significant relationship between Women's Entrepreneurship Index and Global Entrepreneurship Index values? (Method Spearman Correlation Coefficient)","","Non-Parametric Tests in Entrepreneurship Data","Non-Parametric Tests in Entrepreneurship Data","11/21/2021 23:59:00","9"
"2933","987042","653883","12/13/2020 20:04:02","## Task Details
Predict Entrepreneurship Index

## Expected Submission
Notebook

## Evaluation
AUC","","Predict Entrepreneurship Index","","","3"
"2735","984249","5944036","11/20/2020 09:40:24","## Task Details
Aim: Separate the employees to 3 different segment of job turnover intention, using 3 of the variables out of 4 variables of ""T""coded scale items.

Segment Names:  Ready to Leave, On Hold, On Consideration
## Expected Submission
Notebook submission is required.

## Evaluation
Answer of the question below will be considered in evaluation.
1. Which 3 combination  of the 4 scale items would you use better to segment the employees? How would you define it (Check our Correlation and Cronbach Alpha)?
2. Use Chi-Square Test to examine the relation between the Segments and Demographical variables?
3. What are the hypotheses for each test method used, how it is interpreted? 

### Further help
If you need additional inspiration, check out the studies below.

In Turkish  https://medium.com/@krglnahmetcan/rfm-analizi-ile-m%C3%BC%C5%9Fteri-segmentasyonu-b3c7233b4243
In English","","Segmentation (RFM Technique) on Turnover Intention of Retailing Industry Staff","Segmentation of Turnover Intention","11/20/2021 23:59:00","2"
"2860","1015225","5944036","12/04/2020 11:49:11","## Task Details
Make the same animation graph in the kernel in the link below.
https://www.kaggle.com/mathchi/animation-for-time-series-earthquake-dataset","","Animated Graphs","","12/04/2021 23:59:00","0"
"3123","1077181","5956205","01/04/2021 14:45:56","Hello fellow kagglers,
In this task you have to classify the images and correctly recognize what is the diver trying to say.
Basically a Multiclass- classification problem.
So if I provide an image of a diver the model should predict the Name of the gesture performed.","","Identify gestures","Tell what the diver is saying.","","0"
"3412","1140248","5956666","02/05/2021 15:06:30","## Task Details

The goal of this task is try to find file-test links, so that when a new commit arrives, only relevant tests are applied in the beginning.

## Expected Submission

The output of your notebook should be a list of commits from the test set (20%) and for each one, a test schedule, i.e. an ordered list of tests to run.

## Evaluation

The evaluation metric should be the Average Percentage of Transition Detection (APTD), described in this paper:

[https://arxiv.org/pdf/2012.10154.pdf]()","","Predict Optimal Test Schedule per Commit","Re-order tests to maximize likelihood of link","","0"
"4348","1336160","5959315","05/12/2021 12:22:47","## Task Details
Predict the type of crop planted for the next year. We assume that this will help regulate supply-demand in agriculture for farm products. It will also take into account the specialization historically formed in certain territories due to the climatic zone, etc.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) ID
2) Cult_2020

## Evaluation
Evaluation using F1-Score (given the output class imbalance)","","Predict crop culture for 6th year","","","0"
"2769","995013","5959476","11/24/2020 21:24:26","Making a program using ML model which will predict our certain day's class of specific coures  will held or not along with my possibility of attending that class.","","Predict the next class","next class will be held or not","","1"
"2586","947787","5959476","10/30/2020 17:52:58","To find out Selling price .","","Samsung mobile price in Bangladesh","Price in TAKA","","3"
"2594","947787","5959476","10/31/2020 08:45:25","provide a mobile phone's name by using all column","","Provide a Mobile","price at taka","","1"
"2587","948073","5961163","10/30/2020 23:34:13","Se recomienda agregar datos de contraste en im√°genes para detectar el humo presente en ciertas im√°genes dado que este atributo es muy confundido por el algoritmo de arboles de clasificaci√≥n","","Implementar contraste","","","0"
"4241","1267724","5962159","05/01/2021 12:22:02","## Task Details
In the era of social media child safety over the internet has become one of the biggest concerns, So use this data to build a model that can classify the chats into  safe or perverted

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classify between Perverted and normal texts","","","0"
"3194","1097747","5963867","01/14/2021 20:51:31","You will be working with a dataset that contains a sample of 5,000 rentals listings in Manhattan.","","Multiple Linear Regression: Scikit-Learn","","","1"
"2623","953887","792567","11/03/2020 19:02:46","It would be nice to see some super cool visualizations that can be used to understand the at risk  water resources around the world.","","Make pretty maps","","","1"
"2934","1001534","5966437","12/13/2020 23:14:02","Based on the stats, what should the universities focus on in order to improve their standings in next year's ratings?","","Finding out how to imrove rankings","","","0"
"4261","1313803","5966695","05/04/2021 08:50:48","You can do EDA and then build a model that  differentiating between the two species of mushroom","","Object detection","","","0"
"4260","1315364","5966695","05/04/2021 08:47:33","## Task Details
You can do EDA and then build a model that reads letters in sign language.","","Object detection","","","0"
"4263","1315402","5966695","05/04/2021 08:56:42","You could use this dataset to, for example, build a classifier of workers that are abiding by safety codes within a workplace versus those that may not be. It is also a good general dataset for practice.","","object detection","","","0"
"4264","1315426","5966695","05/04/2021 09:03:31","You can do EDA and then build a model for
- Identify a number of boats on the water over a lake.
- Boat object detection
- Identify cars with a drone
- Find which lakes are inhabited and to which degree.","","Object detection","","","0"
"2466","926141","5979879","10/17/2020 15:13:04","## Task Details
This task is made in order to solve the corn infection problem as classification.

## Expected Submission
There is submission expectation, it is for everyone.

## Evaluation
Try using different metrics like f1 score, confusion matrix etc.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Find a best model to classify Infected vs Healthy leaves.","","","6"
"2474","926141","5979879","10/18/2020 01:01:47","## Task Details
Classification is easy task but detection is hard because of image complexity.

## Evaluation
Using different evaluation metrics like IOU, MAP can come to aid.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Train this dataset to successfully detect infected region.","","","1"
"3140","1081718","5979879","01/06/2021 13:16:24","## Task Details
Why not view how the stock market was performing for each company?

## Expected Submission
No date.

## Evaluation
More cleaner and more interactive the better.

### Further help
If you need further help about data, please read the [blog](https://q-viper.github.io/2020/11/21/deploying-nepse-data-visualizer-on-heroku/) or leave a query.
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Do visualization of the data.","Make visualization of Time Vs Buyer, Broker, Amount and Rate","","2"
"4029","1262459","5991444","04/10/2021 10:03:57","Use this data and perform Bitcoin Price prediction using whatever resources you want.","","Bitcoin Price Prediction","","","7"
"2604","951115","5994377","11/02/2020 02:42:40","Mortgage lenders are interested in determining borrower and loan factors that may lead to delinquency or foreclosure. In the file lasvegas.csv are 1,000 observations on mortgages for single family homes in Las Vegas, Nevada during 2008. The variable of interest is DELINQUENT, an indicator variable = 1 if the borrower missed at least three payments (90+ days late), but 0 otherwise. Explanatory variables: are LVR = the ratio of the loan amount to the value of the property; REF = 1 if purpose of the loan was a ‚Äò‚Äòrefinance‚Äô‚Äô and = 0 if loan was for a purchase; INSUR = 1 if mortgage carries mortgage insurance, 0 otherwise; RATE = initial interest rate of the mortgage; AMOUNT = dollar value of mortgage (in $100,000); CREDIT = credit score, TERM = number of years between disbursement of the loan and the date it is expected to be fully repaid, ARM = 1 if mortgage has an adjustable rate, and = 0 if mortgage has a fixed rate.

(a) Estimate probit model explaining DELINQUENT as a function of the remaining variables. Are the signs of the estimated coefficients reasonable?

(b) Compute the predicted value of DELINQUENT for the 500th and 1,000th observations using the probit model. Interpret the values.

(c) Calculate the probability of delinquency for CREDIT = 500, 600, and 700 for a loan of $250,000 (AMOUNT = 2.5). For the other variables, loan to value ratio (LVR) is 80%, initial interest rate is 8%, indicator variables take the value one, and TERM = 30.

(d) Compute the marginal effect of CREDIT on the probability of delinquency for CREDIT = 500, 600, and 700, given that the other explanatory variables take the values in (c). Discuss the interpretation of the marginal effect.

(h) As a loan officer, you wish to provide loans to customers who repay on schedule and are not delinquent. Suppose you have available to you the first 500 observations in the data on which to base your loan decision on the second 500 applications (501‚Äì1,000). Is using the probit model, with a threshold of 0.5 for the predicted probability the best decision rule for deciding on loan applications? If not, what is a better rule?","","determining borrower and loan factors for lenders by R language","","11/05/2020 23:59:00","0"
"2507","934072","6000105","10/22/2020 11:03:47","## Task Details
Train language models using the data provided and compute their perplexity on the test data.

## Expected Submission
Submit a notebook detailing the model used and the training strategy. Then compute and present achieved perplexity on the test set.

## Evaluation
Evaluation is done on perplexity. As defined in [Wikipedia](https://en.wikipedia.org/wiki/Perplexity), perplexity is a measurement of how well a probability distribution or probability model predicts a sample. Lower perplexity means a better model.

### Baseline
Simple n-gram models using the [SRILM toolkit](http://www.speech.sri.com/projects/srilm/) achieved the following baseline perplexities:

* 2-gram --&gt; 252.49
* 5-gram --&gt; 191.60","","Train and test language models on Librispeech","","","1"
"2508","934072","6000105","10/22/2020 11:06:38","## Task Details
Train a model that can generate fake sentences realistic enough to be confused by real sentences from LibriSpeech

## Expected Submission
Submit a notebook detailing the model used and the training strategy. Then generate and present some fake sentences.

## Evaluation
Evaluation is subjective on how realistic the generated sentences look like","","Generate fake samples based on LibriSpeech","","","0"
"3804","1218285","6013830","03/18/2021 04:36:28","## Task Details
I have created this task so that you get an opportunity to work with different timezones using different libraries in python.

## Expected Submission
1. What I intend you to do is create columns for each country present in the 'SOURCE TIMEZONE' column.
2. Now, that you have resp columns for each country, you have to extract the time zone for each country in another column.
3. Finally, calculate the difference between timezones for each country.
e.g- Suppose the first country is kabul and its time zone is +4:30 
The other 89 countries in the columns will have different timezones such as -5:30
calculate the difference between these for each country.

## Evaluation
1. This problem as far as i know cannot be solved using regular PANDAS or NUMPY library explore other libraries.
2.Use object oriented approach.
3. The column name preferabaly should only contain 'Country' name extracted from the string.

### Further help
If you wish to explore the new libraries go ahead:-
https://pendulum.eustace.io/docs/#timezones","","Find the difference between one country in the column wrt other countries.","Exploring new libraries in python to work with different timezones.","04/01/2021 23:59:00","0"
"5297","1131181","4656934","07/23/2021 19:35:24","Predict whether Eyes are opened or closed.","","Eyes Prediction","","","0"
"2581","946036","6019935","10/30/2020 10:05:20","## Task Details
The task is to build a recommender system based on the list of movies. One way of doing it is by building a SAE (Sparse AutoEncoder), but there are many other ways too.

## Expected Submission
There should be a proper split for training and testing the system on this dataset and a loss should be compute (preferably for each epoch).

## Evaluation
A loss under 1.0 is considered optimal for an efficient system.","","Recommender System","","","1"
"3042","1056321","6042712","12/25/2020 23:33:24","Es posible determinar mediante la clasificaci√≥n de clades (variaciones filogen√©ticas en la estructura molecular de un virus o mutaciones) si determinado clade se comporta de forma diferente de otro?

Mediante el entrenamiento de un modelo de clasificaci√≥n, se intentar√° descubrir si cierto clade del virus SARS-CoV se replica con mayor velocidad. 

Expected Submission

- Notebook
- XML del modelo entrenado si se emplea alguna plataforma (Knime, Rapid miner, etc.)
- Dataset

Evaluation

Actualmente se ha detectado cierto clade que se replica m√°s r√°pidamente en el sur de UK. Define cu√°l es.","","Modelo de clasificaci√≥n","COVID clades","","0"
"3375","1131564","6042712","02/02/2021 06:14:37","## Tarea 1
* Grafica de serie temporal
* Medias de 

## Soluci√≥n deseada
Notebook y gr√°ficas

## Evaluation
What makes a good solution? 
I don't know","","Optimizaci√≥n de descenso","ADAGRAD, Nesterov","","0"
"3023","1054920","6042913","12/23/2020 19:15:47","## Task Details
The richness that the coastline presents did not go unnoticed from the eyes of man and brought about a heavy anthropogenic pressure over the coast ecosystem, which affects all the organisms that live in the water column and in the soil. Therefore there is an urgency regarding the identification and assessment of pollutants and their effect on different organisms and ecosystems in the coastal area.
The coastal soil presented us with a dynamic and challenging environment, in terms of adequate research methods and the subsequent interpretation of results.
We conducted during a year a comparison among the meiobenthic fauna in 3 coasts which represent a pollution gradient.


## Expected Submission
Users should submit a notebook that visualizes the data and try to predict the coastal pollution level between the 3 coasts. 
Alternatively, a code that achieves a low MSE can be submitted as well.

## Evaluation
Clear visualization of the data presented in addition to the lowest MSE possible. 

### Further help
For visualization you can try show (optional):
The connection between nema1/2, turb, or foram into pollution gradient
The connection between the time of year to the factors","","Visual data and classification of pollution level","","","1"
"4352","1303868","6054767","05/12/2021 20:33:16","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Given Weather Data you have to forecast the amount of GHI (Global Horizontal Irridiance ) that will be generated after 2 days.

It will help the solar plant operators to manage resources","","Forecast GHI for after 2 days","","","1"
"3279","1120594","6067505","01/26/2021 17:25:51","## Task Details
I want to see if it is possible to cluster the images by its shape without giving any label information.
## Expected Submission
Notebook submission 

## Evaluation
Get a sample of 100 images with it predicted label. You, I and anyone will see if it worked or not.
### Further help
The galaxy shape information is given in the Data Description. You can see more in the Hubble - de Vaucouleurs Galaxy Morphology Diagram. 
[Galaxy Morphological Classification](https://en.wikipedia.org/wiki/Galaxy_morphological_classification)","","Cluster Galaxy Shapes","","","0"
"2631","957445","6094706","11/05/2020 08:44:24","The idea for this dataset is to investigate how prices have been changing for:

 **-Old developments:** Evolution of prices over the years
**-New developments:** How expensive are those compared to old ones? Where are located? Are old ones bigger in comparison?
-Is there any new areas in Hong Kong becoming trendy compared to previous years?
-How long will take for the youth to purchase a flat in Hong Kong? How much has it changed over the years?

The notebook should provide enough information about the evolution of real estate in Hong Kong, not only in terms of prices and sizes, but also in terms of development areas. Including a view of purchase power of newer generations at the time to buy a flat would also help.","","Hong Kong real estate - Impossible dream for new generations?","","","2"
"2668","963565","6094706","11/10/2020 05:36:38","When playing rps, what is the most common action used by agents? (some strategies points at ""rock"" move as the strongest one for first round).

Is that true? Is there any difference between distribution on moves for first round between different tiers of competitors (top 20, top 100, top 200, ...)","","What is the most common action for first round?","First round action","","0"
"4597","1381959","6096594","06/01/2021 11:58:18","Perform an EDA on the 'Covid19 Latest Data-Kerala, India'","","EDA on Covid Data","","","1"
"4615","1384587","6096594","06/03/2021 02:02:08","Perform an EDA on the COVID-19 India Statewise Vaccine Data","","EDA on COVID-19 India Statewise Vaccine Data","","","5"
"4516","1372797","6096594","05/28/2021 10:19:43","Do a time series analysis on the daily Covid-19 confirmed cases, and predict the cases for next two weeks","","Time Series Analysis on Covid-19 Confirmed Cases Kerala","","","1"
"4875","1422721","6096594","06/21/2021 09:45:31","## Task Details

Perform Time Series Analysis on this dataset","","Time Series Analysis","","","0"
"5076","1459033","6096594","07/10/2021 11:04:40","## Task Details

Perform an exploratory data analysis on this data","","Exploratory Data Analysis","","","2"
"5040","1454737","6096594","07/08/2021 06:00:13","## Task Details
Perform EDA on this dataset and find the insights","","Exploratory Data Analysis","","","6"
"4791","1412046","6096594","06/16/2021 09:50:59","## Task Details
Perform an EDA on this dataset","","Exploratory Data Analysis","","","0"
"4778","1408598","6096594","06/15/2021 01:27:59","Perform EDA on this dataset","","EDA on 'Latest Worldwide Vaccine Data'","","","1"
"4833","1417039","6096594","06/18/2021 12:54:42","Perform time series analysis on this data","","Time series analysis","","","0"
"4645","1390187","6096594","06/05/2021 10:13:32","Perform an Exploratory Data Analysis on this dataset-'Latest Covid-19 India Statewise Data'","","EDA on 'Latest Covid-19 India Statewise Data'","","","48"
"5168","1390187","6096594","07/17/2021 17:07:26","Analyze the data","","Explore the data","","","29"
"4265","1315526","5966695","05/04/2021 09:07:50","You can do EDA and then build a model for identifying parking spaces  if they are  occupied or empty","","Object detection","","","0"
"3063","1062904","6111651","12/28/2020 14:17:29","## Task Details
Build a machine learning model to classify which calligrapher's style does a given image belong to.","","Calligraphy Style Classification","","","0"
"4386","993740","6112375","05/14/2021 13:26:37","## Task Details
Somehow I cannot show the animation of the sample directly in the notebook.
It just shows an empty or the first frame

## Expected Submission
If someone has an idea how to solve this, you could simply publish a notebook or request editing right for the one I uploaded. Maybe this needs to be solved deeper in the code than you can also take a look at https://github.com/adrianlubitz/VVAD/blob/main/vvadlrs3/sample.py#L353 and create a PR.","","Make visualization in Notebook work","","","0"
"2652","964028","6117091","11/09/2020 10:42:28","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6117091%2Fb6dd9df10b9dad7ddc06d936f1b8bdd6%2Fxss.svg?generation=1604937105225115&alt=media)","","Test12345678","Test","11/10/2020 23:59:00","3"
"2653","964028","6116865","11/09/2020 10:46:53","Test","","SKNG124","Test","11/10/2020 23:59:00","0"
"2654","964028","6116865","11/09/2020 10:47:47","Test","","SKNG1264","Test123","11/10/2020 23:59:00","0"
"2656","964028","6116865","11/09/2020 11:23:06","task details","","New task","","11/10/2020 23:59:00","0"
"2657","964028","6116865","11/09/2020 11:24:31","task details","","New task by SK","","11/10/2020 23:59:00","1"
"2911","964028","6117091","12/11/2020 20:29:52","Testing the tasks....Please ignore","","122312422","Test","12/13/2020 23:59:00","0"
"2655","964156","6116865","11/09/2020 11:02:35","Test","","SKNG12645789","Test123","11/10/2020 23:59:00","0"
"2646","961490","6118389","11/07/2020 16:56:57","Part 1. First 1000 COVID-19 cases in Singapore

Part 1 of the Excel data file provides selected patient information of the first 1000 confirmed
cases of COVID-19 in Singapore.
Answer all six questions below by analysing the data
For each question, state clearly the sample size used, the assumptions with justifications and
the potential flaws of the approach chosen (no approach is perfect).
If you find the question ambiguous, follow your own interpretation, then provide a
justification of your interpretation.

1. Plot a histogram showing the distribution of COVID-19 patients by age group, e.g. 0 ‚Äì 10
years, 11 ‚Äì 20 years, 21 ‚Äì 30 years, etc. What is the average age to contract the virus?


2. According to the data provided, is it true that males generally take longer to recover than
females? How conclusive is your answer?


3. According to the data provided, is it true that males are less likely to recover (i.e. die from
COVID-19), than females? How conclusive is your answer?



4. Estimate the mean number of days to recover with 95% confidence intervals.



5. According to the data provided, is it true that a Singaporean (nationality-wise) male is
more likely to contract COVID-19 than a Singaporean female? How conclusive is your
answer?


6. According to the data, is it true that a COVID-19 patient at or above the age of 50 takes
longer to recover than a patient who is younger than 50? How conclusive is your answer?","","covid research","recovery time","11/08/2020 23:59:00","0"
"4220","1301363","6118987","04/28/2021 19:20:24","## Task Details
Create visualizations as an exploratory data analysis of the dataset.","","Visualizations","","","0"
"5027","1451791","6120101","07/06/2021 18:35:06","## Task details
Predicting what weapon a person was injured with
## Expected shipment
Model with an accuracy of 80% or higher


## Evaluation
What constitutes a good solution? How do you evaluate which presentation is better than another?

Optimization and exploratory analysis of the dataset","","Weapon use prediction","","","1"
"2841","1004647","6124235","12/02/2020 17:25:16","This task question is:
Choose a car for resale","","Car Resales","","","0"
"3457","1131669","6127421","02/10/2021 08:04:36","## Goal
If you'd like to take a challenge, you can take on the task of predicting the popularity of articles, using `is_popular` as a target feature. 

Suggested task flow: exploratory data analysis --&gt; feature engineering --&gt; train and validate model using 'train.csv' --&gt; evaluate model using 'test.csv'.

## Metric
You can use whatever metric you'd like, but if you want a benchmark to beat, I used an XGBoost Classifier to achieve a ROC-AUC of **0.869** and accuracy of **0.786**.

## Futher Details
Try not to use `n_comments` apart from your initial exploratory data analysis.","","Predicting Article Popularity","A Binary Classification Task","","0"
"2912","1029771","6134942","12/11/2020 20:37:18","**Test123424242**","","SKNG_579","Test","12/13/2020 23:59:00","0"
"4648","1390907","6134942","06/05/2021 17:54:13","Testing","","Testing123","Testing","","0"
"5484","1390907","6121625","08/01/2021 17:40:12","## Task Details","","Test Task","Test","08/29/2021 23:59:00","0"
"2709","975746","6144392","11/16/2020 12:31:18","You can add data that is missing in this IPL dataset like longest sixes, most valuable player, match summary of every match played, etc.","","Task-1","","","0"
"2750","989855","6147881","11/22/2020 23:44:27","## Task Details

When I explored [when motorcycles crash](https://alexgude.com/blog/switrs-motorcycle-crashes-by-date/) I hypothotized that there were **two** types of riders:

- **Commuters**, who ride back and forth between work and most ride during the morning and evening commute on highways on practical, large motorcycles.
- **Recreational**, who ride for fun and are mostly out on the weekend, on sports bikes, and likely off of highways.

## Expected Submission

I would love to see a notebook that explores different makes and models and when they crash. The scientist should divide bikes into different categories either using something like Wikipedia's data or their own judgement to classify them as ""commuter bikes"" or ""sports bikes"" (and maybe ""other""). They should then look at if there is a statistically significant different in when and where crashes happen.

## Evaluation

A good solution should explore the motorcycle crash data, classify the different makes and models, and then look at when they crash and perhaps where. It should then use a statistical test to see if the different makes/models have different crash behavior.","","When do different makes and models of motorcycles crash?","Are sports bikes more likely to crash on the weekend?","","8"
"2936","989855","6147881","12/14/2020 06:09:18","## Task Details

When I first download the SWITRS data, I wanted to look at if crashes happened in different places during the weekday and the weekend. Maybe on weekdays most crashes would be on the highways whereas on weekends they'd happen on smaller, more fun to drive roads.

But unfortunately a lot of the locations are really noisy! Apparently the police sometimes round the numbers, as you can see in the plot off all the accidents below:

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6147881%2Fcb286ba665f7c3486c401f87c648e556%2F__results___8_0.png?generation=1607925715731890&alt=media)

There are even collisions in the middle of the ocean!

## Expected Submission

Can you put together a notebook that fixes this problem? Some points are too far gone to correct (like the crashes dozens of miles out in the Pacific ocean), but could other points be fixed by snapping to the nearest road? Or perhaps we can filter out points without enough precision?

## Evaluation

A good solution will contain both an algorithm (or multiple) to clean and filter the data. Ideally it would show a before and after for a zoomed in part of the map so we can clearly see that the crashes far from roads have been removed.

### Further help

I have an example of how to read the data and generate the above map [**in this starter notebook**](https://www.kaggle.com/alexgude/starter-california-traffic-collisions-from-switrs).","","Fix latitude and longitude data","The highway patrol oftens truncates locations, can you help fix it?","","3"
"5169","989855","6147881","07/17/2021 18:11:16","## Task Details

As states locked down in March 2020 due to COVID, people stopped driving to work and so the number of collisions decreased. You would expect this to reduce the fatality rate, but according to the [National Highway Traffic Safety Administration it did not](https://www.nhtsa.gov/press-releases/2020-fatality-data-show-increased-traffic-fatalities-during-pandemic):

&gt; While Americans drove less in 2020 due to the pandemic, NHTSA‚Äôs early estimates show that an estimated 38,680 people died in motor vehicle traffic crashes‚Äîthe largest projected number of fatalities since 2007. This represents an increase of about 7.2 percent as compared to the 36,096 fatalities reported in 2019.

Can we see this trend in the California data?

## Expected Submission

I would love to see notebooks that explore questions like:

- Did the fatality rate increase?
- Did the absolute number of fatalities increase?
- Does it differ by county?
- Can you see differences in the rate by type of cars? Do Corolla drivers drive safer than F150 drivers?

## Evaluation

A good solution should explore the data and use statistical tests to show the effect is real.","","Traffic Fatalities Post COVID Stay-at-home Order","","","1"
"4795","1414194","6153656","06/16/2021 21:25:12","I. You have to write a code that gets the information from 8 different csv files and summarizes the information in another csv file that is attached as a sample. The sample output has the information for only two players, but you need to summarize the whole information.
II.
After summarizing the data into one csv file, you will use that file to answer the following question:
a. Choose the best 5 teams with a great transfer:
You will rank the players that had a different team in two seasons according to their new season ranking. Choose first 5 players and print their new team‚Äôs name.
b. Print the name of 5 best players that promoted themselves; in other words, the players whose rankings are increased in comparison to the last season will be sorted and first 5 players will be chosen. (If a player was not playing last year, he will not be included on the list).
c. Repeat the steps in part b, and print the name of the players that have demoted themselves.
d. Consider two formations: 3.5.2 and 4.4.2. There is an image attached to this file explaining these formations. Note that if a player is labeled as Defender, he can play in any post in the defense line. Similarly, a Midfielder and Forward can play in any position in the middle and front line, respectively. You are supposed to choose players according to these formations
based on their average scores in both seasons.
1) The best teams. (You are supposed to print two lists of size 11 with the highest average
ranking for both seasons).
2) The most offensive teams. (You are supposed to print two lists of size 11 with players
that have scored more goals; if there are more than one players with the same amount
of goals, choose the player that has played fewer minutes than the other players)
3) The fair play teams. (You are supposed to print two lists of size 11 with players the least fouls; if there are more than one options, choose the player that has player more
minutes than the other players.)
4) The youngest teams. (You are supposed to print two lists of size 11 with the youngest
ages; if there are more one options, choose the player with the higher ranking).
e. Considering the teams of the year, choose the best player that was on the teams of the years, if
there are more than one, rank them according to the average of their ranking for both years.","","Missions","","","0"
"4077","1272936","6164625","04/15/2021 07:27:11","Check the sentiment for tweets","","Check Sentiment Analysis","","","0"
"4079","1272936","6164625","04/15/2021 10:29:41","Text Preprocessing/Cleaning/visualization","","Text clearning","","","0"
"3962","1250051","6165260","04/05/2021 04:41:00","## Task Details
Use the database to create model that can suggest next word for the input string","","Next word Prediction","","06/30/2021 23:59:00","1"
"4366","1334109","6165260","05/13/2021 14:51:07","Each image has its text on it. 
Build a model to read those texts.","","Read text from image","","11/19/2021 23:59:00","0"
"4390","1334447","6168551","05/14/2021 20:37:26","I have created a classification model with around 80% accuracy. Can you create a one for regression and predict actual % vote for remain and leave?","","Create Regression Model","Can you predict the %vote using regression","","0"
"3453","1144673","6168551","02/09/2021 22:40:32","## Task Details
As a fan of rugby I wanted to see if home advantage, historical results and word rugby rankings were a good predictor of success. 

## Expected Submission
Can you optimize the results to predict the scores for the 2021 championship and beyond?

## Evaluation
I have been using absolute mean error as a measure of success. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the results","","","0"
"4034","1148458","6168551","04/10/2021 13:14:11","## Task Details
Can you use this data to predict the winner of games or even the winning margin?","","Predict Game Margin or Home_win / Away_win","","","0"
"3026","1055760","6169692","12/24/2020 00:11:44","Use the marker, X, and Y data to classify the calls using ML. Have fun and enjoy :)","","Marker call classification [ML]","","","0"
"2894","975848","6172614","12/09/2020 12:16:40","## Task Details
LUFlow '20 receives periodic updates with new attack telemetry captured by diverse honeypots. This ensures LUFlow '20 always incorporates evolving attack patterns. As the modern threat landscape contains ever-changing attacks orchestrated against critical infrastructure, it is now more than ever necessary to capture these attacks to defend against them. 

This task requests the analysis of LUFlow to identify the extent to which the attacks captured evolve over time. Furthermore, we are requesting the evaluation of classification algorithms to understand how effective older training data can be used to classify new data. Is it necessary to re-train models on new data in order detect the newer attacks captured and incorporated within LUFlow '20?


## Expected Submission
User's should submit notebooks which use multiple dates, e.g. once a week, of captured telemetry to establish whether the attacks captured in LUFlow '20 are evolving. This should include time-series analysis of attack properties.

In order to evaluate the effectiveness of re-training models on new data, users should create models using older training data and attempt to classify old and new data. If the results are not accurate enough, attempt to re-train with new data to understand how recent the training data must be to detect the most emerging attack patterns.


## Evaluation
Is there significant differences between old and new attacks within LUFlow '20?

Are the differences substantial enough to warrant the re-training of models to detect newer attacks?

### Further help
If you need additional inspiration, check out a similar approach in research - https://www.sciencedirect.com/science/article/abs/pii/S0167739X18307635","","Attack Evolution","Analysis and classification of the evolving attacks within LUFlow","","1"
"2943","1035858","6178325","12/15/2020 03:01:50","Use both the form tables and the league tables to assess and compare the competitiveness of the leagues.","","Which soccer league is the most competitive?","","","0"
"2726","982382","6178325","11/19/2020 00:20:50","## What can you do with these datasets?

Here are some questions to explore using the datasets in this directory:
- How competitive is the Premier League? How do the rankings of the teams change over the years?
- Which teams are the most consistent in terms of performance?
- Which team(s) is/are the strongest in terms of defense/attacking?
- Which team pairing(s) is/are the most competitive (in terms of goals scored, goal difference, shots taken, fouls, etc.)
- Is there a clear home-team advantage?","","Exploring English Premier League Statistics","Competitiveness, performance consistency, team strengths and weaknesses, etc.","","0"
"2722","979765","6182134","11/18/2020 14:28:53","Integrate new attacks against MQTT","","Integrate new attacks","","","0"
"4085","1095741","6186871","04/16/2021 02:41:49","## Task Details
This NY restaurant inspections result data contain lots of interesting data. Here, see if you can come up with interesting visualizations of the inspection results. I believe there are many ways to interpret and visualize the data.

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
Creative and unique visualizations !","","Visualize New York City Restaurant Inspection results","","","0"
"4151","1288041","6186871","04/22/2021 09:47:09","## Task Details
There is a lot of data here for you to play with. Some examples can be time series analysis, correlation between age/sex to the disease, compare rate of conditions/diseases between countries etc.

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
Creative and unique visualizations !","","EDA on leading causes of DALYs and Deaths","","","0"
"3912","1242124","6187105","03/30/2021 19:44:18","## Task Details
Freely available demographic data reported by government agencies is often reported as counts within ranges (pre-binned). For instance, the IRS reports censored AGI data by zip code. While such data can provide general insights about a population, it can complicate the data scientist's ability to provide more granular insights.

In this task you are provided with censored salary data for employees of the California Community College system. Each campus reports the number of employees with salaries in specified ranges as well as the average salary of those employees, by employee category (see ReadMe and Directory descriptions for details). The top salary bracket is open ended which further complicates the problem.

Can you build a model that provides counts of employees by salary in $5000 intervals for each employee category each year? Use counts from all districts for this task as there is insufficient data to make individual models for most campuses.

## Expected Submission
Users should submit a notebook using the salary data provided by the California Community College Salaries dataset for the years 2001-2018. Feel free to use any freely available packages or tools. 

## Evaluation
A good submission will be a model which provides counts in $5000 intervals and reproduces the average salary reported. A great submission will be a model with smooth (i.e. does not directly replicate the steps in counts observed in the censored data) changes between $5000 brackets. An excellent submission will be a model that meets all of the previously mentioned goals and reproduces the original counts when re-binned according to the intervals of the censored data.

### Further help
If you need guidance on where to start, check out some of these links:
- http://freerangestats.info/blog/2019/08/25/fitting-bins
- https://stats.stackexchange.com/questions/354671/fitting-distributions-on-censored-data

Additionally, these papers describe methods that can be applied to solve this task:
- https://core.ac.uk/download/pdf/208957231.pdf
- https://arxiv.org/pdf/1910.02958.pdf
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4493979/","","Uncensoring Censored Salary Data from Community Colleges","Making granular insights from censored data","","0"
"3074","1066190","6193310","12/29/2020 20:38:27","## Task Details

The last 2 decade in Brazil the High Education became possible and accessible for many people. The aim of this task is try to identify if:

1) Is higher education in Brazil improving?
2) Where Brazil have the best number in education? per √°rea and also per courses.
3) In relation to the past generation, is higher education improving?
4) In what modalities of higher education is Brazil improving?
5) Which courses in which college are best rated?

## Expected Submission
This is just for curiosity and maybe can help universities in Brazil to identify opportunities to new offerings and improving.

## Evaluation
I believe that good findings in this dataset such as trends and also relations could be very intersting approach.","","Is the High Education in Brazil Improving?","","","0"
"2834","1010308","6205192","12/01/2020 19:43:08","**Welcome to MBTI Personality Classification AI Challenge!**

The aim of this challenge is to design a model to predict the Myers-Briggs Personality Type (MBTI) based on the given post data.

The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:

    1. Introversion (I) ‚Äì Extroversion (E)
    2.ntuition (N) ‚Äì Sensing (S)
    3.Thinking (T) ‚Äì Feeling (F)
    4.Judging (J) ‚Äì Perceiving (P)

Leading to total 16 classes.

The dataset consists of 3 files:

    1.TRAIN.csv (training data)
    2.TEST.csv (test data for prediction)
    3.sample_submission.csv (sample submission format)


The ""TRAIN.csv"" consists of the following attributes:

   1. idx
    2.type (Target attribute)
    3.posts

The ""TEST.csv"" consists of the data to be predicted and has the following attributes:

    1.idx
    2.posts","","MBTI Personality Classification AI Challenge","","12/12/2020 23:59:00","0"
"2744","988856","6210861","11/22/2020 06:42:20","## Task Details
1. Create a notebook in which you will load the dataset and perform common EDA tasks (you choose which ones, Higher scores awarded for more details and techniques)
2. Provide your detailed analysis of the data, your observations and any additional inference you can obtain from the dataset.
 
*Bonus points for derived data and associated analysis.","","Question #1 - Exploratory Data Analysis","Difficulty Level - Beginner","","2"
"2745","988856","6210861","11/22/2020 08:15:46","## Task Details
1. Create a notebook in which you will load the dataset and build a model that predicts the outcome of a match based on Toss and Venue of the Match.

2. Split the provided data into a training dataset with 70% of the data and a testing dataset with the remaining 30% of the data. 

3. You are free to use any algorithm or libraries to solve this (you can also use more than one and select the best)

4. Validate the accuracy of your model using any of the standard techniques.","","Question #2 - Simple Prediction Model","Difficulty Level - Beginner","","2"
"2802","988856","6210861","11/27/2020 16:50:01","1. Load the given data set and show as many meaningful, non obvious inferences that you can make with this data You need to make up your own questions here and try to answer that question with insights from the data provided.

 Some examples of the kid of insights you can come up :
- Which RCB batsmen would you choose to send at #4 when RCB is batting at home and is chasing a target &gt;160 with 3 wickets down at the half way mark of the innings?

- Would it be better for MI to field an extra left arm pacer or an extra spinner when playing CSK at Chennai ? 

The more challenging the question and more meaningful the insights the better the points. You can submit any number of insights within a range of 3 minimum insights and a maximum of 10 insights.","","Question #3 - Data Insignts","","","1"
"2757","992214","6216818","11/23/2020 18:11:58","Evaluation criteria: r^2
Evaluation criteria: residuals skewness

Build a model with the best r^2, but pay attention to the residuals chart. Since the data is unbalanced, it's a challenge to make the the prediction homoscedastic, i.e. have a random distribution of residuals along the price axis.","","Build an apartment price prediction model","But beware that the data is unbalanced!","","0"
"2758","992214","6216818","11/23/2020 18:18:13","Professional house flippers use a sort of k-nearest neighbors algorithm, picking a set of closest and similar listings to the one being evaluated, and then finding an average (if you are optimistic) or a minimum (if you are pessimistic) price, while adjusting for differences in the parameters of the listings selected for valuation.

See if you can emulate this approach by using a k-nearest neighbours algorithm!","","Predict the price using k-nearest neighbors regressor","Like the pros do!","","0"
"3625","1186801","6221254","03/01/2021 10:44:08","What you can find? Enjoy!","","Find Anything!","","","1"
"2845","991690","6224505","12/03/2020 08:59:53","## Task Details
Our current analysis pipeline does a pretty good job in classifying this brain data.   But, we know that it can be done better -- and if it can be done better then users (including patients) experience using the BCI will also improve.

## Expected Submission
You should submit a new notebook which when run with the multi-dataset analysis produces a final cross-dataset performance at least 10% better than with our default analysis.

## Evaluation
The core performance metric is the final cross-dataset AUDC performance.  Secondary metrics include computational resources used.","","Improve EEG-cVEP classification performance","","","1"
"2833","1009390","6226122","12/01/2020 16:51:18","When is the best time of day/day of week/time of year to fly to minimise delays?
Do older planes suffer more delays?
How does the number of people flying between different locations change over time?
How well does weather predict plane delays?
Can you detect cascading failures as delays in one airport create delays in others? Are there critical links in the system?","","find out why flights were delayed","do some data analysis and data visualization","","0"
"3002","1030442","6232890","12/21/2020 17:35:59","## Task Details
Max Strength hero, max agi hero and max int hero.

## Expected Submission
A resulting dataframe or a list","","Highest stat hero for each role","","","0"
"3003","1030442","6232890","12/21/2020 17:37:50","## Task Details
Least Strength hero, least agi hero and least int hero.

## Expected Submission
A resulting dataframe or a list","","Worst stat hero for each role","","","0"
"3004","1030442","6232890","12/21/2020 17:45:36","## Task Details
For each role, hero with best strength, int, agi at lvl 30.

## Expected Submission
Example-

Carries
Strength: Axe
Agi: AM
Int: CM

Support
Strength: Snapfire
Agi: Venge
Int: CM

### Further help
Strength data format - 23 + 1.3. base str = 23, gain/level = 1.3","","Best hero for each role - highest stat gain.","","","0"
"3005","1030442","6232890","12/21/2020 17:59:00","## Task Details
best and worst movement speed.

## Expected Submission
list or df","","Fastest & slowest hero for each role","","","1"
"4422","1346846","6244300","05/17/2021 16:30:34","Make a model using using any technology to predict the price of stock in future","","Make a model to predict Stock Price","","","1"
"2825","999187","3606121","11/30/2020 15:02:42","## Task Details
A company's market values are variable and affected by different factors. The most important factor could be the thoughts of the public. In this task, you are responsible to analyze the effect of public opinion about a company on that company's market values.

## Expected Submission
We are expecting a correlation rate that clarifies the public opinion vs market values. You can intuitively use a creative method to evaluate public opinion, but we prefer to use sentiment analysis. You can get help from different datasets or notebooks. But we leave a dataset here to reach the companies' market values.
https://www.kaggle.com/omermetinn/values-of-top-nasdaq-copanies-from-2010-to-2020

## Evaluation
Inherently, the solutions giving better correlation rates are better solutions. But we are expecting good reasoning and clarifications for your assumptions and solution.","","Are the tweets are really effective?","","","227"
"2826","999187","3606121","11/30/2020 15:45:36","## Task Details
This data set consists of the tweets about the top 6 NASDAQ companies. Every week, we read articles and news about their investments, projects, services and etc. As we experience new things about these companies, our thoughts change and we share them on social media. As a data scientist, you are responsible to visualize the public opinion of a company day by day. And explaining sudden declines and rises in this graph.


## Expected Submission
We expect you to submit a graph with comments that clarify the rapid changes. 
You do not need to reason for every decline or rise. The three biggest declines and rises are enough for this task.
For example, after the release of news that states Google has an attitude against the principle of data privacy, public opinion has changed to negative as 12.5% on January 1, 2017.

## Evaluation
Here is the key element is sentiment analysis. The sentiment analysis has been used for a long time. Used dictionaries vary from sector to sector. You should pick a good dictionary that covers the social life of people. In other words, pick a dictionary with the content of social life rather than targeting a special group or sector, such as finance. Furthermore, manual dictionaries are preferred.","","When do we suddenly love or hate companies?","","","5"
"2899","999187","3606121","12/09/2020 17:05:29","## Task Details
Social media is full of trolls and fake accounts. These accounts cause several vital problems affecting people or organizations. For example, a group of fake accounts coming together can suspend a post or keep a tag in the trend list. In this task, you will be responsible to find the accounts that permanently spam tweets to occupy the trend list on Twitter. 

## Expected Submission
You should submit a writer list with the label of 1 or 0 (1 represents troll, 0 represents not troll) with your approach.  


## Evaluation
Evaluating troll users is a questionable topic. So we will analyze and discuss the approach you proposed and if your approach is valid then we will use the accuracy to understand which solution is better than the other.","","Buddy, are you Troll?","Evaluating troll users trying to occupy the social agenda.","","5"
"3177","1094733","6249203","01/13/2021 06:59:21","In given datasets, wikipedia.html contains data of metropolitan regions and associated sports teams. Each of these regions may have one or more teams from the ""Big 4"": NFL (football, in nfl.csv), MLB (baseball, in mlb.csv), NBA (basketball, in nba.csv or NHL (hockey, in nhl.csv). The task is from the perspective of the metropolitan region, and that this file is the ""source of authority"" for the location of a given sports team. Thus teams which are commonly known by a different area (e.g. ""Oakland Raiders"") need to be mapped into the metropolitan region given (e.g. San Francisco Bay Area). This will require some human data understanding outside of the data given (e.g. you will have to hand-code some names, and might need to google to find out where teams are)!


Task:

To check the hypothesis that given that an area has two sports teams in different sports, those teams will perform the same within their respective sports. Use a series of paired t-tests (so use ttest_rel) between all pairs of sports. Are there any sports where we can reject the null hypothesis? Again, average values where a sport has multiple teams in one region. Remember, you will only be including, for each sport, cities which have teams engaged in that sport, drop others as appropriate. Use data from year 2018 only for your analysis. The performance of the team can be determined by the Win to Loss ratio, so that needs to be computed and suitable statistical analysis should be performed.

Note:
Do not include data about the MLS or CFL in any of the work you are doing, we're only interested in the Big 4 in this task.","","Hypothesis (Paired T-tests)","","","0"
"3550","1171471","6252455","02/20/2021 17:52:37","**Background**
Consider you are a Data Analyst with a private bank or a loan distribution firm. Your organization receives many applications in a given day. In order to process the applications, you sometimes miss out on accepting applications from people who are able to pay loans in time and end up sanctioning loans to those who later turn out to be defaulters.

**Datasets**
You are now provided with two datasets:

1. Current_app: This file gives you information on the existing loan applications. Whether or not clients have payment difficulties
2. Previous_app: This file contains information on the previous loan applications with status details of the previous applications being Approved, Cancelled, Refused or Unused offer.

Exploratory Data Analysis is really fun! You get to select how to approach the problem with the defined objectives. Here in this analysis, you are required to identify the loan application patterns and recommend the bank/firm on how they can build their loan portfolios and avoid giving loans to defaulters.

You have to recommend ways in which the bank/firm can maximize their loan sanction applications to the clients who can repay the installments. This can be really tricky since there could be new clients with no credit history and can take advantage of the bank and turn out to be defaulters in the future.","","Credit Analysis","","","0"
"4302","1323536","6257315","05/08/2021 05:45:13","Perform a detailed exploratory data analysis on the dataset.","","Perform EDA","","","0"
"3692","1189263","6267609","03/08/2021 15:13:53","## Task Details
This was what the data was creates for in my Bachelor's thesis.","","How do athletes from different sports differ in their independence/interdependence?","","","0"
"2822","1007466","6279457","11/30/2020 12:30:54","https://www.kaggle.com/c/denoising-dirty-documents/data","","Denoising Dirty Documents","Remove noise from printed text","","0"
"2847","1014128","6285891","12/03/2020 20:23:30","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6285891%2F5c5059326a714d0bb09892892fb085d3%2F..pdf?generation=1607027002768729&alt=media)
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","https://stihi.ru/avtor/bayashka2016","https://stihi.ru/avtor/bayashka2016","","0"
"3964","1251874","6286972","04/05/2021 07:41:17","## Task Details

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","find the accuracy of dataset using any ml model technique","","","0"
"3649","1190256","6287626","03/03/2021 08:19:28","DESCRIPTION

Background of Problem Statement :

The US Census Bureau has published California Census Data which has 10 types of metrics such as the population, median income, median housing price, and so on for each block group in California. The dataset also serves as an input for project scoping and tries to specify the functional and nonfunctional requirements for it.

Problem Objective :

The project aims at building a model of housing prices to predict median house values in California using the provided dataset. This model should learn from the data and be able to predict the median housing price in any district, given all the other metrics.

Districts or block groups are the smallest geographical units for which the US Census Bureau
publishes sample data (a block group typically has a population of 600 to 3,000 people). There are 20,640 districts in the project dataset.

Domain: Finance and Housing

Analysis Tasks to be performed:

1. Build a model of housing prices to predict median house values in California using the provided dataset.

2. Train the model to learn from the data to predict the median housing price in any district, given all the other metrics.

3. Predict housing prices based on median_income and plot the regression chart for it.

1. Load the data :

Read the ‚Äúhousing.csv‚Äù file from the folder into the program.
Print first few rows of this data.
Extract input (X) and output (Y) data from the dataset.
2. Handle missing values :

Fill the missing values with the mean of the respective column.
3. Encode categorical data :

Convert categorical column in the dataset to numerical data.
4. Split the dataset : 

Split the data into 80% training dataset and 20% test dataset.
5. Standardize data :

Standardize training and test datasets.
6. Perform Linear Regression : 

Perform Linear Regression on training data.
Predict output for test dataset using the fitted model.
Print root mean squared error (RMSE) from Linear Regression.
            [ HINT: Import mean_squared_error from sklearn.metrics ]

7. Bonus exercise: Perform Linear Regression with one independent variable :

Extract just the median_income column from the independent variables (from X_train and X_test).
Perform Linear Regression to predict housing values based on median_income.
Predict output for test dataset using the fitted model.
Plot the fitted model for training data as well as for test data to check if the fitted model satisfies the test data.
Dataset Description :

Field	Description
longitude	(signed numeric - float) : Longitude value for the block in California, USA
latitude	(numeric - float ) : Latitude value for the block in California, USA
housing_median_age	(numeric - int ) : Median age of the house in the block
total_rooms	(numeric - int ) : Count of the total number of rooms (excluding bedrooms) in all houses in the block
total_bedrooms	(numeric - float ) : Count of the total number of bedrooms in all houses in the block
population	(numeric - int ) : Count of the total number of population in the block
households	(numeric - int ) : Count of the total number of households in the block
median_income	(numeric - float ) : Median of the total household income of all the houses in the block
ocean_proximity	(numeric - categorical ) : Type of the landscape of the block [ Unique Values : 'NEAR BAY', '&lt;1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'  ]
median_house_value	(numeric - int ) : Median of the household prices of all the houses in the block","","California Housing Price Prediction Project","","","3"
"3937","1241132","6288076","04/02/2021 08:06:40","## Task Details üî• 
Lux is recently trending  among data analysts which has features like automated exploratory analysis. To get into hands on Lux Library, here is your Task

##Dataset
[Click Here](https://www.kaggle.com/qwqextra/suicidalpost)

## Expected Submission üí• 
Your are expected to clean and plot various exploratory analysis on text data by using Lux

## Evaluationüëà 
‚úîÔ∏è Detailed explanation and description about Lux
‚úîÔ∏è Explanation of various plots ad usage
‚úîÔ∏è Overall, a good looking and beautiful Notebook which can serve as documentation/example for Lux library is excepted as outcomeüíï .

### Further helpüôå 
#### If you need additional inspiration, check out these existing high-quality articles:
- https://towardsdatascience.com/intelligent-visual-data-discovery-with-lux-a-python-library-dc36a5742b2fdataset/tasks?taskId=508 
#### PIP installation 
- https://pypi.org/project/lux-api/

#And Don't forget to ENJOYüòÉ","","Text Analysis using Lux #Python #Lux","Hands on Lux - Exploratory Data Analysis","04/30/2021 23:59:00","0"
"3938","1241132","6288076","04/02/2021 08:42:20","##Purpose and Usage‚ú® 
Suicidal cases are rising worldwide at an alarming rate and it has become a third most causes for deaths. The rapid adaptability of Electronic devices and Social media directly influences or act as a platform for people who want to explore or express about their depressive or Suicidal state or related query.
Here, we collected a large set of data on Suicidal data and Non Suicidal data which we feel may help us all in differentiate between normal and Suicidal data.

## Task Detailsüî• 
Your task is to do various all possible Data analysis on these textual data and ### Tell us what your found through the analysis

##Dataset
[Click Here](https://www.kaggle.com/qwqextra/suicidalpost)

## Expected Submissionüëà 
üí• A solid notebook which shows 
üí• Explanation about attributes
üí• Detailed Preprocessing
üí• Various plot 
&gt; Note: 
     ‚úîÔ∏è Should Categories all plots into relevant sections with description about plot 
         category describing about the purpose of plot.
    ‚úîÔ∏è Short description about findings for each plot at bottom.

üí• A detailed description (should be located at last) or summary which tells a story about the overall analysis and their uncovered truth.

## Format‚úã 
üëâ Title and Description about Task
üëâ Table of Content
üëâ Various Plot - Title, Short Description, Plot, Findings or Summary
üëâ Finally: A DETAILED STORY OF YOUR WORK
üëâ Link for your profile for LIKEüëç 

## Evaluationüëç 
‚úîÔ∏è Excepted a Good Looking Notebook.  (10 Points)
‚úîÔ∏è Large number of Different Plots. (40 Points)
‚úîÔ∏è EXTRA points if you use rarely used plot types. (20 Points)
‚úîÔ∏è Perfect detailed description for each plot. (10  Points)
‚úîÔ∏è And A Best Looking STORY which make us WOWüò≤. (20 Points)

### Further helpüôå 
Help full Links and Tips
https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/
Widgets and More
https://towardsdatascience.com/bringing-the-best-out-of-jupyter-notebooks-for-data-science-f0871519ca29
Interactive Visualization And Plotly
https://towardsdatascience.com/interactive-visualizations-in-jupyter-notebook-3be02ab2b8cd
Additional Links
- https://www.dataquest.io/blog/jupyter-notebook-tutorial/
- https://towardsdatascience.com/jupyter-notebook-best-practices-f430a6ba8c69

# Do the Most Beautiful Work EVERüíï","","Suicidal Analysis #Python #DataAnalysis","Data Exploration On Textual Data","05/31/2021 23:59:00","1"
"4245","1310720","6288608","05/02/2021 10:03:57","The task

You need to develop an algorithm for classifying images of Russian road signs according to the classes specified in the traffic rules.

The algorithm must be presented in the form of a product, into which it will be possible to upload a photograph of a road sign and obtain information on what class it belongs to.
Please note that some signs (for example, 3.24 - maximum speed limit) have separate attributes, such as numbers on the sign indicating the maximum speed limit. For such characters, in addition to the class, these attributes must be defined.","","authors task","","","0"
"3724","1166617","6307910","03/10/2021 15:36:46","## Task Details
The data represents real USA retail sales ,the data set was created with the pur[oses of exploring the Retail market and deploy some Time series analysis,Clustering as well as future Sales prediction.

## Expected Submission
 Tasks can be solved using Notebooks or Datasets. 
the solution should contain the MSE of the Model

## Evaluation
What makes a good solution? The lowest MSE is the best model
### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Retail sales","USA real retail sales & index","05/10/2021 23:59:00","0"
"3725","1203526","6307910","03/10/2021 15:52:34","## Task Details
This is a Bigginer Data set with less than a 100 observations.A mini survey to try and collect data on the demand of medical deliveries as well as the average Delivery Price. Data cleaning as well as Time series analysis of Number of People .Data was collected in a 3 day period.
## Expected Submission
What should users submit?  Notebooks or Datasets. Predicted Delivery Price as well as Medical Cover or Not.

## Evaluation
RMSE ,the lowest  RMSE.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)","","Medrush Survey","Online survey of Potentential Medical deliveries to private citizens","08/10/2021 23:59:00","0"
"2888","1022714","6316844","12/08/2020 12:42:36","## Task Details
Creating the most beautiful data visualization.

## Expected Submission
**What should users submit? **
- Their Notebook

**What must the submission contain?**
- COVID19 Geospatial visualization output
- Graphs and other visualization of your choice
- Your own specific country's data must be used

## Evaluation
- Best designed graphs and geospatial data
- Data prediction is a plus
- Precision data is a plus

### Further help
If you need additional inspiration email me @ ramdevcalope1995@gmail.com","","COVID19 Tracking Data Design Contest","Design the most beautiful COVID19 visualization","02/19/2021 23:59:00","0"
"3968","1071543","6322339","04/05/2021 09:20:18","Pending work in the dataset is kept in the Issues section of `player-scores`. Kaggle dataset improvements are tagged with [`dataset-improvements`](https://github.com/dcaribou/player-scores/issues?q=is%3Aissue+is%3Aopen+label%3A%22dataset+improvements%22)","","Dataset Improvements","","","0"
"4018","1155465","6338326","04/09/2021 07:45:51","Try to explore trends and discover where profits are coming from.","","Do Analysis for this sales Excel","","","0"
"3621","1186558","6340858","03/01/2021 07:53:49","What was the change in price of the stock over time?","","Analysis the price of the stock over time","","","0"
"3622","1186558","6340858","03/01/2021 07:56:30",".","","Find Hidden Patterns","","","0"
"3623","1186558","6340858","03/01/2021 07:57:19",".","","Predict future stock behavior","","","1"
"4510","1367816","6347974","05/27/2021 10:51:36","Using the columns provided in the dataset, try and predict what the prices will look like and compare it to the values provided. It will be good practice for people starting out in machine learning! 

A step to doing this can be figuring out what columns are important to the prediction of prices and what are not. Performing EDA can be a critical step to that end.","","Perform EDA and Predict Electricity Prices (Column 'SMPEP2' in the dataset)","","","1"
"3259","1114104","6348108","01/23/2021 13:30:20","## Task Details
Use various tools and techniques to perform sentiment analysis and evaluate the polarity, subjectivity and other sentiment measurement metrics.

## Expected Submission
You are expected to submit a Python Notebook or an R Markdown report, including the following:

Data analysis;
Method for sentiment analysis;
Results;
Comments and conclusions.","","Covishield Sentiment Analysis","Perform sentiment analysis on tweets","","2"
"3232","1109013","6348108","01/20/2021 20:06:57","Task Details
Suggest best movies of a various genres","","Data Analysis of movies and movie recommender","","","0"
"3434","1145569","6348108","02/08/2021 09:57:29","## Task Details
Recommend players that a club should sign, help find good replacement for players and suggest the best formation for any club","","Scouting players and suggesting best formation","","","0"
"3287","1122258","6348987","01/27/2021 13:07:06","## Content
Information about Russian airlines that have an operator's certificate for commercial air transportation. Taken from the official website of the Federal Air Transport Agency (Ministry of Transport of the Russian Federation), translated and geocoded by me. Information provided as of 31.12.2020","","Russian Airlines 2021","","","0"
"4816","1415682","6360901","06/17/2021 16:11:03","Attached Bank Nifty time series data of 5 min.
- Backtest using Python.
- Convert data into 15 min time frame.
  
- Intraday trades are to be taken and mandatorily closed in the same day.
- Trade starts only after 2nd 15-minute candle (9:30 to 9:45)
  	
- If candle closes above VWAP, we will buy and if candle closes below VWAP, we will sell
- Example: If 2nd candle is closing above VWAP, we will buy 1 tick above candle high and Stop Loss (SL) will be 1 tick below candle low.
  
- If stop loss is not hit, then all trades are exited at 3:15 PM.


- Calculate the overall profit and loss. 
- Segregate the Profit and Loss Year wise and Day wise
- Suggest best performing Year.","","Complete the given tasks","","","0"
"2908","1029005","6363672","12/11/2020 12:26:58","Are you looking for a **[Therapist and Counsellor in Auckland](https://www.wendymatthews.co.nz)** due to feeling of anxiety, stress and depression? Book your sessions with Wendy Matthew and get rid of that constant feeling of sadness and restlessness that makes it difficult for you to get out of bed in the morning.","","Therapist and Counsellor in Auckland","","","0"
"4982","1445417","6368206","07/03/2021 14:59:56","Perform Exploratory data analysis","","Perform EDA","","","0"
"4874","1422623","6368833","06/21/2021 09:23:09","Perform Exploratory Data Analysis on this Data","","Perform EDA","","","1"
"4835","1417901","6368833","06/18/2021 15:46:24","Use the dataset to analyse the impact of COVID-19 on mental health of migrant workers in India","","Analysis of the Data","","","0"
"4843","1418117","6368833","06/19/2021 08:20:39","Perform Exploratory Data Analysis","","Perform EDA","","","0"
"4864","1421332","6368833","06/20/2021 16:44:54","Perform Exploratory Data Analysis on this data","","Perform EDA","","","0"
"4860","1420507","6374616","06/20/2021 15:38:35","Perform an EDA on this dataset containing statewise details of covid-19 till 20th June 2021 in India .","","Exploratory Data Analysis","","","0"
"4879","1423618","6374616","06/21/2021 17:35:12","Perform an EDA on this dataset.","","Exploratory Data Analysis","","","0"
"7013","1424917","6374616","12/04/2021 18:09:01","Conduct an EDA to find insights from the dataset.","","Exploratory Data Analysis","","","0"
"4846","1419432","6374656","06/19/2021 14:46:08","# Perform EDA on the vote share of political parties indicating their growth","","EDA OF POLITICAL PARTIES","","","2"
"4817","1415796","6374656","06/17/2021 16:34:38","## Task Details
perform EDA and submit data visualizations 

## Expected Submission
Good notebooks of performing EDA","","Perform EDA","Perform exploratory data analysis on this data set","","0"
"4851","1420412","6374959","06/20/2021 07:31:07","## Task Details
EDA of reviews and its relation with other features.","","Exploratory Data Analysis (EDA)","","","1"
"4852","1420412","6374959","06/20/2021 07:33:30","## Task Details
Sentiment analysis of user.","","Performing Sentiment Analysis and analyzing behavior of reviewer.","","","1"
"3680","1192565","6376555","03/07/2021 02:38:22","We use the Computer Vision algorithm to stitching multiple images of a scene","","Image stitching (Panorama Maker)","by using openCV","","1"
"3078","1068264","6379711","12/30/2020 20:51:16","This is a bonus task. Since the dataset can be generalized, the requirement of this task is to create an end-to-end application which will take the data as input and the insights and predictions will be displayed on a dashboard. The dashboard needs to be deployed on a web server and the link to be shared as a part of the submission of this task.","","Create an application","","","0"
"3389","1130261","6384313","02/03/2021 11:28:21","For example, you are the owner of an oil company!üòé 
 And you have to decide which wells are profitable for you and which wells bring a loss for the company.
If during the next three years oil production is more than three m3 per day, then the well is profitable, if less than three m3 per day, then the well is unprofitable for the company and it should be stopped.","","Is an oil well profitable or unprofitable for the next three years?","Try to predict oil and gas production by well for the next three years.","","1"
"3390","1130261","6384313","02/03/2021 11:33:47","For example, you are the owner of an oil company! üòé
And you have to determine from what date the well will start generating losses for the company.
If oil production in a well is more than three cubic meters per day, then the well is profitable, if less than three cubic meters per day, then the well is unprofitable for the company and should be stopped.","","When will the well become unprofitable?","","","1"
"4413","1345656","6384494","05/17/2021 08:46:24","## Task Details
Recognize 8 digit  wagon numbers from live IP camera...

## Expected Submission

Inputs is IP camera or video source or image 

### Further help
It should work like https://www.dtksoft.com/wnrsdk","","AI Wagon number recognition","","05/21/2021 23:59:00","0"
"4866","1144127","6385902","06/20/2021 22:16:34","I uploaded the relationship between GDPR and Personal Data dataset by collecting Google Search Trends. It was my first dataset on Kaggle. I kind of struggle with how to do it. So, the result is not on the expected level. Because of my knowledge of Kaggle, and also most importantly R programming and Statistics. 

I couldn't find a common ground GDPR related search results. The first reason is, on GPDR's website [key issues](https://gdpr-info.eu/issues/) ordered 12 keywords.  The second reason is; I thought that I could find a relationship between those keywords if I look for their searching results. Because, in my mind, I thought people would search those keywords as soon as they read GDPR's articles. My thought process might be biased. Another because the countries in the EU are well-regulated, and people constantly update their knowledge on the new regulations. Little did I know, the world does not run like that :) People think differently! We approach things differently. That makes us unique!

With that being said, I am looking for a task that contains clustered data about [the key issues](https://gdpr-info.eu/issues/) in GDPR. My expected submission could be a notebook, code, or dataset. The decision will be yours only. On the evaluation part, I will be evaluated the submissions on the basis of K-means Clustering, Hierarchical Clustering, Principal Components Analysis, and the other methods that contain clustering algorithms.

Thank you in advance!","","Clustering of GDPR and Personal Data Search Results","","","0"
"4541","1374207","6396210","05/29/2021 03:27:39","## Task Details

Every task has a story. In this Dataset you have to predict the future values or cases using time Series Analysis. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analysis Dataset using Time Series","Can also use Machine Learning Algorithms","","0"
"4551","1376362","6396210","05/30/2021 03:32:54","## Task Details
Every task has a story. This is a very basic and simple dataset for practice purpose only. 

## Expected Submission
Submit your detailed insights from the dataset.","","Get Insights from the Sample Data","","","1"
"4166","1240774","6398333","04/24/2021 02:01:14","Task Details
1. You have to model a LSTM -RNN model.

2. Analyse the dataset to analyse what could be a proper prediction.

3. Describe how it is useful in recognizing gene and variants.

 Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create a RNN LSTM model to find prediction between gene and variants","","05/08/2021 23:59:00","0"
"2995","1047965","6399429","12/21/2020 02:32:05","The class blackboard has a pdf file of a paper by Caspi et al. that reports a finding of a gene-environment interaction. This paper used multiple regression techniques as the methodology for its findings. You should read it for background, as it is the genesis of the models that you will be given. The data that you are analyzing is synthetic. That is, the TA used a model to generate the data. Your task is to find the model that the TA used for your data. For example, one possible model is  
 . 
Y = u + E1:E2 + E3:G4 + N(0,sigma^2)

The class blackboard also contains a paper by Risch et al. that uses a larger collection of data to assess the findings in Caspi et al. These researchers confirmed that Caspi et al. calculated their results correctly but that no other dataset had the relation reported in Caspi et al. That is, Caspi et al. seem to have reported a false positive (Type I error). The class blackboard contains a recent paper about the genetics of mental illness and a technical appendix giving the specifics. Together these papers are an example of the response of the research community to studying the genetics of mental illness, which is a notoriously difficult research area. 

Guidelines for analysis

The first task for this problem is to use the statistical package of your choice to find the correlations between the independent variables and the dependent variable. Transformations of variables may be necessary. The Box-Cox transformation may find potentially nonlinear transformations of a dependent variable. After selecting the transformations of the dependent variable, use model building methods such as stepwise regression to select the important independent variables. The TA will use at most four-way interactions of the independent variables (that is, terms like or E1:E2:G1:G2 ) in generating your data. There may also be non-linear environmental variables, such as  E1^2 or E4^0.5 . 

 
Hints

Remember to consider multiple testing issues. The p-value for the variables that you select should be much smaller than 0.01. Remember that you have 6 environmental variables, 20 genes, 120 gene-environment variables, 190 gene-gene interaction variables, and so on.","","Multiple Regression Analysis","","","4"
"3502","1112253","6402589","02/15/2021 00:40:54","## Task Details
Seek similarities in dataset","","Seek similarities in dataset","","","0"
"3244","1111894","6402661","01/22/2021 08:42:46","## Task Details
Our top priority in this business problem is to identify companies in bankruptcy.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) Row number
2) Bankruptcy?

## Evaluation
Evaluation using F1-Score. 
The F1-Score is defines as the harmonic mean between precision and recall:![](https://datascience103579984.files.wordpress.com/2019/04/capture4-17.png)

### Further help
Liang, D., Lu, C.-C., Tsai, C.-F., and Shih, G.-A. (2016) Financial Ratios and Corporate Governance Indicators in Bankruptcy Prediction: A Comprehensive Study. European Journal of Operational Research, vol. 252, no. 2, pp. 561-572.","","Predict Company Bankruptcy","Classificastion task","","16"
"3281","1120859","6402661","01/26/2021 20:03:32","## Task Details
Our top priority in this health problem is to identify patients with a stroke.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) id
2) Stroke

## Evaluation
Evaluation using F1-Score (given the output class imbalance)

### Acknowledgements
If you use this dataset in your research, please credit the author.","","Predict Stroke","Classificastion task","","94"
"5290","1120859","4656934","07/23/2021 09:58:30","Insights of Stroke Prediction Dataset","","EDA on Stroke Prediction Dataset","","","5"
"4888","1120859","5090441","06/22/2021 21:13:13","## Task Details

The goal is to predict Water Quality.

##Attribute Information
1) id: unique identifier
2) gender: ""Male"", ""Female"" or ""Other""
3) age: age of the patient
4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
6) ever_married: ""No"" or ""Yes""
7) work_type: ""children"", ""Govt_jov"", ""Never_worked"", ""Private"" or ""Self-employed""
8) Residence_type: ""Rural"" or ""Urban""
9) avg_glucose_level: average glucose level in blood
10) bmi: body mass index
11) smoking_status: ""formerly smoked"", ""never smoked"", ""smokes"" or ""Unknown""*
12) stroke: 1 if the patient had a stroke or 0 if not
*Note: ""Unknown"" in smoking_status means that the information is unavailable for this patient

##MODELS USED
-  Logistic Regression
-  Support Vector Machine (SVM)
-  Decission Tree
-  K Nearest Neighbour
-  Naive Bayes
-  Random Forest Classifier

##LIBRARIES NEEDED
- numpy
- pandas
- seaborn
- matplotlib
- scikit-learn","","Stroke Prediction using 6 ML models","","06/23/2021 23:59:00","18"
"3541","1169027","6402661","02/19/2021 09:28:31","## Task Details
You are working with the government to transform your city into a smart city. The vision is to convert it into a digital and intelligent city to improve the efficiency of services for the citizens. One of the problems faced by the government is traffic. You are a data scientist working to manage the traffic of the city better and to provide input on infrastructure planning for the future.

The government wants to implement a robust traffic system for the city by being prepared for traffic peaks. They want to understand the traffic patterns of the four junctions of the city. Traffic patterns on holidays, as well as on various other occasions during the year, differ from normal working days. This is important to take into account for your forecasting.

## Expected Submission
The objective is to predict traffic patterns in each of these four junctions for the next 4 months.

## Evaluation
The evaluation metric for the task is RMSE.","","Traffic Pattern Prediction","Predict traffic patterns in each of these four junctions for the next 4 months.","","1"
"3016","1054434","6402661","12/23/2020 09:36:37","## Task Details
Analysis of the unemployment in 2020 by municipalities. Discover the municipalities with bigger unemployment rates in each demographic category.

## Expected Submission
Solve the task primarily using Notebooks. The objective is to obtain useful visualizations to understand the data.

### Terms of use for reuse of data
According to Article 8 of the law 18 / 2015, 9 July, amending the law 37 / 2007 of 16 November, on the reuse of public sector information, apply the following general conditions for all those datasets offered within section Open Data:

1. The content of the information, including their metadata, is not altered.

2. That is not hinder the meaning of the information.

3. The source is acknowledged.

4. That mention the date of the last update.

5. When information contain personal data, the purpose or specific purposes for which it is possible to reuse future of data.

6. When information, still being provided so decoupled, contain sufficient elements that could enable the identification of stakeholders in the process of reuse, the prohibition of reversing the procedure of decoupling by adding new data from other sources.

More info: https://sede.serviciosmin.gob.es/en-us/Paginas/aviso.aspx","","Analysis of the unemployment in 2020 by municipalities","","","0"
"3008","1052571","6402661","12/22/2020 11:18:45","## Task Details
Predict the number of jets with transverse momentum above 40 GeV based on the values of the other variables.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) Run
2) nJets

## Evaluation
Evaluation using Accuracy metric

### Further help
http://opendata.cern.ch/record/553","","Prediction of number of jets","Predict the number of jets with transverse momentum above 40 GeV.","","1"
"3000","1051216","6402661","12/21/2020 17:19:48","## Task Details
Predict the Category of the observation: distinguishing blood donors from Hepatitis C patients.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) Row number
2) Category

## Evaluation
Evaluation using Accuracy metric

### Further help
Lichtinghagen R et al. J Hepatol 2013; 59: 236-42
Hoffmann G et al. Using machine learning techniques to generate laboratory diagnostic pathways √¢‚Ç¨‚Äú a case study. J Lab Precis Med 2018; 3: 58-67","","Patient Category Prediction","Predict whether the subject is a hepatitis C patient or a blood donor","","4"
"3100","1072569","6402661","01/02/2021 10:01:06","## Task Details
Explore the Coronavirus (COVID-19) Vaccinations dataset and display useful insights with visualizations and statistics.

## Expected Submission
The task should be solved using Notebooks that display useful visualizations to understand the data.

## Source of information country by country
[https://ourworldindata.org/covid-vaccinations#source-information-country-by-country](https://ourworldindata.org/covid-vaccinations#source-information-country-by-country)

## Further help
If you need additional information about the data, check out these links:

- [Data description](https://www.kaggle.com/fedesoriano/coronavirus-covid19-vaccinations-data)
- [Complete Source](https://github.com/owid/covid-19-data/tree/master/public/data)","","Analyze Coronavirus (COVID-19) Vaccinations Data","How many people have received a coronavirus vaccine?","","1"
"3054","1061675","6402661","12/27/2020 12:45:20","## Task Details
The outbreak of Covid-19 is developing into a major international crisis, and it's starting to influence important aspects of daily life. A strong model that predicts how the virus could spread across different countries and regions may be able to help mitigation efforts. The goal of this task is to build a model that predicts the progression of the virus throughout March 2020.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

## Evaluation
Accuracy - How well does the model perform on the real data? Can it be generalized over time? Can it be applied to other scenarios? Was it overfit?","","Predict the Spreading of Coronavirus","Help mitigating the secondary effects of covid-19 by predicting its spread","","0"
"3035","1057064","6402661","12/24/2020 15:29:36","## Task Details
Predict the value of the Temperature given the other attributes.

## Expected Submission
Solve the task primarily using Notebooks.

## Evaluation
Evaluation using RMSE

### Further help
S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005.

Saverio De Vito, Marco Piga, Luca Martinotto, Girolamo Di Francia, CO, NO2 and NOx urban pollution monitoring with on-field calibrated electronic nose by automatic bayesian regularization, Sensors and Actuators B: Chemical, Volume 143, Issue 1, 4 December 2009, Pages 182-191, ISSN 0925-4005.

S. De Vito, G. Fattoruso, M. Pardo, F. Tortorella and G. Di Francia, 'Semi-Supervised Learning Techniques in Artificial Olfaction: A Novel Approach to Classification Problems and Drift Counteraction,' in IEEE Sensors Journal, vol. 12, no. 11, pp. 3215-3224, Nov. 2012.
doi: 10.1109/JSEN.2012.2192425","","Prediction of the Temperature given the air components","Regression analysis","","0"
"3045","1059701","6402661","12/26/2020 09:31:41","## Task Details
CIFAR-100 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 100 object classes, with 600 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.

## Expected Submission
For each image in the test set, predict a label for the given id. Your labels must match the official labels exactly.

## Evaluation
Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly).

### Further help
Please cite this technical report if you use this dataset:
[Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.","","CIFAR-100 - Object Recognition in Images","Identify the subject of 60,000 labeled images","","6"
"3041","1058964","6402661","12/25/2020 19:34:13","## Task Details
Predict the value of the invariant mass given the other attributes.

## Expected Submission
Solve the task primarily using Notebooks.

## Evaluation
Evaluation using RMSE

### Further help
McCauley, Thomas; (2014). Events with two electrons from 2010. CERN Open Data Portal. [https://opendata.cern.ch/record/304](https://opendata.cern.ch/record/304)","","Prediction of the invariant mass of the two electrons","Regression analysis","","7"
"4672","1381830","6402661","06/07/2021 17:31:37","## Task Details
If you have some experience with R or Python and machine learning basics, this is the perfect task for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition. 

## Expected Submission
Prediction for the median value of owner-occupied homes (MEDV) for a test set.

## Evaluation
Evaluation using RMSE

### Further help
Harrison, David & Rubinfeld, Daniel. (1978). Hedonic housing prices and the demand for clean air. Journal of Environmental Economics and Management. 5. 81-102. 10.1016/0095-0696(78)90006-2. [LINK](https://www.researchgate.net/profile/Daniel-Rubinfeld/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air/links/5c38ce85458515a4c71e3a64/Hedonic-housing-prices-and-the-demand-for-clean-air.pdf)","","Boston House Prices Prediction","Predict median value of houses and practice feature engineering, RFs, and gradient boosting","","1"
"4775","1408058","6402661","06/14/2021 16:09:22","### Task Details

If you have some experience with R or Python and machine learning basics, this is the perfect task for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.


### Expected Submission

Prediction for the body fat percentage for a test set. Note that the use of the Density variable is not allowed since the body fat is calculated directly with the density.


### Evaluation

Evaluation using RMSE","","Body Fat Percentage Prediction","Predict body fat percentage and practice feature engineering, RFs, and gradient boosting","","8"
"4682","1396483","6402661","06/08/2021 12:53:57","## Task Details
Implement a classifier using PyTorch and GPUs for Chinese MNIST

## Expected Submission
Submit your Notebook.

## Evaluation
Obtain at least 0.90 accuracy.","","Implement a classifier using PyTorch and GPUs","Implement a classifier for Chinese numbers characters using PyTorch and GPUs","","3"
"4885","1424838","6402661","06/22/2021 10:16:52","## Task Details
CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.

## Expected Submission
For each image in the test set, predict a label for the given id. Your labels must match the official labels exactly.

## Evaluation
Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly).

### Further help
Please cite this technical report if you use this dataset:
[Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.","","CIFAR-10 - Object Recognition in Images","Identify the subject of 10,000 unlabelled images","","2"
"4983","1446091","6402661","07/03/2021 15:51:25","## Task Details
If you have some experience with R or Python and machine learning basics, this is the perfect task for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.

## Expected Submission
Prediction for the median value of owner-occupied homes (MEDV) for a test set.

## Evaluation
Evaluation using RMSE

### Further help
Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297. [LINK](https://www.sciencedirect.com/science/article/abs/pii/S016771529600140X)","","California House Prices Prediction","Predict median value of houses and practice feature engineering, RFs, and gradient boosting","","2"
"3857","1230610","6404512","03/24/2021 14:52:50","Objectives
Note: This is an individual assignment.

Project Brief
You work for Spark Funds, an asset management company. Spark Funds wants to make investments in a few companies. The CEO of Spark Funds wants to understand the global trends in investments so that she can take the investment decisions effectively.

 

Business and Data Understanding
Spark Funds has two minor constraints for investments:

It wants to invest between 5 to 15 million USD per round of investment

It wants to invest only in English-speaking countries because of the ease of communication with the companies it would invest in

For your analysis, consider a country to be English speaking only if English is one of the official languages in that country

You may use this list: Click here for a list of countries where English is an official language.

 

These conditions will give you sufficient information for your initial analysis. Before getting to specific questions, let‚Äôs understand the problem and the data first.

 

1. What is the strategy?

Spark Funds wants to invest where most other investors are investing. This pattern is often observed among early stage startup investors.

 

2. Where did we get the data from? 

We have taken real investment data from crunchbase.com, so the insights you get may be incredibly useful. For this assignment, we have divided the data into the following files:

 

You have to use three main data tables for the entire analysis (available for download on the next page):

 

3. What is Spark Funds‚Äô business objective?

The business objectives and goals of data analysis are pretty straightforward.

Business objective: The objective is to identify the best sectors, countries, and a suitable investment type for making investments. The overall strategy is to invest where others are investing, implying that the 'best' sectors and countries are the ones 'where most investors are investing'.
Goals of data analysis: Your goals are divided into three sub-goals:
Investment type analysis: Comparing the typical investment amounts in the venture, seed, angel, private equity etc. so that Spark Funds can choose the type that is best suited for their strategy.
Country analysis: Identifying the countries which have been the most heavily invested in the past. These will be Spark Funds‚Äô favourites as well.
Sector analysis: Understanding the distribution of investments across the eight main sectors. (Note that we are interested in the eight 'main sectors' provided in the mapping file. The two files ‚Äî companies and rounds2 ‚Äî have numerous sub-sector names; hence, you will need to map each sub-sector to its main sector.)
 

4. How do you approach the assignment? What are the deliverables?

The entire assignment is divided into checkpoints to help you navigate. For each checkpoint, you are advised to fill in the tables into the spreadsheet provided in the download segment. The tables are also mentioned under the 'Results Expected' section after each checkpoint. Since this is the first assignment, you have been provided with some additional guidance. Going forward you will be expected to structure and solve the problem by yourself, just like you would be solving problems in real life scenarios.","","Investment Analysis","","","1"
"3482","1149830","6409983","02/13/2021 10:59:59","## Task Details

Cookie Cats is a mobile puzzle game developed by Tactile Entertainment. It's a classic ""connect three""-style puzzle game.

As players progress through the levels of the game, they will occasionally encounter gates that force them to wait a non-trivial amount of time or make an in-app purchase to progress. These gates serve the important purpose of giving players an enforced break from playing the game, hopefully resulting in that the player's enjoyment of the game being increased and prolonged.

Initially, the first gate was placed at level 30. The company moved the first gate from level 30 to level 40. But where should the gates be placed? Is there any change when the first gate moved to level 40 in terms of customer satisfaction?","","Analyze an A/B test from the popular mobile puzzle game, Cookie Cats.","","","1"
"3393","1136174","6421113","02/03/2021 13:53:13","analysis","","analysis","","","0"
"3392","1136198","6421113","02/03/2021 13:43:37","analysis","","Analysis","","","0"
"4272","1311225","6425789","05/05/2021 12:16:56","## Task Details
Sign language is one of the oldest and most natural forms of language for
communication, but since most people do not know sign language and
interpreters are very difficult to come by. The task is to develop a neural network to classify the ASL alphabet images in the dataset.

## Expected Submission
Users should submit the notebook performing the image classification using the dataset.

## Evaluation
Evaluation will be done on the basis of the accuracy and size of the neural network.","","American Sign Language recognition","","","1"
"3926","1245316","6429483","04/01/2021 11:29:12","# Task description
Create a binary target variable **alc_prob** as follows:
- Calculate the Gini index for the target variable **alc_prob** and the Gini index for each variable with respect to **alc_prob**. Determine the 5 variables with the highest Gini Gain.
- Learn 2 different decision trees with alc_prob as target variable. For the first tree, nodes should be further partitioned until the class distribution of all resulting leaf nodes is pure. For the second tree, nodes with a cardinality of less than 20 instances should not be further partitioned. Determine the quality of the trees by calculating sensitivity (True Positive Rate) and specificity (True Negative Rate) for a 70%:30% split in training and test sets. Display the decision trees graphically and discuss the differences in quality measures.
- Use **randomForest::randomForest()** to create a random forest with 200 trees. As candidates
for a split within a tree a random sample of 5 variables should be drawn. Calculate Accuracy,
Sensitivity and Specificity for the Out-of-the-Bag instances and show the most important
variables (**?importance**).","","What factors explain excessive alcohol consumption among students","","","1"
"3930","1245685","6434964","04/01/2021 14:43:25","## Task Details
Here I want to make a language detector system

## Expected Submission
You have to train a model that can detect language that like from which language these words belong to.

## Evaluation
You shouldn't break this dataset into a test set and train set because in my opinion, all words are necessary for this.

### Further help
This dataset is created using selenium webscraping and in the first column or feature there are belongs to a language and then the language name in the second column or feature.","","Create Language Detector","","","0"
"4464","1258183","6437624","05/23/2021 07:42:59","Performe Visualization and Analysis.","","Perform EDA","","","1"
"3291","1116066","1314380","01/27/2021 22:58:09","## Task Details

Plot Gamestop (GME) sentiment on reddit vs share price (over time).  Are price increases correlated with improvements in reputation?  Use data from both `comment_analysis.csv` and `stockticker_history.csv`.

## Expected Submission

A public Kaggle Notebook that accomplishes the task.

## Evaluation

None","","Plot Gamestop (GME) sentiment on reddit vs share price (over time)","Are price increases correlated with improvements in reputation?","","2"
"3733","1205389","6443424","03/11/2021 15:06:29","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
Visualize the gradient descent for given data.
Predict the grades for 300 and 127 minutes respectively.","","Regression","Visualize the gradient descent","12/31/2021 23:59:00","0"
"4350","1334397","6448792","05/12/2021 15:26:59","## Task Details
Let's build an efficient ML model that predicts posttest scores with minimal error and high accuracy

## Expected Submission
Please submit notebooks of your work

## Evaluation
A good MAE and high accuracy score","","Predict Post Test Scores","Let's build an efficient ML model that predicts posttest scores with minimal error and high accuracy","","18"
"6136","1457173","6450367","09/20/2021 03:07:09","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)



perform and understand correalation between the data by hierarchical clustering graphs","","hierarchical clustering(cereal.csv)","","","1"
"3307","1126409","6453808","01/29/2021 15:15:45","build any suitable ML model","","ML model","","","0"
"3652","1175926","6456693","03/03/2021 21:21:39","## Task Details
    In this project we will try to predict the AQI using the temperature datas. We will also search which weather condition has higher effect on air quality. We will use Shanghai as the base city, since the air quality of Shanghai has seasonal air conditions, close to sea and has low air quality. 


Air Quality Index (AQI) is measured with the indicators below.  
PM 2.5

    Though invisible to the naked eye, the air we breathe is loaded with tiny particulates made up of chemicals, allergens, dust, smoke, or soil in the formof (microscopic) solids, gases or liquids. When we burn fossil fuels forproduction, energy or mobility, it generates chemicals and gases whichpollute the air. This poses a risk to human health and earth as awhole. One of the most minuscule categories of these airborne hazards are called particulate matter (PM) 2.5 

PM10
    
    Organic particles, or particulate matter, as in smoke, measuring between 2.5 and 10 microns in diameter.

Ozone (O3)

    Ozone is a gas whose molecules are composed of three oxygen atoms. Ozone is not emitted directly into the air, but is created by a chemical reaction between two precursors ‚Äî oxides of nitrogen (NOx) and volatile organic compounds (VOC) ‚Äî in the presence of sunlight. 

Nitrogen dioxide (NO2)

    Nitrogen dioxide (NO2) is one of a group of highly reactive gases known as nitrogen oxides (NOX). NO2 forms quickly from emissions from cars, trucks and buses, power plants, off-road equipment and other combustion sources. In addition to contributing to the formation of ground-level ozone and fine particle pollution, NO2 is associated with a number of adverse effects on the human respiratory system. 

Sulfur Dioxide (SO2)

    Sulfur dioxide (SO2) is a highly reactive gas, and the largest sources of SO2 emissions are power plants and other industrial facilities that combust fossil fuels. Smaller sources of SO2 emissions include the burning of high sulfur-containing fuels by locomotives and non-road equipment, and non-combustion industrial processes, such as extracting metal from ore. SO2 is linked with a number of adverse effects on the respiratory system. 

Carbon Monoxide (CO)

    Carbon monoxide (CO) is a colorless, odorless gas emitted directly from motor vehicles, off-road equipment, and other sources of equipment powered by fossil-fuels. High CO concentrations are a health concern because the pollutant is readily absorbed through the lungs into the blood, where it binds with hemoglobin and reduces the ability of the blood to carry oxygen. 

Air Quality Index and Effects on Human Health

Depending on how healthy a person is, fine particulate matter will have various short term and long term health impacts. If you are exposed to levels between ""Unhealthy for Sensitive Groups"" (AQI 100) to ""Hazardous"" (300+), you might experience one or more of the following effects:

‚Ä¢ Asthma attacks

‚Ä¢ Lowered heart function, possibly leading in heart attack

‚Ä¢ Reduced lung function and exacerbation of lung conditions

‚Ä¢ Wheezing or coughing

‚Ä¢ Throat, nose or eye irritation

‚Ä¢ Shortness of breath 

# According to both feature selection models the fallowing weather conditions has the highest impact on the AQI 

1) Minimum Temp C

2) Feels Like C 

3) UV Index : 
The ultraviolet index, or UV index, is an international standard measurement of the strength of sunburn-producing ultraviolet (UV) radiation at a particular place and time.

4) DewPointC : 
Represents the temperature to which air would have to be cooled to reach a level of moisture saturation. When it reaches the dew point, droplets of water, or dew, begin to form on solid objects like grass and cars.

5) Heat Index C : 
represents how hot the temperature actually feels when humidity is considered. The more humid the air is, the less perspiration is able to evaporate, which cripples the human body‚Äôs cooling system and makes it feel hotter when it‚Äôs humid outside.

6) Wind Chill C : 
Also known as the ‚Äúfeels-like‚Äù temperature, wind chill represents how cold the weather feels on human skin when the chilling effect of the wind is taken into consideration.","","Temperature and Climate effects on AIR QUALITY INDEX (AQI) based on Shanghai measurements","Temperature and Climate effects on AIR QUALITY INDEX (AQI) based on Shanghai measurements","03/05/2021 23:59:00","0"
"4729","1400621","6457522","06/11/2021 07:10:52","Build model to classify & localize different signs of ageing suchas puffy eyes, wrinkles, dark spots etc. on the face.Use different algorithms and try to check weather the person is old or not.","","Classify and Localize different signs of ageing","","","10"
"3326","1127151","6459548","01/30/2021 03:24:39","This is historical data of the stock price, Try to Forcast the adjusted close stock prices for the next months in 2021","","Forcasting Stock Prices for the next months in 2021","","","0"
"3308","1126269","6459595","01/29/2021 15:45:38","## Task Details
Which book received the Average rating.

## Expected Submission
A notebook with the solution.

## Evaluation
A clear representation of the results of your analysis.","","Book with the highest Average rating.","","","0"
"3269","1117432","6461110","01/25/2021 08:51:58","**Use the test_esrb.csv and make a machine learning model to predict the rating of the games in the test file.**
- **Show the accuracy of your prediction by using accuracy score.**","","Predict the rating for each game in test data","Classification task","","5"
"3286","1121606","6461353","01/27/2021 09:18:06","hi , guys if you can do recommendation system try to do it","","recommendation system","recommend a book to the users","","1"
"3337","1128117","6462498","01/30/2021 14:23:49","**Perfume notes:**  let the model predict perfume notes.

**Department prediction:** build a model to decide the perfume department based on the notes features

**Price prediction:** predict the prices of the products based on the num_seller_ratings and other features

**Brand prediction:** it seems that some brands have a favorite note, lets the machine guess the brand based on the notes features

**Concentration prediction:** it seems that some concentration has a common note, lets the machine guess the concentration based on the notes features

**Feature Engineering:** do magic and play around to create new features from this dataset","","model tp predict","","","0"
"3360","1130528","6462967","01/31/2021 19:23:45","Book with the highest text reviews.","","Best Books of the 19th Century","","","1"
"3353","1128844","6462982","01/30/2021 22:07:29","## Task Details
Predict the prices of the cars in the neighboring countries. And compare.

## Expected Submission
The best country to buy a new car based on that prediction?

## Evaluation
What makes a good solution? getting 80% accuracy and providing the solution on where to buy the next car.","","Predict the difference.","different country different prices","","1"
"3341","1128226","6462986","01/30/2021 15:47:02","**Context**

The Kingdom of Saudi Arabia has dealt excellently with the Covid-19 situation compared to other countries. Moreover, the possibility of collecting detailed data related to the Covid-19 in KSA is motivating to publish it in Kaggle to explore and extract hidden knowledge to predict the future.

**Content**

The dataset contains the following detailed sub-datasets:
1- A dataset contains 29852 daily cases from 2020-03-02 to 2021-01-28.
2- A dataset includes cases by governorates.
3- A dataset includes cases by 13 regions.
4- A dataset contains 307 cumulated cases by date from 2020-03-02 to 2021-01-02.
5- A dataset contains 338 total daily tests from 2020-03-02 to 2021-02-02.
6- A dataset contains 29769 critical cases from 2020-03-02 to 2021-01-27.","","Predict the effect of the second covid 19 strain spreading if it spreads in KSA?","There are six datasets, merging some of them to create creative ideas to extract hidden knowledge and predict the future.","","0"
"3352","1128787","6462991","01/30/2021 21:56:04","## About
I have been working on a few time-series forecasts lately and found them quite fun to analyze. I have used many ready-to-use datasets in past. It's time to give back to the community as a token of appreciation.

## Task Details
- Inspect, plot, and check for seasonality
- Fit an ARIMA model with statsmodels
- Forecast with the fitted model
- Evaluate the model using RMSE as an indicator
- Retrain the model on the full data, and forecast the future

Good luck,
Adam","","Use the past to predict the future","","","0"
"4067","1269864","6465362","04/13/2021 22:15:19","## Task Details
The IMF World Economic Outlook  Africa 2019-2020 as the name indicates is the economic outlook for African countries dealing with the IMF.

## Expected Submission
My aim was to get a better understanding of the current outlook of African countries, their population and finally their economic position.

## Evaluation
Please feel free to add any additional insights that I might have overlooked. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","IMF WEO Africa 2019-2020","","","0"
"3299","1121877","6468663","01/28/2021 15:22:46","By using the free lancers skills, classify the free lancers to the services fields i.e. Programming and Development, Marketing and Sales.. etc.","","Classify the free lancers.","","","0"
"3398","1136849","6469303","02/03/2021 22:42:01","It is expected you apply sentiment analsis  while working this dataset","","Evaluating Sentiment","","","1"
"4063","1269074","6470511","04/13/2021 13:06:42","## Task Details
Work on the data and extract the best result

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Work on the data and extract the best result","Work on the data and extract the best result","10/10/2021 23:59:00","1"
"4091","1276562","6470511","04/16/2021 23:03:21","You can do EDA and then classification if a person is ill or not.","","Classification","","","0"
"4141","1284643","6470511","04/20/2021 23:25:59","# Task Details:
What should users provide for this data? Should they primarily solve the task with laptops or data sets? What should this solution contain?

## Evaluation:
What makes a good solution? How do you evaluate which submission is better than another?","","work on this data set","Every task has an interesting story. Tell users what this task is all about and why you created it.","","0"
"3888","1236723","6470511","03/27/2021 21:50:36","## Task Details
Provide your code to benefit those who want to learn

## Expected Submission
Provide your code to benefit those who want to learn

## Evaluation
Provide your code to benefit those who want to learn","","learn  k_mean","Specialized in education in nachklashay","01/05/2023 23:59:00","1"
"3239","1110747","6473725","01/21/2021 16:34:08","## Task Details
it is fun task . it all about playing around with our monthly internet usage dataset","","find all interactions and also find what is the weekday at which max is the usage?","","","0"
"4665","1393914","6477455","06/07/2021 09:25:59","kdsjfkds
## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","mytask","","","0"
"3821","1222684","6479040","03/20/2021 11:47:05","## Task Details
This task is about making a deep learning model for image segmentation of crystals.

## Expected Submission
You can submit a notebook containing the implementation of a segmentation model like U-Net using this dataset. The model should output the mask of the input image. You should provide the best solution for maximum accuracy.

## Evaluation
I will test your solution on a microscopic crystal image, if it successfully detects all crystals it means yours is the best model.","","Image Segmentation of Crystals","","","1"
"3167","1086876","6479964","01/10/2021 22:11:54","## Task Details
Create a Trend of progression and update the results.

## Expected Submission
Submit Notebooks, images, etc.

## Evaluation
Humans are good till now in making cognitive decisions through visualizations. So it's a good approach to represent a data through visualization.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Trend of progression of Covid in Vit√≥ria da Conquista","","","0"
"4356","1274690","6497833","05/13/2021 04:48:50","## Task Details
Predict rainfall using Recurrent Neural Network. **Try taking only last 100 days into account**.

## Expected Submission
The solution should contain the Prdicted rainfall values.","","Rainfall Prediction","","","0"
"3543","1120133","6498242","02/19/2021 12:22:42","## Task Details
Migration flow can be visualize through the world map, in order to get accuratte data and data visualizations to overcome stigma.

## Expected Submission
Solutions should (but we are always open to original ideas) visualizations or predictions all above the world or each country.

## Evaluation
A good solution is about easy data everyone can understand to be used on educational purposes

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualizations","Lets build a visualization map about migration through the years to analyze the trends","","0"
"3173","1091766","6504146","01/11/2021 20:58:25","## Task Details
Create a 360 viewer that takes the dataset provided (360 pano images) and stitches them into a virtual tour experience. User should be able to click through environment where different 360 panos were taken.

Using any relevant APIs to to automate the process

## Expected Submission
Python notebook using provided dataset
User interface where users can navigate through 360 viewpoints

## Evaluation
Quality of virtual tour walkthrough","","Create a 360 Viewer with walkthrough capabilities","360 Viewer with the ability to move through the space (virtual tour)","","0"
"3592","1179715","6505909","02/25/2021 06:13:34","## Task Details
Analysis of each Variable 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Multi Variate Analysis","","05/25/2021 23:59:00","0"
"4014","1259547","6521428","04/09/2021 03:08:10","In December 2020, news media reported a new variant of the coronavirus that causes COVID-19, and since then, other variants have been identified and are under investigation. The new variants raise questions: Are people more at risk for getting sick? Will the COVID-19 vaccines still work? Are there new or different things you should do now to keep your family safe?","","Perform analysis to find trend of new COVID Strain","","","1"
"3267","1116216","6529944","01/24/2021 17:10:48","## Task Details
Extract features and try to increase the accuracy of model","","Use CNN model to predict if the given sample in covid positive or a Normal one","","","0"
"4274","1319704","6534793","05/05/2021 17:54:21","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)


**","","Create need and clean Dashboard to analyse easily to understandable.","Dashboard","","0"
"4262","1311358","5966695","05/04/2021 08:54:16","You can do EDA and then build a model that differentiating between three classes: WBC (white blood cells), RBC (red blood cells), and Platelets.","","Object detection","","","2"
"3712","1200454","6535768","03/09/2021 11:07:50","## Task Details
In this part, you have to implement a code that generates these frequent sequential patterns
(closed or maximal) that have support g greater than or equal to the threshold fixed by the user.
To do so, you may generate as a first step frequent sequential patterns that have a frequency
greater than or equal to the threshold fixed by the user.","","Task mandatory","","04/06/2021 23:59:00","1"
"3713","1200454","6535768","03/09/2021 11:09:00","## Task Details
In this part, you have to use the mined sequential patterns to predict when the next event of a sequence. The approach should be studied and compared to state-of-the-art approaches using the quality measure discussed in the lecture.","","Data engineering task","","04/06/2021 23:59:00","1"
"3577","1175998","6541579","02/23/2021 05:28:30","## Data Visualizations
-- Map volcanoes that produced the highest tsunamis.
-- Plot volcanoes by type, VEI, and experiences tsunamis
-- Chart volcano characteristics
-- Explore","","Data Visualization","","","0"
"3578","1175998","6541579","02/23/2021 05:35:44","## Prediction
-- Apply various machine learning models to predict the likelihood of a tsunami occurring.
-- Evaluate the correlation between characteristics and volcano type.
-- Explore","","Prediction and Correlation","","","0"
"3229","1108061","6547331","01/20/2021 13:06:30","We can use this dataset to develop many useful A.I applications in fisheries industry.Also this dataset can be used for search engine purpose too.","","In A.I applications and for search engines","","12/31/2021 23:59:00","0"
"3248","1112463","6573957","01/22/2021 14:41:44","**Tom & Jerry [2021]
EXCLUSIVE! ‚Äî Tom & Jerry (2021) on Warner Bros. Pictures | FULL STREAMING of ‚ÄúTom & Jerry‚Äù**
*Directored by Tim Story*
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6573957%2Fde1eb74d5bfc9b343a86e6209176deeb%2Fe06BpqZIxRSpvNSbItcGcgs0S5I.jpg?generation=1611326532212764&alt=media)
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6573957%2F0c9a77014d30da2c13565a2e145c03a1%2F89379236_522017008517693_4951110635220893696_n.png?generation=1611326556451569&alt=media)

WATCH    ‚ûî‚ûî https://t.co/CKvqnXbLfl?amp=1
DOWNLOAD ‚ûî‚ûî https://t.co/CKvqnXbLfl?amp=1

Adaptation of the classic Hanna-Barbera property, which reveals how Tom and Jerry first meet and form their rivalry.
Tom & Jerry
Movie Tom & Jerry Tom the cat and Jerry the mouse get kicked out of their home and relocate to a fancy New York hotel‚Ä¶

Tom & Jerry
Tom & Jerry Cast
Tom & Jerry Trailer
Tom & Jerry Review
Tom & Jerry 2021
Tom & Jerry full movie
Tom & Jerry full movie 2021
Tom & Jerry full online
Tom & Jerry full streaming
Tom & Jerry online
Tom & Jerry streaming
Tom & Jerry watch full online
Tom & Jerry full streaming online
Tom & Jerry watch online
Tom & Jerry watch streaming

Film, also called movie, motion picture or moving picture, is a visual art-form used to simulate experiences that communicate ideas, stories, perceptions, feelings, beauty, or atmosphere through the use of moving images. These images are generally accompanied by sound, and more rarely, other sensory stimulations.[1] The word ‚Äúcinema‚Äù, short for cinematography, is often used to refer to filmmaking and the film industry, and to the art form that is the result of it.
‚ùè STREAMING MEDIA ‚ùè
Streaming media is multimedia that is constantly received by and presented to an end-user while being delivered by a provider. The verb to stream refers to the process of delivering or obtaining media in this manner.[clarification needed] Streaming refers to the delivery method of the medium, rather than the medium itself. Distinguishing delivery method from the media distributed applies specifically to telecommunications networks, as most of the delivery systems are either inherently streaming (e.g. radio, television, streaming apps) or inherently non-streaming (e.g. books, video cassettes, audio CDs). There are challenges with streaming content on the Internet. For example, users whose Internet connection lacks sufficient bandwidth may experience stops, lags, or slow buffering of the content. And users lacking compatible hardware or software systems may be unable to stream certain content.
Live streaming is the delivery of Internet content in real-time much as live television broadcasts content over the airwaves via a television signal. Live internet streaming requires a form of source media (e.g. a video camera, an audio interface, screen capture software), an encoder to digitize the content, a media publisher, and a content delivery network to distribute and deliver the content. Live streaming does not need to be recorded at the origination point, although it frequently is.
Streaming is an alternative to file downloading, a process in which the end-user obtains the entire file for the content before watching or listening to it. Through streaming, an end-user can use their media player to start playing digital video or digital audio content before the entire file has been transmitted. The term ‚Äústreaming media‚Äù can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered ‚Äústreaming text‚Äù.
‚ùè COPYRIGHT CONTENT ‚ùè
Copyright is a type of intellectual property that gives its owner the exclusive right to make copies of a creative work, usually for a limited time.[1][2][3][4][5] The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself.[6][7][8] A copyright is subject to limitations based on public interest considerations, such as the fair use doctrine in the United States.
Some jurisdictions require ‚Äúfixing‚Äù copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.[citation needed][9][10][11][12] These rights frequently include reproduction, control over derivative works, distribution, public performance, and moral rights such as attribution.[13]
Copyrights can be granted by public law and are in that case considered ‚Äúterritorial rights‚Äù. This means that copyrights granted by the law of a certain state, do not extend beyond the territory of that specific jurisdiction. Copyrights of this type vary by country; many countries, and sometimes a large group of countries, have made agreements with other countries on procedures applicable when works ‚Äúcross‚Äù national borders or national rights are inconsistent.[14]
Typically, the public law duration of a copyright expires 50 to 100 years after the creator dies, depending on the jurisdiction. Some countries require certain copyright formalities[5] to establishing copyright, others recognize copyright in any completed work, without a formal registration.
It is widely believed that copyrights are a must to foster cultural diversity and creativity. However, Parc argues that contrary to prevailing beliefs, imitation and copying do not restrict cultural creativity or diversity but in fact support them further. This argument has been supported by many examples such as Millet and Van Gogh, Picasso, Manet, and Monet, etc.[15]
‚ùè GOODS OF SERVICES ‚ùè
Credit (from Latin credit, ‚Äú(he/she/it) believes‚Äù) is the trust which allows one party to provide money or resources to another party wherein the second party does not reimburse the first party immediately (thereby generating a debt), but promises either to repay or return those resources (or other materials of equal value) at a later date.[1] In other words, credit is a method of making reciprocity formal, legally enforceable, and extensible to a large group of unrelated people.
The resources provided may be financial (e.g. granting a loan), or they may consist of goods or services (e.g. consumer credit). Credit encompasses any form of deferred payment.[2] Credit is extended by a creditor, also known as a lender, to a debtor, also known as a borrower.","","EXCLUSIVE! [Watch] TOM & JERRY [2021] Full MOVIE Free Download UHD | HD | 720p","","01/23/2021 23:59:00","0"
"3249","1112463","6573957","01/22/2021 15:02:24","**WAY DOWN 2021 VOLLEDIGE FILM Streaming**
*Directored Jaume Balaguer√≥*

**‚ÄúFREE.Movies+]-SOUND.Cloud++!-JWPLayer*GoogleDrive/4K.Downloads-! How to watch WAY DOWN online Free? HQ Reddit Video [DVD-ENGLISH] WAY DOWN 2021 Full Movie Watch online free Dailymotion [#WAY DOWN ] Google Drive/[DvdRip-USA/Eng-Subs] WAY DOWN!
Streaming online WAY DOWN
(2021) Full Movie
**
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6573957%2Fa70400bd2a04d639456b84ac55c31c09%2FzEIs9GqgCadPloE5GeSp84JkPiV.jpg?generation=1611327699000131&alt=media)
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6573957%2Ff7b8aed668545bc1e902b1deb47c8267%2F89379236_522017008517693_4951110635220893696_n.png?generation=1611327720246874&alt=media)


- Watch WAY DOWN (2021) Full Movie
- Download HD Quality WAY DOWN (2021) WAY DOWN (2021)
‚á®[One click to play] ¬ª‚û´[ https://worldmovie.site/nl/movie/630004/way-down]( https://worldmovie.site/nl/movie/630004/way-down)
Bekijk WAY DOWN Volledige film online gratis


Vrijgegeven: 2021‚Äì01‚Äì15
Looptijd: 0 minuten
Genre: Actie, Thriller
Sterren: Freddie Highmore, Liam Cunningham, Famke Janssen, Astrid Berg√®s-Frisbey, Sam Riley
Regisseur: Freddie Highmore, Daniel Hubbard, Nina Haun, √Ålvaro August√≠n, Michel Gaztambide

Welcome to the Best Film ACTION TV SERIES From Various HD Quality Products:
Action. Adventure. Animation. Biography. Comedy. Crime. Documentary.

üì∫Enjoy And Happy Watchingüì∫
WAY DOWN (2021) full Movie Watch Online
WAY DOWN (2021) full English Full Movie
WAY DOWN (2021) full Full Movie,
Watch WAY DOWN (2021) full English FullMovie Online
WAY DOWN (2021) full Film Online
Watch WAY DOWN (2021) full English Film
WAY DOWN (2021) full Movie stream free
Watch WAY DOWN (2021) full Movie subtitle
Watch WAY DOWN (2021) full Movie spoiler
WAY DOWN (2021) full Movie tamil
WAY DOWN (2021) full Movie tamil download
Watch WAY DOWN (2021) full Movie download
Watch WAY DOWN (2021) full Movie telugu
Watch WAY DOWN (2021) full Movie tamildubbed download
WAY DOWN (2021) full Movie to watch Watch Toy full Movie vidzi
WAY DOWN (2021) full Movie vimeo
Watch WAY DOWN (2021)
Its somewhat ironic that a movie about time travel can‚Äôt be reviewed properly until your future self rewatches the movie.
It‚Äôs bold of Nolan to make such a thoroughly dense blockbuster. He assumes people will actually want to see WAY DOWN more than once so they can understand it properly, which some may not. This movie makes the chronology of Inception look as simplistic as tic-tac-toe.
Ergo, it‚Äôs hard for me to give an accurate rating, without having seen it twice, as I‚Äôm still trying to figure out whether everything does indeed make sense. If it does, this movie is easily a 9 or 10. If it doesn‚Äôt, it‚Äôs a 6.
It‚Äôs further not helped by the fact that the dialogue in the first 15 minutes of the movie is painfully hard to understand / hear. Either they were behind masks; they were practically mumbling; the sound effects were too loud; or all of the above. The exposition scenes are also waayyy too brief for something this complex ‚Äî a problem also shared with WAY DOWN actually.
(WAY DOWN had this minimalist exposition problem explaining Blight, where if you weren‚Äôt careful, you‚Äôd miss this one sentence / scene in the entire movie explaining that Blight was a viral bacteria:
‚ÄúEarth‚Äôs atmosphere is 80% nitrogen, we don‚Äôt even breathe nitrogen. Blight does, and as it thrives, our air gets less and less oxygen‚Äù).
I guess it‚Äôs a Nolan quirk. Hopefully, a revision of the film audio sorts the sound mixing out. I do like the soundtrack, but it‚Äôs too loud initially.
I liked all the actors. You think John Washington can‚Äôt act at first, but he can, and he grows on you as the film progresses. And Pattinson is his usual charming self. Elizabeth is a surprise treat. And so on.
Its worth a watch either way. See it with subtitles if you can. And definitely don‚Äôt expect to fully understand whats going on the first time around.
Its one hell of a complicated film. It will be very hard for an average viewer to gather all the information provided by this movie at the first watch. But the more you watch it, more hidden elements will come to light. And when you are able to put these hidden elements together. You will realize that this movie is just a ‚Äúmasterpiece‚Äù which takes the legacy of Christopher Nolan Forward
If I talk about acting, Then I have to say that Robert Pattinson has really proved himself as a very good actor in these recent years. And I am sure his acting skills will increase with time. His performance is charming and very smooth. Whenever he is on the camera, he steals the focus John David Washington is also fantastic in this movie. His performance is electrifying, I hope to see more from him in the future. Other characters such as Kenneth Branagh, Elizabeth, Himesh Patel, Dimple Kapadia, Cl√©mence Po√©sy have also done quite well. And I dont think there is a need to talk about Michael Caine
Talking about Music, its awesome. I dont think you will miss Hans Zimmer‚Äôs score. Ludwig has done a sufficient job. There is no lack of good score in the movie
Gotta love the editing and post production which has been put into this movie. I think its fair to say this Nolan film has focused more in its post production. The main problem in the movie is the sound mixing. Plot is already complex and some dialogues are very soft due to the high music score. It makes it harder to realize what is going on in the movie. Other Nolan movies had loud BGM too. But Audio and dialogues weren‚Äôt a problem
My humble request to everyone is to please let the movie sink in your thoughts. Let your mind grasp all the elements of this movie. I am sure more people will find it better. Even those who think they got the plot. I can bet they are wrong.
WAY DOWN is the long awaited new movie from Christopher Nolan. The movie that‚Äôs set to reboot the multiplexes post-Covid. It‚Äôs a manic, extremely loud, extremely baffling sci-fi cum spy rollercoaster that will please a lot of Nolan fan-boys but which left me with very mixed views.
John David Washington (Denzel‚Äôs lad) plays ‚ÄúThe Protagonist‚Äù ‚Äî a crack-CIA field operative who is an unstoppable one-man army in the style of Hobbs or Shaw. Recruited into an even more shadowy organisation, he‚Äôs on the trail of an international arms dealer, Andrei Sator (Kenneth Branagh in full villain mode). Sator is bullying his estranged wife Kat (Elizabeth Debicki) over custody of their son (and the film unusually has a BBFC warning about ‚ÄúDomestic Abuse‚Äù). Our hero jets the world to try to prevent a very particular kind of Armageddon while also keeping the vulnerable and attractive Kat alive.
This is cinema at its biggest and boldest. Nolan has taken a cinema ‚Äòsplurge‚Äô gun, filled it with money, set it on rapid fire, removed the safety and let rip at the screen. Given that Nolan is famous for doing all of his ‚Äòeffects‚Äô for real and ‚Äòin camera‚Äô, some of what you see performed is almost unbelievable. You thought crashing a train through rush-hour traffic in ‚ÄúInception‚Äù was crazy? You ain‚Äôt seen nothing yet with the airport scene! And for lovers of Chinooks (I must admit I am one and rush out of the house to see one if I hear it coming!) there is positively Chinook-p*rn on offer in the film‚Äôs ridiculously huge finale.
The ‚Äòinversion‚Äô aspects of the story also lends itself to some fight scenes ‚Äî one in particular in an airport ‚Äòfreeport‚Äô ‚Äî which are both bizarre to watch and, I imagine, technically extremely challenging to pull off. In this regard John David Washington is an acrobatic and talented stunt performer in his own right, and must have trained for months for this role.
Nolan‚Äôs crew also certainly racked up their air miles pre-lockdown, since the locations range far and wide across the world. The locations encompassed Denmark, Estonia, India, Italy, Norway, the United Kingdom, and United States. Hoyte Van Hoytema‚Äôs cinematography is lush in introducing these, especially the beautiful Italian coast scenes. Although I did miss the David Arnold strings that would typically introduce these in a Bond movie: it felt like that was missing.
The ‚Äòtimey-wimey‚Äô aspects of the plot are also intriguing and very cleverly done. There are numerous points at which you think ‚ÄúOh, that‚Äôs a sloppy continuity error‚Äù or ‚ÄúShame the production design team missed that cracked wing mirror‚Äù. Then later in the movie, you get at least a dozen ‚ÄúAha!‚Äù moments. Some of them (no spoilers) are jaw-droppingly spectacular.
Perhaps the best twist is hidden in the final line of the movie. I only processed it on the way home.
And so to the first of my significant gripes with WAY DOWN. The sound mix in the movie is all over the place. I‚Äôd go stronger than that‚Ä¶ it‚Äôs truly awful (expletive deleted)! Nolan often implements Shakespeare‚Äôs trick of having characters in the play provide exposition of the plot to aid comprehension. But unfortunately, all of this exposition dialogue was largely incomprehensible. This was due to:
the ear-splitting volume of the sound: 2021 movie audiences are going to be suffering from ‚ÄòWAY DOWNis‚Äô! (LOL);
the dialogue is poorly mixed with the thumping music by Ludwig G√∂ransson (Wot? No Hans Zimmer?);
a large proportion of the dialogue was through masks of varying description (#covid-appropriate). Aaron Taylor-Johnson was particularly unintelligible to my ears.
Overall, watching this with subtitles at a special showing might be advisable!
OK, so I only have a PhD in Physics‚Ä¶ but at times I was completely lost as to the intricacies of the plot. It made ‚ÄúInception‚Äù look like ‚ÄúThe Tiger Who Came to Tea‚Äù. There was an obvious ‚ÄòMcGuffin‚Äô in ‚ÄúInception‚Äù ‚Äî ‚Äî (‚ÄúThese ‚Äòdream levels‚Äô‚Ä¶ how exactly are they architected??‚Äù‚Ä¶. ‚ÄúDon‚Äôt worry‚Ä¶ they‚Äôll never notice‚Äù. And we didn‚Äôt!) In ‚ÄúWAY DOWN‚Äù there are McGuffins nested in McGuffins. So much of this is casually waved away as ‚Äúfuture stuff‚Ä¶ you‚Äôre not qualified‚Äù that it feels vaguely condescending to the audience. At one point Sator says to Kat ‚ÄúYou don‚Äôt know what‚Äôs going on, do you?‚Äù and she shakes her head blankly. We‚Äôre right with you there luv!
There are also gaps in the storyline that jar. The word ‚ÄúWAY DOWN‚Äù? What does it mean. Is it just a password? I‚Äôm none the wiser.
The manic pace of WAY DOWN and the constant din means that the movie gallops along like a series of disconnected (albeit brilliant) action set pieces. For me, it has none of the emotional heart of the Cobb‚Äôs marriage problems from ‚ÄúInception‚Äù or the father/daughter separation of ‚ÄúWAY DOWN‚Äù. In fact, you barely care for anyone in the movie, perhaps with the exception of Kat.
It‚Äôs a talented cast. As mentioned above, John David Washington is muscular and athletic in the role. It‚Äôs a big load for the actor to carry in such a tent-pole movie, given his only significant starring role before was in the excellent BlacKkKlansman. But he carries it off well. A worthy successor to Gerard Butler and Jason Statham for action roles in the next 10 years.
This is also a great performance by Robert Pattinson, in his most high-profile film in a long time, playing the vaguely alcoholic and Carr√©-esque support guy. Pattinson‚Äôs Potter co-star Clemence Po√©sy also pops up ‚Äî rather more un-glam that usual ‚Äî as the scientist plot-expositor early in the movie.
Nolan‚Äôs regular Michael Caine also pops up. although the 87-year old legend is starting to show his age: His speech was obviously affected at the time of filming (though nice try Mr Nolan in trying to disguise that with a mouth full of food!). But in my book, any amount of Caine in a movie is a plus. He also gets to deliver the best killer line in the film about snobbery!
However, it‚Äôs Kenneth Branagh and Elizabeth Debicki that really stand out. They were both fabulous, especially when they were bouncing off each other in their marital battle royale.
So, given this was my most anticipated movie of the year, it‚Äôs a bit of a curate‚Äôs egg for me. A mixture of being awe-struck at times and slightly disappointed at others. It‚Äôs a movie which needs a second watch, so I‚Äôm heading back today to give my ear drums another bashing! And this is one where I reserve the right to revisit my rating after that second watch‚Ä¶ it‚Äôs not likely to go down‚Ä¶ but it might go up.
(For the full graphical review, check out One Mann‚Äôs Movies on t‚Äôinterweb and Facebook. Thanks.)
As this will be non-spoiler, I can‚Äôt say too much about the story. However, what I can is this: WAY DOWN‚Äôs story is quite dynamic in the sense that you won‚Äôt understand it till it wants you to. So, for the first half, your brain is fighting for hints and pieces to puzzle together the story. It isn‚Äôt until halfway through the movie that WAY DOWN invites you to the fantastic storytelling by Christopher Nolan.
Acting is beyond phenomenal, and I‚Äôd be genuinely surprised if neither Robert Pattinson nor John David Washington doesn‚Äôt receive an Oscar nomination for best actor. It‚Äôs also hard not to mention how good Elizabeth Debicki and Aaron Johnson both are. All around, great acting, and the dialogue amps up the quality of the movie.
The idea of this movie is damn fascinating, and while there are films that explore time-travelling, there‚Äôs never been anything quite like this. It has such a beautiful charm and for the most part, explains everything thoroughly. It feels so much more complex than any form of time-travelling we‚Äôve seen, and no less could‚Äôve been expected from Nolan.
Oh my lord, the score for this film fits so perfectly. Every scene that‚Äôs meant to feel intense was amped by a hundred because of how good the score was. Let me just say though, none of them will be found iconic, but they fit the story and scenes so well.
In the end, I walked out, feeling very satisfied. Nevertheless, I do have issues with the film that I cannot really express without spoiling bits of the story. There are definitely little inconsistencies that I found myself uncovering as the story progressed. However, I only had one issue that I found impacted my enjoyment. That issue was understanding some of the dialogue. No, not in the sense that the movie is too complicated, but more that it was hard to make out was being said at times. It felt like the movie required subtitles, but that probably was because, at a time in the film, there was far too much exposition.
Nevertheless, I loved this film, I‚Äôll be watching it at least two more times, and I think most of you in this group will enjoy it. I definitely suggest watching it in theatres if possible, just so you can get that excitement.
(4/5) & (8.5/10) for those that care about number scores.","","Way Down 2021 VOLLEDIGE FILM Streaming","Way Down 2021 VOLLEDIGE FILM Streaming","01/23/2021 23:59:00","0"
"3648","1188848","6597586","03/03/2021 04:50:25","## Task Details
Social Media is a relatively new phenomenon with a vast range of direction and style. Conduct an in-depth Exploratory Data Analysis on the dataset to find out what users are commenting about!

## Good Submission
A good submission should outline steps and processes, so an aspiring data scientist can learn and expand from your notebook!

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- [NLP with Disaster Tweets - EDA, Cleaning and BERT
](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)","","Exploratory Data Analysis on the Comments Data","Find patterns and discover insights with an in-depth EDA","","1"
"4767","1406689","6602076","06/13/2021 17:15:56","## Task Details
Use CNN to classify 1s and 0s","","Binary Classification","Classify 1s and 0s","","1"
"3806","1218991","6605353","03/18/2021 11:03:55","For the given ‚ÄúSBI_Historical_Data‚Äù. Below is the data description:

Column	Description
Date	date on which data is recorded
Price	Unique number assigned to each category of the video
Open	current day open point
High	current day highest point
Low	current day lowest point
Vol	the amount of a security that was traded during a given period of time. For every buyer, there is a seller, and each transaction contributes to the count of total volume.
Change %	% change in the current value and the previous day's market close



	Perform the following tasks:	
Q. 1	As part of EDA, perform the following tasks:
a.	Print dimensions of the data
b.	Dimensions of Dataset
c.	Statistical Summary
d.	Converting Date
e.	Check Data Type and Missing Values
f.	Index the dataset with Date	
Q. 2	Perform time series analysis:
a.	Visualize time series data
b.	Check Stationarity with:
-	ADF Test
-	KPSS Test
c.	Perform decomposing	
Q. 3	Forecast about the stock price using ARIMA. Steps to be performed:
a.	Parameter Selection using gridsearch
b.	Fit ARIMA model as per the selected optimum value of parameters
c.	Validate forecast
d.	Calculate the MSE and RMSE
e.	Visualize the forecast","","Forecast the Price","Using Time Series","","2"
"4183","1287848","6605504","04/25/2021 06:53:25","## Task Details
Create an interactive EDA on the geographical location of the billionaires and find out which place will have the most number of billionaires in upcoming years

## Expected Submission
The solution should contain a notebook or an web page of the Geospatial Analysis

## Evaluation
So, probably the best interactive and deductive notebook will be the best submission

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Geospatial Analysis","Build Interactive visualization of the Geographical Location of the Billionaires","","1"
"4713","1398360","6614570","06/10/2021 09:46:18","## Task Details
Figure out the  Day/ Night , Day of the week, Months  during which  Rail road accidents usually occur.

## Expected Submission
A notebook that outputs the time that accidents usually occur. Trespasser Casualties Over Hours should be visualized.

## Evaluation
A proper ranking of the times that accidents occur. The time that accidents usually occur should be at the top of the list together with the number of times accidents occurred during that specific time. Other times at lower ranks should follow. This is done to provide a more detailed solution.","","At what time do Rail Road accidents usually occur in the US","","","0"
"4714","1398360","6614570","06/10/2021 09:49:50","## Task Details
Which US state  and county that has the highest number of rail road accidents, and a description of the accidents that usually occur in that state in  recent 5 years.

## Expected Submission
A notebook that shows the state with the highest number of accidents and the main cause of the accidents in that state.
## Evaluation
All states should be ranked with the first one being the state with the highest accidents. This is done for better comparisons and to provide a more detailed solution.","","The state and county that has the highest number of accidents in recent 5 years","","","0"
"4715","1398360","6614570","06/10/2021 09:51:55","## Task Details
This is a countrywide  accident dataset. The purpose of this task is to visualize the dataset states wise.

## Expected Submission
A notebook with beautiful visualizations.","","Visualize The dataset","","","0"
"4716","1398360","6614570","06/10/2021 10:01:23","## Task Details
Were there any Suicide Causalities .? If yes in which states
Expected Submission

## Expected Submission
In Notebook , Analyze data for any suicide casualties and Visualize the data .","","Suicide Casualties","","","0"
"4717","1398360","6614570","06/10/2021 10:07:50","## Task Details
Find out how many people have died and how many were injured, state wise, year wise.

## Expected Submission
A notebook that shows the state wise  , Year wise  death & injuries .","","How many people died and how many were injured.?","","","0"
"4149","1287922","6627950","04/22/2021 07:21:31","Predict Covid cases for next 3 Quarter (3 Months)
- Create a dashboard to have real-time information about Country's Covid 19 status (Data Visualization)","","PREDICTION FOR THE MONTH OF MAY, JUNE AND JULY","","","1"
"3474","1153632","6638048","02/12/2021 07:50:48","## Task Details
American professional sports owners have contributed nearly $47 million in federal elections since 2015, according to research by ESPN in partnership with FiveThirtyEight, including $10 million to Republican causes and $1.9 million to Democratic causes so far in the 2020 election cycle.

## Expected Submission
The sports-political-donations.csv contains every confirmed partisan political contribution from team owners and commissioners in the NFL, NBA, WNBA, NHL, MLB, and NASCAR. Only contributions while owners were involved with the team are included. The data is from the Federal Election Commission and OpenSecrets.

## Evaluation
Questions that need to be answered:

1. Which party received the maximum donations?
2. Which Team, Team Owners, League made donations?
3. Shift in donations, owners, party, amount from 2016-2020?

### Further help
If you need additional inspiration, check out this article by FiveThirtyEight to have a better understanding of the scenario.

https://fivethirtyeight.com/features/inside-the-political-donation-history-of-wealthy-sports-owners/

**DO UPVOTE!**","","Inside The U.S. Political Donation History Of Wealthy Sports Owners (2016-2020)","","","1"
"4998","1446923","6648246","07/04/2021 13:36:34","Explore data using exploratory data analysis and also make visualizations.","","Make beautiful visualizations on players performance","","","1"
"4996","1446923","7526686","07/04/2021 13:28:02","Can you do innings by innings dataset of t20 matches?","","Dataset","","","0"
"4848","1419764","6654327","06/19/2021 18:38:47","In writing your first resume, you need to focus on the contents rather than the structure. Although it is important that you also use a quality formatting style, the contents of your resume will be the basis on whether a company will hire you or not. In this case, we will talk about one of the major types of resumes in the job market, the clerical resume. Clerical jobs are very important segments in any companies. That being the case, companies are always on the look for the most qualified applicants fit to do clerical tasks. We will give you some tips how to write your clerical resumes.

It is important that you know the basic parts of a resume. This will ultimately guide you to write a quality article that you can fill up with any details pertaining to your choice of job direction for [english homework help](https://mcessay.com/english-homework-help/). For your guidance, here are the basic parts of a resume:

Profile details ‚Äì it contains the information about you; name, address, contact information.
Objective sentence ‚Äì this is a simple part that speaks everything about your goal. You should put your main objective in applying for a certain position in a company.
Work experience ‚Äì it provide info about your past employers and your achievements and accomplishments in those companies.","","resume writing","","","0"
"4368","1338672","6654327","05/13/2021 15:47:39","The audience has to be considered as they get exposed to the original material. All the arguments should be presented considering this fact. The reader has to get a better view of the topic.

Each paragraph should try to elaborate on a separate point you wish to convey. Examples have a way of conveying much more than just an argument would. You have to be careful here. Like in an art essay, you would have to support your thesis statement with [programming homework help](https://mcessay.com/programming-homework-help/).
When you conclude, the reader should have reached a point where clarity is not an issue. You would have justified every statement you made in the body of the essay. The conclusion has to be brief and to the point.

Finally, if you have jotted down all the points that you could think of, relevant or irrelevant, you are in a position to expand on each and every point and then decide which would justify the thesis statement completely.
When you write an analytical essay, short and sweet would be the motto you would have to follow to capture your reader‚Äôs attention.","","essay writing","","","0"
"4222","1303851","6659698","04/29/2021 05:33:32","#Task Details
Every task has a story. Tell users what this task is all about and why you created it.

# Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

# Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualization","","","1"
"3832","1225872","6660745","03/22/2021 07:22:35","Explore and analyze the dataset and uncover any relevant insights from this public CMS Part D drug spending and utilization data.","","Utilization and Spending Analysis","","","0"
"4461","1156289","6666360","05/22/2021 18:09:28","# Optimize machine learning/ deep learning models to obtain effective results","","Stocks Sentiment Analysis","Use certain NLP techniques to obtain best results in predictions of Sentiment of Stocks tweets.","","0"
"5750","1458765","6700236","08/15/2021 15:21:24","# explore and submit :)","","Exploratory Data Analysis","Be creative","","0"
"4933","1433788","6700236","06/28/2021 08:06:03","Clean: Clean the data and Prepare for Analysis
- Map: Create Map with Geolocation to show the locations of current volcanoes.
- Plot: 
- Chart
- EDA","","Data Visualization","","","0"
"5749","1435854","6700236","08/15/2021 15:19:46","Use Any language to perform EDA","","Exploratory Data Analysis","","","0"
"3527","1153297","6709787","02/17/2021 17:49:35","## Task Details
Perform real time object detection on playing cards to create applications for card games using computer vision.","","Object detection","","","0"
"3591","1179056","6716004","02/24/2021 19:04:20","use machine learning classifying models to find out the best classifier for given data and compare each classifying model","","classification of audio tracks","","","0"
"3500","1158050","6725801","02/14/2021 19:28:38","You need to analyze the data and come up with the insights for below questions: 

1. Your insights will help management decide upon a minimum number of images to be made mandatory for a listing that would ensure bookings.

2. Also, come up with an optimal number of images that we can suggest the host to post along with a listing that would attract the most bookings and ensure success.","","Insights generation for bookings","","","0"
"4581","1380275","6752868","05/31/2021 17:28:20","1.identify cost savings regular product and purchasing trends
2.which company code has more order according to plant location
3.price forecasting trend of order budget of coming year
4.which material group has more contribution on price
5.purchs doc and item are primary key
6.Cluster the regular purchase product
7.80% of cost and 20% material","","spend analysis","","06/02/2021 23:59:00","0"
"3586","1168505","6758536","02/23/2021 16:37:01","## Task Details
The usage statistics are interesting on their own, but they could include even more helpful information. The idea behind this task is to take Pokedex data from this [dataset](https://www.kaggle.com/fruityfritz/generation-8-pokedex) and merge it with the usage statistics given here.

## Expected Submission
The submission should simply be a `csv` or similar type of data file that contains information from the usage statistics and Pokedex datasets. There should be a dataset for each month of usage statistics data.

## Evaluation
There are two main criteria:
-  Make sure the submission can iterate over all the datasets in this database or the final dataset is made by calling a function.
- Make sure your code is generalizable to new months of data.","","Merge Usage and Pokedex Data","Include typical Pokedex data with the Usage data","","0"
"3585","1168600","6758536","02/23/2021 16:28:52","## Task Details
The moveset data contains information on the relative percentage of items used by each Pokemon. This task is meant to synthesize that information to determine which items are the most commonly used in general.

## Expected Submission
This solution can be solved using notebooks primarily and the datasets provided in this database. Submissions can use data from each month separately or the aggregated data - whatever the user would like. 

## Evaluation
The top five items should be displayed. There are two ways to consider the items usage statistics:
1. **Unweighted**: Under this scheme, you would look at each of the items used by each Pokemon and simply count the instances of a certain item. The item with the most instances would be the most common.
2. **Weighted**: A more complex procedure would be to take the ideas from the Unweighted scheme and weight the item instances by Pokemon usage. For instance, Incineroar has a high usage statistic which would weight the items typically used on this Pokemon more than a Pokemon like Crobat who has much less usage.","","Most Common Items","What are the most common items in VGC Series 7?","","0"
"3548","1170243","6762435","02/20/2021 00:43:08","Collect images for Military Tanks and store them in the respective folder. 
Larger the dataset, the better it is.","","Completion of Image Data Collection","","03/31/2021 23:59:00","0"
"4266","1316317","6762435","05/04/2021 14:41:13","Train different variants of yolov4 
* yolov4 large
* yolov4- csp
* yolov4-tiny","","Experiment with various variants of yolov4","yolov4-large, yolov4-csp, yolov4-tiny","06/30/2021 23:59:00","0"
"4269","1316317","6762435","05/04/2021 23:47:39","Experimentation with various combinations of yolov5.","","Perform experimentation using yolov5","Experiment using various possible variants and combinations of yolov5","","0"
"4270","1316317","6762435","05/04/2021 23:48:38","Try various combinations for mobilenet ssd v2 and report the best possible combination.","","Experiment with MobileNet SDD v2","","06/30/2021 23:59:00","1"
"4975","1444034","5156746","07/02/2021 15:59:53","Acupressure Reflexology to clear the Brain Paranasal Area

Task Details -
Do Dormant Viruses or Plaque Build Up cause shingles in older Adults?

Are heart attacks and skin infections related to ingrown toenails that cause pus to build up as plaque in the body, creating acne, cyst, cavities and ulcers?

- Universalis Alopecia 
-Cicatricial Alopecia
-Lichen Planopilaris
-Androgenetic Alopechia
-Telogen
-Discoid Lupus
-Areta
-cytamegolo
-Porokeratosis


Expected Submission
Images of Toenails and Finger nails


Evaluation
how the skin and nail grows back after cleaning

Further help - YouTube Channels
-Podologia Integral
-DC Foot Doctor
-DR. Toe Bro
-Jaws Podiatry
-The Meticulous Manicurist","","Test of Leprosy","Leviticus 13","09/29/2021 23:59:00","0"
"4755","1405619","6773938","06/13/2021 06:27:59","## COvid CLusters DAtaset

* Hey there Data lovers this dataset can be used to perform simple EDA","","Perform EDA on the Covid Clusters Datasets","","","1"
"4828","1416132","6776149","06/18/2021 09:12:46","Loading dataset
As a training dataset, we will use pairs of short English and Russian sentences (source: http://www.manythings.org/anki/). Let's take the first 10,000 phrases (they are sorted by length, so we'll take the shortest ones for simplicity).

For this code to work, you need to load the rus.txt file into Colab.

We read the lines from this file, parse them and put the sentences in the input_texts andtarget_texts lists (input and output sentences, respectively).

Preparing dictionaries
As before, we will use one character (not a word) as an element of the sequence. This is fine for our simple short sentence task.

Let's prepare two dictionaries (mapping index to character and character to index), and we will do this for input texts (input_texts) and output texts (target_texts), since they are in different languages and consist of different characters.

In addition, we need special tokens for the beginning and end of the chain (","","Machine translate","Test your knowledge in machine translation","","0"
"4829","1417162","6776149","06/18/2021 09:49:04","Classificate genre of a film using nlp","","Classification","","","0"
"4404","1343787","6788409","05/16/2021 09:26:22","## Task Details
Cluster users based on their frequent connection between 110 different access points distributed on different geolocation.

## Expected Submission
The best cluster between users and corresponding representation","","Cluster Users based on frequent access on Access points","","","0"
"3981","1253797","6799648","04/06/2021 08:33:46","Predict the percentage of an student based on the no. of study hours. 
This is a simple linear regression task as it involves just 2 variables.
You can use R, Python, SAS Enterprise Miner or any other tool","","Prediction using Supervised ML","","","0"
"3982","1253846","6799648","04/06/2021 08:51:10","Use R or Python or perform this task","","From the given ‚ÄòIris‚Äô dataset, predict the optimum number of clusters  and represent it visually","","","0"
"3985","1253890","6800718","04/06/2021 19:31:59","The ""FineTech"" company launch there android and iOS mobile base app and want to grow there business. But there is problem how to recommended this app and offer who really want to use it. So for that company decided to give free trial to each and every customer for 24 hour and collect data from the customers. In this scenario some customer purchase the app and someone not. According to this data company want to give special offer to the customer who are not interested to buy without offer and grow there business.
Follow the ‚ÄúDirecting Customers to Subscription Through Financial App Behavior Analysis ‚Äù","","Directing Customers To Subscription Through Financial App Behavior Analysis","","","3"
"4850","1419750","6805697","06/19/2021 22:42:24","## Task Details
Right now the model and its associated repo are not working on Kaggle due to an out-of-memory error. This is likely fixable, and may be fixed at the repo level. In the meantime, does anyone have ideas? Submit them here.","","Find a way to make it work","Kaggle TPU goes OOM, but it ought to be fixable","","0"
"3619","1185933","6829001","02/28/2021 21:16:51","## En √ßok toprak kaybeden padi≈üah Sultan Abdulhamid Han deƒüil, me≈ürutiyet d√∂nemi sonrasƒ± olduƒüu i√ßin toprak kaybƒ±na neden olan ittihat ve terakki cemiyeti uzantƒ±larƒ±dƒ±r.

### ittihat ve terakki cemiyeti uzantƒ±larƒ± sayesinde Tunus, Mƒ±sƒ±r, Kƒ±brƒ±s, Sƒ±rbƒ±stan, Karadaƒü ve Romanya olmak √ºzere 1 milyon 592 bin 806 kilometre kare toprak kaybedilmi≈ütir.
Sultan Abd√ºlhamid Han ƒ±n tahtta kaldƒ±ƒüƒ± tarihler 31 Aƒüustos 1876 ‚Äì 27 Nisan 1909

### Birinci Me≈ürutiyet, Osmanlƒ± ƒ∞mparatorluƒüu'nda 23 Aralƒ±k 1876'da II. Abd√ºlhamid tarafƒ±ndan ilan edilen, anayasal monar≈üi rejiminin ilk d√∂nemi.
Me≈ürutiyet d√∂nemi meclis d√∂nemine ge√ßi≈ü s√ºrecidir yani tek karar merci meclistir.

## ƒ∞ttihat ve terakki uzantƒ±larƒ±nƒ±n  Ba≈üarƒ±sƒ±z ≈üekilde y√∂netmesi elbette toprak kayƒ±plarƒ±na neden olacaktƒ±r.
Zaten tek karar merci padi≈üah olmu≈ü olsaydƒ± ne toprak kaybƒ± olur ne de kendi ba≈üarƒ±sƒ±zlƒ±klarƒ±nƒ± padi≈üaha y√ºkleyip onu tahttan indirebilirlerdi.

## Gelin ondan √∂ncesine gidelim bakƒ±n ne yapmƒ±≈ü bu hain uzantƒ±lar

### Daha √∂nce Sened-i ƒ∞ttifak (1808) ile ba≈ülayan, padi≈üahƒ±n yetkilerini kƒ±sma s√ºreci, Tanzimat (1839) ve Islahat (1856) Fermanlarƒ± ile devam etmi≈ü, ancak Ali ve Fuat Pa≈üalarƒ±n √∂l√ºmleriyle birlikte bir duraklama evresine girmi≈üti.

### ƒ∞lerleyen yƒ±llarda ƒ∞talya‚Äônƒ±n ihtilal mantalitesine sahip olan Carbonari √∂rg√ºt√ºn√º √∂rnek alarak 1865 yƒ±lƒ±nda ‚ÄúGen√ß Osmanlƒ±lar Cemiyeti‚Äù adƒ± altƒ±nda gizli bir √∂rg√ºt kurmu≈ülar ve me≈üruti rejimi kurmak i√ßin ihtilal yoluna ba≈üvurmu≈ülardƒ±. ƒ∞lk olarak da, 1867 yƒ±lƒ±nda Babƒ±√¢li‚Äôyi basarak Sadrazam Ali Pa≈üa‚Äôyƒ± indirerek Mahmut Nedim Pa≈üa‚Äôyƒ± sadrazamlƒ±ƒüa getirmek istemi≈ülerdi. Ancak h√ºk√ºmet bu durumu √∂nceden haber almƒ±≈ü ve cemiyetin pek √ßok √ºyesi tutuklanmƒ±≈ütƒ±. Kurtulabilenler ise Avrupa‚Äôya ka√ßmƒ±≈ülar ve ‚Äúsadrazam Fuad Pa≈üa ile ge√ßinmediƒüi i√ßin Avrupa‚Äôya gitmi≈ü olan Mustafa Fazƒ±l Pa≈üa‚Äônƒ±n ekonomik desteƒüinde, daha sistemli ve g√ºr sesli bir muhalefeti ba≈ülatmƒ±≈ülardƒ±‚Äù.Etkin muhalefetlerini bir s√ºre daha devam ettiren cemiyet; mali sƒ±kƒ±ntƒ±, Sultan Abd√ºlaziz‚Äôin √ºmit verici konu≈ümalarƒ± ve 1871 yƒ±lƒ±nda Ali Pa≈üa‚Äônƒ±n √∂l√ºm√º gibi nedenler ƒ±≈üƒ±ƒüƒ±nda muhalefetlerine ara vermi≈üler ve √ºyelerin b√ºy√ºk bir b√∂l√ºm√º √ºlkelerine d√∂nm√º≈ülerdi.


### Ancak, me≈üruti bir idarenin kurulmasƒ±nƒ± ana gaye edinen ve ba≈üƒ±nƒ± Mahmut Nedim Pa≈üa‚Äônƒ±n √ßektiƒüi grup, bu y√∂ndeki faaliyetlerine devam etmi≈üler ve bu uƒüurda yapƒ±lacak her yolu da m√ºbah g√∂rm√º≈ülerdi. Nitekim, √ºmitlerini kestikleri Sultan Abd√ºlaziz‚Äôi bir darbe ile tahttan indirerek bunu a√ßƒ±k√ßa g√∂stermi≈ülerdi.Sultan Abd√ºlaziz‚Äôi tahttan indiren darbeciler, birka√ß g√ºn sonra onu √∂ld√ºrerek intihar s√ºs√º vermi≈üler ve i≈übirliƒüi i√ßinde olduklarƒ± V. Murad‚Äôƒ± tahta √ßƒ±karmƒ±≈ülardƒ±. Ne yazƒ±k ki, me≈ürutiyetin ilanƒ± i√ßin umut baƒüladƒ±klarƒ± V. Murad‚Äôƒ±n, ya≈üanan geli≈ümelerden dolayƒ± ruh saƒülƒ±ƒüƒ± bozulmu≈ü ve V. Murad, bu halinden dolayƒ± sadece 3 ay tahtta kalabilmi≈üti. V. Murad‚Äôƒ±n da b√∂ylece tahttan inmesi √ºzerine, Veliaht Abd√ºlhamid Efendi‚Äônin √∂nemi bir kat daha artmƒ±≈ü ve me≈ürutiyet√ßiler, istemeyerek de olsa Abd√ºlhamid‚Äôe umut baƒülamƒ±≈ülardƒ±.
Bunun √ºzerine Mithat Pa≈üa, Abd√ºlhamid ile g√∂r√º≈üm√º≈ü ve me≈ürutiyeti ilan edeceƒüini vadeden Abd√ºlhamid, 31 Aƒüustos 1876 Per≈üembe g√ºn√º b√ºy√ºk bir t√∂renle tahta c√ºlus etmi≈üti.Balkanlarƒ±n fokur fokur kaynadƒ±ƒüƒ±, √ºst √ºste gelen taht deƒüi≈üiklikleri, devletin i√ßine girdiƒüi ekonomik sƒ±kƒ±ntƒ±lar gibi kaotik bir d√∂nemde tahtta ge√ßen II. Abd√ºlhamid‚Äôi bekleyen pek √ßok sorun vardƒ± ve bunlardan biri de me≈ürutiyetin ilanƒ± hadisesiydi. Nihayet Mithat Pa≈üa‚Äônƒ±n tazyikleri ile anayasa √ßalƒ±≈ümalarƒ± ba≈ülamƒ±≈ü ve 23 Aralƒ±k 1876 tarihinde Osmanlƒ± Devleti‚Äônin ilk anayasasƒ± olan Kanun-ƒ± Esasi y√ºz bir pare top atƒ±≈üƒ±yla ilan edilmi≈üti.Sened-i ƒ∞ttifak yani PADƒ∞≈ûAHIN YETKƒ∞LERƒ∞Nƒ∞ KISMA S√úRECƒ∞ ile ba≈ülayan s√ºrecin √∂nemli bir √ºr√ºn√º olarak ortaya √ßƒ±kan Kanun-ƒ± Esasi‚Äônin alelacele ilan edilmesinde ‚Äúdiplomatik bir taktik‚Äù vardƒ±. ≈û√∂yle ki, ‚Äú1875 Hersek Ayaklanmasƒ±‚Äôyla ba≈ülayan olaylar Rusya ile Osmanlƒ± Devletini kar≈üƒ± kar≈üƒ±ya getirmi≈ü ve Osmanlƒ± kuvvetlerinin 29 Ekim1876‚Äôda tarihinde Osmanlƒ± Devleti‚Äônin ilk anayasasƒ± olan Kanun-ƒ± Esasi y√ºz bir pare top atƒ±≈üƒ±yla ilan edilmi≈üti.Sened-i ƒ∞ttifak ile ba≈ülayan s√ºrecin √∂nemli bir √ºr√ºn√º olarak ortaya √ßƒ±kan Kanun-ƒ± Esasi‚Äônin alelacele ilan edilmesinde ‚Äúdiplomatik bir taktik‚Äù vardƒ±. ≈û√∂yle ki, ‚Äú1875 Hersek Ayaklanmasƒ±‚Äôyla ba≈ülayan olaylar Rusya ile Osmanlƒ± Devletini kar≈üƒ± kar≈üƒ±ya getirmi≈ü ve Osmanlƒ± kuvvetlerinin 29 Ekim1876‚Äôda Morova‚Äôda Sƒ±rp kuvvetlerine saƒüladƒ±ƒüƒ± √ºst√ºnl√ºkten sonra Rus h√ºk√ºmeti Babƒ±√¢li‚Äôye bir √ºltimatom vermi≈üti. Bu d√∂nemde Osmanlƒ±‚Äônƒ±n toprak b√ºt√ºnl√ºƒü√ºnden yana olan ƒ∞ngiltere ise bu √ºltimatoma kar≈üƒ± bir konferansƒ±n toplanmasƒ± √∂nerisinde bulunmu≈ütu. Bu √∂nerinin kabul edilmesi √ºzerine B√ºy√ºk Devletler, ‚ÄúDoƒüu Sorunu‚Äùnun √ß√∂z√ºm√ºnde √ßƒ±karlarƒ±nƒ± uzla≈ütƒ±rmak i√ßin ƒ∞stanbul‚Äôda bir araya gelmi≈ülerdi‚Äù.ƒ∞stanbul Konferansƒ± ya da toplandƒ±ƒüƒ± Hali√ß Tersanesi‚Äône izafeten Tersane Konferansƒ± 23 Aralƒ±k‚Äôta toplanmƒ±≈ü ve ‚Äúb√ºt√ºn devlet temsilcilerinin hazƒ±r bulunduƒüu esnada ‚Äòdƒ±≈üarƒ±dan deh≈üetli bir surette atƒ±lmaya ba≈ülayan top sesleri‚Äô duyulmu≈ütu. Bunun √ºzerine Hariciye Nazƒ±rƒ± s√∂z alarak delegelere, padi≈üahƒ±n halkƒ±n me≈üru isteklerine g√∂re uygulanmasƒ±nƒ± gerekli g√∂rd√ºƒü√º yeni idare y√∂nteminden ve me≈ürutiyet idaresinin getirdiƒüi √∂zg√ºrl√ºklerden bahisle, ‚Äòbu inkƒ±l√¢p kar≈üƒ±sƒ±nda toplantƒ±nƒ±n zait(gereksiz) kaldƒ±ƒüƒ±nƒ±‚Äô dile getirmi≈üti‚Äù. Ancak delegeler Kanun-ƒ± Esasi‚Äônin ilanƒ±na pek itibar etmemi≈üler ve bu hareketi, ‚Äúdiplomatik bir taktik‚Äù olarak addetmi≈ülerdi. Sonrada, hi√ßbir ≈üey olmamƒ±≈ü gibi kaldƒ±klarƒ± yerden g√∂r√º≈ümelere devam etmi≈ülerdi. Me≈ürutiyetin ilanƒ±, konferansa katƒ±lan delegeler √ºzerinde herhangi bir olumlu tesir uyandƒ±rmamasƒ±na raƒümen, ƒ∞stanbul‚Äôda bulunan Gayr-ƒ± M√ºslim cemaat √ºyeleri tarafƒ±ndan b√ºy√ºk bir co≈üku ve bayram havasƒ± i√ßinde kutlanmƒ±≈ütƒ±. Neticede, Osmanlƒ± Devleti, devletin b√ºt√ºnl√ºƒü√ºne ve baƒüƒ±msƒ±zlƒ±ƒüƒ±na b√ºy√ºk bir darbe vuran maddeler ihtiva eden konferansƒ±n kararlarƒ±nƒ± reddetmi≈ü ve zaten sava≈ü i√ßin fƒ±rsat kollayan Rusya, bunu bahane ederek Nisan 1877‚Äôde Osmanlƒ± Devleti‚Äône sava≈ü ilan etmi≈üti. B√∂ylece, Osmanlƒ± Devleti i√ßin b√ºy√ºk bir felaketle sonu√ßlanan 1877‚Äì78 (93 Harbi) Osmanlƒ±-Rus sava≈üƒ±...

Abd√ºlhamid han tahttan indirilme meselesi


Serefsiz ƒ∞ttihat Terakkicilerin √∂n planda olduƒüu Meclis-i Mebusan Sultan II.Abd√ºlhamit'i  25 Nisan 1909'da tahttan indirilmesi y√∂n√ºnde karar verdi.","","En √ßok toprak kaybeden padisah","En √ßok toprak kaybeden padisah kimdir","","0"
"3636","1187084","6833379","03/02/2021 03:21:08","## Task Details
1. Collect at least 1000 RuBISCO proteins structure in 3d.
2. Install rust 
3. Build cluster in Kubernetes.
4. Generate Deep Fake method on the data. 
5. Store images in same folder under Gan. 
6. Build Git, Git-lab, and bit-bucket Repository 

## Expected Submission
1. Git repository
2. Whitepaper or demonstration
3. Kubernetes Results 
        1. No of clusters [at-least(1)]
        2. CPU/GPU/TPU, Cloud, ANSI [at-least(1)]
        3. Members [at-least(1)]
        4. Cluster Health (&gt;75-85)
        5. Noise ratio(0.01-5.00)
 4. Solution build up on rust language. 
## Evaluation
Good Solution results ratio 75-90%, low latency, low noise. 

### Further help
email: wizdwarfs@gmail.com","","Generate Artifical Proteins by using GAN and generate protein data (.pdb & .stl)","","","1"
"3647","1187740","6836449","03/03/2021 02:53:24","Shrimp are at the bottom of the food chain, and base resources like this dataset can help you lay a foundation for how shrimp behave. Finding a correlation between the behavior of shrimp and the presence of fish can show how predators affect creatures at the bottom of the food chain. Don't forget to account for other variables, such as the time of day or size of the pool.","","Find correlation between quantity of shrimp and presence of fish/time of day","","","1"
"4697","1398268","6840786","06/09/2021 09:30:02","## Task Details
THIS DATASET IS A CLEANED VERSION OF SOME GERMAN,ENGLISH PAIRS.THIS DATASET CAN BE USED FOR BUILDING A NEURAL MACHINE TRANSLATOR THAT CONVERTS ENGLISH SENTENCES TO GERMAN AND VICE VERSA.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NEURAL MACHINE TRASLATION","","","0"
"5093","1441802","6851005","07/12/2021 09:29:46","## Task Details
Why do some breast cancer patients die and others survive? Can we identify mechanisms and/or risk factors that could help understand this problem?

## Expected Submission
A notebook describing your thinking around breast cancer risk factors based on the provided multi-omics dataset. What are the linear/trivial factors? How do different factors play together to predict risk of death?

## Evaluation
A good solution tackles the problem from various angles. Do not overcomplicate, try to keep it simple and provide ideas rather than solutions.","","Identify Risk Factors in Breast Cancer","A multi-omics investigation","09/30/2021 23:59:00","1"
"3695","1200014","6862491","03/08/2021 18:08:05","## Task Details
Fantasy premier league allocates points to each player depending on their actual game performance that week.

## Expected Submission
With the help of this particular data set, you can predict the performance of a player for each gameweek in FPL. With this, you can create an optimal fantasy team to maximize your points. Team selection should be done according to the FPL rules.

## Evaluation
The model must predict the total points a player is likely to get each gameweek.","","Player Performance Prediction","","","1"
"4655","1391792","6873711","06/06/2021 08:56:42","Compare the result of Logistic regression with different classification models and visualize.","","Classification using Logistic Regression.","Use random_state = 123","","1"
"4815","1413996","6873711","06/17/2021 16:09:19","Explore data and prepare a predictory model using different target column..","","Exploratory Data Analysis","","","0"
"6259","1290722","8514513","10/05/2021 06:03:58","Mini Project
## Task Details 
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Traffic flow pattern detection in the city based on the Bikeshare application  data.","Mini Project","10/06/2021 23:59:00","0"
"3788","1211161","6902991","03/16/2021 16:35:54","Objectives:
1.	Become familiar with building databases using Pyramid Analytics.
2.	Learn and apply simple data visualisation techniques to better understand data.
3.	Develop sound conclusions based on data.
4.	Have fun :D

The sample scenario for this dataset is as follows:

You have been given the results of a survey conducted recently covering three specific communities by zip code.  The dataset includes response to questions including education, employment status, income, homeownership, banking, and utilization of different services that already established in the area.  

ABC, a venture capitalist company, has hired you as a consultant to provide investment recommendations based on the current consumer behavior and demographics in their target region.  Utilizing different visualization techniques and analysis, please answer the following questions below:

1.	What relationships between the different data are you able to observe?
2.	Are there any specific target demographic groups that would be interesting for ABC?
3.	Is there a zip code in the region better suited for ABC?
4.	What are the industries and services that ABC should pay particular attention to?
5.	Supported by data, could you identify and make a case for emerging opportunities that ABC should consider?  Why? 
6.	As someone with domain knowledge in marketing, list two other information you can uncover with the help of data visualisation techniques. 
7.	Based on your analysis, what three recommendations are you going to give ABC?
8.     Name one other data point you'd like to incorporate in your analysis.

Be as creative as you like and use different visualization techniques to come up with your answer.

Submit screenshots of your visualization with your responses.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Identifying Investment Opportunities in a Community by Analyzing Consumer Spending Habits","For Beginners","","0"
"3789","1208329","6902991","03/16/2021 17:29:03","This project was designed for Pyramid Analytics.  However any analytics can be used on the data set.  For reference. the accompanying directions and screenshots for Pyramid can be found [here](https://drive.google.com/file/d/1FVtqKNHaloP0uRj19vmgOZLnRlXfnOYU/view?usp=sharing).","","Finding Whales, Baby Whales, and Potential Whales","Exercises for Beginners Just for Fun!","","0"
"4856","1420878","6908661","06/20/2021 13:02:43","Make object Detection  use tensorflow 2 model zoo","","Object Detection","","12/20/2022 23:59:00","1"
"4857","1421042","6908661","06/20/2021 13:17:55","make object detection use tensorflow 2 model zoo or custom models","","Object Detection","","12/30/2022 23:59:00","1"
"4142","1285406","6909319","04/21/2021 06:19:16","## Task Details
You are required to use the training dataset to identify patterns that predict default. Apply the patterns on test dataset to identify ‚Äúpotential‚Äù defaulters.","","identify customers with risk_flag","","","3"
"3880","1236179","6921900","03/27/2021 14:33:46","# Begin your journey of the world of Data Scientist with a small piece of cakeüòÑ","","First Task","","","0"
"3881","1236249","6921900","03/27/2021 15:15:25","# Do whatever you like but go inside learn first üòÉ","","Task 1","","","1"
"4449","1354437","6921900","05/20/2021 06:21:45","Do whatever you like, just learn üòä","","Learn and share","","","1"
"4242","1309296","6921900","05/01/2021 17:51:27","just learn everyday","","Do whatever u like...","","","0"
"3851","1228848","6929669","03/23/2021 16:32:33","Task Details :

**Recommendation system is required in subscription-based OTG platforms. **
Recommended engine generally in three types 1.content Based recommended engine 2.collaborative recommender engine and 3.hybrid recommended engine
Expected Submission

With the help of this particular data set, you have to build a recommended engine. And your recommended engine will return a maximum of 10 movies name if a user searches for a particular movie.
Evaluation

Recommended engine must return 5 movie names and maximum it can return 10 movie names if task Details :","","Movie Recommendation System","","05/03/2021 23:59:00","1"
"3852","1228848","6929669","03/23/2021 16:33:56","Netflix is known for its strong recommendation engines. They use a mix of content-based and collaborative filtering models to recommend tv shows and movies. In this task, one can create a recommendation engine based on text/description similarity techniques.","","What to watch on Netflix ?","","05/02/2021 23:59:00","0"
"3759","1209362","6930714","03/13/2021 23:36:52","Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","11111111111111111111","1111111111111111111111","03/18/2021 23:59:00","0"
"3803","1217808","6934003","03/18/2021 00:40:08","## Task Details
Build a model that‚Äôs able to detect spam posts within out dataset.

## Expected Submission
Solve the problem within a notebook. The output should be post ids with suspected spam.

## Evaluation
We will compare your outputted spam post ids against our database of known spam posts based on user reports and manual moderator flagging.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Detect spam","","06/17/2021 23:59:00","0"
"4928","1434320","6938559","06/27/2021 12:28:57","These are the assignment questions.

Questions on operators:

**Q 1.**	Working with **arithmetic operators**:
a.	Add 5 to the fifth record of ‚ÄòMonthlyCharges‚Äô column
b.	Subtract 9.65 from the sixth record of ‚ÄòMonthlyCharges‚Äô column
c.	Multiply the 1st record of ‚ÄòMonthlyCharges‚Äô column with 3
d.	Divide the 37th record of ‚ÄòMonthlyCharges‚Äô column with 3

**Q 2.**	Working with the **relational operators**:
a.	Check if the value of ‚Äòtenure‚Äô in the 1st row is greater than the value of ‚Äòtenure‚Äô in the 10th row
b.	Check if the value of ‚Äòtenure‚Äô in the 3rd row is equal to the value of ‚Äòtenure‚Äô in the 5th row

**Q 3.**	Working with **logical operators**:
a.	Get the count of those customers who has theseThese are the assignment questions.
b.	Extract those customers whose ‚ÄòInternetService‚Äô is either ‚ÄòDSL‚Äô or ‚ÄòFiber optic‚Äô & store the result in ‚ÄòInternet_dsl_fiber‚Äô","","Introduction to ""R"" Assignment Module 1","Assignment Module 1","","1"
"5083","1434320","6938559","07/11/2021 07:10:25","""The path to get success is to practice your weaknesses."" 
Here in this assignment, I am coding for the Data exploration.

**Data Exploration Assignment**

**Problem Statement:**

Consider yourself to be Sam, who is a Data Scientist at ‚ÄòVerizon‚Äô. In the past 6 months customers of your company have been churning out to other networks. So, you as a data scientist have to understand what are the factors leading to the customers churning out and what can you do to reduce the churn rate.
Sam starts off with data exploration. So, Sam wants to understand the structure of the data first.

**Questions on Data Extraction:**

1.	Extract these individual columns with the ‚Äò$‚Äô symbol:
1. a.	Extract ‚ÄòInternetService‚Äô column and store it in ‚Äòcustomer_internet_service‚Äô
1. b.	Extract ‚ÄòPaperlessBilling‚Äô column and store it in ‚Äòcustomer_Paperless_Billing‚Äô
1. c.	Extract ‚ÄòStreamingTV‚Äô column and store it in ‚Äòcustomer_Streaming_TV‚Äô

2.	Extract the 3rd, 6th and 9th columns from the ‚Äòcustomer_churn‚Äô data.frame & store it in ‚Äòcustomer_random_columns‚Äô

3.	Extract all the columns from column number-10 to column-number 20 and store the result in ‚Äòcustomer_10_20‚Äô

4.	Extract only these row numbers: 65, 765, 3726 & 7000 and store the result in ‚Äòcustomer_random_rows‚Äô

5.	Extract all the rows starting from row number-655 to row number-6550 & store the result in ‚Äòcustomer_continuous_rows‚Äô

6.	Extract row numbers- 10, 100 & 1000 & column numbers- 5, 10, 15 & store the result in ‚Äòcustomer_random_data‚Äô


Now, Sam decides to implement some flow control statements to explore the data


**Questions on Flow Control Statements:**

1.	Check if the value in the 6th cell of ‚ÄòPaymentMethod‚Äô column is ‚ÄòElectronic check‚Äô. If yes, print ‚ÄúYes, the payment method is Electronic check‚Äù

2.	Check the value present in 12th cell of ‚ÄòContract‚Äô column. 
If it‚Äôs ‚Äòmonth-to-month‚Äô, print ‚ÄòThe contract is on a month to month basis‚Äô
If it‚Äôs ‚ÄòOne year‚Äô, print ‚ÄòThe contract is on a yearly basis‚Äô
If it‚Äôs ‚ÄòTwo year‚Äô, print ‚ÄòThe contract is on a two-year basis‚Äô

3.	Use switch to check the gender in 6th cell of ‚Äògender‚Äô column.
If it‚Äôs ‚ÄòMale‚Äô, give a discount of 20% in ‚ÄòMonthlyCharges‚Äô
If it‚Äôs ‚ÄòFemale‚Äô, give a discount of 50% in ‚ÄòMonthlyCharges‚Äô

4.	Use for loop to get the count of customers whose ‚ÄòInternetService‚Äô is ‚ÄòDSL‚Äô

5.	Use while to find the number of customers whose tenure is exactly ‚Äò2‚Äô months

After this, Sam would be applying some inbuilt functions on the data to get some insights

**Questions on Inbuilt Functions:**

1.	Do these operations with the head() function:
1. a.	Get the first 4 records from ‚ÄòPhoneService‚Äô column
1. b.	Get the first 8 records from ‚ÄòContract‚Äô column

2.	Do these operations with the tail() function:
2. a. Get the last record of ‚ÄòTotalCharges‚Äô column
2. b. Get the last 5 records of ‚Äòtenure‚Äô column

3.	Find the average, minimum, maximum & range from the ‚Äòtenure‚Äô column

4.	Get 10 random values from the ‚ÄòTotalCharges‚Äô column using sample()

5.	Find the count of different levels in ‚ÄòPaymentMethod‚Äô & ‚ÄòContract‚Äô columns using table()","","Data Exploration using ""R"" Assignment Module 2","R- Assignment module 2","","0"
"5084","1434320","6938559","07/11/2021 07:25:28","**Data Structures in R Assignment**

**Problem Statement:**

Implement these data structures in R to get a better understanding
Questions:

1.	Create these vectors:
1. a.	A character vector named ‚Äòfruits‚Äô with these values: ‚ÄòApple‚Äô, ‚ÄòGuava‚Äô, ‚ÄòBanana‚Äô, ‚ÄòMango‚Äô
1. b.	A numeric vector named ‚Äòhundred‚Äô comprising of the first 100 natural numbers
1. c.	A logical vector named ‚Äòlogic_game‚Äô with these values: ‚ÄòTRUE‚Äô,‚ÄôTRUE‚Äô,‚ÄôFALSE‚Äô,‚ÄôFALSE‚Äô

2.     Create a list named ‚Äòjumbo‚Äô which comprises of:
2. a. A character vector comprising of alphabets from A to D
2. b. A numeric vector comprising of numbers from 55 to 60
2. c. A logical vector comprising of just these two values: True, False
2.c. i. Now, access the third value from the first element of the list
2.c. ii. Access the 2nd value from the 2nd element of the list
2.c. iii. Access the 1st value from the 3rd element of the list

3.	Create a matrix named ‚Äòfour_trouble‚Äô, with the numbers 1 to 16. The matrix should have 4 rows & 4 columns
3. a.	Arrange the elements by row

4.	Create an array named ‚Äòsky_maze‚Äô with the numbers 1 to 32. The array should comprise of two 4*4 matrices.","","Data Structures in ""R"" Assignment module 2","""R"" Assignment Module 2","","0"
"4445","1353809","6940204","05/19/2021 19:36:05","Apply a preexisting sentiment analysis model on this dataset to track how the general sentiment of the paper has changed over time.

Alternatively, track how the sentiment towards different topics has changed over time.","","Apply sentiment analysis model to news titles, track changes over time","","","0"
"4205","1300326","6953359","04/27/2021 18:02:56","## Task Details
Determine with what probability a part of the text contains a bibliographic link. To do this, you must first split the text with a sliding window into equal segments, for example, 250 or 300 lengths.

## Evaluation
In this experiment, completeness is more important, that is, it is necessary to select all windows containing bibliographic references.","","Punctuation marks","Highlighting bibliographic descriptions","","1"
"3929","1234246","6966166","04/01/2021 14:10:44","Essentially, using Natural Language Process Methods, create a model that can differentiate between progressive rock and pop lyrics.","","Create a Model that predicts a songs genre by its lyrics.","","","2"
"4211","1302448","6975336","04/28/2021 13:12:14","## Task Details
 As a mobile application developer/company, I want to get a rough idea about my future earning and what can be increased the earnings.

## Expected Submission
Visualization and earnings Predictions for the next 30 days","","series analysis and Predictions","","","0"
"4728","1392611","7648505","06/11/2021 03:31:36","## Task Details
Every task has a story. Tell users what this task is all about and why you created it

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Dog find","Dog","","0"
"3933","1245958","6990402","04/01/2021 17:55:41","## Task Details
Create a new data frame with 3 columns to have these values Month, Year, Number of Orders","","Create a new data frame with 3 columns to have these values Month, Year, Number of Orders","","","1"
"3934","1245958","6990402","04/01/2021 17:58:50","## Task Details
 Number of orders for each country and region combination","","Number of orders for each country and region combination","","","1"
"3993","1245958","6990402","04/07/2021 17:41:58","## Task Details
EDA USING VISUALISATIONS","","EDA USING VISUALISATIONS","","","1"
"4396","1295524","6990402","05/15/2021 17:39:49","MAKE EDA","","EXPLORATORY DATA ANALYSIS","EDA","","0"
"4155","1288453","6994541","04/22/2021 12:11:04","## Task Details
Extend the dataset to other Gym environments. Code for running the algorithm is [here](https://github.com/Ryan-Rudes/minimal_goexplore). Make sure to implement frame saving using an image library such as `opencv-python` when a new cell is discovered. The code linked above excludes this component, for it is just a pure implementation of the algorithm itself.","","Extend dataset","Collect cells from more environments","","0"
"3923","1243538","6995363","03/31/2021 13:56:11","This is a task for introductory level only. 

How have the emissions increased since 1990 in Brazil?
What is the highest sector in emissions in 1991?
Comparing the energy and agriculture sectors what is the data showing?

I would like to hear your ideas about this. 

Please submit your notebook and answer to the questions above.","","Time series","Practice your line charts and bar charts.","","0"
"5803","1260007","6900019","08/19/2021 12:40:40","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks","","Predict satisfaction of airline passengers","Using a multitude of data sources predict which passengers are satisfied","","0"
"4308","1329018","7012016","05/09/2021 12:48:52","## Task Details
The audio in this corpus is provided as a set of long audio files; the Elan (`.eaf`) files contain the transcripts, with start and end times. The speech and transcript should be extracted from the Elan and audio files.","","Extract spoken segments","","","0"
"4309","1325240","7012016","05/09/2021 12:48:58","## Task Details
The audio in this corpus is provided as a set of long audio files; the Elan (`.eaf`) files contain the transcripts, with start and end times. The speech and transcript should be extracted from the Elan and audio files.","","Extract spoken segments","","","0"
"4307","1325316","7012016","05/09/2021 12:46:12","## Task Details
The audio in this corpus is provided as a set of long audio files; the Elan (`.eaf`) files contain the transcripts, with start and end times. The speech and transcript should be extracted from the Elan and audio files.","","Extract spoken segments","","","0"
"4841","1415024","7029381","06/19/2021 07:52:15","## Task Details
The task is to create a model that predict the price of a house in Nigeria given some parameters.

## Expected Submission
Submit your notebooks that contains your solution","","Predict the prices of the Houses","","","1"
"4028","1261997","7042817","04/10/2021 06:26:38","This data set has various Indicators of Import and Export in it. Based upon their economic importance one could create an Indicator to make valuable insight into the performance of the sector.","","Performance of External Sector of Ghana.","","","1"
"4125","1281160","7042817","04/19/2021 08:57:39","## Task Details
One Can Try EDA with the Data from the perspective of different stakeholders of Amazon","","Tweet analysis of AMAZON In","","","0"
"4122","1280044","7042817","04/18/2021 17:24:23","## Task Details
One Can analyze the Tweets and the reactions to those In the Indian Context.","","Text Analysis with respect to Indian Context","","","0"
"4112","1279409","7042817","04/18/2021 11:03:44","One can try to identify that what are the other important news apart from Covid in this time period.","","Popular News determination","","","0"
"4291","1323524","7042824","05/07/2021 08:36:00","Show the best performance by using Auto-Encoder and DC-GAN module structures.","","Auto-Encoder and DC-GAN","Machine Learning","","0"
"4293","1323700","7042824","05/07/2021 09:58:17","Build and test the best classifier model","","Prediction Models","Machine Learning","","1"
"4315","1329786","7042824","05/09/2021 18:00:11","Create your detection and prediction models with computer vision.","","Detection System","Machine Learning and Computer Vision","","0"
"4435","1351460","7042824","05/18/2021 19:43:30","Show your best detection system with computer vision","","DETECTION SYSTEM","Deep Learning Process","","0"
"4406","1344099","7042824","05/16/2021 12:18:38","The galaxy images are convolved or smoothed with a kernel that would act to turn a single point into a blurry image. Part of the challenge is to attempt to remove or account for that blurring effect.

The aim is to measure the shapes of galaxies to reconstruct the gravitational lensing signal in the presence of noise and a known Point Spread Function. The signal is a very small change in the galaxies‚Äô ellipticity, an exactly circular galaxy image would be changed into an ; however real galaxies are not circular.

Show your best model.","","AUTO-ENCODER PROCESS","Machine Learning","","1"
"4376","1339948","7042824","05/14/2021 09:54:01","Show your best performance about DC-GAN process and create a new eye","","DC-GAN","Machine Learning and Deep Learning","","0"
"4391","1341135","7042824","05/14/2021 22:04:06","You can use this data for DC-GAN or prediction models.
Show your best model.","","DC-GAN AND MACHINE LEARNING","Deep Learning Process","","0"
"4591","1381626","7042824","06/01/2021 09:08:06","Create a prediction system with DC-GAN module.","","GAN-DCGAN PREDICTION SYSTEM","Machine Learning","","0"
"4540","1373932","7042824","05/28/2021 20:51:47","Create your best DCGAN or GAN model","","DCGAN - GAN","Machine Learning Process","","1"
"4470","1362209","7042824","05/23/2021 17:09:20","Show your best machine learning system with computer vision.

You can set it as detection system","","DETECTION SYSTEM","Machine Learning","","0"
"4768","1406833","7042824","06/13/2021 19:03:25","Create your DCGAN models on this dataset.","","DCGAN & COMPUTER VISION","Machine Learning","","0"
"4662","1393574","7042824","06/07/2021 06:35:13","Create a prediction model with DC-GAN or GAN models.","","GAN - DCGAN PREDICTION MODELS","Deep Learning Process","","0"
"4706","1399071","7042824","06/09/2021 15:42:54","Create your prediction model with computer vision.","","PREDICTION MODELS & AUTO-ENCODER","Machine Learning and Deep Learning Process","","0"
"4725","1401538","7042824","06/10/2021 19:38:32","Create your detection models with examples and computer vision","","Detection Models","","","0"
"4742","1404051","7042824","06/12/2021 08:28:01","Create your segmentatƒ±on model with Auto-Encoder","","SEGMENTATION MODEL","Deep Learning","","0"
"4955","1440931","7042824","06/30/2021 20:21:46","## Task Details

There are many files in different formats. You can examine them and build a prediction model on them.","","ANALYSIS","Data Analysis Process & Machine Learning","","0"
"4958","1441462","7042824","07/01/2021 05:26:38","Perform analysis on the images extracted from the simulation and specify all possible parameters. 


Be creative. You can also set up a DCGAN-GAN structure.","","ANALYSIS AND COMPUTER VISION","Deep Learning Process","","1"
"4969","1443806","7042824","07/02/2021 09:12:35","Create your best perspective!","","ANALYSIS","Deep Learning Process","","0"
"4892","1426821","7042824","06/23/2021 07:58:10","## Task Details
Create your best Auto-Encoder structure with computer vission.","","DEEP LEARNING PROCESS","Auto-Encoder","","0"
"4914","1431542","7042824","06/25/2021 17:36:59","Create your GAN and show how the universe is excellent","","GAN & DC-GAN","Deep Learning","","0"
"5077","1459402","7042824","07/10/2021 13:39:48","## Task Details
Analysis

Show your best performance!","","ANALYSIS","Data Science","","0"
"5097","1462723","7042824","07/12/2021 15:18:55","## Task Details
This data is published for the first time. This is the first example of the M87 shared on this platform. We want a broad analysis study. You can use all kinds of methods and approaches.","","ANALYSIS","Data Science Process","","0"
"4097","1276787","7054983","04/17/2021 07:30:31","## Task Details
This dataset is a good material for ML study.

## Expected Submission
- Notebook is preferred.
- The solution should contain the model, the prediction result and the description of the method

## Evaluation
The description is the most important part!","","Tumor Area Segmentation","","","1"
"4068","1270687","7055034","04/14/2021 07:44:24","## Task Details
This dataset was created to identify the type of Activity being performed by a Human. Can be used for surveillance in Hospitals, Schools and Old Age Homes.

## Expected Submission
Create a ML model to classify into 6 categories Sitting, Standing, Walking, Walking on Stairs and Control. 

## Evaluation
Use data augmentation and CNN to improve the model and prevent overfitting. The model should be able to generalize well for any conditions (with proper illumination).

### Further help
Category Control used to prevent Random images to be classified into any of the remaining 5 categories. Dataset has 150x150x3 ie the image of size 150x150 and a 3 channel(RGB). The dataset involved humans of various backgrounds, ages, skin tone, time of day.
Links:
https://github.com/JithLord/Activity_Detection","","Model to classify Human Activity","","","0"
"4002","1248471","7055034","04/08/2021 07:27:23","## Task Details
Modified FER2013 Dataset which has 5 categories including control to prevent the model from fitting random images into one of the human emotion categories. 

## Expected Submission
The Model should not only have an accuracy greater than 0.8 for training and validation but also have greater than 0.8 as precision, recall values. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Perform Multi Class classification","","","0"
"4303","1326138","7055034","05/08/2021 09:24:59","## Task Details
Inspired by the use of arrow orientation detection in rovers.

## Evaluation
Model should be able to handle random noise and reflection on the surface of arrows.  

### Further help
https://dir-class-jlx.netlify.app/classifiers.html
Model developed with the help of teachable machine","","Model to detect orientation of arrows","Create a ML model which can be used for live classification of arrows into 4 main directions.","","0"
"6394","1277772","6564094","10/17/2021 16:40:55","Introduction
Welcome to the Cyclistic bike-share analysis case study! In this case study, you will perform many real-world tasks of a junior data analyst. You will work for a fictional company, Cyclistic, and meet different characters and team members. In order to answer the key business questions, you will follow the steps of the data analysis process: ask, prepare, process, analyze, share, and act. Along the way, the Case Study Roadmap tables ‚Äî including guiding questions and key tasks ‚Äî will help you stay on the right path.

Three questions will guide the future marketing program:

How do annual members and casual riders use Cyclistic bikes differently?
Why would casual riders buy Cyclistic annual memberships?
How can Cyclistic use digital media to influence casual riders to become members?","","Data Exploration","Exploration and Visualization","","1"
"3961","1251331","7059647","04/05/2021 02:16:15","So, the main problem was figuring out what are the most used words in the whole TV-Show and running the following query I get these results: 

```
SELECT WORD, COUNT(*)
FROM SUMMARY_INFO
group by WORD
HAVING COUNT(*)&gt;100
ORDER by count(*) DESC
```

Filtering top's results for a specific season:

```
SELECT WORD, COUNT(*)
FROM SUMMARY_INFO
group by WORD
HAVING COUNT(*)&gt;10 AND SEASON=""1""
ORDER by count(*) DESC

```

What new question would you ask to this dataset? Maybe knowing how many times protagonist's names were said or discovering the insult-per-minute's rate...

Share with me your questions and let's begin our journey through Billion's dialogues.","","Getting used to the Billion's dataset.","Knowing the basis.","","0"
"4024","1261780","7078464","04/10/2021 03:49:40","fdsafa
fdsafafasd","","abcdeffdsfas fadfs","","04/11/2021 23:59:00","0"
"4201","1297306","7080190","04/27/2021 12:46:50","## Task Details
Add USA deaths information, sources and quality. Columns are `total_yr_deaths_USA	per_100_000_inhabitants_/yr_USA	female_yr_deaths_/yr_USA	female_yr_deaths_=&lt;_4yo_USA	female_yr_deaths_&gt;=5yo,_&lt;=64yo_USA	female_yr_deaths_&gt;=_65_USA	male_yr_deaths_USA	male_yr_deaths_=&lt;_4yo_USA	male_yr_deaths_&gt;=5yo,_&lt;=64yo_USA	male_yr_deaths_&gt;=_65_USA successor_array	total_deaths_source_year_USA	sources_total_deaths_USA	deaths_quality_USA`

## Expected Submission
A new CSV file with same headers and format as the existing ones, with old correct data, new data and corrected data. 
For data, see example with *_FRANCE data on the existing dataset.

## Evaluation
Random verifications.

### Further help
Data might be scrapped from https://wonder.cdc.gov/ucd-icd10.html .","","(prio=high,level=easy) insert data for country=USA","","","1"
"4204","1297306","7080190","04/27/2021 15:17:31","## Task Details
search the dataset concept_id values in the https://icd.who.int/browse10/2019/en and copy, correct or correct the format of ICD-10(s) number(s) in the column CIM10

## Expected Submission
A new CSV file with same headers and format as the existing ones, with old correct data, new data and corrected data. 
Format of ICD10 numbers is e.g. C71
For multiple ICD10 values, use comma e.g. C71,C72,C73
Subgroup detail is not necessary e.g. E66.2

## Evaluation
Random verifications","","(prio=medium,level=medium) add/correct ICD-10 disease classification number","","","0"
"4636","1297306","7080190","06/04/2021 10:39:37","## Task Details
Update, fill missing data, add a country
This task will always  be open 

## Expected Submission
New df/CSV by notebook (or contact message)

## Evaluation
Please specify sources and data reliability in the quality column

### Further help
Use the discussion section of the dataset to ask questions","","CONTRIBUTORS ARE WELCOME :-) e.g. update, fill missing data, add a country","","","0"
"4193","1298181","7089743","04/26/2021 17:12:51","## Task Details
My favorite thing about data is to see how different people bring their own lens to analysis and ask their own questions that get at different connections in the data.

## Expected Submission
Please use any form of clear and understandable visual communications, to make the insights accessible to not just those experienced with data, but anyone who might be looking.

## Evaluation
Any unique insights are appreciated, but even solid looks at the state of the world that can confirm expected results can be really valuable in data communication.","","Find unexpected connections","use a wide variety of indicators to bring light to correlation between worldwide measures","","0"
"4061","1248936","7093758","04/13/2021 09:38:03","Data visualization can be a good start for social network analysis. So, please share your data visualizations üòä","","Data Visualization","","","2"
"4475","1362878","6392042","05/24/2021 08:38:27","dtdsdfsdfdsfsdfsdfsdfsdfsd","","testddddddd","test","05/30/2021 23:59:00","1"
"4484","1366092","7104855","05/25/2021 11:07:00","DESCRIPTION

For safe and secure lending experience, it's important to analyze the past data. In this project, you have to build a deep learning model to predict the chance of default for future loans using the historical data. As you will see, this dataset is highly imbalanced and includes a lot of features that make this problem more challenging.

Objective: Create a model that predicts whether or not an applicant will be able to repay a loan using historical data.

Domain: Finance

Analysis to be done: Perform data preprocessing and build a deep learning prediction model.

Steps to be done: 

‚¶Å    Load the dataset that is given to you
‚¶Å    Check for null values in the dataset
‚¶Å    Print percentage of default to payer of the dataset for the TARGET column
‚¶Å    Balance the dataset if the data is imbalanced
‚¶Å    Plot the balanced data or imbalanced data
‚¶Å    Encode the columns that is required for the model
‚¶Å    Calculate Sensitivity as a metrice
‚¶Å    Calculate area under receiver operating characteristics curve","","House Loan Data Analysis","Deep Learning with Keras and Tensorflow","","5"
"4296","1323529","7107670","05/07/2021 11:56:45","Not always will your data be in a readily usable form, so one will need to format it accordingly to make it usable","","formatting the dataset","","","0"
"3980","1253463","7112557","04/06/2021 05:03:16","vdfgdf ffhdf xfdgf","","cvxcbxc ff","dgdf xfdf","04/07/2021 23:59:00","0"
"4769","1406973","7119949","06/13/2021 21:38:07","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

This task is for everyone. Please submit your best work or practice! :) Good Luck.","","Give me your best Exploratory Data Analysis","Share your skills to the public","","0"
"4916","1255352","7071177","06/26/2021 03:04:01","1. Problem Definition

How we can use various python based Machine Learning Model to and the given parameters to predict the resale prices of HBD
2. Data

Data set from Data.gov.sg link: https://data.gov.sg/dataset/resale-flat-prices?resource_id=42ff9cfe-abe5-4b54-beda-c88f9bb438ee
3. Evaluation

It will be done with the Root Mean Square Error (RMSE), Mean Absolute Error (MAE) and with the R2 Score (Accuracy)","","Prediction of Resale HDB Prices","","","0"
"4326","1271455","7131360","05/11/2021 11:38:34","## Task Details
Defects or anomalies in industrially manufactured objects may produce a faulty or even dangerous final product. However, early detection of these defects is tedious and inaccurate because it is usually a manual work. For these reason an automated inspection would help to achieve higher production standards in the industry reducing costs and increasing productivity.

In this task defective or abnormal zones must be detected in the wooden objects. Image must be segmented detecting any anomaly in the texture in order to decide if the object is defective or not. As all the anomalies that might occur cannot be compiled, the task is presented in a one-class classification approach, where only non-defective textures are used in the training stage.

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation.

With this model, a batch of  a batch of resulting image segmentation, binary masks, indicating the defective pixels / zones in the image should be produced as the masks provided in the dataset.

## Evaluation
Good solutions are capable of finding all the defective pixels without producing false positives. In order to evaluate this different measures might be used: ROC AUC at pixel level, precision at total recall and finally a precision-recall AUC.  The higher value in these metrics the better result.

As stated in the dataset two mask images are provided ""AND"" and ""XOR"". When evaluation only the ""AND"" pixels should be considered as positives while ""XOR"" pixels are ""optional"".","","Wood texture anomaly detection","One-Class classification","","0"
"4664","1393720","6477455","06/07/2021 09:18:45","## Task Details
EÂÆöÊó∂ÂèëÈÄÅvery task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","mytask","111","","0"
"4074","1271616","7184919","04/14/2021 15:11:06","## Task Details

There are people who were not affected by covid but still they had to go through RT-PCR test which resulted in delay of actually detecting the disease. To reduce this, we need data analysis

## Expected Submission

Users should submit an analysis of the dataset, and the solution should contain a model that predicts on the basis of the symptoms given that whether a person requires a covid test or not.

## Evaluation
Data analysis should be visualized by graphs if possible.
Accuracy of the model should be more than 0.5","","Predict Covid 19 if you know these symptoms","","04/19/2021 23:59:00","0"
"4251","1312442","7198515","05/03/2021 04:22:54","## Task Details
As per the drastic changes in covid -19  its get important to use multiple and advance models to predict the situation. 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analyze Dataset with SIR model","SIR predictions","05/10/2021 23:59:00","0"
"4954","1439498","4997746","06/30/2021 19:16:30","You can use the dataset to predict the probability of a course score from 1-5","","Predict the probability how much a course will get score(1-5)","","","0"
"4277","1319788","7210206","05/05/2021 19:22:08","Remove the missing values and Reduce the errors","","Data Cleaning","","","2"
"4288","1319788","7210206","05/06/2021 21:47:29","Show the relationships between the variables","","Exploratory Data Analysis","","","0"
"4369","1335617","7225220","05/13/2021 18:21:57","The resources sheet has a lot of null values that I want to replace with N/A just so they don't throw nulls.","","Update Nulls to N/A","","","0"
"4341","1335617","7225220","05/12/2021 07:07:46","Need a sheet for forge recipes","","Forge Recipes","","","0"
"4342","1335617","7225220","05/12/2021 07:09:47","Need to update the Workbench Recipes sheet with the complete list of recipes. I didn't pull from a wiki because I didn't want spoilers, but this will need to be done eventually.","","Update Workbench Recipes","","","0"
"4254","1313173","7261501","05/03/2021 11:17:11","You have to use list in python in which you will create one list that will store user inputs like if customer like pesto/mushroom/onion etc or not . And then another list will be like a 2Darray in which we will store pizza flavors along with ingredients.","","Pizza Flavor Recommendation System using Python code","Python Project","","0"
"4169","1292611","7261501","04/24/2021 09:29:28","Make a Python program using conditions and list .In the list you can have 10 pizza flavor names.Then ask input from the user and matches it with some conditions.And on the basis of inputs display a message that recommends user a pizza flavor to customer.","","Pizza Flavor Recommendation System Python Project","Python food recommendation system Project","","0"
"4433","1309334","7270108","05/18/2021 17:41:59","Data should be visualized and analyzed.","","Data Visualization","","","0"
"4197","1299482","7285184","04/27/2021 08:27:20","You can use Opencv or any other library to detect the number of broken rice in the different images.","","Using OpenCV or any other library to detect number of broken rice","","","0"
"5501","1335560","7288698","08/02/2021 15:13:27","# Task Details
# Predict the number of positive.
# 1)covid19_osaka_daily_data.csv
#   This data is a training data set
# 2)050921_073121covid19_osaka_daily_data.csv
#   This data is a test data set.","","Anything is okay. For instance is predict the number of positive.","","09/30/2021 23:59:00","0"
"4984","1446413","7288698","07/03/2021 19:17:55","1.Task 
Classify 

2.Expected Submission
1)Build your machine learning model by using ""test_data.csv"", and
   classify images into 3 shapes; square, circle, triangle.<br> 
2)Predict correct labels of ""quiz_dataset.csv"".

3.Sample Notebook<br>
https://www.kaggle.com/nohrud/hand-written-images-classification-on-keras","","Image classification","Simple image classification square/circle/triangle","","1"
"4224","1304213","7300883","04/29/2021 08:51:02","Create a Segmentation Model to identify cars","","Create a Segmentation Model to identify cars","","","0"
"4351","1332969","7314510","05/12/2021 16:57:12","EDA","","Exploratory Data Analysis of the dataset","","","5"
"4440","1352781","7314510","05/19/2021 12:02:05","build a Machine Learning model to predict the delays of Tunisair flights.","","Predict airline delays.","","","5"
"4250","1311896","7326206","05/02/2021 22:56:41","Use an IP geolocation database or API to enrich the data with the country, region, state, and/or city of each IP address.","","Add IP Geolocation","","","0"
"4289","1315359","7337879","05/07/2021 05:05:27","## Task Details
Get maximum IOU/PQ score on the test set.

## Expected Submission
Code weights/ Results
If you get good results send them to me and I will buy you a beer:)

## Evaluation
See the readme file and evaluation script in the dataset

### Further help
Contact me: sagieppel@gmail.com for any questions","","Solve visual recognition for autonmous chemistry lab","","","0"
"4286","1321802","7343459","05/06/2021 16:08:39","## Task Details

About 350 movies from MovieLens 25M have a cover missing. You could impute the missing data with screenshots or fan art.

## Expected Submission

A notebook and a dataset exploring the missing movies.
 
## Evaluation

Screenshots must be clean and look like a real cover for the movie.","","Add posters for the movies that miss a cover on IMDb","Imputing missing data","","0"
"4304","1325547","7360673","05/08/2021 17:06:23","The challenge is hosted at https://eval.ai/web/challenges/challenge-page/800/overview","","Streaming Perception Challenge","Low-latency object detection for self-driving","","0"
"4800","1414524","7372161","06/17/2021 03:20:47","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

I have an assignment to visualize all sorts of valuable information from this data set. Need to do lots of chart related. Also, the data is already cleaned with no noise.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

Visualizing all sorts of chart from pie to scatter plot and etc.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

Good visualization representation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualize this dataset","i have an assignment to visualize all sorts of valuable information from this dataset. Need to do lots of chart related","07/02/2021 23:59:00","0"
"4305","1326895","7368558","05/08/2021 19:47:01","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)


Mettre en √©vidence les similitudes  du point de vue sant√© des diff√©rents pr√©sidents am√©ricain.
Pr√©dire √† quel groupe pourrait appartenir selon leurs donn√©es 
des individus √©tant candidat √† la pr√©sidence am√©ricaine.","","Pr√©diction","","","0"
"4625","1387486","7373442","06/03/2021 23:44:09","This is a detailed study with a part of the company database and there are more facts and information is stored but since I am taking these two datasets only, so will not talk much about the other datasets.

## Expected Submission
Total Packages sold by dealers.
Find Dealer details who sold Maximum & Minimum number of ‚ÄòLongPND‚Äô package(as we found that the ‚ÄòLongPND‚Äô has been sold the highest a number of times).
How many packages have sold monthly/quarterly/yearly in 2020 & 2021
Dealers 2021 and 2020""s total sales. With Pivot chart.
Performance of the CC's monthly.
Find the area where the demand for PND is higher
From the data set find the rank of the dealers from the active customer‚Äôs list.
Customers have already taken 2 or more PND

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Pick & Drop Sales and booking Analysis","","","1"
"4820","1416444","7425932","06/18/2021 01:27:57","#This data set is for beginners to help you haw to clean big data and then manipulate it to help you with your EDA. 

##Analyze data and provide some trends.
1. Year with best rating
2. Does length of movie have any impact with the rating?
3. Top movies according to rating per year and overall.
4. Number of Movies released each year.
5. Genre through the year.
6. Which director, directed the most movies
7. Which actor starred in the most movies movie
8. Plot 6 and 7 with the year.
9. Any other trends you can find
10. Any other trends or future prediction you may have


Thank you for viewing my dataset, looking forward to seeing some codes.","","EDA of Movies","","","2"
"4811","1415455","7425932","06/17/2021 13:55:20","## Task Details
#### Every data set has a story lets see if we can unravel it.

1. Clean the data by removing missing values or adding average values this process will help to manipulate the data to help with your EDA.

2. Analyze data and provide some trends, like 
- Year with best rating.
- Does length of movie have any impact with the rating?
- Top 10 movies according to rating per year and overall.
- Number of popular movies released each year and was there any impact because of the pandemic.
- Counting the number of votes which movies preformed better in rating per year and overall.

3. Any other trends or future prediction you may have

Thank you for viewing my dataset, looking forward to seeing some codes.","","EDA of Bollywood Movies","","","1"
"4467","1361526","7485040","05/23/2021 11:18:42","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.cortm/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","testtest","test","","0"
"4466","1361539","7485152","05/23/2021 11:16:34","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submissiondfgdfg
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","<h1>testing</h1><h1></h1>","<h1>test</h1>","","1"
"5152","1461623","7494464","07/16/2021 19:10:25","## Task Details
Users today depend a lot on reviews before shopping online. So, perform a sentiment analysis that will help people in shopping online

## Expected Submission
Try to preprocess data more effectively, and then perform sentiment analysis

## Evaluation
Make all the necessary steps before analysis

### Further help
Feel free to ask for any help","","Sentiment Analysis","Amazon Product Reviews Sentiment Analysis","","3"
"4711","1399800","860369","06/10/2021 03:32:53","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","TestTask","Test2","06/12/2021 23:59:00","1"
"4923","1433785","7594800","06/27/2021 07:00:42","A company named Cyclistic located in Chicago is trying to increase their business in the coming year by analyzing past one-year data. According to Marketing Director 'Lily Moreno', it is believed that maximizing the number of annual memberships would be an efficient one. The business task is how members and casual riders use cyclist bikes differently because the company wants to increase the number of annual members.","","Bike share case study","","","0"
"4753","1405505","7604920","06/13/2021 03:51:35","Users should predict how many personal protection equipments each region in Brazil will recieve for use in COVID-19 patients assistance, in the next 6 months after the last date of this database

sep= ';'
code = 'UTF-8'","","Material distribuition by region","","","0"
"4943","1437681","7638894","06/29/2021 07:59:34","## Task Details
Can we use a year daily stock data to predict future TSMC stock price? How accurate is that? Is the prediction reliable or does it need periodically adjust?

## Expected Submission
A notebook with a visual / textual explanation.

## Evaluation
This is highly subjective, but whatever looks most convincing.","","Is stock price of TSMC predictable?","","","1"
"4719","1400705","7641184","06/10/2021 12:02:26","https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2021-movie-online-full-free-hd
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2021-full-movie-download-online
https://ccsc.instructure.com/courses/5835/pages/movies-hd-watch-the-conjuring-3-2021-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/the-conjuring-3-2021-full-movie-online-hd-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2021-full-online-for-free-hd
https://ccsc.instructure.com/courses/5835/pages/the-conjuring-3-2021-full-movie-free-hd-download
https://ccsc.instructure.com/courses/5835/pages/full-watch-the-conjuring-3-2021-movie-online
https://ccsc.instructure.com/courses/5835/pages/full-watch-the-conjuring-3-2021-hd-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/hd-watch-the-conjuring-3-2021-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2021-full-hd-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-full-the-conjuring-3-2021-online-mp4-movie
https://ccsc.instructure.com/courses/4957/pages/watch-the-conjuring-3-2021-movie-online-full-free-hd
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-movie-%3E%3E-the-conjuring-3-2021-full-for-free-hd
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2020-full-movies-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-2021-full-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-online-hd720p
https://ccsc.instructure.com/courses/5835/pages/official-the-conjuring-3-the-devil-made-me-do-it-2021-watch-movie-full-hd
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-hd-movie-online-on-free
https://ccsc.instructure.com/courses/5835/pages/free-watch-the-conjuring-3-the-devil-made-me-do-it-2021-online-full-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-online-full-hd-movies-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-movie-online-full-free-hd
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-download-online
https://ccsc.instructure.com/courses/5835/pages/movies-hd-watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-online-hd-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-online-for-free-hd
https://ccsc.instructure.com/courses/5835/pages/the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-free-hd-download
https://ccsc.instructure.com/courses/5835/pages/full-watch-the-conjuring-3-the-devil-made-me-do-it-2021-movie-online
https://ccsc.instructure.com/courses/5835/pages/full-watch-the-conjuring-3-the-devil-made-me-do-it-2021-hd-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/hd-watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-movie-online-free
https://ccsc.instructure.com/courses/5835/pages/watch-the-conjuring-3-the-devil-made-me-do-it-2021-full-hd-movie-online-free
https://lemon.shivtr.com/forum_threads/3467905
http://snomoto.com/forums/timbersled/conjuring/
http://ptits.net/boards/t/14774/filmshdlive.aspx
https://torgi.gov.ru/forum/posts/list/0/156330.page#192328
http://pressure-vessel-steels.co.za/forum.php/read.php?1,39246
https://blog.goo.ne.jp/filmshd_live/e/426a44144b617bc93e62d07da4d52ec6
https://www.bankier.pl/forum/temat_filmshd-live,47578897.html","","filmshd.live","filmshd.live","06/11/2021 23:59:00","0"
"4799","1414481","7664801","06/17/2021 02:56:04","## Task Details
To develop a deep learning algorithmto for classify the anteroposterior radiographs of the shoulder with can achieve 98% accuracy, 98% precision, and 98% recall.

## Evaluation
A deep learning algorithmtocan achieve 98% accuracy, 98% precision, and 98% recall.

### Further help
What makes a good solution? How do you evaluate which submission is better than another?
classifier is able to detect correctly base on given values.","","Shoulder X-ray Classification","","06/30/2021 23:59:00","1"
"4862","1418621","7686044","06/20/2021 16:04:53","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
EXAMPLE
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","NEW_TASK","","06/23/2021 23:59:00","0"
"4838","1418621","7686044","06/19/2021 05:22:30","## Task Details
Narrowing down dataset to view Oakland, CA information of real cost measure,
Narrow down to Oakland income, poverty level, education level, ...by race.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","oakland_data","Oakland Realcost measure Variables","06/20/2021 23:59:00","0"
"4796","1414210","7708052","06/16/2021 21:32:52","I. You have to write a code that gets the information from 8 different csv files and summarizes the information in another csv file that is attached as a sample. The sample output has the information for only two players, but you need to summarize the whole information.
II.
After summarizing the data into one csv file, you will use that file to answer the following question:
a. Choose the best 5 teams with a great transfer:
You will rank the players that had a different team in two seasons according to their new season ranking. Choose first 5 players and print their new team‚Äôs name.
b. Print the name of 5 best players that promoted themselves; in other words, the players whose rankings are increased in comparison to the last season will be sorted and first 5 players will be chosen. (If a player was not playing last year, he will not be included on the list).
c. Repeat the steps in part b, and print the name of the players that have demoted themselves.
d. Consider two formations: 3.5.2 and 4.4.2. There is an image attached to this file explaining these formations. Note that if a player is labeled as Defender, he can play in any post in the defense line. Similarly, a Midfielder and Forward can play in any position in the middle and front line, respectively. You are supposed to choose players according to these formations
based on their average scores in both seasons.
1) The best teams. (You are supposed to print two lists of size 11 with the highest average
ranking for both seasons).
2) The most offensive teams. (You are supposed to print two lists of size 11 with players
that have scored more goals; if there are more than one players with the same amount
of goals, choose the player that has played fewer minutes than the other players)
3) The fair play teams. (You are supposed to print two lists of size 11 with players the least fouls; if there are more than one options, choose the player that has player more
minutes than the other players.)
4) The youngest teams. (You are supposed to print two lists of size 11 with the youngest
ages; if there are more one options, choose the player with the higher ranking).
e. Considering the teams of the year, choose the best player that was on the teams of the years, if
there are more than one, rank them according to the average of their ranking for both years.","","Just Do below Tasks!","","06/18/2021 23:59:00","0"
"6814","1424921","4561355","11/10/2021 16:19:09","## Task Details

Predict Spending Score using any regression model

## Expected Submission

Create a simple regression model and train using required feature and predict spending score","","Predict Spending Score","","","1"
"4873","1422524","7730819","06/21/2021 08:47:07","Purpose: automatically transform CT Datasets into DRR and generate the DRRs with the special angles (according to the EXCEL).","","Automatically transform shoulder CT Datasets into DRRs","Automatically transform shoulder CT Datasets into DRRs and rotate","06/30/2021 23:59:00","0"
"5030","1452954","7778348","07/07/2021 08:36:19","Perform EDA üôå","","EDA & Visualizing","","08/29/2021 23:59:00","0"
"5298","1446632","7780007","07/23/2021 20:06:07","#","","Clustering","find player archtypes and similarities","","0"
"4950","1438826","7788373","06/30/2021 02:44:23","Use multivariable linear regression to predict the masses of brains from the dataset, plot a graph and display your model score also.","","Predict masses of brains from the datset","","","1"
"5066","1458216","7806547","07/09/2021 18:46:44","## Expected Submission
Only data visualization and the submission that have explaining what is going on in that notebook","","Data Visualization","Make Data Visualization for this data set","","0"
"5080","1460047","7806547","07/10/2021 21:13:50","## Expected Submission
%80 or more hight for each eval folder 

## Evaluation
Any solution with good describe to steps will be accepted.","","recognition model","Create a model which can make recognition for Knife and/or Pistol","","1"
"5163","1460047","7533616","07/17/2021 10:58:14","if you make it tray to check a data cause ther's some image can't open
and if you have another methode please tell me cause i'am beginer üòÖ","","create classification","if you make it tray to check a data cause ther's some image can't open","07/31/2021 23:59:00","0"
"6558","1651810","376354","10/24/2021 08:07:32","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Dirty Dozen - Data visualization","Practice data visualization on a simple dataset!","","1"
"6559","1651810","376354","10/24/2021 08:08:01","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Dirty Dozen - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6531","1651813","376354","10/24/2021 07:56:05","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Dark Triad - Find the outliers!","Find the test takers outliers!","","1"
"6532","1651813","376354","10/24/2021 07:56:18","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Dark Triad - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6533","1651813","376354","10/24/2021 07:56:29","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Dark Triad - Background analysis","Practice EDA on a simple dataset!","","1"
"6534","1651813","376354","10/24/2021 07:56:42","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Dark Triad - Data cleaning","Practice data cleaning on a simple dataset!","","2"
"6535","1651813","376354","10/24/2021 07:56:56","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Dark Triad - Data visualization","Practice data visualization on a simple dataset!","","1"
"6615","1651820","376354","10/24/2021 08:38:16","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Face Memory - Warming-up! Predict the age based on the questions answers and other features","This should be pretty easy!","","1"
"6616","1651820","376354","10/24/2021 08:38:30","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Face Memory - Find the outliers!","Find the test takers outliers!","","1"
"6617","1651820","376354","10/24/2021 08:38:42","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Face Memory - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6618","1651820","376354","10/24/2021 08:38:54","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Face Memory - Background analysis","Practice EDA on a simple dataset!","","1"
"6619","1651820","376354","10/24/2021 08:39:07","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Face Memory - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6620","1651820","376354","10/24/2021 08:39:23","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Face Memory - Data visualization","Practice data visualization on a simple dataset!","","1"
"6621","1651822","376354","10/24/2021 08:42:40","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Sex Role - Warming-up! Predict the age based on the questions answers and other features","This should be pretty easy!","","1"
"6622","1651822","376354","10/24/2021 08:42:56","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Sex Role - Find the outliers!","Find the test takers outliers!","","1"
"6623","1651822","376354","10/24/2021 08:43:07","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Sex Role - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6624","1651822","376354","10/24/2021 08:43:25","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Sex Role - Background analysis","Practice EDA on a simple dataset!","","2"
"6625","1651822","376354","10/24/2021 08:43:38","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Sex Role - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6626","1651822","376354","10/24/2021 08:43:53","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Sex Role - Data visualization","Practice data visualization on a simple dataset!","","1"
"6525","1651831","376354","10/24/2021 07:52:39","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Artistic Taste - Warming-up! Predict the age based on the questions answers and other features","This should be pretty easy!","","1"
"6526","1651831","376354","10/24/2021 07:52:55","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Artistic Taste - Find the outliers!","Find the test takers outliers!","","1"
"6527","1651831","376354","10/24/2021 07:53:12","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Artistic Taste - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6528","1651831","376354","10/24/2021 07:53:29","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Artistic Taste - Background analysis","Practice EDA on a simple dataset!","","1"
"6529","1651831","376354","10/24/2021 07:53:45","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Artistic Taste - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6530","1651831","376354","10/24/2021 07:54:00","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Artistic Taste - Data visualization","Practice data visualization on a simple dataset!","","1"
"6542","1651832","376354","10/24/2021 08:01:49","### Warming-up: Train a model to predict the score_full based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'score_full' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the score_full based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Vocabulary IQ - Warming-up! Predict the score_full based on the questions answers and other features","This should be pretty easy!","","1"
"6543","1651832","376354","10/24/2021 08:02:03","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Vocabulary IQ - Find the outliers!","Find the test takers outliers!","","1"
"6544","1651832","376354","10/24/2021 08:02:14","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Vocabulary IQ - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6545","1651832","376354","10/24/2021 08:02:27","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Vocabulary IQ - Background analysis","Practice EDA on a simple dataset!","","1"
"6546","1651832","376354","10/24/2021 08:02:39","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Vocabulary IQ - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6547","1651832","376354","10/24/2021 08:02:52","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Vocabulary IQ - Data visualization","Practice data visualization on a simple dataset!","","1"
"6597","1651838","376354","10/24/2021 08:31:18","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Immediacy - Warming-up! Predict the age based on the questions answers and other features","This should be pretty easy!","","1"
"6598","1651838","376354","10/24/2021 08:31:33","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Immediacy - Find the outliers!","Find the test takers outliers!","","1"
"6599","1651838","376354","10/24/2021 08:31:44","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Immediacy - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6600","1651838","376354","10/24/2021 08:31:58","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Immediacy - Background analysis","Practice EDA on a simple dataset!","","1"
"6601","1651838","376354","10/24/2021 08:32:12","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Immediacy - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6602","1651838","376354","10/24/2021 08:32:25","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Immediacy - Data visualization","Practice data visualization on a simple dataset!","","1"
"6609","1651842","376354","10/24/2021 08:36:12","### Warming-up: Train a model to predict the age based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'age' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the age based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Introvert vs Extrovert - Warming-up! Predict the age based on the questions answers","This should be pretty easy!","","1"
"6610","1651842","376354","10/24/2021 08:36:24","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Introvert vs Extrovert - Find the outliers!","Find the test takers outliers!","","1"
"6611","1651842","376354","10/24/2021 08:36:36","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Introvert vs Extrovert - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6612","1651842","376354","10/24/2021 08:36:50","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Introvert vs Extrovert - Background analysis","Practice EDA on a simple dataset!","","1"
"6613","1651842","376354","10/24/2021 08:37:02","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Introvert vs Extrovert - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6614","1651842","376354","10/24/2021 08:37:16","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Introvert vs Extrovert - Data visualization","Practice data visualization on a simple dataset!","","1"
"6405","1651842","376354","10/19/2021 20:13:10","Introvert vs Extrovert","","Predict Introvert vs Extrovert","","","1"
"6351","1647582","376354","10/14/2021 12:15:31","Predict the score from gender, age, is it possible? what is this telling us?","","Predict the score from gender, age","","","1"
"6352","1647582","376354","10/14/2021 12:16:41","Train a model to maximize the score based on the Images. 
Pay close attention to cross validation! there aren't many questions!","","Defeat the human IQ Tests with CNN model","","","1"
"6490","1647582","376354","10/24/2021 07:33:04","### Warming-up: Train a model to predict the score based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regession algorithm to make a prediction on the 'score' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the score based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Defeating IQ Tests - Warming-up! Predict the score based on the questions answers and other features","This should be pretty easy!","","1"
"6491","1647582","376354","10/24/2021 07:33:30","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Defeating IQ Tests - Find the outliers!","Find the test takers outliers!","","1"
"6492","1647582","376354","10/24/2021 07:33:49","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Defeating human IQ Tests - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6493","1647582","376354","10/24/2021 07:34:07","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Defeating human IQ Tests - Background analysis","Practice EDA on a simple dataset!","","1"
"6494","1647582","376354","10/24/2021 07:34:21","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Defeating human IQ Tests - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6495","1647582","376354","10/24/2021 07:34:40","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Defeating human IQ Tests - Data visualization","Practice data visualization on a simple dataset!","","1"
"6518","1651747","376354","10/24/2021 07:48:00","### Warming-up: Train a model to predict the accuracy based on the questions answers and other features
#### This actually should be pretty easy..

### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a regression algorithm to make a prediction on the 'accuracy' column of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Just as a quick warm-up!
Train a model to predict the accuracy based on the answers to the questions. 
this should be pretty easy!

### Evaluation Criteria
Low error, and easy to understand output. Should be able to put in new task takers and get a value estimate.
The best model should be the one that evaluates to have the lowest RMSE overall, please indicate the RMSE you get on out of fold validation set containing all observations.

### Expected Submission
A model with a lower RMSE score. Please submit a Kernel where the final cell outputs the RMSE score on the observations.","","Empathizing-Systemizing - Warming-up! Predict the accuracy based on the questions answers","This should be pretty easy!","","1"
"6519","1651747","376354","10/24/2021 07:48:19","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use an outlier detection algorithm to make an estimate on the rows of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Find outliers
#### Find the test takers outliers! 
Hint: the ""most baseline"" outlier detection algorithm can be found in the sklearn package at: ""sklearn.ensemble.IsolationForest""","","Empathizing-Systemizing Test - Find the outliers!","Find the test takers outliers!","","1"
"6520","1651747","376354","10/24/2021 07:48:34","### Don't Panic!
This task is suitable for beginners who have just stepped into the world of data science. It requires you to use a clustering algorithm based on the columns of the dataset. You are encouraged to explore the different parameters you can work with to improve your model and understand the importance of data understanding and feature selection.

### Cluster the takers based on the answers
#### Are the takers generally come from a single cluster or multiple clusters? find out using a clustering algorithm! 
Hint: the ""most baseline"" clustering algorithm can be found in the sklearn package at: ""sklearn.cluster.KMeans""","","Empathizing-Systemizing Test - Cluster the test takers!","Are the takers generally come from a single cluster or multiple clusters?","","1"
"6521","1651747","376354","10/24/2021 07:48:47","### Background analysis
This task relates to the background understanding of the problem statement.
Steps include understanding the functional background of the problem, the dataset provided and attributes in the dataset.
Outcome of this task will be understanding of the attributes of the dataset.","","Empathizing-Systemizing Test - Background analysis","Practice EDA on a simple dataset!","","1"
"6522","1651747","376354","10/24/2021 07:49:01","### Data cleaning
This task relates to the data cleaning aspect of the analysis. The requirement is to find the attributes that needs to be handled based on the data visualization task.
It is preferred that functions are created as a part of the data cleaning pipeline and dataset is prepared for further analysis.","","Empathizing-Systemizing Test - Data cleaning","Practice data cleaning on a simple dataset!","","1"
"6523","1651747","376354","10/24/2021 07:49:17","### Data visualization
This task details the visualization aspect of the data analysis. In this task, the requirement is to visualize the dataset without cleaning the data and understand the distribution of the dataset.

### Expected Submission
The submission must be a Notebook containing the process of Exploratory Data Analysis and making of a regression model. You can split the data into testing and training data and are required to show how well your model does on the testing data using the 'RMSE' metric.

#### Evaluation
The aim is to understand the regression algorithm you used and its parameters, and evaluation would be based on the RMSE of the model.","","Empathizing-Systemizing Test - Data visualization","Practice data visualization on a simple dataset!","","1"
"6524","1651747","376354","10/24/2021 07:50:03","Do Male & Female Differ?
Try and answer the age-long question","","Do Male & Female Differ?","Try and answer the age-long question","","1"
"6962","1754143","9003780","11/29/2021 11:04:33","perform the  omicron variant data analysis","","Omicrone variant","","12/12/2021 23:59:00","0"
"6890","1734027","390940","11/21/2021 07:11:08","## Task Details
Build a regression model on the dataset to predict age of the crab.

## Expected Submission
The dataset is more for exploration and discussion. Feel free to reach out for any conclusion","","Age Prediction","","","0"
"5601","1510756","115002","08/08/2021 07:13:49","## Task Details
I have always heard that smaller earthquake lead to a bigger earth quake in a given area & time. People are told to evacuate the building whenever a smaller earthquake hit. Does the data corresponds to this general saying?

## Expected Submission
Gather data of series of earthquakes where source were nearby and time was within a week. Cluster the data by hard constraint of source within 10 kilometres and date within  7 days. Check if the magnitude of such cluster records follows a pattern.
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?
For each cluster, sorted by time, what was the magnitude of all earthquakes.
Is the distribution random? Solve it using statistical hypothesis testing techniques provided by statistical libraries in any language/tool of your choice (python/R etc)

## Evaluation","","Do smaller earthquake lead to a bigger earth quake in a given area & time","","","2"
"6992","1758902","438781","12/02/2021 09:22:46","## Task Details
AMD website contains only new processors (2016+). As result we haven't full AMD processors database. It will be good to get old processors from old AMD website: https://web.archive.org/web/2/http://www.amd.com.","","Add old AMD processors","","","1"
"5541","1509720","590240","08/04/2021 07:32:34","## Task Details
NMR is the criptocurrency used as the staking token in [Numerai](https://numer.ai/). But maybe we can make profit simply by trading.

## Expected Submission
The solution should contain a trading strategy regarding ""when to sell, when to buy"" . For example, ""Buy on Monday, sell on Friday"". Also it should include a backtesting result.

## Evaluation
Return based on your backtest, readability of your notebook","","When to buy, when to sell","simple rule to make profits by NMR buy/sell","","0"
"6903","1727728","644036","11/22/2021 17:08:54","## Do some Exploration

Can you find some interesting insights from the rollercoaster dataset?","","Explore the data for insights","Find something interesting in the data.","","0"
"6194","1611656","748277","09/26/2021 01:45:10","## Task Details
Your task is to predict the correct categories given a paper abstract. This type of classifier can be useful for conference submission portals like [OpenReview](https://openreview.net/). Given a paper abstract, the portal could provide suggestions on which areas the underlying paper would best belong to.

The dataset comes with paper titles too, so you are free to use that to build your machine learning model. 

## Expected Submission
It should be a notebook that builds and trains a model on the provided dataset. It's expected that you will create held-out sets out of the provided dataset for ensuring a good evaluation. Additionally, data collection notebooks are also welcome that would help us collating a more comprehensive dataset. 

## Evaluation
Submissions will be evaluated based on the F1 score.","","Map arXiv paper abstracts to right categories","Can we map the arXiv papers to their correct categories given their abstracts?","","0"
"6909","1740245","766452","11/23/2021 13:10:25","## Task Details

Can you classify the survey respondents with machine learning? The target variable would be E1r1 ""If an effective COVID-19 vaccine becomes available and is recommended for me, I would get it.

## Expected Submission
A classification algorithm which classifies into the categories of `E1r1`. 

## Evaluation
The suggested metric is the AUC.","","Can you help to classify people into vaccine hesitancy groups?","A variety of social-demographic factors are responsible for Covid-19 Vaccine Hesitancy.","","1"
"6910","1740245","766452","11/23/2021 13:28:00","## Task Details
Understanding . Multiple variables 


## Expected Submission

Application of different clustering techniques to identify segments of the correspondents and derive interesting insights into these segments. Also, their attitudes towards Covid-19 would be of interest.

## Evaluation
A good solution describes the goals, general approach, methods and outcomes clearly and for others.","","Segmentation/Clustering Analysis of Respondents","Can you identify important sub-groups of respondents based on socio-demographic factors? Do they differ Covid-19 attitude?","","1"
"6052","1581514","769452","09/10/2021 09:03:52","## Task Details

Explore the rich dataset of Ask Reddit posts and comments.

## Expected Submission

A Notebook providing insights in the subjects discussed, topics, dynamic of discussion, trends.

## Evaluation

Your contribution should be able to highlight the important information from this subreddit.
Analysis should be original, easy to understand, use good quality graphics and include your own comments.","","Ask This Dataset what Redditors Asks","What topics are discussed in the popular Ask Reddit subreddit?","10/31/2021 23:59:00","0"
"5347","1487666","769452","07/26/2021 11:28:04","## Task Details

Explore Tokyo Olympics 2020 Tweets

## Expected Submission
Use a Notebook for submission

## Evaluation
 Your submission would need to include a detailed analysis of the tweets corpus.","","Explore Tokyo Olympics 2020 Tweets","","09/10/2021 23:59:00","0"
"5210","1467201","769452","07/19/2021 08:00:34","## Task Details
There are multiple European datasets that uses NUTS code for European regions and countries. This dataset contains the real names of these regions (3-levels, from country level to sub-region level).

## Submission evaluation
Provide interesting integration with rich European datasets.","","Combine the Europe Territorial Units data with other European datasets","","09/30/2021 23:59:00","0"
"5206","1472587","769452","07/19/2021 07:37:00","## Task Details
Perform a detailed data exploration for COVID-19 variants. Check data quality, use visualization to show various data features.

## Expected Submission
A rich visualization Notebook highlighting the various features of the dataset. The data has geographical, temporal and COVID-19 variants dimensions.


## Evaluation
The following will be considered for the submission evaluation:
- completeness of the analysis;
- attention to details;
- quality of visualization;
- originality;
- comments and conclusions.","","Explore COVID-19 Variants","Perform a detailed data exploration for COVID-19 variants","08/31/2021 23:59:00","1"
"5258","1479445","769452","07/21/2021 08:35:37","# Introduction
This is a task about answering questions from analyzing the data. To address this task, you will have to answer to questions like:  
* What are the testing reporting mode used by each country?
* Are there countries that report(ed) in more than one way?  
* How testing progressed in each country?
* Can you correlate testing, vaccination and covid19 occurrences data to create a comprehensive analysis of factors driving the dynamics of covid19 in different geographies?

# Expected submission

A successful submission will have to include:
* Exploratory data analysis;  
* Comments on the facts revealed by various graphs, diagrams;  
* Original and insightful interpretations.

# How the submission is evaluated 

The submission is evaluated on:
* Quality of data analytics and graphs;  
* Originality of approach;  
* Quality of insights.","","Explore COVID-19 testing data","","08/31/2021 23:59:00","0"
"5254","1479445","7693666","07/20/2021 22:20:16","Trying to discover how this works","","First tast","I hope it works","","1"
"5953","1479445","769452","09/01/2021 07:06:52","## Introduction

This is a task about answering questions from analyzing the data. To address this task, you will have to answer to questions like:

* What are the testing reporting mode used by each country?
* Are there countries that report(ed) in more than one way?
* How testing progressed in each country?
* Can you correlate testing, vaccination and covid19 occurrences data to create a comprehensive analysis of factors driving the dynamics of covid19 in different geographies?

## Expected submission
A successful submission will have to include:

* Exploratory data analysis;
* Comments on the facts revealed by various graphs, diagrams;
* Original and insightful interpretations.
* How the submission is evaluated

The submission is evaluated on:

* Quality of data analytics and graphs;
* Originality of approach;
* Quality of insights.","","Exploration of COVID-19 World Testing Progress Data","Fall edition","11/30/2021 23:59:00","0"
"5889","1555484","769452","08/27/2021 10:33:32","## Task Details

Explore the text of r/Cricket subreddit

## Expected Submission

Prepare a Notebook with text analysis (from word frequencies to topic modelling to sentiment analysis).

## Evaluation

Your work will be evaluated based on:

* Analysis quality and completeness; things to consider: include a preliminary exploratory data analysis, evaluate data quality, review possible analysis paths;  
* Degree of detail of your analysis;
* Originality;","","Analyze discussions on cricket from Reddit","Explore the rich textual content from r/Cricket subreddit","10/31/2021 23:59:00","0"
"5761","1533360","793761","08/16/2021 10:49:42","## Task Details
Classify, from a given retina image, which of the 46 diseases are present.

## Suggested metrics
- ROC Auc (ovr)
- PR Auc (ovr)","","Retinal Multi Disease Classification","","","0"
"5762","1533360","793761","08/16/2021 10:50:28","## Task Details
Create a classifier that distinguishes normal vs abnormal retinal images

## Suggested Metrics
- Roc Auc
- Accuracy","","Retinal Disease Eye Screening","","","0"
"6900","1736753","793761","11/22/2021 07:30:55","## Task Details
Using audio recordings, classify the audio into one of the 3 or 4 labels (the 4th can be 'other_vocalizations' located in the 'extra' folder).


## Suggested Metrics
- Accuracy
- AUPRC, AUROC","","Cat Meow Classification","","","0"
"6257","1629190","793761","10/05/2021 05:48:37","## Task Details
- What characteristics do the most successful board games have in common?
- What game mechanics users enjoy the most?
- How are board games distributed across genres?
- What board games are hidden gems?

These questions are just for reference, use one or come up with your own and enjoy the data :)","","Explore Board Games","","","0"
"6258","1629190","793761","10/05/2021 05:51:24","## Task Details
Predict a game Rating Average based of its other features, excluding columns such as ranking, id, name, users rated and owned users.

## Suggested Metrics
- MAE
- RMSE","","Predict Board Game Average Rating","","","0"
"5142","1470543","803207","07/16/2021 06:29:46","## Task Details
Visualize Nullah's by Coordinates.

## Expected Submission
Geo Spatial Visualization Task

## Evaluation
Visualization Link

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.dropbox.com/sh/b1gorubv0zcuhv0/AABJBQipBu-UUdLBriXkatcja?dl=0&preview=Karachi+map+showing+documented+nullahs+and+drains+for+sewage-Karachi-MAP-SOURCE+CLICK-WB.pdf","","Visualize Nullah's by Coordinates","Geo Spatial Visualization Task","","0"
"6966","1748371","844176","11/29/2021 18:39:04","## Task Details
If ""parallel"" databases can be designed, there are quite a few interesting experiments to be done.
For example, make data of the same commands, but uttered by different automatic assistants. To what extent are algorithms able to distinguish mechanical sound from live speech?
Or build a database of the same commands in other languages. How similar are the same words in different languages on the spectrograms?

## Expected Submission
A database with 2-5 thousand sounds —Åould be a good result.

## Evaluation
Successful can be considered a classification that allows you to determine whether it is mechanical or live speech with an accuracy of over 90 percent.

### Further help
I think it is not possible to realize this project individually.","","What could we learn from audio processing and recognition?","Audio Deep Learning","","0"
"5552","1475786","861427","08/04/2021 15:47:56","## Task Details

`athletes.csv` file contains column `athlete_medals`. It has a number of GOLD, SILVER, and BRONZE medals with descriptions for each athlete. The goal of this task is to split the column into 3 columns called `gold`, `silver`, and `bronze` with a separate number of medals respectively.

## Expected Submission
Submit a notebook that implements the splitting using the pandas library.

## Evaluation
This is not a competition. It's just a data preprocessing task.","","Split athlete medals into 3 columns","","","0"
"5147","1471146","863388","07/16/2021 13:57:51","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Product Details","","09/30/2021 23:59:00","0"
"5118","1466958","863388","07/14/2021 09:28:45","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Amazon France Product Details","","09/30/2021 23:59:00","0"
"5517","1510138","863388","08/03/2021 08:19:05","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Canada Product Review Dataset","","09/30/2021 23:59:00","0"
"5581","1516155","863388","08/06/2021 13:01:30","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Monster UK Job Listing Dataset","","09/30/2021 23:59:00","0"
"5420","1500708","863388","07/30/2021 10:05:29","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Indeed Job Listings","","09/30/2021 23:59:00","0"
"5771","1535471","863388","08/17/2021 11:36:12","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Indeed Job Listings","","09/30/2021 23:59:00","0"
"5713","1528200","863388","08/13/2021 12:12:18","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Amazon France Product Dataset","","09/30/2021 23:59:00","0"
"5656","1523126","863388","08/10/2021 11:41:20","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Ebay Product Dataset","","09/30/2021 23:59:00","0"
"5813","1542284","863388","08/20/2021 08:22:35","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Indeed USA Job Listings","","09/30/2021 23:59:00","0"
"6045","1579970","863388","09/09/2021 11:38:24","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of CareerBuilder USA Job Listings","","10/31/2021 23:59:00","0"
"6029","1576023","863388","09/07/2021 10:29:45","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Seek Australia Job Listings","","10/31/2021 23:59:00","0"
"6184","1608885","863388","09/24/2021 13:21:34","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Monster USA Job Listings","","12/31/2021 23:59:00","0"
"6146","1603071","863388","09/21/2021 11:49:17","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Amazon Product Dataset","","10/31/2021 23:59:00","1"
"6880","1731088","863388","11/19/2021 14:48:37","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Monster USA Job Listings","","12/31/2021 23:59:00","0"
"6941","1747952","863388","11/26/2021 14:39:06","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Monster USA Job Listings","","12/31/2021 23:59:00","0"
"6323","1643081","863388","10/12/2021 10:45:49","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Amazon Product Dataset","","12/31/2021 23:59:00","0"
"6341","1647197","863388","10/14/2021 10:31:33","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Indeed India Job Listings","","12/31/2021 23:59:00","0"
"6404","1658085","863388","10/19/2021 13:47:15","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Amazon France Product Dataset","","12/31/2021 23:59:00","0"
"6435","1664503","863388","10/22/2021 11:37:30","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Product Review Dataset","","12/31/2021 23:59:00","0"
"6907","1739843","863388","11/23/2021 10:21:29","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Seek Australia Job Listings","","12/31/2021 23:59:00","0"
"6855","1724086","863388","11/16/2021 14:47:35","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Indeed USA Job Listings","","12/31/2021 23:59:00","0"
"6831","1712473","863388","11/12/2021 12:44:14","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 5K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Canada Review Dataset","","12/31/2021 23:59:00","0"
"6791","1704850","863388","11/09/2021 13:12:31","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Walmart Product Listing Dataset","","12/31/2021 23:59:00","0"
"6686","1672746","863388","10/26/2021 14:06:50","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Monster UK Job Listings","","12/31/2021 23:59:00","0"
"6710","1681361","863388","10/29/2021 09:04:13","###Task Details
Every task has a story. this task is being created to understand in what other ways can this data be used. 

###Expected Submission
Users must submit a full-blown analysis of the sample dataset that is available here. 

###Evaluation
The submissions will be equally reviewed by us and the winner will get a special prize at the end of the competition. 

This dataset was created in house by our web scraping and data mining teams at PromptCloud and DataStock

###Context
This dataset was created by our in house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records.

###Content

###Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.

###Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world. This data will help them with their projects and analysis. Do download the sample first before making the purchase of the dataset.","","Analysis of Ebay Product Listing Dataset","","12/31/2021 23:59:00","0"
"5796","1539671","1116576","08/19/2021 01:52:25","## Task Details
Find the proportions of each user type that use each station. Visualize your findings somehow, possibly annotating any stations or areas of note.

## Expected Submission
A notebook with analysis and visualizations. Map-based visualizations would probably be very useful on a dataset like this. You should expect to find some kind of pattern in the station usage, either time- or location-based.

## Evaluation
A good submission should visualize the results in such a way that any patterns found are immediately apparent to someone viewing it for the first time.

![Map showing section of downtown Chicago with busy bicycle stations](https://i.imgur.com/wInxtoL.png)","","Which stations do users prefer?","The Divvy bike-sharing service has members and casual users. Which stations do each type prefer?","","1"
"5600","1517723","4656934","08/07/2021 16:18:21","Perform Time Series Analysis and Methods.","","Time Series and EDA","","","1"
"5745","1529737","4656934","08/15/2021 08:42:13","Perform Time Series Analysis and EDA.","","Time Series Analysis and EDA","","","0"
"5844","1545861","1165599","08/22/2021 16:05:40","Have a favourite team? Prove to the world that they are the best!
- Have a favourite player? Prove to the world that he is the best!

Use Visualisation / Analysis tools to show why your favourite team/player is the best among all.

Brownie points for notebooks that can analyse everyone objectively and thus, will help cricket newbies pick their favourites!","","Is your favourite the best?","","","0"
"5845","1545861","1165599","08/22/2021 16:14:09","Do you think some matches were weird/peculiar compared to others? Why so? 

Give your metric for ""weirdness"" and give which match IDs are weird acc. to you!","","Weird/Peculiar matches?","","","0"
"5846","1545861","1165599","08/22/2021 16:17:05","What effect does the powerplay have on the overall score? Is there a strong correlation? Which teams behave differently? Which venues behave differently?","","Powerplay vs. Complete","","","0"
"5842","1545861","1165599","08/22/2021 15:59:49","Using past to predict the powerplay score in the 2021 season! (Powerplay = first 6 overs) 
Also, free to use any external sources too!

Metric: MSE between the predicted and actual scores (Lower the better) 

Brownie points for simpler architectures!","","Predict 2021 Powerplay Score","","","0"
"5996","1569945","1260510","09/03/2021 20:50:15","## Task Details
Suicide is grave, maybe data can save lives!","","Exploratory Data analysis based on profession of suicide victims","Maybe if we reach out on time, we can save some lives","","1"
"6936","1746565","1272482","11/26/2021 06:16:20","## Task Details
Generate Assamese poem using  recurrent neural network (RNN) and its variants

## Expected Submission
Submit a notebook that implements the full lifecycle of data preparation, model creation and evaluation. Feel free to use this datasets  plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model to generate Assamese text(poems). 

## Evaluation
we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).","","Assamese Text Generation","Generate Assamese poem using  recurrent neural network (RNN) and its variants","","0"
"6279","1634953","1353846","10/07/2021 20:32:50","Undertake any cleansing or pre-processing you think is necessary on the dataset. In your report, explain clearly what you have done and why you have done it. Some cleaning could be to remove any feature/column if 60% missing values, detect/remove outliers or to remove duplicate and highly correlated information.

Split the data into a training set and a test set once cleansing is done. Use suitable toolkit and libraries (Python, Orange, Weka, or R whichever platform you are comfortable with) to train models (e.g. Decision Tree, Random Forest or SVM) from the training set to build the Loan Status Classifier.
Make sure to explicitly discuss methods applied to deal with class imbalance, feature selection and other tuning or validation methods you used to improve the quality of your developed models.","","Data Mining the Lending Club Loan Dataset","","","0"
"5117","1465051","1371046","07/14/2021 08:40:59","## Task Details
This task comprises of visualizations to effectively arrange and express data. In this task, the requirement is to visualize the dataset with/without cleaning and understand the distribution of the dataset. Can be further used to generate insights.

## Expected Submission
Users are free to submit notebooks or pictures of their work.","","Data Visualization","","","0"
"5236","1478862","1383867","07/20/2021 07:35:34","Remove duplicate records from the dataset","","Remove duplicate records from the dataset","","","5"
"5237","1478862","1383867","07/20/2021 07:36:31","Create a word and emoji cloud for the most words used","","Create a word and emoji cloud for the most words used","","","4"
"5238","1478862","1383867","07/20/2021 07:37:11","Create a plot between positive and negative word counts","","Create a plot between positive and negative word counts","","","3"
"5518","1510179","1383867","08/03/2021 08:26:23","Detect whether a person is in deep sleep or not","","Detect whether a person is in deep sleep or not","","","0"
"6432","1663855","1383867","10/22/2021 04:49:41","## Task Details
The levels are of zelda game. There are equal number of playable and unplayable levels. Classify levels into these two categories.


## Evaluation
Better F1 score","","Classify playable levels","","","3"
"6712","1663855","4071979","10/29/2021 10:53:32","## Task Details
Trsnafer learning has mad e life easier by using the model trained on different dataset and predicting on our objects

## Expected Submission
for starting accuracy ca be around 70 but it would be excellent if its around 85 or more

## Evaluation
model should have good recall and fscore

### Further help
@adizafar is the main contributor his code can be really helpful if you are just starting but try to be innovative as well","","Use Transfer Learning to predict playable and un playabel","transfer learning","","0"
"6969","1756890","1383867","11/30/2021 07:00:30","Perform EDA
- Shortlist Important Features 
- Generate new features from existing ones 
- Use different machine learning models to predict landslides 
- Evaluation will be done on AUC Metric","","Predict Landslide","","01/10/2022 23:59:00","5"
"5187","1474781","1488252","07/18/2021 09:46:34","## The Create visualizations of Top 10 salaries among 22046 rows of data","","Top 10 Salaries","Create Visualizations","","0"
"5837","1545171","1541942","08/22/2021 03:13:57","## Task Details
Pemerintah akan mengubah warna dasar plat kendaraan pribadi dari hitam ke putih dengan alasan bahwa warna dasar hitam itu sulit dideteksi oleh ETLE 

## Expected Submission
Buatlah sebuah analisis citra digital dengan bahasa Python (Jupyter notebook).
Metode yang dipakai adalah:
1. Edge detection (minimal) baik yang filter spasial atau HPF
2. Teknik lain 

## Evaluation
Penilaian:
1. Metode
- edge detection (60%)
- metode lain (10%)
2. Analisis (30%)
- membandingkan apakah ada perbedaan antara warna dasar hitam dengan putih","","Amati perbedaan plat","","08/24/2021 23:59:00","0"
"5675","1525182","1541942","08/11/2021 14:43:19","## Task Details
Klasifikasikan dengan model Decision Tree dan Naive Bayes Classifier

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
Bandingkan hasil akurasinya mana yang lebih bagus

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Klasifikasi Tumor","","08/13/2021 23:59:00","2"
"5673","1525182","1541942","08/11/2021 13:24:41","## Task Details
Prediksi tumor

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Praktikum Regresi Logistik","","08/12/2021 23:59:00","0"
"6251","1603525","1571785","10/04/2021 08:20:58","Using Text Mining and NLP, identify most common terms, keyphrases, and ngrams in the complications dataset.","","Find most frequently occurring terms in the complications database","","","1"
"6212","1615112","1571785","09/29/2021 08:15:13","## Task Details
Create a notebook(s) and analyze the data of tech company fundings since January 2020. Identify insights such as - mean funding amounts, top regions, unique regions, frequency in tech fundings, etc. Which other categories have similar trends as AI fundings? Try to share a clear narrative and insights related to funding trends","","Artificial Intelligence Funding Trends","Create a notebook to analyse tech funding trends for the AI category","","0"
"6284","1615112","1571785","10/08/2021 14:12:24","Analyse the data and identify different patterns in fundings in USA, UK and other countries","","Compare Funding Trends - USA, UK, Others","","","0"
"6233","1624191","1571785","10/02/2021 02:45:28","Use exploratory analysis and data visualizations to compare the differences in insights / trends among Disney+ and other platforms such as Netflix. The netflix data is available here - https://www.kaggle.com/shivamb/netflix-shows","","How does the insights compare with Other Platforms (Netflix)","Compare the top insights of Disney+ with other platforms","","8"
"6977","1624191","1571785","12/01/2021 02:33:27","Inputs: 
1. User Previous Watched Movies List

Outputs: 
1. Similar Movies / TV Shows on the basis of similarity in the text, genre, actors etc.","","Create a Disney+ Recommendation Engine","Create a framework to recommend similar movies / shows","","1"
"6822","1661658","1571785","11/11/2021 14:31:14","Use NER, Ngram Analysis, text mining, identify - 

1. Top / Popular Keywords used 
2. Popular Entities 
3. Popular Topics","","Find Most Popular Named Entities, Ngrams, and Keywords","Use NLP to identify popular terms mentioned in the company's mission statements","","0"
"6306","1636669","1571785","10/11/2021 04:39:58","Identify Interesting Correlations between Video Title features and the Video views","","Interesting Correlations ?","","","1"
"6994","1636669","1571785","12/03/2021 05:04:15","using nlp , text mining, analyse how this company writes the headlines","","Analyse headline / title writing styles","","","0"
"6684","1672910","1571785","10/26/2021 13:46:23","Perform clustering using one of the popular methods and identify interesting trends related to different clusters, also identify definitions and rules corresponding to each cluster.","","Customer Segmentation","segment customers based on demographics + transaction data","","6"
"6685","1672910","1571785","10/26/2021 13:47:28","RFM is a popular type of analysis in the banking sector. This data can be used to perform RFM analysis and further identify trends associated with different clusters.","","Recency, Frequency, Monetary Analysis","","","1"
"6673","1669380","1571785","10/25/2021 04:01:45","All 4 top OTT platforms data is updated on kaggle. an analysis of how these platform differs would be interesting 

- [Amazon Prime Video Movies and TV Shows](https://www.kaggle.com/shivamb/amazon-prime-movies-and-tv-shows)
- [Disney+ Movies and TV Shows](https://www.kaggle.com/shivamb/disney-movies-and-tv-shows)
- [Netflix Prime Video Movies and TV Shows](https://www.kaggle.com/shivamb/netflix-shows)
- [Hulu Movies and TV Shows](https://www.kaggle.com/shivamb/hulu-movies-and-tv-shows)","","Compare OTT Platforms","Compare netflix, prime, disney+, hulu","","0"
"6821","1697740","1571785","11/11/2021 14:27:22","There are two targets - Failure (binary) and Failure Type (multiple)","","Create a Modelling Strategy to predict Failure and Failure Type","Create multimodels pipeline to predict if a machine requires maintainence","","1"
"6823","1710559","1571785","11/11/2021 15:57:54","Expected submission - NLP model which can analyse case title and case text to classify likely outcome","","Create a NLP Model to classify legal text","legal case text classification using nlp","","0"
"6857","1725235","1571785","11/17/2021 04:21:39","One can restrict the problem to top 5 or top 10 classes.","","Model to predict Emotion (Multi Label)","Create NLP Models to predict the probability of different labels","","0"
"6978","1735467","1571785","12/01/2021 02:40:16","perform exploratory data analysis to identify most popular ones","","Identify Most Popular Tenders","find popular tenders by amount, agencies, date etc","","0"
"6898","1735543","1571785","11/22/2021 01:59:30","Using Clustering, identify which brands produce similar colored products","","How Similar are the Brands","","","1"
"6958","1754460","1585870","11/29/2021 08:41:32","Predicting income and and expenses for the next month of each user.","","Predicting income and and expenses for the next month of each user","","","0"
"5315","1488710","1596400","07/24/2021 23:43:32","## Task Details
Each row is a relationship between a Dune character and another person.  You can add a row for additionally relationships you know of, based on the events described in the novel.  Please add a URL as a source for your addition.","","Add relationships for Dune characters","","","0"
"5696","1504463","1626886","08/12/2021 14:45:22","## Task Details
There are only 47 data points and 15 predictors fitting any model would result in overfit. 
Apply Principal Component Analysis and then create a regression model using the first few principal components to perform dimension reduction.

## Expected Submission
Notebook with the implementation of PCA on data dataset

## Evaluation
Specify the model in terms of the original variables (not the principal components), and compare its quality to that of the simple linear regression model specified [here](https://www.kaggle.com/shilpagopal/basic-data-exploration)

### Further help
Basic EDA - https://www.kaggle.com/shilpagopal/basic-data-exploration
https://learnche.org/pid/latent-variable-modelling/principal-components-regression","","Principle Component Analysis","","","2"
"5698","1504463","1626886","08/12/2021 15:18:59","## Task Details
For small datasets where the collection of new data points are costly, it's important to perform variable selection so that model overfit can be reduced. 

Using the dataset build regression models using:
1. Stepwise regression 
2. Lasso
3. Elastic net

## Expected Submission
Notebook with analysis on there three different types of variable selection methods. 

## Evaluation
Compare its quality(Adjusted R-Square) to that of the simple linear regression model specified [here](https://www.kaggle.com/shilpagopal/basic-data-exploration)


### Further help
[https://www.kaggle.com/shilpagopal/basic-data-exploration ](https://www.kaggle.com/shilpagopal/basic-data-exploration )
[https://link.springer.com/article/10.1057/jt.2009.26 ](https://link.springer.com/article/10.1057/jt.2009.26)","","Variable Selection","","","1"
"6030","1576323","1669992","09/07/2021 13:15:27","NOTE:  The ""Other"" subfolder in the dataset consists of images of ""SuperCab"" which can be either used as an additional class or can be merged in the existing ""Cab"" class.","","Car Body Type Classifier","Build an image classifier to successfully predict body type of a car such as Cab, Hatchback, Sedan, SUV etc.","","0"
"6942","1744034","1858684","11/26/2021 21:39:47","## Task Details
Use the 12 month dataset from Cyclistic bike-share to understand how members and casual riders use bikes differently. Understanding this difference in customer behavior is key to the profitability of cyclistic bikes.

## Expected Submission
Submit a notebook containing visualizations and recommendations about how members and casual riders use bikes differently.","","How do members and casual riders use Cyclistic bikes differently?","","","1"
"6968","1754040","1900190","11/30/2021 03:48:46","Exploratory Data Analysis","","Exploratory Data Analysis","","","2"
"6954","1754040","1900190","11/29/2021 05:09:02","Top issues that the user's are facing in each of these Apps","","Top issues that the user's are facing in each of these Apps","","","0"
"6955","1754040","1900190","11/29/2021 05:10:19","Analyze how responsive is the App management team for each application","","Analyze how responsive is the App management team for each application","","","0"
"6699","1674549","1900190","10/28/2021 03:26:58","Analyze which countries have the highest % of child marriage among both boys and girls.","","Analyze which countries have the highest % of child marriage among both boys and girls.","","","0"
"6701","1674549","1900190","10/28/2021 03:29:32","Find out which countries have made good progress when it comes to diminishing the child marriage","","Find out which countries have made good progress when it comes to diminishing the child marriage","","","0"
"6724","1683894","1900190","10/30/2021 14:39:04","Exploratory Data Analysis","","Exploratory Data Analysis","","","0"
"6749","1692658","1900190","11/03/2021 14:47:05","Exploratory Data Analysis","","Exploratory Data Analysis","","","1"
"6374","1651530","1900190","10/16/2021 04:11:55","Visualize and compare the nutritional content of various serial brands","","Visualize and compare the nutritional content of various serial brands","","","1"
"6375","1651530","1900190","10/16/2021 04:12:39","Are there any cereals that are virtually identical?","","Are there any cereals that are virtually identical?","","","0"
"6446","1651530","8406617","10/24/2021 05:53:27","## Task Details



I'd like to see a visual representation of healthy sizes of best brands for you.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Nutritional size","Using nutritional data to viz best cereal","","1"
"6162","1605920","1900190","09/23/2021 04:08:16","What additional insights can you uncover and present to give a more thorough analysis of these two players?","","What insights can you uncover and present to give a more thorough analysis of these two players?","","","0"
"5129","1466697","1900190","07/15/2021 02:38:50","Calculate the total Number of vaccinated people per country","","Calculate the total Number of vaccinated people per country","","","6"
"5130","1466697","1900190","07/15/2021 02:39:57","Name the country where maximum vaccination has taken place in a day","","Name the country where maximum vaccination has taken place in a day","","","0"
"5131","1466697","1900190","07/15/2021 02:40:54","Trend of vaccination doses with time","","Trend of vaccination doses with time","","","1"
"5132","1468382","1900190","07/15/2021 04:37:06","Sentiment Analysis of people about ZomatoIPO","","Sentiment Analysis","","","0"
"6824","1710686","1901912","11/11/2021 17:02:25","## Task Details
This is a simple classification problem used to challenge students to build a better model. 

## Expected Submission
 Solve the task primarily using Notebooks. 

## Evaluation
Typically we use AUC or LogLoss but also useful for Precision AUPR

--- dataset summary --- 
output_file            = churn_100k.csv
n_samples            = 101000
n_features            = 28
pct_missing          = 1.00%

--- model features --- 
numeric_features     = ['monthly_minutes', 'customerServiceCalls', 'streaming_minutes', 'TotalBilled', 'PrevBalance', 'latePayments']
categorical_features = ['ip_address_asn', 'phone_area_code', 'customer_reg_date', 'email_domain', 'phoneModel', 'billing_city', 'billing_postal', 'billing_state', 'partner', 'PhoneService', 'MultipleLines', 'streamingPlan', 'mobileHotspot', 'wifiCallingText', 'OnlineBackup', 'device_protection', 'number_phones', 'contract_code', 'currency_code', 'maling_code', 'paperlessBilling', 'paymentMethod']

--- dataset performance ---
Train AUC Score        : 0.967279
Eval  AUC Score        : 0.958073
Test  AUC Score        : 0.946909","","Identify Churners","","","1"
"6731","1686035","1996737","10/31/2021 17:40:32","## Task Details
Detect object on image along with class prediction

## Expected Submission
Since there are only one object per image, IoU is considered as the best metric","","Object detection","","","0"
"6732","1686035","1996737","10/31/2021 17:43:00","## Task Details
Segment single object on image. Class prediction is not required for this task

## Expected Submission
Since there are only one object on image, mask IoU is considered as the best metric","","Object instance segmentation","","","0"
"6733","1686035","1996737","10/31/2021 17:44:21","## Task Details
Localize single object on image

## Expected Submission
Since there are only one object per image, IoU is considered as the best metric","","Object localization","","","0"
"6734","1686035","1996737","10/31/2021 17:46:34","## Task Details
Segment objcet on image and predict it class

## Expected Submission
mAP is the best metric for this class - it consider predicted along with IoU","","Object semantic segmentation","","","0"
"6735","1686035","1996737","10/31/2021 17:48:08","## Task Details
Classify object presented on image","","Image classification","","","0"
"5625","1520152","2016045","08/08/2021 22:55:19","Characters with more dialogue.
- Most repeated words.
- Use wordclouds to visually represent the most repeated words.
- Most repeated bigrams (and trigrams, if you want)
- Network of bigrams (or trigrams). 
- Visualize all of the relationships among words
- Most repeated words for each character
- Most relevant words for each character using tf‚Äìidf (bind_tf_idf() function)
- Categorize the words in a binary fashion into positive and negative categories using the bing lexicon (get_sentiments() function)
- Categorize the words using the nrc lexicon (get_sentiments() function)
- Categorize the words using the AFINN (get_sentiments() function)
- Sentiment analysis for each character","","Text Mining tasks","""Stupid is as stupid does"" - Forest Gump","","0"
"5296","1479724","2048048","07/23/2021 17:53:55","## Task Details
In this task, you need to create a predictive model using machine learning which can predict future prices of Gold

## Expected Submission
The detailed notebook with sample prediction to demonstrate the capability of model.

## Evaluation
Evaluate your model based on RMSE, Adjusted R2 and MAE","","Building machine learning based Gold price predictor model","","","2"
"5293","1486303","2048048","07/23/2021 15:49:22","## Task Details
Develop a Machine Learning or Deep Learning model to predict whether the borrower will default in the future based on his/her demographic, financial  and loan details.

## Expected Submission
A well-explained notebook that accomplishes the task.

## Evaluation
After making the model try to answer the following questions:

Given the customer's demographic, financial and loan details can you answer this customer will default in future?

ROC is the evaluation metrics for judging your model performance.

Train model on 80%, validate on 10% and finally test your model on 10% unseen data","","Predict Default Loans in advance","Predict if the loan is going to default in future","11/01/2021 23:59:00","2"
"6741","1690571","2102167","11/02/2021 17:17:20","## Task Details
Heart Disease is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year and exerting a significant financial burden on the economy.

Build a binary classifier for heart disease using the CDC's BRFSS 2015 survey data. None of the features in this dataset require medical intervention to obtain, and can all be asked as questions. See how well these classifiers compare to more standard ones that take blood measurements or use medical equipment to obtain.

## Expected Submission
Try to split up the data and obtain high evaluation metrics. 
* Accuracy
* Sensitivity
* Specificity
* PPV
* NPV

## Evaluation
This is not a formal competition or task. Just share code and results as well as thoughts on the applicability of the CDC's BRFSS datasets towards health condition prediction using machine learning.

NOTE: I am not associated with the CDC or the collection of BRFSS data. I just cleaned the data from [here](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system) using the code [here](https://www.kaggle.com/alexteboul/heart-disease-health-indicators-dataset-notebook).","","Binary Classification Heart Disease","Use a cleaned version of the CDC's BRFSS 2015 dataset for heart disease prediction.","01/01/2025 23:59:00","4"
"6744","1690571","2102167","11/03/2021 04:06:43","## Task Details
Heart Disease is among the most prevalent chronic diseases in the United States, impacting millions of Americans each year and exerting a significant financial burden on the economy.

Conduct Exploratory Data Analysis (EDA) on this dataset. 

## Expected Submission
Submit a notebook that has performed EDA on the dataset. 

Check out class imbalances, data types of the variables, potential problems with the dataset, concerns for the model building process, and visualizations.

## Evaluation
This is not a formal competition or task. Just share code and results as well as thoughts on the applicability of the CDC's BRFSS datasets towards health condition prediction using machine learning based on your EDA.

NOTE: I am not associated with the CDC or the collection of BRFSS data. I just cleaned the data from [here](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system) using the code [here](https://www.kaggle.com/alexteboul/heart-disease-health-indicators-dataset-notebook).","","Exploratory Data Analysis (EDA)","Explore the data and produce visualizations","01/01/2025 23:59:00","1"
"6787","1703281","2102167","11/08/2021 22:23:25","## Task Details
Conduct Exploratory Data Analysis (EDA) on this dataset. 

## Expected Submission
Submit a notebook that has performed EDA on the dataset. 

Check out class imbalances, data types of the variables, correlations between variables, potential problems with the dataset, concerns for the model building process, and visualizations.

## Evaluation
This is not a formal competition or task. Just share code and results as well as thoughts on the applicability of the CDC's BRFSS datasets towards health condition prediction using machine learning based on your EDA.

NOTE: I am not associated with the CDC or the collection of BRFSS data. I just cleaned the data from [here](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system) using the code [here](https://www.kaggle.com/alexteboul/diabetes-health-indicators-dataset-notebook).","","Exploratory Data Analysis (EDA)","Explore the data and produce visualizations","01/01/2025 23:59:00","3"
"6788","1703281","2102167","11/08/2021 22:26:46","## Task Details
Create predictive models for diabetes risk (binary and multi-class when including prediabetes). Important for public health surveillance and outreach to know how well ML models can predict diabetes risk.

## Expected Submission
Experiment on the full dataset and binary balanced and unbalanced datasets to obtain high evaluation metrics.

Accuracy
Sensitivity/Recall
Specificity
Precision/PPV
NPV

## Evaluation
This is not a formal competition or task. Just share code and results as well as thoughts on the applicability of the CDC's BRFSS datasets towards health condition prediction using machine learning.

NOTE: I am not associated with the CDC or the collection of BRFSS data. I just cleaned the data from [here](https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system) using the code [here](https://www.kaggle.com/alexteboul/diabetes-health-indicators-dataset-notebook).","","Model Building","Create predictive models for diabetes risk (binary and multi-class when including prediabetes)","01/01/2025 23:59:00","4"
"6321","1642776","2126864","10/12/2021 08:28:32","Generate a complete analytics dashboard for the given log file analyzing all aspects of this log data individually.","","Generate Dashboard for the Log File","Generate an Analytics Dashboard for the given log file","","0"
"6828","1709366","6982946","11/12/2021 07:08:26","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.","","Visualization Forbes World's Billionaires 2018","","","2"
"7032","1740769","2226027","12/09/2021 06:27:38","## Task Details
The reviews are already preprocessed. You have to perform some vectorization and feed that into a suitable classifier which can predict the sentiment from a given preprocessed review.

## Expected Submission
You are expected to submit the implementation of classification and prediction task. Perform some data visualization which may include the accuracy evaluation with different encodings and hyperparameters of the classifier.","","Sentiment Classification","Performing sentiment analysis and prediction","","0"
"6721","1683429","2226962","10/30/2021 10:24:59","## Task Details
you want to know which video / image is Fake

## Expected Submission
Select F1 scope mertics and approach","","Classification task","build model to find Fake images","","1"
"5331","1490083","2226962","07/25/2021 14:43:21","## Task Details
try to predict if image contains Pneumothorax or not

## Expected Submission
Use any library for best results 

## Evaluation
ROC_AUC 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Binary classification task with CNN","","","2"
"5597","1517966","2226962","08/07/2021 13:19:54","## Task Details
You want to train your model to identify positive or negative the review is

## Expected Submission
You can use any metrics w","","Classify the review -&gt; trying to predict positive or negative","","","1"
"5606","1519649","2226962","08/08/2021 15:25:16","Using a dataset with indicators of user behavior, you need to solve the classification problem and predict whether the user is a fraud or not. 

You need to get the maximum recall for fraudsters, while the precision is at least 95%","","Get precision at least 95%","use any models","","7"
"6393","1595976","2337246","10/17/2021 06:42:42","New York saw a drop over two weeks in their search conversion ‚Äì Can you, with the given data set identify, what are the biggest drivers of this drop and predict the next 4 weeks conversion?","","Search Drop and Conversion","","","0"
"6140","1600799","2361711","09/20/2021 11:39:20","## Task Details
The Quran has 6236 verses in total. For each Arabic verse, we want to find out the number of letters and symbols.

## Expected Submission
The expected submission is a CSV file with 2 additional columns - no_of_letters and no_of_symbols for each verse

## Evaluation
The evaluation depends on the accuracy. This is slightly difficult since the language is Arabic.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Arabic letter and symbol count","","","0"
"6141","1600799","2361711","09/20/2021 11:48:15","## Task Details
The Quran has 6236 verses and 18,994 unique words. The task is to get a count of every word using the data set and create a word cloud. This is tricky since the language is Arabic.

## Expected Submission
The expected submission is  notebook with the word-cloud and a CSV file with word in Arabic and it's frequency.

## Evaluation
A good solution should be accurate and take care of all the nuances of the language. For example - Ÿ±ŸÑŸíÿ®ŸéÿµŸêŸäÿ±Ÿè (AL-Baseer) is two words - 'The' and 'All-Seeing' 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Arabic Word Frequency and Word Cloud","","","0"
"6218","1620965","2376244","09/30/2021 10:44:51","Fake Reviews are the biggest roadblocks that hinder the decision-making abilities of a potential consumer of a product or a future employee of a firm. Fake Reviews are often misleading. There are three parties who practice posting fake reviews on company online review platforms and they are: 

1. The company: When a company pumps fake reviews on a platform it is mostly positive. They rate themselves high and give themselves a great review to attract skilled labor and lure them.

2. The competitors: When competitors post fake reviews the nature of the review is mostly negative. They bully the target firm to mislead its potential skilled candidates. 

3. The Platform: No one likes to visit a platform with fewer reviews and hence it is a practice followed by the platforms to post fake reviews and attract other potential reviewers and consumers to engage on their platform. 

AmbitionBox is one such platform where one can find a lot of fake reviews. And especially some reviews might not be fake but they do not sound like a good review due to the lack of interest in elaborating.

This data repo contains two files: a train set where every review is a fake review. 
test set where most of the reviews are fake. 


Can you explore these datasets and figure out what reviews are fake or not fake, understand the nature of the fake review (positive, negative, neutral) and classify reviews as fake or not fake. Further classify the fake reviews as Competitor's Fake Reviews, Company's Fake Reviews, and platform's fake reviews (time series analysis should give you some information on when and how platform engages in posting fake reviews)?","","NLP Fake Company Reviews Classifier","Create a boolean field and tag fake and non-fake reviews","","0"
"6229","1622972","2382789","10/01/2021 16:58:15","To use this dataset, please cite ""Kobat, M.A.; Tuncer, T. Coronary Angiography Print: An Automated Accurate Hidden    Biometric Method Based on Filtered Local Binary Pattern Using Coronary Angiography Images. J. Pers. Med. 2021, 11, 1000. https://doi.org/ 10.3390/jpm11101000""","","Citation","Cite","","0"
"6293","1555134","2405002","10/10/2021 06:22:18","This is a weblog file.
A weblog file is nothing but a file that gives information about the events that happens in a software or operating system. The software keeps the log and the file has all the logs.
More on weblog files üëá
https://en.wikipedia.org/wiki/Logging_(software)

The task is to clean the data and transform it into a more readable format. Then just analyze the data.

Here are few things that you can do to analyze.

1. Write a regular expression to convert the data in a structured o/p (sample o/p shown below)
‚Ä¢ Use the regular expression to read the data using pandas read_csv function.

2. Identify number of requests for each hour and plot the same using line plot. Use the following instructions
‚Ä¢ Convert the data type of time column to datetime using pd.to_datetime() with appropriate time format
‚Ä¢ Use resample function to identify number of requests per hour and plot the same using line plot
‚Ä¢ Interpret the chart and identify from which hour of the day, the number of page visits increases

3. Identify different types of request_type along with their frequencies.

4. Analyze size column using the following instruction
‚Ä¢ Convert size column to numeric (Remove special characters, if necessary)
‚Ä¢ Using pd.cut, divide size column in to 10 bins.
‚Ä¢ Identify the bin which has the maximum count

5. Using url column, create a new column file_type based on following conditions
‚Ä¢ If the url ends with either .gif or .jpg, file_type will be image
‚Ä¢ If the url ends with .html, file_type will be html
‚Ä¢ Else file_type will be Other
‚Ä¢ Identify how many times images were rendered","","Data cleaning, wrangling and analysis","","","0"
"6043","1469970","5090286","09/08/2021 22:34:03","## Task Details
Try to predict the age of the gender using different algorithms.
Explore the data using different libraries.
Try to visualise the data as much as possible.
ENJOY!","","Predict The AGE!","","","0"
"6133","1599617","2439894","09/19/2021 18:24:32","## Task Details
Dataset contains voices in Azerbaijan language  with 4 different confirmation words (yes-b…ôli,no-xeyr,h…ô-yes,no-yox) 
Recognize confirmation word from the voice or detect if it is positive or negative

## ∆ètraflƒ±
Dataset-d…ô Az…ôrbaycan dilind…ô 4 f…ôrqli t…ôsdiq s√∂zl…ôri olan s…ôsl…ôr yer alƒ±r (h…ô,yox,b…ôli,xeyr)
Veril…ôn s…ôsin onun t…ôsdiq yoxsa inkar olduƒüunu, yaxud s√∂z√ºn √∂z√ºn√º(h…ô,yox,b…ôli,xeyr) t…ôyin edin","","Recognize confirmation word from voice in Azerbaijan language","","","0"
"6356","1648531","2448949","10/14/2021 21:22:08","Task Details
Build an Exploratory Data Analysis for one or more countries of your choice.

Expected Submission
Solve using a Notebook, and be creative!

Evaluation
Solutions that show correlations between variables and good graphical analysis are considered the best in this case.","","Global Nuclear Energy EDA","Exploratory Data Analisys for Global Nuclear Energy Generation","","0"
"6973","1633303","2487225","11/30/2021 13:14:26","# Î™®ÏùòÍ≥†ÏÇ¨ 
- Ïù¥ÏßÑ Î∂ÑÎ•ò
- Ïù¥ÏßÅÌï† ÏÇ¨ÎûåÏùÑ Ï∞æÎäî Î¨∏Ï†ú(ÏßÄÏõêÏûêÍ∞Ä ÏùºÌï† ÌôïÎ•†)

## ÌèâÍ∞Ä
- ÌôïÎ•†Í∞í ÏòàÏ∏°
- ÌèâÍ∞Ä : ROC-AUC (Ïù¥ÏßÅÌï† ÌôïÎ•†)

## ÏòàÏ∏°

- target: 0 ‚Äì Ïù¥ÏßÅÌï† ÎßàÏùå ÏóÜÏùå, 1 ‚Äì Ïù¥ÏßÅÌï† ÌöåÏÇ¨ Ï∞æÍ≥† ÏûàÏùå

## ÏãúÏûë
- ÏãúÏûë Î∞è ÌèâÍ∞Ä ÌÖúÌîåÎ¶ø : ""[MOCK EXAM] T2. EXAM template"" copy&edit
- Ï∞ê ÏûÖÎ¨∏ÏûêÎ•º ÏúÑÌïú ÏΩîÎìú : [MOCK EXAM] T2. Ï∞êÏûÖÎ¨∏ÏûêÏö© Starter","","[MOCK EXAM1] TYPE2. HR-DATA","","02/02/2222 23:59:00","9"
"6946","1633303","2487225","11/28/2021 01:28:18","### Î∞±ÌôîÏ†ê Í≥†Í∞ùÏùò 1ÎÖÑ Í∞Ñ Íµ¨Îß§ Îç∞Ïù¥ÌÑ∞Î•º ÌôúÏö©Ìï¥
- Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
- Feature Engineering
- Î™®Îç∏ÎßÅ (Î∂ÑÎ•ò ÏïåÍ≥†Î¶¨Ï¶ò ÏÇ¨Ïö©)
- ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù (Ï¥àÎß§Í∞úÎ≥ÄÏàò ÏµúÏ†ÅÌôî)
- Î™®Ìòï ÏïôÏÉÅÎ∏î
- csvÏ†úÏ∂ú

### Ïú†ÏùòÏÇ¨Ìï≠
- ÏàòÌóòÎ≤àÌò∏.csv ÌååÏùºÏù¥ ÎßåÎì§Ïñ¥ÏßÄÎèÑÎ°ù ÏΩîÎìúÎ•º Ï†úÏ∂úÌï®
- Ï†úÏ∂úÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÄ ROC-AUC ÌèâÍ∞ÄÏßÄÌëúÏóê Îî∞Îùº Ï±ÑÏ†êÌï®

### Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò Î∞è Ïó∞Í≤∞
- data Ï∂úÏ≤ò: https://www.dataq.or.kr/ - Í≥µÏßÄÏÇ¨Ìï≠ - 759Î≤à Ï†ú2Ìöå ÎπÖÎç∞Ïù¥ÌÑ∞Î∂ÑÏÑùÍ∏∞ÏÇ¨ Ïã§Í∏∞ ÏïàÎÇ¥ - Ï≤®Î∂ÄÌååÏùº

### Îç∞Ïù¥ÌÑ∞ÏÖã ÏóÖÎ°úÎìú
- Îç∞Ïù¥ÌÑ∞ÏÖã ÌîÑÎùºÏù¥Îπó ÏóÖÎ°úÎìú : https://youtu.be/BZlEQ5JwLiA
    - Datasets - new dataset - (drag&drop) - Create / Î∞òÎìúÏãú Private
- ·Ñå·Ö°·Ü®·Ñã·Ö•·Ü∏·Ñí·Öß·Üº2 ·Ñã·Ö®·Ñâ·Öµ: https://youtu.be/_GIBVt5-khk","","T2. Exercise","","02/02/2222 23:59:00","6"
"6930","1633303","2487225","11/25/2021 23:34:02","### Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞(basic2.csv)ÏóêÏÑú Ï£º Îã®ÏúÑ SalesÏùò Ìï©Í≥ÑÎ•º Íµ¨ÌïòÍ≥†, Í∞ÄÏû• ÌÅ∞ Í∞íÏùÑ Í∞ÄÏßÑ Ï£ºÏôÄ ÏûëÏùÄ Í∞íÏùÑ Í∞ÄÏßÑ Ï£ºÏùò Ï∞®Ïù¥Î•º Íµ¨ÌïòÏãúÏò§(Ï†àÎåÄÍ∞í)
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic2.csv
- ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù : Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Ï†ïÎãµ : 91639050","","T1-22. Time-Series4 (Weekly data)","","02/02/2222 23:59:00","17"
"6934","1633303","2487225","11/26/2021 01:11:50","## f1Ïùò Í≤∞Ï∏°ÏπòÎ•º Ï±ÑÏö¥ ÌõÑ age Ïª¨ÎüºÏùò Ï§ëÎ≥µ Ï†úÍ±∞ Ï†ÑÍ≥º ÌõÑ, f1Ïùò Ï§ëÏïôÍ∞í Ï∞®Ïù¥Î•º Íµ¨ÌïòÏãúÏò§ 
### - Í≤∞Ï∏°ÏπòÎäî f1Ïùò Îç∞Ïù¥ÌÑ∞ Ï§ë ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ ÌõÑ 10Î≤àÏß∏ Í∞íÏúºÎ°ú Ï±ÑÏõÄ
### - Ï§ëÎ≥µ Îç∞Ïù¥ÌÑ∞ Î∞úÏÉùÏãú Îí§Ïóê ÎÇòÏò§Îäî Îç∞Ïù¥ÌÑ∞Î•º ÏÇ≠Ï†úÌï®
### - ÏµúÏ¢Ö Í≤∞Í≥ºÍ∞íÏùÄ Ï†àÎåÄÍ∞íÏúºÎ°ú Ï∂úÎ†•

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-23. Drop Duplicates","","02/02/2222 23:59:00","13"
"6935","1633303","2487225","11/26/2021 01:53:51","## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞(basic2.csv)ÏóêÏÑú ÏÉàÎ°úÏö¥ Ïª¨Îüº(1Ïùº Ïù¥Ï†Ñ ÏãúÏ∞® Ïª¨Îüº)ÏùÑ ÎßåÎì§Í≥†, EventsÍ∞Ä 1Ïù¥Î©¥ÏÑú SalesÍ∞Ä 1000000Ïù¥ÌïòÏù∏ Ï°∞Í±¥Ïóê ÎßûÎäî ÏÉàÎ°úÏö¥ Ïª¨Îüº Ìï©ÏùÑ Íµ¨ÌïòÏãúÏò§

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic2.csv
- ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-24. Time-Series5 (Lagged Feature)","","02/02/2222 23:59:00","10"
"7012","1633303","2487225","12/04/2021 16:20:43","# ÏïàÎÇ¥
- import pandas as pd
- a = pd.read_csv(""../input/traval-insurance-exam/train.csv"")
- b = pd.read_csv('../input/traval-insurance-exam/test.csv')
- pd.DataFrame(Î≥ÄÏàò).to_csv('000000000.csv')

## Î≥¥ÌóòÍ∞ÄÏûÖ ÌôïÎ•†ÏùÑ Î¨ªÎäî Î¨∏Ï†ú

##Ï∂úÎ†•ÌòïÌÉú 
index,y_pred
- 0,0.15246735144689827
- 1,0.9615112924356346
- 2,0.15131398160778645
- 3,0.23663288296712173
- 4,0.9385824940846587
- 5,0.15738583559845978","","T2. Exam Question (3rd round)","","12/22/2222 23:59:00","1"
"6979","1633303","2487225","12/01/2021 05:42:00","# ÏûëÏóÖÌòï1 Î™®ÏùòÍ≥†ÏÇ¨ 
- data: basic1.csv

## 1. 1. Ï≤´Î≤àÏß∏ Îç∞Ïù¥ÌÑ∞ Î∂ÄÌÑ∞ ÏàúÏÑúÎåÄÎ°ú 50:50ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º ÎÇòÎàÑÍ≥†, ÏïûÏóêÏÑú Î∂ÄÌÑ∞ 50%Ïùò Îç∞Ïù¥ÌÑ∞(Ïù¥Ìïò, AÍ∑∏Î£π)Îäî 'f1'Ïª¨ÎüºÏùÑ AÍ∑∏Î£πÏùò Ï§ëÏïôÍ∞íÏúºÎ°ú Ï±ÑÏö∞Í≥†, Îí§ÏóêÏÑúÎ∂ÄÌÑ∞ 50% Îç∞Ïù¥ÌÑ∞(Ïù¥Ìïò, BÍ∑∏Î£π)Îäî 'f1'Ïª¨ÎüºÏùÑ BÍ∑∏Î£πÏùò ÏµúÎåÄÍ∞íÏúºÎ°ú Ï±ÑÏö¥ ÌõÑ, AÍ∑∏Î£πÍ≥º BÍ∑∏Î£πÏùò ÌëúÏ§ÄÌé∏Ï∞® Ìï©ÏùÑ Íµ¨ÌïòÏãúÏò§
- Îã®, ÏÜåÏàòÏ†ê Ï≤´Ïß∏ÏûêÎ¶¨ÍπåÏßÄ Íµ¨ÌïòÏãúÏò§ (ÎëòÏß∏ÏûêÎ¶¨ÏóêÏÑú Î∞òÏò¨Î¶º)

## 2. 'f4'Ïª¨ÎüºÏùÑ Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨Í≥º 'f5'Ïª¨ÎüºÍ∏∞Ï§Ä Ïò§Î¶ÑÏ∞®Ïàú Ï†ïÎ†¨ÏùÑ ÏàúÏÑúÎåÄÎ°ú Îã§Ï§ë Ï°∞Í±¥ Ï†ïÎ†¨ÌïòÍ≥†ÎÇòÏÑú ÏïûÏóêÏÑúÎ∂ÄÌÑ∞ 10Í∞úÏùò Îç∞Ïù¥ÌÑ∞ Ï§ë 'f5'Ïª¨ÎüºÏùò ÏµúÏÜåÍ∞í Ï∞æÍ≥†, Ïù¥ ÏµúÏÜåÍ∞íÏúºÎ°ú ÏïûÏóêÏÑú Î∂ÄÌÑ∞ 10Í∞úÏùò 'f5'Ïª¨Îüº Îç∞Ïù¥ÌÑ∞Î•º Î≥ÄÍ≤ΩÌï®. Í∑∏Î¶¨Í≥† 'f5'Ïª¨ÎüºÏùò ÌèâÍ∑†Í∞íÏùÑ Í≥ÑÏÇ∞Ìï®
- Îã® ÏÜåÏàòÏ†ê ÎëòÏß∏ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•(ÏÖãÏß∏ÏûêÎ¶¨ÏóêÏÑú Î∞òÏò¨Î¶º)

## 3. 'age' Ïª¨ÎüºÏùò IQRÎ∞©ÏãùÏùÑ Ïù¥Ïö©Ìïú Ïù¥ÏÉÅÏπò ÏàòÏôÄ ÌëúÏ§ÄÌé∏Ï∞®*1.5Î∞©ÏãùÏùÑ Ïù¥Ïö©Ìïú Ïù¥ÏÉÅÏπò Ïàò Ìï©ÏùÑ Íµ¨ÌïòÏãúÏò§
- IQRÎ∞©Ïãù : Q1 - 1.5 * IQR, Q3 + 1.5 * IQRÏóêÏÑú Î≤óÏñ¥ÎÇòÎäî ÏòÅÏó≠ÏùÑ Ïù¥ÏÉÅÏπòÎùºÍ≥† ÌåêÎã®Ìï® (Q1ÏùÄ Îç∞Ïù¥ÌÑ∞Ïùò 25%, Q3Îäî Îç∞Ïù¥ÌÑ∞Ïùò 75% ÏßÄÏ†êÏûÑ)
- ÌëúÏ§ÄÌé∏Ï∞®*1.5Î∞©Ïãù: ÌèâÍ∑†ÏúºÎ°úÎ∂ÄÌÑ∞ 'ÌëúÏ§ÄÌé∏Ï∞®*1.5'Î•º Î≤óÏñ¥ÎÇòÎäî ÏòÅÏó≠ÏùÑ Ïù¥ÏÉÅÏπòÎùºÍ≥† ÌåêÎã®Ìï®


## ÌíÄÏù¥ ÎÖ∏Ìä∏Î∂Å : https://www.kaggle.com/agileteam/mock-exam1-type1-1-tutorial","","[MOCK EXAM1] TYPE1","","02/02/2222 23:59:00","11"
"6892","1633303","2487225","11/21/2021 12:32:02","data: https://www.kaggle.com/mirichoi0218/insurance","","T2-5. Insurance Forecast (Regression)","","12/22/2222 23:59:00","11"
"6895","1633303","2487225","11/22/2021 00:30:15","data: https://www.kaggle.com/c/bike-sharing-demand/","","T2-6. Bike-sharing-demand (Regression)","","02/02/2222 23:59:00","11"
"6896","1633303","2487225","11/22/2021 01:12:11","## ÎÇòÏù¥ Íµ¨Í∞Ñ ÎÇòÎàÑÍ∏∞
### basic1 Îç∞Ïù¥ÌÑ∞ Ï§ë 'age'Ïª¨Îüº Ïù¥ÏÉÅÏπòÎ•º Ï†úÍ±∞ÌïòÍ≥†, ÎèôÏùºÌïú Í∞úÏàòÎ°ú ÎÇòÏù¥ ÏàúÏúºÎ°ú 3Í∑∏Î£πÏúºÎ°ú ÎÇòÎàà Îí§ Í∞Å Í∑∏Î£πÏùò Ï§ëÏïôÍ∞íÏùÑ ÎçîÌïòÏãúÏò§
### (Ïù¥ÏÉÅÏπòÎäî ÏùåÏàò(0Ìè¨Ìï®), ÏÜåÏàòÏ†ê Í∞í)
- data: basic1.csv
- ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-21. Binning Data","","02/02/2222 23:59:00","21"
"6868","1633303","2487225","11/18/2021 01:37:35","## Í≥†Í∞ùÍ≥º Ïûò ÎßûÎäî ÌÉÄÏûÖ Ï∂îÏ≤ú :)
### basic1 Îç∞Ïù¥ÌÑ∞ Ï§ë 'f4'Î•º Í∏∞Ï§ÄÏúºÎ°ú basic3 Îç∞Ïù¥ÌÑ∞ 'f4'Í∞íÏùÑ Î≥ëÌï©ÌïòÍ≥†, 
### Î≥ëÌï©Ìïú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú r2Í≤∞Ï∏°ÏπòÎ•º Ï†úÍ±∞Ìïú Îã§Ïùå, ÏïûÏóêÏÑú Î∂ÄÌÑ∞ 20Í∞ú Îç∞Ïù¥ÌÑ∞Î•º ÏÑ†ÌÉùÌïòÍ≥† 'f2'Ïª¨Îüº Ìï©ÏùÑ Íµ¨ÌïòÏãúÏò§

- basic1.csv: Í≥†Í∞ù Îç∞Ïù¥ÌÑ∞ 
- basic3.csv: Ïûò Ïñ¥Ïö∏Î¶¨Îäî Í¥ÄÍ≥Ñ Îç∞Ïù¥ÌÑ∞ (Ï∂îÏ≤ú1:r1, Ï∂îÏ≤ú2:r2)

Ï†ïÎãµ: 15","","T1-20. Combining Data","","02/02/2222 23:59:00","15"
"6836","1633303","2487225","11/14/2021 07:03:25","Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 2022ÎÖÑ 5Ïõî salesÏª¨ÎüºÏùò Ï§ëÏïôÍ∞íÏùÑ Íµ¨ÌïòÏãúÏò§","","T1-17. Time-Series1","","12/12/2222 23:59:00","19"
"6837","1633303","2487225","11/14/2021 07:25:40","## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 2022ÎÖÑ 5Ïõî Ï£ºÎßêÍ≥º ÌèâÏùºÏùò salesÏª¨Îüº ÌèâÍ∑†Í∞í Ï∞®Ïù¥Î•º Íµ¨ÌïòÏãúÏò§ (ÏÜåÏàòÏ†ê ÎëòÏß∏ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•, Î∞òÏò¨Î¶º)
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic2.csv
- ÌïÑÏÇ¨: ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-18. Time-Series2 (weekend)","","12/12/2222 23:59:00","16"
"6838","1633303","2487225","11/14/2021 13:11:07","### Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 2022ÎÖÑ ÏõîÎ≥Ñ Sales Ìï©Í≥Ñ Ï§ë Í∞ÄÏû• ÌÅ∞ Í∏àÏï°Í≥º
### 2023ÎÖÑ ÏõîÎ≥Ñ Sales Ìï©Í≥Ñ Ï§ë Í∞ÄÏû• ÌÅ∞ Í∏àÏï°Ïùò Ï∞®Ïù¥Î•º Ï†àÎåÄÍ∞íÏúºÎ°ú Íµ¨ÌïòÏãúÏò§.
### Îã®, EventsÏª¨ÎüºÏù¥ '1'Ïù∏ Í≤ΩÏö∞ 80%Ïùò SalseÍ∞íÎßå Î∞òÏòÅÌï® 
### (ÏµúÏ¢ÖÍ∞íÏùÄ ÏÜåÏàòÏ†ê Î∞òÏò¨Î¶º ÌõÑ Ï†ïÏàò Ï∂úÎ†•)

Îç∞Ïù¥ÌÑ∞ÏÖã : basic2.csv
ÌïÑÏÇ¨: ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-19. Time-Series3 (monthly total)","","12/12/2222 23:59:00","16"
"6858","1633303","2487225","11/17/2021 04:33:51","# ÏÑ±Ïù∏ Ïù∏Íµ¨Ï°∞ÏÇ¨ ÏÜåÎìù ÏòàÏ∏°

- age: ÎÇòÏù¥
- workclass: Í≥†Ïö© ÌòïÌÉú
- fnlwgt: ÏÇ¨ÎûåÏùò ÎåÄÌëúÏÑ±ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî Í∞ÄÏ§ëÏπò(final weight)
- education: ÍµêÏú° ÏàòÏ§Ä
- education.num: ÍµêÏú° ÏàòÏ§Ä ÏàòÏπò
- marital.status: Í≤∞Ìòº ÏÉÅÌÉú
- occupation: ÏóÖÏ¢Ö
- relationship: Í∞ÄÏ°± Í¥ÄÍ≥Ñ
- race: Ïù∏Ï¢Ö
- sex: ÏÑ±Î≥Ñ
- capital.gain: ÏñëÎèÑ ÏÜåÎìù
- capital.loss: ÏñëÎèÑ ÏÜêÏã§
- hours.per.week: Ï£ºÎãπ Í∑ºÎ¨¥ ÏãúÍ∞Ñ
- native.country: Íµ≠Ï†Å
- income: ÏàòÏùµ (ÏòàÏ∏°Ìï¥Ïïº ÌïòÎäî Í∞í)

dataset : https://www.kaggle.com/uciml/adult-census-income","","T2-3. Adult Census Income (classification)","","02/02/2222 23:59:00","10"
"6859","1633303","2487225","11/17/2021 05:33:45","data: https://www.kaggle.com/c/house-prices-advanced-regression-techniques","","T2-4. House Prices (Regression)","","02/02/2222 23:59:00","14"
"6804","1633303","2487225","11/09/2021 23:42:26","## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú ageÏª¨Îüº ÏÉÅÏúÑ 20Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º Íµ¨Ìïú Îã§Ïùå 
## f1Ïùò Í≤∞Ï∏°ÏπòÎ•º Ï§ëÏïôÍ∞íÏúºÎ°ú Ï±ÑÏö¥Îã§.
## Í∑∏Î¶¨Í≥† f4Í∞Ä ISFJÏôÄ f5Í∞Ä 20 Ïù¥ÏÉÅÏù∏ 
## f1Ïùò ÌèâÍ∑†Í∞íÏùÑ Ï∂úÎ†•ÌïòÏãúÏò§!

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv 
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- File -&gt; Editor Type -&gt; Script

- Ï†ïÎãµ: 73.875","","T1-15. Slicing & Condition","","02/02/2222 23:59:00","18"
"6805","1633303","2487225","11/09/2021 23:52:25","## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú f2Í∞Ä 0Í∞íÏù∏ Îç∞Ïù¥ÌÑ∞Î•º ageÎ•º Í∏∞Ï§ÄÏúºÎ°ú Ïò§Î¶ÑÏ∞®Ïàú Ï†ïÎ†¨ÌïòÍ≥†
## ÏïûÏóêÏÑú Î∂ÄÌÑ∞ 20Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌïú ÌõÑ 
## f1 Í≤∞Ï∏°Ïπò(ÏµúÏÜåÍ∞í)Î•º Ï±ÑÏö∞Í∏∞ Ï†ÑÍ≥º ÌõÑÏùò Î∂ÑÏÇ∞ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏãúÏò§ (ÏÜåÏàòÏ†ê ÎëòÏß∏ ÏûêÎ¶¨ÍπåÏßÄ)

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv 
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- File -&gt; Editor Type -&gt; Script

- Ï†ïÎãµ : 38.44","","T1-16. Variance","","02/02/2222 23:59:00","16"
"6778","1633303","2487225","11/08/2021 03:39:46","T1 ÏòàÏãúÎ¨∏Ï†ú Python
ÏûêÎèôÏ∞® Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú ÌäπÏ†ï Ïª¨ÎüºÏùÑ Min-Max ScaleÎ°ú Î≥ÄÌôò ÌõÑ 0.5Î≥¥Îã§ ÌÅ∞ Í∞íÏùÑ Í∞ÄÏßÄÎäî Î†àÏΩîÎìú(row) ÏàòÎ•º Î¨ªÎäî Î¨∏Ï†úÏûÖÎãàÎã§.

data Ï∂úÏ≤ò: https://www.kaggle.com/ruiromanini/mtcars
data Ï∂îÍ∞ÄÎ∞©Î≤ï : Ïö∞Ï∏° Î©îÎâ¥ -&gt; +Add data -&gt; mtcar(ruiromanini) ADD
ÏòÅÏÉÅ ÎßÅÌÅ¨ : https://youtu.be/E86QFVXPm5Q","","T1. Exercise","","12/31/2022 23:59:00","26"
"6789","1633303","2487225","11/08/2021 23:53:24","## cityÏôÄ f4Î•º Í∏∞Ï§ÄÏúºÎ°ú f5Ïùò ÌèâÍ∑†Í∞íÏùÑ Íµ¨Ìïú Îã§Ïùå, f5Î•º Í∏∞Ï§ÄÏúºÎ°ú ÏÉÅÏúÑ 7Í∞ú Í∞íÏùÑ Î™®Îëê ÎçîÌï¥ Ï∂úÎ†•ÌïòÏãúÏò§ (ÏÜåÏàòÏ†ê ÎëòÏß∏ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•)
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv 
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- File -&gt; Editor Type -&gt; Script

### Ï†ïÎãµ : 643.68","","T1-14. Multi Index & Groupby","","12/31/2222 23:59:00","19"
"6742","1633303","2487225","11/03/2021 00:19:52","## Ïó¨-Ï°¥Ïä®Í≥º Î∞ïÏä§ÏΩïÏä§ Î≥ÄÌôò

- Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 20ÏÑ∏ Ïù¥ÏÉÅÏù∏ Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌïòÍ≥† 'f1'Ïª¨ÎüºÏùÑ Í≤∞Ï∏°ÏπòÎ•º ÏµúÎπàÍ∞íÏúºÎ°ú Ï±ÑÏö¥ ÌõÑ, f1 Ïª¨ÎüºÏùò Ïó¨-Ï°¥Ïä®Í≥º Î∞ïÏä§ÏΩïÏä§ Î≥ÄÌôò Í∞íÏùÑ Íµ¨ÌïòÍ≥†, Îëê Í∞íÏùò Ï∞®Ïù¥Î•º Ï†àÎåÄÍ∞íÏúºÎ°ú Íµ¨ÌïúÎã§Ïùå Î™®Îëê ÎçîÌï¥ ÏÜåÏàòÏ†ê ÎëòÏß∏ ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•(Î∞òÏò¨Î¶º)ÌïòÏãúÏò§
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-10. Yeo-Johnson and Box‚ÄìCox","","12/31/2022 23:59:00","14"
"6743","1633303","2487225","11/03/2021 02:20:17","## Î¨∏Ï†ú1
- Îç∞Ïù¥ÌÑ∞ÏÖã(basic1.csv)Ïùò 'f5' Ïª¨ÎüºÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏÉÅÏúÑ 10Í∞úÏùò Îç∞Ïù¥ÌÑ∞Î•º Íµ¨ÌïòÍ≥†,
- 'f5'Ïª¨Îüº 10Í∞ú Ï§ë ÏµúÏÜåÍ∞íÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º ÎåÄÏ≤¥Ìïú ÌõÑ,
- 'age'Ïª¨ÎüºÏóêÏÑú 80 Ïù¥ÏÉÅÏù∏ Îç∞Ïù¥ÌÑ∞Ïùò'f5 Ïª¨Îüº ÌèâÍ∑†Í∞í Íµ¨ÌïòÍ∏∞

## Î¨∏Ï†ú2
- Îç∞Ïù¥ÌÑ∞ÏÖã(basic1.csv)Ïùò ÏïûÏóêÏÑú ÏàúÏÑúÎåÄÎ°ú 70% Îç∞Ïù¥ÌÑ∞Îßå ÌôúÏö©Ìï¥ÏÑú,
- 'f1'Ïª¨Îüº Í≤∞Ï∏°ÏπòÎ•º Ï§ëÏïôÍ∞íÏúºÎ°ú Ï±ÑÏö∞Í∏∞ Ï†ÑÌõÑÏùò ÌëúÏ§ÄÌé∏Ï∞®Î•º Íµ¨ÌïòÍ≥†
- Îëê ÌëúÏ§ÄÌé∏Ï∞® Ï∞®Ïù¥ Í≥ÑÏÇ∞ÌïòÍ∏∞
(ÌëúÎ≥∏ÌëúÏ§ÄÌé∏Ï∞® Í∏∞Ï§Ä)

## Î¨∏Ï†ú3
- Îç∞Ïù¥ÌÑ∞ÏÖã(basic1.csv)Ïùò 'age'Ïª¨ÎüºÏùò Ïù¥ÏÉÅÏπòÎ•º ÎçîÌïòÏãúÏò§!
- Îã®, ÌèâÍ∑†ÏúºÎ°úÎ∂ÄÌÑ∞ 'ÌëúÏ§ÄÌé∏Ï∞®*1.5'Î•º Î≤óÏñ¥ÎÇòÎäî ÏòÅÏó≠ÏùÑ Ïù¥ÏÉÅÏπòÎùºÍ≥† ÌåêÎã®Ìï®

Ï∞∏Í≥†ÏòÅÏÉÅ : https://youtu.be/Jh3rJaZlEg0","","T1. Exam Question (2nd round)","","12/31/2022 23:59:00","20"
"6750","1633303","2487225","11/04/2021 03:40:25","##min-maxÏä§ÏºÄÏùºÎßÅ Í∏∞Ï§Ä ÏÉÅÌïòÏúÑ 5% Íµ¨ÌïòÍ∏∞
## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 'f5'Ïª¨ÎüºÏùÑ min-max Ïä§ÏºÄÏùº Î≥ÄÌôòÌïú ÌõÑ, ÏÉÅÏúÑ 5%ÏôÄ ÌïòÏúÑ 5% Í∞íÏùò Ìï©ÏùÑ Íµ¨ÌïòÏãúÏò§
- ÏÉÅÏúÑ 5%ÏßÄÏ†ê Í∞í + ÌïòÏúÑ 5%ÏßÄÏ†ê Í∞í

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- **File -&gt; Editor Type -&gt; Script**","","T1-11 min-max scaling","","12/31/2022 23:59:00","21"
"6751","1633303","2487225","11/04/2021 03:44:34","## ÏÉÅÏúÑ 10Í∞ú, ÌïòÏúÑ 10Í∞ú Ï∞®Ïù¥
## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÉÅÏúÑ 10Í∞ú Íµ≠Í∞ÄÏùò Ï†ëÏ¢ÖÎ•† ÌèâÍ∑†Í≥º ÌïòÏúÑ 10Í∞ú Íµ≠Í∞ÄÏùò Ï†ëÏ¢ÖÎ•† ÌèâÍ∑†ÏùÑ Íµ¨ÌïòÍ≥†, Í∑∏ Ï∞®Ïù¥Î•º Íµ¨Ìï¥Î≥¥ÏÑ∏Ïöî  (Ïù¥ÏÉÅÏπò - 100%Í∞Ä ÎÑòÎäî Ï†ëÏ¢ÖÎ•† Ï†úÍ±∞, ÏÜåÏàò Ï≤´Ïß∏ ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•)
## Í≤∞Í≥º Í∞íÏùÄ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏Ïóê Îî∞Îùº Îã¨ÎùºÏßà Ïàò ÏûàÏùå

- Îç∞Ïù¥ÌÑ∞ÏÖã : ../input/covid-vaccination-vs-death/covid_vaccination_vs_death_ratio.csv 
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- File -&gt; Editor Type -&gt; Script","","T1-12. top10-bottom10","","12/31/2022 23:59:00","22"
"6755","1633303","2487225","11/05/2021 02:37:18","## ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Íµ¨ÌïòÍ∏∞
## Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Íµ¨ÌïòÍ≥†, qualityÏôÄÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä Í∞ÄÏû• ÌÅ∞ Í∞íÍ≥º, Í∞ÄÏû• ÏûëÏùÄ Í∞íÏùÑ Íµ¨Ìïú Îã§Ïùå ÎçîÌïòÏãúÏò§!
#### Îã®, qualityÏôÄ quality ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Ï†úÏô∏, ÏÜåÏàòÏ†ê ÎëòÏß∏ ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•

- Îç∞Ïù¥ÌÑ∞ÏÖã : ../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Ïä§ÌÅ¨Î¶ΩÌä∏ Î∞©Ïãù Ïã§Ïäµ Í∂åÏû•: File -&gt; Editor Type -&gt; Script","","T1-13. Correlation","","12/31/2022 23:59:00","17"
"6769","1633303","2487225","11/06/2021 14:46:17","## ÏÉùÏ°¥Ïó¨Î∂Ä ÏòàÏ∏°Î™®Îç∏ ÎßåÎì§Í∏∞
### ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ (X_train, y_train)ÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÏÉùÏ°¥ ÏòàÏ∏° Î™®ÌòïÏùÑ ÎßåÎì† ÌõÑ, Ïù¥Î•º ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞(X_test)Ïóê Ï†ÅÏö©ÌïòÏó¨ ÏñªÏùÄ ÏòàÏ∏°Í∞íÏùÑ Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌòïÏãùÏùò CSVÌååÏùºÎ°ú ÏÉùÏÑ±ÌïòÏãúÏò§(Ï†úÏ∂úÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÄ accuracy ÌèâÍ∞ÄÏßÄÌëúÏóê Îî∞Îùº Ï±ÑÏ†ê)

(Í∞Ä) Ï†úÍ≥µ Îç∞Ïù¥ÌÑ∞ Î™©Î°ù
- y_train: ÏÉùÏ°¥Ïó¨Î∂Ä(ÌïôÏäµÏö©)
- X_trian, X_test : ÏäπÍ∞ù Ï†ïÎ≥¥ (ÌïôÏäµÏö© Î∞è ÌèâÍ∞ÄÏö©)

(ÎÇò) Îç∞Ïù¥ÌÑ∞ ÌòïÏãù Î∞è ÎÇ¥Ïö©
- y_trian (712Î™Ö Îç∞Ïù¥ÌÑ∞)

**ÏãúÌóòÌôòÍ≤Ω ÏÑ∏ÌåÖÏùÄ ÏòàÏãúÎ¨∏Ï†úÏôÄ ÎèôÏùºÌïú ÌòïÌÉúÏùò X_train, y_train, X_test Îç∞Ïù¥ÌÑ∞Î•º ÎßåÎì§Í∏∞ ÏúÑÌï®ÏûÑ**

### Ïú†ÏùòÏÇ¨Ìï≠
- ÏÑ±Îä•Ïù¥ Ïö∞ÏàòÌïú ÏòàÏ∏°Î™®ÌòïÏùÑ Íµ¨Ï∂ïÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Ï†ÅÏ†àÌïú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨, ÌîºÏ≤òÏóîÏßÄÎãàÏñ¥ÎßÅ, Î∂ÑÎ•òÏïåÍ≥†Î¶¨Ï¶ò, ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù, Î™®Ìòï ÏïôÏÉÅÎ∏î Îì±Ïù¥ ÏàòÎ∞òÎêòÏñ¥Ïïº ÌïúÎã§.
- ÏàòÌóòÎ≤àÌò∏.csvÌååÏùºÏù¥ ÎßåÎì§Ïñ¥ÏßÄÎèÑÎ°ù ÏΩîÎìúÎ•º Ï†úÏ∂úÌïúÎã§.
- Ï†úÏ∂úÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÄ accuracyÎ°ú ÌèâÍ∞ÄÌï®

csv Ï∂úÎ†•ÌòïÌÉú 
- indexÎäî ÏóÜÍ≥†
- PassengerId, Survived Îßå ÏûàÎäî ÌòïÌÉú","","T2-1. Titanic (classification)","","12/31/2022 23:59:00","16"
"6770","1633303","2487225","11/06/2021 15:51:58","## ÎãπÎá®Î≥ë Ïó¨Î∂Ä ÌåêÎã®
- Ïù¥ÏÉÅÏπò Ï≤òÎ¶¨ (Glucose, BloodPressure, SkinThickness, Insulin, BMIÍ∞Ä 0Ïù∏ Í∞í)
- Outcome ÏòàÏ∏° (ÎãπÎá®Î≥ë Ïó¨Î∂Ä)","","T2-2. Pima Indians Diabetes (classification)","","12/31/2022 23:59:00","15"
"6771","1633303","2487225","11/06/2021 16:00:43","## Ï†ÑÏûêÏÉÅÍ±∞Îûò Î∞∞ÏÜ° Îç∞Ïù¥ÌÑ∞
### Ï†úÌíà Î∞∞ÏÜ° ÏãúÍ∞ÑÏóê ÎßûÏ∂∞ Î∞∞ÏÜ°ÎêòÏóàÎäîÏßÄ ÏòàÏ∏°Î™®Îç∏ ÎßåÎì§Í∏∞
- ÌïôÏäµÏö© Îç∞Ïù¥ÌÑ∞ (X_train, y_train)ÏùÑ Ïù¥Ïö©ÌïòÏó¨ Î∞∞ÏÜ° ÏòàÏ∏° Î™®ÌòïÏùÑ ÎßåÎì† ÌõÑ, Ïù¥Î•º ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞(X_test)Ïóê Ï†ÅÏö©ÌïòÏó¨ ÏñªÏùÄ ÏòàÏ∏°Í∞íÏùÑ Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌòïÏãùÏùò CSVÌååÏùºÎ°ú ÏÉùÏÑ±ÌïòÏãúÏò§(Ï†úÏ∂úÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÄ ROC-AUC ÌèâÍ∞ÄÏßÄÌëúÏóê Îî∞Îùº Ï±ÑÏ†ê)

### [ÏãúÌóòÏö© Îç∞Ïù¥ÌÑ∞ÏÖã ÎßåÎì§Í∏∞] ÏΩîÎìúÎäî ÏòàÏãúÎ¨∏Ï†úÏôÄ ÎèôÏùºÌïú ÌòïÌÉúÏùò X_train, y_train, X_test Îç∞Ïù¥ÌÑ∞Î•º ÎßåÎì§Í∏∞ ÏúÑÌï®ÏûÑ

### (Ïú†ÏùòÏÇ¨Ìï≠)

- ÏÑ±Îä•Ïù¥ Ïö∞ÏàòÌïú ÏòàÏ∏°Î™®ÌòïÏùÑ Íµ¨Ï∂ïÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Ï†ÅÏ†àÌïú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨, ÌîºÏ≤òÏóîÏßÄÎãàÏñ¥ÎßÅ, Î∂ÑÎ•òÏïåÍ≥†Î¶¨Ï¶ò, ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù, Î™®Ìòï ÏïôÏÉÅÎ∏î Îì±Ïù¥ ÏàòÎ∞òÎêòÏñ¥Ïïº ÌïúÎã§.
- ÏàòÌóòÎ≤àÌò∏.csvÌååÏùºÏù¥ ÎßåÎì§Ïñ¥ÏßÄÎèÑÎ°ù ÏΩîÎìúÎ•º Ï†úÏ∂úÌïúÎã§.
- Ï†úÏ∂úÌïú Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÄ ROC-AUCÌòïÌÉúÎ°ú ÏùΩÏñ¥ÎìúÎ¶∞Îã§.","","T2. Exam Question (2nd round)","","12/31/2022 23:59:00","15"
"6666","1633303","2487225","10/24/2021 12:15:48","## Ïù¥ÏÉÅÏπòÎ•º Ï∞æÏïÑÎùº
Îç∞Ïù¥ÌÑ∞ÏóêÏÑú IQRÏùÑ ÌôúÏö©Ìï¥ 'Fare'Ïª¨ÎüºÏùò Ïù¥ÏÉÅÏπòÎ•º Ï∞æÍ≥†, Ïù¥ÏÉÅÏπò Îç∞Ïù¥ÌÑ∞Ïùò Ïó¨ÏÑ± ÏàòÎ•º Íµ¨ÌïòÏãúÏò§

- Îç∞Ïù¥ÌÑ∞ÏÖã : titanic
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Îç∞Ïù¥ÌÑ∞ ÏúÑÏπò ""../input/titanic/train.csv"" (copy&editÍ∞Ä ÏïÑÎãê Í≤ΩÏö∞ Î≥ÑÎèÑÎ°ú Îç∞Ïù¥ÌÑ∞ÏÖã Î∂àÎü¨ÏôÄÏïº Ìï®)

Ï∞∏Í≥† ÏòÅÏÉÅ : https://youtu.be/ipBW5D_UJEo","","T1-1. Outlier(IQR)","","12/31/2022 00:00:00","73"
"6667","1633303","2487225","10/24/2021 12:18:32","## Ïù¥ÏÉÅÏπòÎ•º Ï∞æÏïÑÎùº(ÏÜåÏàòÏ†ê ÎÇòÏù¥)
Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïù¥ÏÉÅÏπò(ÏÜåÏàòÏ†ê ÎÇòÏù¥)Î•º Ï∞æÍ≥† Ïò¨Î¶º, ÎÇ¥Î¶º, Î≤ÑÎ¶º(Ï†àÏÇ¨)ÌñàÏùÑÎïå 3Í∞ÄÏßÄ Î™®Îëê Ïù¥ÏÉÅÏπò 'age' ÌèâÍ∑†ÏùÑ Íµ¨Ìïú Îã§Ïùå Î™®Îëê ÎçîÌïòÏó¨ Ï∂úÎ†•ÌïòÏãúÏò§¬∂

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Ï∞∏Í≥† ÏòÅÏÉÅ : https://youtu.be/c3Fr9G-ZYdw","","T1-2. Outlier(age)","","12/31/2022 00:00:00","26"
"6668","1633303","2487225","10/24/2021 12:19:58","## Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨
Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≤∞Ï∏°ÏπòÍ∞Ä 80%Ïù¥ÏÉÅ ÎêòÎäî Ïª¨ÎüºÏùÄ(Î≥ÄÏàòÎäî) ÏÇ≠Ï†úÌïòÍ≥†, 80% ÎØ∏ÎßåÏù∏ Í≤∞Ï∏°ÏπòÍ∞Ä ÏûàÎäî Ïª¨ÎüºÏùÄ 'city'Î≥Ñ Ï§ëÏïôÍ∞íÏúºÎ°ú Í∞íÏùÑ ÎåÄÏ≤¥ÌïòÍ≥† 'f1'Ïª¨ÎüºÏùò ÌèâÍ∑†Í∞íÏùÑ Ï∂úÎ†•ÌïòÏÑ∏Ïöî!

- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Ï∞∏Í≥† ÏòÅÏÉÅ : https://youtu.be/WqlpqBRn7x4","","T1-3. Missing data","","12/31/2022 00:00:00","19"
"6669","1633303","2487225","10/24/2021 12:23:36","## ÏôúÎèÑÏôÄ Ï≤®ÎèÑ Íµ¨ÌïòÍ∏∞
Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ Ï§ë train.csvÏóêÏÑú 'SalePrice'Ïª¨ÎüºÏùò ÏôúÎèÑÏôÄ Ï≤®ÎèÑÎ•º Íµ¨Ìïú Í∞íÍ≥º, 'SalePrice'Ïª¨ÎüºÏùÑ Ïä§ÏºÄÏùºÎßÅ(log1p)Î°ú Î≥ÄÌôòÌïú Ïù¥ÌõÑ ÏôúÎèÑÏôÄ Ï≤®ÎèÑÎ•º Íµ¨Ìï¥ Î™®Îëê ÎçîÌïú Îã§Ïùå ÏÜåÏàòÏ†ê 2Ïß∏ÏûêÎ¶¨ÍπåÏßÄ Ï∂úÎ†•ÌïòÏãúÏò§

- Îç∞Ïù¥ÌÑ∞ÏÖã : House Prices - Advanced Regression Technique (https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë
- Ï∞∏Í≥† ÏòÅÏÉÅ : https://youtu.be/_ft7ZlDlk7c","","T1-4. Skewness and Kurtosis (Log Scale)","","12/31/2022 00:00:00","18"
"6670","1633303","2487225","10/24/2021 12:27:02","## Ï°∞Í±¥Ïóê ÎßûÎäî Îç∞Ïù¥ÌÑ∞ ÌëúÏ§ÄÌé∏Ï∞® Íµ¨ÌïòÍ∏∞
Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ Ï§ë basic1.csvÏóêÏÑú 'f4'Ïª¨Îüº Í∞íÏù¥ 'ENFJ'ÏôÄ 'INFP'Ïù∏ 'f1'Ïùò ÌëúÏ§ÄÌé∏Ï∞® Ï∞®Ïù¥Î•º Ï†àÎåÄÍ∞íÏúºÎ°ú Íµ¨ÌïòÏãúÏò§
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-5. Standard deviation","","12/31/2022 00:00:00","16"
"6704","1633303","2487225","10/29/2021 06:53:44","## Í≤∞Ï∏°Ïπò Ï†úÍ±∞ Î∞è Í∑∏Î£π Ìï©Í≥ÑÏóêÏÑú Ï°∞Í±¥Ïóê ÎßûÎäî Í∞í Ï∞æÏïÑ Ï∂úÎ†•
- Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ Ï§ë basic1.csvÏóêÏÑú 'f1'Ïª¨Îüº Í≤∞Ï∏° Îç∞Ïù¥ÌÑ∞Î•º Ï†úÍ±∞ÌïòÍ≥†, 'city'ÏôÄ 'f2'ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Î¨∂Ïñ¥ Ìï©Í≥ÑÎ•º Íµ¨ÌïòÍ≥†, 'cityÍ∞Ä Í≤ΩÍ∏∞Ïù¥Î©¥ÏÑú f2Í∞Ä 0'Ïù∏ Ï°∞Í±¥Ïóê ÎßåÏ°±ÌïòÎäî f1 Í∞íÏùÑ Íµ¨ÌïòÏãúÏò§
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- Ï∞∏Í≥†Ìï† ÎÖ∏Ìä∏Î∂Å ÌÅ¥Î¶≠ - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-6. Groupby Sum","","12/31/2022 23:59:00","18"
"6705","1633303","2487225","10/29/2021 06:57:10","## Í∞í Î≥ÄÍ≤Ω Î∞è 2Í∞ú Ïù¥ÏÉÅÏùò Ï°∞Í±¥

- 'f4'Ïª¨ÎüºÏùò Í∞íÏù¥ 'ESFJ'Ïù∏ Îç∞Ïù¥ÌÑ∞Î•º 'ISFJ'Î°ú ÎåÄÏ≤¥ÌïòÍ≥†, 'city'Í∞Ä 'Í≤ΩÍ∏∞'Ïù¥Î©¥ÏÑú 'f4'Í∞Ä 'ISFJ'Ïù∏ Îç∞Ïù¥ÌÑ∞ Ï§ë 'age'Ïª¨ÎüºÏùò ÏµúÎåÄÍ∞íÏùÑ Ï∂úÎ†•ÌïòÏãúÏò§!
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- ÏõêÌïòÎäî ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-7. Replace","","12/31/2022 23:59:00","16"
"6706","1633303","2487225","10/29/2021 07:00:12","## ÎàÑÏ†ÅÌï© Í∑∏Î¶¨Í≥† Î≥¥Í∞Ñ(Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨)
- Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ ÏÖãÏóêÏÑú 'f2' Ïª¨ÎüºÏù¥ 1Ïù∏ Ï°∞Í±¥Ïóê Ìï¥ÎãπÌïòÎäî Îç∞Ïù¥ÌÑ∞Ïùò 'f1'Ïª¨Îüº ÎàÑÏ†ÅÌï©ÏùÑ Í≥ÑÏÇ∞ÌïúÎã§. Ïù¥Îïå Î∞úÏÉùÌïòÎäî ÎàÑÏ†ÅÌï© Í≤∞Ï∏°ÏπòÎäî Î∞îÎ°ú Îí§Ïùò Í∞íÏùÑ Ï±ÑÏö∞Í≥†, ÎàÑÏ†ÅÌï©Ïùò ÌèâÍ∑†Í∞íÏùÑ Ï∂úÎ†•ÌïúÎã§. (Îã®, Í≤∞Ï∏°Ïπò Î∞îÎ°ú Îí§Ïùò Í∞íÏù¥ ÏóÜÏúºÎ©¥ Îã§ÏùåÏóê ÎÇòÏò§Îäî Í∞íÏùÑ Ï±ÑÏõåÎÑ£ÎäîÎã§)
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-8. Cumulative Sum","","12/31/2022 23:59:00","19"
"6707","1633303","2487225","10/29/2021 07:02:09","## ÏàòÏπòÌòï Î≥ÄÏàò Î≥ÄÌôòÌïòÍ∏∞
- Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú 'f5'Ïª¨ÎüºÏùÑ ÌëúÏ§ÄÌôî(Standardization (Z-score Normalization))ÌïòÍ≥† Í∑∏ Ï§ëÏïôÍ∞íÏùÑ Íµ¨ÌïòÏãúÏò§
- Îç∞Ïù¥ÌÑ∞ÏÖã : basic1.csv
- ÎÖ∏Ìä∏Î∂Å ÏÑ†ÌÉù - Ïò§Î•∏Ï™Ω ÏÉÅÎã® copy&edit ÌÅ¥Î¶≠ -&gt; ÏòàÏÉÅÎ¨∏Ï†ú ÌíÄÏù¥ ÏãúÏûë","","T1-9. Standardization","","12/31/2022 23:59:00","18"
"6984","1760133","2566870","12/01/2021 11:09:14","## Task Details
Would be interesting if someone could develop a model predicting genre, taking into account the variation of music in one genre","","Predict Genre based on audio features","","","0"
"6985","1760133","2566870","12/01/2021 11:11:03","Exploring and visualizing audio features divided by genres to see their difference","","Exploratory Data Analysis","","","0"
"5319","1486681","2589790","07/25/2021 05:47:17","## Task Details
Clean, analyze and do exploratory data analysis regarding COVID19 based on the tweets.","","EDA with CDC Guidelines Tweets","","","1"
"5438","1494679","2626211","07/30/2021 20:23:27","As of today, there are no good Speech-to-Text models for the Ukrainian language. To tell the truth, as far as I know, there are only a few Speech-to-Text systems that support the Ukrainian language, but they are all commercial.
Until recently, there was not a large enough and prepared dataset to build such systems in the public domain, but thanks to a team of volunteers, such a dataset appeared (https://github.com/egorsmkv/speech-recognition-uk).

The main purpose of this task is to involve the kaggle community in creating the Speech-to-Text models for the Ukrainian language.

**
Dataset Ukrainian Open Speech To Text Dataset STT consists of 18 parts (according to the sources from which it was formed) and is divided into 3 kaggle datasets.**

Columns
UkrainianOpenSpeechToText Dataset.csv

path- path to each wav file
text - text from each wav file
dataset - the name of the dataset that includes the wav file
kaggle_dataset - the name of the kaggle dataset

To download the wav file you need to add at least one of the following Data Sources
https://www.kaggle.com/aikhmelnytskyy/ukrainian-open-speech-to-text-dataset-42
https://www.kaggle.com/aikhmelnytskyy/ukrainian-open-speech-to-text-dataset-42-part-2
https://www.kaggle.com/aikhmelnytskyy/ukrainian-open-speech-to-text-dataset-42-part-3","","Ukrainian Open Speech To Text Dataset models","Ukrainian Speech To Text","","1"
"6025","1574713","2655849","09/06/2021 18:34:03","## Task Details
Case Study
You have just been hired as a Data Scientist by Gamma, one of the world leaders in the field of inspection, analysis, technical assistance and digital solutions to ensure the traceability, security and compliance of certain goods in the public and private sectors.
You are in charge of analyzing a source file, received from a partner, in order to detect potential fraudsters.
Your employee's mission is to ensure that the transportation of merchandise from a stopover point to the final customer has gone smoothly.
The process of loading the goods by the carrier is divided into the following steps:
 
The control concerns particularly two stages: number 2 and number 5.
During his usual audit process, your recruiter initiated a batch control. To do so, he checked the total weight of the goods coming from the import with the sum of the goods loaded by truck. He found a significant discrepancy.
 
In order to conduct a thorough investigation, your recruiter requested the data entered by his partner specializing in terminal and port dock operations. In return, he received a file containing the following fields:
- Num_escale : Number of the ship loaded with goods
- Num_vehicle : Number of the carrier
- Num_Trailer : Number of the trailer attached to the vehicle
- Customer_Number : Customer number
- Date_entrance : The date of the carrier's entry at the weighbridge
- Entry_Time : The time of the taking of the weight (empty) at the weighbridge
- Bridge_in : Number of the bridge at the entry
- Exit_bridge : Number of the weighbridge where the weight of the loaded truck was taken 
- Exit_Date: The date of the taking of the weight (loaded) at the bridge
- Exit_Time: The time of the weight pickup (loaded) at the bridge
- Empty_Weight:		The weight of the empty truck
- Weight_Loaded:	The weight of the truck loaded with the goods
- Product:		The merchandise
Your recruiter has identified a list of concerns:
- The truck did not position itself correctly 
- The carrier goes to a weighbridge for the empty weight measurement and changes the weighbridge for the loaded weight measurement
- The carrier hides heavy objects during the first empty weighing and gets rid of the weight or objects on the way to his load.
In order to highlight potential fraudsters. Your recruiter asks you to analyze the file received along the following lines:
1.	The minimum observed empty weight (including the driver) and the analysis of the distribution

2.	Identify the standard deviation per vehicle/trailer pair

3.	Propose a logical deviation: + 150 kg more than the observed minimum weight: we assume that the driver cannot come with an empty tank but with a minimum of fuel to drive. The maximum filling is 400 kg

4.	Propose classifications by standard deviation: Example:
a.	Black list: trucks coming with 2 times standard deviation
b.	Red list : trucks coming with 1 time standard deviation
c.	Yellow list : trucks coming with + 150 kg more than the observed minimum empty weight

5.	Waiting time between empty weight and fully loaded weight at exit 

6.	Analysis by destination

7.	Analysis by raw material

8.	Weighing at different weighbridges: entry at one weighbridge and exit at another: are trucks entering at different weighbridges susceptible to fraud?

9.	Seasonality analysis: 
a.	Difference per month
b.	Gap by Time Period: Morning, Afternoon, Evening
c.	Period before Covid (before March 16) and during Covid
10.	Using the machine learning technique, could you classify the carriers by risk level, in order to propose detailed controls on the risky carriers.","","Get Hired","Data Scientist","01/01/2022 23:59:00","1"
"6438","1664353","8397327","10/22/2021 22:59:31","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)****","","U.S MILITARY DATA SETS","Falaj's dataset","","0"
"7021","1478095","6530303","12/06/2021 19:39:07","## Task Details

Credit Card Transactions are a rich and diverse data source. This particular synthetic dataset, consists of demographic and transaction details. Although the primary usage of this dataset has been for fraud detection, it would be interesting to mine this data and extract different trends and patterns.
The goal of this task is to showcase exploratory data analysis with visualizations that help in data understanding and trend identification.

## Expected Submission

A notebook providing in-depth analysis by making use of User, Card as well as Transaction data. The dataset consists of these separate csv files.
For reference, you can checkout different notebooks in the [Code section](https://www.kaggle.com/apoorvanitsureibm/credit-card-transactions-eda)

 The notebook should show:
-  Different visualizations to understand different attributes of the data
-  Statistical Analysis (Mean, Median, Mode, Correlation etc)
-  Trends Identification
-  Anything else you find interesting! 

It would be nice to see all intermediate steps and data preparation/ feature engineering methods employed as well.

## Evaluation

We will identify and recognize top submissions by the end of January 2022 based on the following criteria:

1. Engagement: How engaging is the submission? Would a viewer's interest be  
    maintained while going through the notebook? 
2. Simplicity: A good submission will ensure that different insights are presented in 
    an easily understandable but at the same time technically rich fashion.
3. Novelty: Does the submission have interesting elements beyond standard 
    metrics?","","Credit Card Transactions Exploratory Analysis","","01/31/2022 23:59:00","0"
"6899","1674428","4143489","11/22/2021 05:26:20","Basic Statistics tasks

1) Which store has maximum sales

2) Which store has maximum standard deviation i.e., the sales vary a lot. Also, find out the coefficient of mean to standard deviation

3) Which store/s has good quarterly growth rate in Q3‚Äô2012

4) Some holidays have a negative impact on sales. Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together

5) Provide a monthly and semester view of sales in units and give insights

Statistical Model

For Store 1 ‚Äì Build prediction models to forecast demand

Linear Regression ‚Äì Utilize variables like date and restructure dates as 1 for 5 Feb 2010 (starting from the earliest date in order). Hypothesize if CPI, unemployment, and fuel price have any impact on sales.

Change dates into days by creating new variable.

Select the model which gives best accuracy.","","TASKWalmart","","12/05/2021 23:59:00","1"
"6066","1585155","2836169","09/12/2021 12:12:17","Perform time series analysis and visualization on data","","Time Series Analysis","","","0"
"6067","1585155","2836169","09/12/2021 12:13:12","I don't know this is a good task with this data but maybe someone can build a model for time series prediction for this data.","","Google Stock Predictions","","","0"
"5636","1520929","2895306","08/09/2021 09:15:49","## Task Details
The task for the students in the school of information of Bohai University.","","BHU_wineQuality","BHU_wineQuality","10/10/2021 23:59:00","0"
"5233","1478543","2906858","07/20/2021 05:01:33","## Task Details
Dataset has all the Movies released by Walt Disney , but not cleaned , try to clean and perform EDA. to know types of movies. , which year highest release.","","Cleaning the dataset and Perform EDA","","","0"
"6766","1697612","2911977","11/06/2021 00:58:53","## Task Details
The communication and public participation of climate change on social media are particularly important, which makes social media a great resource for research on scientific communication and public opinions. We constructed this climate change Twitter dataset and hope people can be inspired from the data perspective in order to make a comprehensive analysis and tackle climate change issues using machine learning.","","Understanding Climate Change on Twitter using Machine Learning","","","0"
"6239","1625272","2925843","10/02/2021 20:14:30","## Task Details
You can have fun with the wide variety of features, and in fact, add or delete any features that you feel are necessary or unnecessary, and show your creativity in presenting the data.

## Expected Submission
The users can submit in Notebook or Script in either Python or R

## Evaluation
A good solution is anything that is clear, concise and is easily understandable by other users, so try to add comments on what you have done.","","Create data visualisation to show the different features","","","0"
"6240","1625272","2925843","10/02/2021 20:19:58","## Task Details
We often listen to songs and judge the artists whether they make happy, sad, emotional, aggressive or passionate ones. Just this time you have to do it using ML and DL algorithms.

## Expected Submission
I expect users to add a column at the end of the dataset displaying clearly what mood or vibe each song has.","","Determine the mood of different songs in the dataset","","","0"
"5458","1502719","2992577","07/31/2021 20:38:00","Disease and Remote Sensing assessments were regularly made in marked soybean quadrats (plots). The data set contains information about the dates when disease and remote sensing sensing assessments were made. Use your coding skills to visualize the trends in disease progress and canopy reflectance over time. 


Also, you can divided the quadrats into two categories i.e., healthy or diseased based on the last or second last disease assessment, and then compare these trends for healthy and diseased quadrats. 

What did you find?","","Exploratory Data Analysis","Explore the patterns of disease and soybean canopy reflectance","","0"
"5459","1502719","2992577","07/31/2021 20:40:08","As the Remote Sensing measurements were made before the disease onset. Can you early detect the soybean quadrats which developed SDS later in the season?","","Early detection of SDS","Can you detect soybean sudden death syndrome before onset?","","0"
"6311","1641176","3027960","10/11/2021 11:16:48","## Task Details
Find the top 10 countries in Asia, that are suffering pretty bad because of the Covid 19 Pandemic situation.

As a Data Scientist or ML engineer, provide your insights and help Health Authorities take remedial actions!","","Can you identify which countries need immediate attention?","","","0"
"7014","1755402","3032694","12/05/2021 01:55:03","## Task Details
The task we are trying to tackle is how to convert casual riders into annual members. The approach is trying to find out how annual members and casual riders use Cyclistic bikes differently. It means to work on the patterns of users' behaviors to describe similarities and differences between these two kinds of users. Using the convincing information from the findings to persuade the casual riders to consider the annual members will be beneficial.

## Expected Submission
Submit the notebook that contains the data wrangling, data exploring, visualization, and conclusions. 
It is expected that using three approaches to dig into the riders' behaviors, including 
- Temporal features: ride start/end time
- Spatial features: ride start/end station
- Categorical features: membership and bike type

## Evaluation
This task is one of the activties of the Google Data Analytics Certificate Program. 
My finished report has been uploaded as the notebook ""Divvy_trips_report_part2_dataAnalysis"", which is under R markdown format. It will be highly welcomed to share your thoughts and finding with the your notebooks. It would be nice that the notebook is composed of list of items, including
- Data wrangling: description of packages and methods that are used for data cleaning, and transformation. 
- Data exploring & visualization: approach that seeks for the insights into diverse data from categorical, temporal, to spatial points of view. Using visualization as communication bridges to audiences.
- Conclusions: summarize the analysis outcome with recommadations and relevent actions-to-do as takeaways for audiences.","","How to convert the casual riders into annual members","Using the convincing information from the findings to persuade the casual riders to consider the annual members will be beneficial.","","0"
"6069","1585678","3084406","09/12/2021 17:17:42","Explore how the situation has developed in the last few years in the Asian country!","","Afghanistan - Exploratory Data Analysis","Explore how the situation has developed in the last few years in the Asian country!","","0"
"6715","1682185","3134988","10/29/2021 17:17:49","## Task Details
Top priority is to predict the car price basis the top 14 cities of India.

## Expected Submission
The solution should contain 2 columns:
1) Car Model
2) Car Price

## Evaluation
Evaluation using F1-Score

## Acknowledgement
If you use this dataset in your research, please credit the author.","","Predict Car Price Basis The 14 Cities Provided","","","1"
"6274","1634033","3203431","10/07/2021 12:58:03","This dataset only covers events till 2019 end. It would be nice to have more data for period before or after start.","","Add More Events","","","0"
"6243","1624737","3215504","10/03/2021 02:37:24","Investigate the dataset with EDA and classify the mammals","","Classify the mammals","","","1"
"6032","1576150","3215504","09/07/2021 14:45:59","This task is to develop a classification model that can accurately classify the types of bears in the dataset. The evaluation criteria is upto the user/developer as this task & the dataset is intended for practise purposes","","Classify the bears","","","1"
"6196","1612313","3264446","09/26/2021 09:01:06","Can you cluster the customers and show their clusters?","","Cluster the customers","","","1"
"5574","1516110","3264446","08/06/2021 12:24:32","Create an EDA and show breakdowns of medals.","","Create an EDA","","","9"
"5596","1517733","3264446","08/07/2021 10:43:57","Create an EDA and let's show the details about transfers","","Create an EDA","","","1"
"5743","1531161","3264446","08/15/2021 07:32:41","Create an EDA and show breakdowns.","","Create an EDA","","","0"
"5634","1520585","3264446","08/09/2021 05:55:33","Create an EDA and show breakdowns of companies.","","Create an EDA","","","0"
"6326","1642787","3264446","10/12/2021 14:48:49","Create Clusters","","Create Clusters","","","0"
"5776","1537105","3302498","08/17/2021 16:42:34","## Task Details
The behavior and activities done by student should be stored digitally , so that can be analyses. The communication between parent and teacher describes lots about student's behavior and interests. If it keep on get documented can gives meaningful insights that will help student and parent to choose career and help to take decisions.

Task is to create lots of data and create ML model (text classification ) to tag the messages with it's intent.","","Create model to extract personality trait of person using texts used in communication between people","Extract student personality trait through parent teacher communication","","0"
"5522","1511119","3338793","08/03/2021 17:22:20","## Task Details
Use your Ml skills to predict whether a person has a heart disease or not using given features.

## Expected Submission
Given a data set split it into three Train-Test-Validate  ratio (70,15,15).
Use train and test for hyper parameter tuning and training and final prediction should be done score should be calculated with validate set. 

Try to use different thresolds for higher accuracy. (hint)

## Features 
Age
Sex : male : 1
female : 0

chest pain type
-- Value 1: typical angina
-- Value 2: atypical angina
-- Value 3: non-anginal pain
-- Value 4: asymptomatic

resting blood pressure (in mm Hg on admission to the hospital

serum cholestoral in mg/dl

(fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)

resting electrocardiographic results

-- Value 0: normal
-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)
-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria

thalach: maximum heart rate achieved

exercise induced angina (1 = yes; 0 = no)

Angina is chest pain or discomfort caused when your heart muscle doesn't get enough oxygen-rich blood.
It may feel like pressure or squeezing in your chest.

oldpeak = ST depression induced by exercise relative to rest

slope: the slope of the peak exercise ST segment

--Value 1: upsloping
-- Value 2: flat
-- Value 3: downsloping

vessels colored by flourosopy : number of major vessels (0-3) colored by flourosopy

A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)

Target : 0 No Heart disease
1 Heart disease","","Rules,Suggestions and About Features","","","0"
"6847","1692121","3364208","11/15/2021 16:10:18","## Task Details
The Bechdel Test (https://en.wikipedia.org/wiki/Bechdel_test) is a measure of female representation in fiction. Given the Bechdel Test Score dataset, what trend can you extract? Are more movies passing the test over the years? 

## Expected Submission
Submit a notebook that implements data wrangling, data visualization, and exploratory data analysis. 

## Evaluation
There is no specific metric for evaluation. Feel free to explore as much as you want.

### Example Analysis
I have published my notebook `Bechdel_vis.ipynb`: https://www.kaggle.com/alisonyao/bechdel-vis

See if you can find out more!","","Analyze Female Representation in Movies","What is the general trend of movie passing/failing the Bechdel Test over the year?","","0"
"6205","1614429","3369888","09/27/2021 09:21:06","Tasks:
Determine which state has the most crimes.
Determine which  state has the least crimes for children less than 6.
Plot the data for a state of your choice.
Plot the USA colored map by the crimes in each state.","","Perform Exploratory Data Analysis on the Dataset","","","2"
"5710","1527998","3387210","08/13/2021 07:44:34","Simple data analysis.","","Analyse the data?","","","0"
"5437","1501932","3442866","07/30/2021 19:58:33","Use this dataset to build a classification model, that can predict whether the next IPO can have listing gains or not. 

*If required, you can add variables related to the company's performance using the link provided in the dataset.*","","Predict the next IPO to have gains or loss","Investing using predictive analytics","","0"
"6000","1554964","3758223","09/04/2021 08:22:09","## Task Details
This dataset contains all related information to customers and travel packages.  You may use the package as your target for the prediction.

## Expected Submission
EDA and develop ML algorithm to predict the taken package.

## Evaluation
The accuracy of the model and comparison to other notebooks

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Which customer is more likely to buy the latest voyage packages","Prediction the customer group","","7"
"5965","1566331","3445455","09/01/2021 18:06:30","Perform an Exploratory Data Analysis on this dataset-'Latest Covid-19 Europe Data'","","Exploratory Data Analysis of the Covid-19 Situation in Europe and Its neighbor countries","","","0"
"6253","1622617","3675887","10/04/2021 10:05:03","What if you could predict if a song performs well?

The task would be to predict if a given song will perform and idealy find the optimal parameters for a trendy song.","","Popularity prediction","Predict the popularity of a song with given parameters","","0"
"6148","1603134","3445991","09/21/2021 12:41:09","## Task Details
Explore the data and identify the per cent of vaccination completed

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Explore the Vaccination across Countries","","","1"
"7011","1753898","3490494","12/04/2021 16:10:24","## ‚ñ∂ Task Details üìÑ
Forecasting daily new confirmed COVID-19 cases using time series forecasting techniques.

## ‚ñ∂ Expected Submission üì•
Solve the task using notebook. There is no restriction for programming language used.

## ‚ñ∂ Evaluation üìä
A model that have better performance and accuracy.","","Daily New Confirmed COVID-19 Cases Forecasting","Daily new confirmed COVID-19 cases time series forecasting","","0"
"5918","1560515","3494031","08/30/2021 05:28:03","This is a classification problem to distinguish between a signal process that produces Higgs bosons and a background process that does not.","","Classification","","","0"
"5717","1527146","3499709","08/13/2021 14:43:39","## Task Details
This dataset is meant for Federated (Collaborated) Learning. The directories are different clients that can learn a local model based on their train and test data, and the goal is to be able to combine their local model without sharing their data. Each client has cases of three classes of 3D printing quality. These classes are tagged at the beginning of the image filenames as follows: Normal, Underfill\_50FR, Underfill\_Fan.","","Federated Learning","","","0"
"7007","1767660","3520469","12/04/2021 12:15:15","## Task Details

Write a python script to take file input which will contain a sentence per line. For eachsentence, you have to identify if it‚Äôs a question or not by detecting question phrases like what,how, etc., and tag it yes or no. Write tagged data into another output file

Format of input file:-

how are you doing
My name is Jerry
I am not going to school

Format of output file:-

how are you doing.         Yes
My name is Jerry             No
I am not going to school No

## Expected Submission

Write tagged data into another output file

## Evaluation

Model with highest accuracy and best methods used","","Model to Check for Interrogative Sentences ?","Find if text is a Question or NOT?","","0"
"6216","1620260","6925136","09/30/2021 04:14:33","## Task Details
Create all possible relational graphs try to use plotly

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
better visualization get upvotes

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and visualisation of tokyo medal dataset","plot all possible relation graphs","10/16/2030 23:59:00","0"
"5646","1522275","3528145","08/10/2021 00:58:12","Perform an Exploratory Data Analysis on This dataset for the first step","","Exploratory data analysis","","","2"
"5218","1476840","7933086","07/19/2021 12:35:06","Try to predict next wave 1




## Task Details



Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict 1","Predict 1","07/23/2021 23:59:00","2"
"7025","1773407","9105966","12/07/2021 04:03:40","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
*This task is made for my training*
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

*user should submit a Dataset.*

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?


### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Training","","12/08/2021 23:59:00","0"
"6209","1605105","3590277","09/27/2021 13:15:22","## Task Details
There are a lot of parties in the political scene in Brazil so one interesting challenge is to understand which of these parties are in practice supporting each other.

Concordance rate: among all voting sessions, how frequently each party voted the same as the next one and so on.

## Expected Submission
This is not a formal competition but the idea would be to have a notebook submitted showing how the 'concordance' analysis was made and which are the parties that have the highest and lowest concordance rate among all. 

## Evaluation
Whoever the community judges to be more interesting and insightful.","","Party Concordance","What parties agree the most or disagree the most among all others?","","0"
"6998","1764954","3617395","12/03/2021 07:45:47","Predict the strength of Concrete Based on its Characteristics.","","Predict strength of Concrete","","","0"
"6718","1681244","3682357","10/30/2021 04:17:31","Comic Book Recommendation System","","Comic Book Recommendation System","","","1"
"6747","1681244","3682357","11/03/2021 09:03:10","Exploratory Data Analysis","","Exploratory Data Analysis","","","0"
"6848","1721518","3682357","11/15/2021 16:15:21","Perform Analysis on the data, and visualize some interesting findings, for eg.

1. Find the link between peak birth months and different seasons.
2. Find the link between peak birth months and latitude (Find the pattern between peak birth months for countries in the northern hemisphere vs the ones in the southern hemisphere ).","","Find Pattern Using Visualization","","","0"
"6337","1646173","3682357","10/14/2021 05:53:32","Perform exploratory data analysis for batsman and bowlers.","","EDA-In-Depth","","","1"
"6272","1631883","3682357","10/07/2021 04:17:44","Create a text-based EDA of the data.","","EDA-Tweets","","","5"
"6313","1641574","3682357","10/11/2021 14:55:49","Time series analysis.","","Analysis of historical data.","","","0"
"6423","1661718","3682357","10/21/2021 05:27:16","Visualize the connection between the characters.","","Visualize the connection between the characters.","","","0"
"6433","1664009","3682357","10/22/2021 07:03:53","Predict the behaviour of the swarm.","","Predict the behaviour of the swarm.","","","1"
"6381","1651731","3682357","10/16/2021 07:00:22","## Task Details
Classify if the crah  is multiple or single.","","Classify the Crash Type","","","0"
"6398","1654442","3682357","10/18/2021 07:03:13","Find out which era had the best video games ü§ì.","","Exploratory Data Analysis","","","1"
"7029","1654442","9101461","12/07/2021 22:04:41","How to properly predict what the meta score of a video game from this list would be given the platform as an input? If anyone can help I would greatly appreciate it!!! I am lost at trying to achieve this.","","Create a linear regression model using platform as an input and meta score as output","How to properly predict what the meta score of a video game from this list would be given the platform as an input?","","0"
"5496","1506771","4971832","08/02/2021 06:58:30","Perform image classification using pretrained architectures","","Devise a classification model with best AUC score","","","0"
"6403","1656820","3758223","10/18/2021 20:51:21","## Task Details
Build different models to predict different problems (classification/regression)

## Expected Submission
one Juypter Notebook including EDA and implementing models to predict defined problems.

## Evaluation
Quality of the description, EDA, and model performance

### Further help
prepare, explore the data, and chose the correct tools for the type of data","","Building the Models","Predict different problems (classification/regression)","","1"
"5144","1470929","3796693","07/16/2021 10:21:44","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Perform EDA and make a model with high accuracy

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","facebook usage over the time","","","0"
"5781","1537647","7939485","08/18/2021 05:18:34","IF-Kaggle testtest","","IF-Kaggle testtest","IF-Kaggle testtest","09/10/2021 23:59:00","1"
"5851","1541738","3828601","08/23/2021 02:55:43","Top & bottom 3 countries with high/low tree cover loss (and associated gross emission) in last 5 years

- Top & bottom 3 countries with high/low tree cover loss in primary forest (and associated gross emission) in last 5 years

- Year with the highest tree cover loss and gross emission (also its dominant driver)

- Year with the lowest tree cover loss and gross emission (also its dominant driver)

- And Basic EDA","","List of Tasks","","","0"
"5850","1546701","3828601","08/23/2021 02:49:20","Top & bottom 3 raider(s) with high/low average and super raids across 7 seasons

- Top & bottom 3 tackler(s) with high/low average and super tackles across 7 seasons

- Top 3 club(s) with super raiders / tacklers across 7 seasons

- Consistent raider(s) & tackler(s) across 7 seasons

- If & when Season8 happens, which club(s),raider(s) & tackler(s) is most likely to perform better based on the players stats from previous 7 seasons","","List of Tasks","","","0"
"5853","1546886","3828601","08/23/2021 05:14:49","User sentiments for all the 5 phones based on their reviews
- Create a 'rating' for the phones based on their sentiment score & total reviews","","Some Tasks","","","0"
"6281","1635244","3828601","10/08/2021 01:36:30","Trend Visualisation","","Trend Visualisation","","","1"
"6267","1628787","7951006","10/06/2021 09:09:29","Find out the country which has highest values of Quantity,
Value and UnitValues and plot it on a bar plot","","Max Values","","03/10/2022 23:59:00","3"
"6357","1648905","3828601","10/15/2021 02:25:02","## Tasks
- Country with most high ranked player
- Top players across Classic / Rapid / Blitz game
- Top youngest / oldest player","","Some Tasks","","","0"
"5621","1519876","3838650","08/08/2021 22:07:01","## Task Details
A simple task to evaluate models' performance on lab images but from a separate viewpoint.


## Evaluation
F1-score","","Image segmentation in the lab","","","0"
"5622","1519876","3838650","08/08/2021 22:09:00","## Task Details
Evaluate models' performance on the wild data using only lab images (and optionally additional backgrounds) for training.

## Evaluation
F1-score","","Image segmentation in the wild","","","0"
"5970","1567286","3927080","09/02/2021 08:38:32","## Task Details
This task is to fill with random data the missing data about NA.

## Expected Submission
The most people faster learn how much they can profit on each career the better they can choose one for picking accurately

## Evaluation
Find please some libray wich wears statistical algorithms to fill the missing data.

### Further help
Search por statics, mean","","First Stage Cleaning NAs","Apply some method to filling missing data, the completely random","09/02/2023 23:59:00","0"
"5999","1570216","3955165","09/04/2021 05:53:34","## Task Details
These data files are generated by my python script. I try to looking for a free solution for holding the script. Run it scheduled cronjob daily to automatically the process.","","Automatic run the python script to update the dataset (already have the script)","","","0"
"5393","1493500","3991483","07/28/2021 17:25:43","Analyze the data","","Explore the data","","","0"
"5394","1493500","3991483","07/28/2021 17:29:53","Find good features that can help predict price better.
Few features that might influence Cryptocurrency price - 
- Start and end of year
- Festival season
- Thanksgiving
- Quarter end","","Feature engineering","","","0"
"5395","1493500","3991483","07/28/2021 17:30:59","Try to predict the future price for Binance coin using the data given.
Use any model of your choice.","","Forecasting future price","","","0"
"5396","1496701","3991483","07/28/2021 17:32:33","Analyze EDA for the data.","","Data analysis","","","0"
"5397","1496701","3991483","07/28/2021 17:33:51","How does a cryptocurrency like Bitcoin effect other currencies like ethereum, litecoin, dogecoin?","","Study influence of one crypto on another","","","0"
"5398","1488313","3991483","07/28/2021 17:35:54","Find any trend or keywords in the data.","","EDA for tweets","","","0"
"5399","1488313","3991483","07/28/2021 17:37:11","Perform sentiment analysis for the tweets.
Is there a correlation between sentiments of tweets and geography of tweets?","","Sentiment analysis","","","0"
"6144","1601728","4016385","09/20/2021 18:24:58","This dataset was obtained from UC Berkeley, Department of Statistics and D-Lab; Data presented by Chris Paciorek. 

The dataset contains relevant characteristics/ features of US airlines from 2005 to 2008. You can find the raw data & official analysis here: https://github.com/berkeley-scf/r-bootcamp-2016/tree/master/data.","","Commercial Air Travel Analysis","Exploratory Data Analysis","","0"
"6078","1588059","4042963","09/14/2021 04:12:16","## Task Details
The aim of the task is to predict the mortality risk of covid -19 patients (i.e the model is able to classify that the patient is in severe condition or not OR he/she is going to die in the future(0/1)).


## Expected Submission
You should submit the notebook which would perform the EDA on dataset to understand the data better and implement a prediction model which gives the best AUC score for the classification of mortality risk in near future.


## Evaluation
The notebooks would be evaluated on the basis of best AUC Score.","","Predict Mortality Risk of Patients in Early stages","Classify the Covid-19 patient iMortality Risk","11/30/2021 23:59:00","1"
"6222","1622512","4042963","10/01/2021 04:13:57","## Task Details
**Perform Full Exploratory Data Analysis on ""COVID19 Cases(total,active,recovered) Countrywise""** Dataset and find out the insights from the Datset such as How number of cases are varying, number of deaths in each country .","","EDA on ""Covid-19 Dataset for each country in Tennessee","Perform Full Exploratory Data Analysis on this Dataset.","","0"
"6834","1711482","6982946","11/12/2021 18:39:57","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.","","NIfty Stock Market data visualization","","","0"
"6132","1563531","4126237","09/19/2021 14:12:37","Perform EDA on Stack Overflow Survey Results and try to figure out the necessary features","","EDA on Stack Overflow Survey Results","","","0"
"5610","1513368","4135531","08/08/2021 17:07:38","## Task Details
###The goal
Use a ML model to predicts the price of the used car using the dataset. You can use one currency or all of them depending on the approach you chose.
###Make Sure that:
1- For the same Make, Model, Submodel (or Make and Model) combination, the higher the number of miles, the lower the price for the same year.
2- For the same Make, Model, Submodel (or Make and Model) combination, the lower the year, the lower the price, do miles matter?
###Questions
1- How can you enforce monotonicity of year and/or mileage with respect to the price?
2- Predict the price of the car per each currency
3- Create visuals of price depreciation (price curve for same Make, Model, Submodel (or Make and Model) for different years)
4- What is the impact of submodel_id on price? Do you find a problem in the lack of values for this column?","","Used car prediction","","","0"
"5441","1487944","4163969","07/31/2021 04:58:14","EDA on Countries participated, disciplines, events & coaches.","","EDA on Countries participated, disciplines, events & coaches.","","","55"
"5391","1496240","4163969","07/28/2021 16:39:17","Time Series Analysis of the Companies starting from 2017-2021","","Time Series Analysis","","","1"
"5392","1496240","4163969","07/28/2021 16:40:43","Effect on these companies before and during the pandeic","","Pre-Covid & Post-Covid Analysis","","","1"
"5442","1494503","4163969","07/31/2021 04:59:14","Time series analysis for the historical data from the year 1987-2013","","Time Series Analysis","","","0"
"5384","1496072","4181516","07/28/2021 08:46:31","## Task Details
classify the email as spam or ham. There are 10 different features and one target variable. Depending upon the words that is contained in the email you have to tell that email is spam or not.

 solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Classify the email as spam or ham","","10/28/2025 23:59:00","0"
"6839","1718779","4227306","11/14/2021 15:53:27","## Problem Statement:
The intermittent nature and low control over the wind conditions bring up the same problem to every grid operator in their successful integration to satisfy current demand. In combination with having to predict demand and balance it with the supply, the grid operator now also must predict the availability of wind and solar generation plants in the next hour, day, or week. Apart from holding
back the benefits of renewable energy, incorrectly scheduling of wind generation plants may lead to unnecessary reservations, higher costs passed over to the consumer, and use of other more expensive and polluting power resources.
Working with real data is challenging due to noise and missing periods. 

## Dataset details
The provided full-year hourly time-series are simulated using the National Renewable Energy Laboratory (NREL) software for a location in Texas, US. It has perfect data completeness, and no noisy data; challenges that hinder forecasting tasks with real datasets and distract from the goal.

The dataset contains various weather features which can be analyzed and used as predictors.

## Task Details
Predict the next day of power output = the next 24 steps ahead.

# Wind Turbine details
General Electric Wind Turbine installed onshore:
- Rotor diameter 111m 
- Rated output 3600kW
- Hub height 80m
- Single Wind Turbine

## Evaluation
A good solution will include a simple baseline and a more advanced forecasting approach. Visualization of both on the same graph will be ace!

Recommended evaluation metrics:
- Mean Error (Bias)
- Root Mean Sqare Error (RMSE)
- Mean Absolute Error (MAE)
- Mean Absolute Percentage Error (MAPE)
- Correlation

No deadline for the task, the provided dataset is purely for education purposes.","","Forecast Wind Power for next day (24 steps ahead)","","","0"
"6759","1695961","4286690","11/05/2021 04:37:37","To know the forecast of all the mentioned crypto currency for the next year and match the actual forecast

This is no competition just a little help if any one can share the input for the uploaded data.","","Forecasting the next pricing for all the crypto.","","","0"
"6713","1680593","4308674","10/29/2021 13:09:07","## Task Details

I wanted to test different algorithms of text generation, and saw a bunch of good tutorials on this, such as : https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/

But I wanted it to work on french text so that I can easily see qualities and drawbacks of the generated text.

So I gathered (quickly) some data (the book ""Flowers of Evils"", in french ""Les Fleurs du Mal"") and try to generate some text as would Baudelaire do.

It can generate this text char par char or word by word, and use any algorithm, but I prefer to rely on neural networks....

## Expected Submission
Please have fun, and produce notebooks.

## Evaluation
This is poetry. We might evaluate the grammar correctness of the generated text, but it should also capture the style of the text. It should also be creative, in the sens it should not simply paste element of the original text.","","French Text Generation","toward artificial poetry ?","","0"
"7000","1765929","4310004","12/03/2021 15:24:00","Please ! 

Someone use fucking StyleGAN and those images and fixing emotions to generate goddamn faces !  Need more high quality FER dataset.","","Fixing Emotions & Change Other Attributes","","","0"
"7001","1765939","4310004","12/03/2021 15:29:01","Plz!!!


Someone use StyleGAN to generate goddamn emotion fixed data !! 

The other emotion data is lack of quality !!!","","Fixing Emotions & Change Other Attributes","","","0"
"7019","1771378","4314579","12/06/2021 04:31:18","## Task Details
Tasked with identifying first-party physical damage fraudulence and explaining the indicators of fraudulent claims.

## Expected Submission
Models to be evaluated using F1 score. The submission file should be a CSV containing two columns: claim_nbr and prediction (the predicted fraud indicator (0 or 1), not the probability)

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Fraudulent Claim Classification","","","0"
"6832","1707933","4333519","11/12/2021 13:18:12","Data Set Name: Anuran Calls (MFCCs)

Abstract: (less than 200 characters).

Acoustic features extracted from syllables of anuran (frogs) calls, including the family, the genus, and the species labels.","","Anuran Calls (MFCCs)","Acoustic features extracted from syllables of anuran (frogs) calls, including the family, the genus, and the species labels.","06/05/2030 23:59:00","0"
"6200","1613286","4333519","09/26/2021 18:48:46","The ""Total Tests"" and ""Positive Tests"" columns show totals based on the collection date. There is a lag between when a specimen is collected and when it is reported in this dataset. As a result, the most recent dates on the table will temporarily show NONE in the ""Total Tests"" and ""Positive Tests"" columns. This should not be interpreted as no tests being conducted on these dates. Instead, these values will be updated with the number of tests conducted as data is received.","","COVID-19 Time-Series Metrics by County and State","COVID-19 Time-Series Metrics by County and State","10/27/2021 23:59:00","0"
"5807","1541836","4356057","08/20/2021 02:39:23","## Task Details
Find and Translate the Images into proper equations and match them with the original ones.","","Detecting Equations From Images","","","1"
"5808","1541836","4356057","08/20/2021 02:40:13","## Task Details
Explore the Equation and find the Proper Roots for the Equations.","","Finding The Roots","","","1"
"5498","1505846","4378523","08/02/2021 08:19:13","## Task Details
Find whether the crime rate is increasing or decreasing. Find out where most of the crimes are taking place. Analyze different types of crime rates per year.","","Find insight","Analysis the data to find insight of the data","","0"
"5542","1510802","4388004","08/04/2021 10:10:10","We have descriptions of each movie and their scripts. Find similar episodes and recommend.","","Recommender Systems","","","1"
"6207","1614935","4426458","09/27/2021 12:18:20","Perform EDA and detect commonalities between genres, cast and timeline","","IMDB Indian data set","Source : https://data.world/adrianmcmahon/imdb-dataset-all-indian-movies/workspace/file?filename=IMDb+Movies+India.csv","11/30/2021 23:59:00","0"
"6202","1612616","4432707","09/27/2021 03:34:39","## Task Details
This dataset has a good story to tell about the NetFlix stock. What would be your story? How would you analyze, predict and forecast the performance of this Dataset?

## Expected Submission
A notebook/kernel that educates fellow Kagglers

## Evaluation
No evaluation. Every submission is a winner. The votes and comments will be the currency for recognition and evaluation

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Analyze and Predict Performance of Netflix","How would you analyze, predict and forecast the performance of this Dataset?","","0"
"6227","1623229","4432707","10/01/2021 13:55:29","![TCS](https://www.tcs.com/content/dam/tcs/images/Newtcslogo/logo-white1x.png)
Tata Consultancy Services (TCS) is an Indian multinational information technology (IT) services and consulting company headquartered in Mumbai, Maharashtra, India with its largest campus located in Chennai, Tamil Nadu, India. As of February 2021, TCS is the largest IT services company in the world by market capitalisation ($200 billion). It is a subsidiary of the Tata Group and operates in 149 locations across 46 countries.

TCS is the second largest Indian company by market capitalisation and is among the most valuable IT services brands worldwide.In 2015, TCS was ranked 64th overall in the Forbes World's Most Innovative Companies ranking, making it both the highest-ranked IT services company and the top Indian company. As of 2018, it is ranked eleventh on the Fortune India 500 list.In April 2018, TCS became the first Indian IT company to reach $100 billion in market capitalisation and second Indian company ever (after Reliance Industries achieved it in 2007) after its market capitalisation stood at ‚Çπ6.793 trillion (equivalent to ‚Çπ7.3 trillion or US$100 billion in 2019) on the Bombay Stock Exchange.

In 2016‚Äì2017, parent company Tata Sons owned 72.05% of TCS and more than 70% of Tata Sons' dividends were generated by TCS. In March 2018, Tata Sons decided to sell stocks of TCS worth $1.25 billion in a bulk deal.As of 15 September 2021, TCS has recorded a market capitalisation of US$200 billion, making it the first Indian IT firm to do so.

## Task

Analyze
Visualize 
Predict","","Visualize and Forecast TCS Stock performance","How will you analyze Tata Consultancy Services Stock?","","0"
"6172","1607002","4432707","09/23/2021 13:44:48","How would you use this data?","","Analyze Microsoft Stock Performance and Predict","Perform EDA and forecasting","","2"
"6075","1576491","4432707","09/13/2021 11:41:32","Can you try this task and upload your notebooks and kernels? 

## Task Details
Every _data_ has a story to tell. Tell users what this data is all about and how you can weave it. How would you use this data to make brilliant data visualization stories?

## What is needed

Any figment of imagination. Any brilliant output in terms of visualization are welcome

## Evaluation
Everything is a brilliant submission. Go for it. The audience will evaluate with the comments and recognition.

Thanks for your inputs!","","Data Visualisation for the metro dataset","How would you analyse this data?","","1"
"6058","1562748","4432707","09/11/2021 08:27:41","## Task Details
There are plenty of tools and techniques to visualize the data in millions of ways. 
How would you visualize this dataset?

## Expected Submission
Different types of visualization techniques and picturesque summary

## Evaluation
No evaluation - It is only for fun.","","Requesting for your data visualization examples","How would you visualize the data available in this dataset?","","1"
"6131","1562748","4432707","09/19/2021 09:26:14","## Task Details
Use WorldPopulation.csv dataset and show your innovative visualization skills to the world!","","How would you visualize World Population?","Visualize World Population Dataset","","0"
"6993","1762076","9059875","12/02/2021 09:26:36","`****##` Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","/WM.org","For easy update and fast response","12/18/2021 23:59:00","1"
"5766","1534478","4537098","08/17/2021 04:21:43","Given UAV Images try to segment foreground and background objects","","Background Subtraction","","","0"
"5697","1527006","4542396","08/12/2021 15:02:46","Analysis Task

To perform these tasks, you can use any of the different Python libraries such as NumPy, SciPy, Pandas, sci-kitAnalysis Task

To perform these tasks, you can use any of the different Python libraries such as NumPy, SciPy, Pandas, scikit-learn, matplotlib, and BeautifulSoup.

- Import data into Python environment.
- Provide the trend chart for the number of complaints at monthly and daily granularity levels.
- Provide a table with the frequency of complaint types.

Which complaint types are maximum i.e., around the internet, network issues, or across any other domains.
- Create a new categorical variable with value as Open and Closed. Open & Pending is to be categorized as Open and Closed & Solved is to be categorized as Closed.
- Provide state-wise status of complaints in a stacked bar chart. Use the categorized variable from Q3. Provide insights on:

Which state has the maximum complaints
Which state has the highest percentage of unresolved complaints
- Provide the percentage of complaints resolved till date, which were received through the Internet and customer care calls.","","Exploration data analysis - consumer complaints","","09/30/2021 23:59:00","0"
"6812","1698290","4561355","11/10/2021 13:33:59","## Task Details

Train multiple model on data to predict BMD and find model that has less RMSE.

## Expected Submission

1. EDA(Optional)
2. Train multiple models
3. Use cross val score","","Predict BMD using regression models","Find best model to predict BMD","","0"
"6152","1604734","4575748","09/22/2021 11:51:37","Electromagnetic Radiation patterns for CyberSecurity protection.","","Signals Level - 400 to 900 MHZ","Electromagnetic Radiation patterns - ISM EU Bands","","0"
"5590","1516968","4616014","08/06/2021 23:56:19","The following dataset is obtained from https://github.com/AsadAliDD/pkwheels_scraper and was cleaned during preprocessing to make the data feasible enough to generate a prediction.
This dataset is scraped from PakWheels and contains information regarding almost all the cars for sale on the Platform.

Fields:
Ad Ref No.
Name
Price
Model Year
Location
Mileage
Registered City
Engine Type
Engine Capacity
Transmission
Color
Assembly
Body Type
Features
Last Updated
URL
Disclaimer: All Data belongs to PakWheels and the code scraper(Asad Ali, Profile link: https://www.kaggle.com/spideysloth)","","Prediction of car prices","","","0"
"5573","1515604","4642142","08/06/2021 06:49:02","## Task Details
To build a classification model  to predict sell conditions of forex data.

## Expected Submission
Submit the code to solve this problem. Solve the task primarily using  Datasets. 
Solution must be in notebook format which contain all the classification models used along with predictor score and explanation.

## Evaluation
Evaluation is based on accuracy metrics and one without overfitting or underfitting.","","Classification model building","Sell Strategy","","1"
"6924","1743222","4671367","11/24/2021 14:10:09","## Task Details
Predict the employee attrition of the insurance company
Say You are working as a data scientist with HR Department of a large insurance company focused on sales team attrition. Insurance sales teams help insurance companies generate new business by contacting potential customers and selling one or more types of insurance. The department generally sees high attrition and thus staffing becomes a crucial aspect.

To aid staffing, you are provided with the monthly information for a segment of employees for 2016 and 2017 and tasked to predict whether a current employee will be leaving the organisation in the upcoming two quarters (01 Jan 2018 - 01 July 2018) or not, given:
1. Demographics of the employee (city, age, gender etc.)
2. Tenure information (joining date, Last Date)
3. Historical data regarding the performance of the employee (Quarterly rating, Monthly business acquired, designation, salary)

## Evaluation matrix
F1Score","","Predict Employee attrition","Problem statement is to predict the employee attrition of the insurance company.","","0"
"5222","1477642","4719143","07/19/2021 15:42:53","building a speech to text system using speech corpus","","Speech to text","STT system","","0"
"5180","1474506","4745228","07/18/2021 07:36:19","## Task Details
You are required to Analyze the Players' and Teams' Performance, **especially in Playoffs.**

## Expected Submission
You are expected to submit the notebook.","","Analyze Players' and Teams' Performance.","","","0"
"6116","1594834","4758542","09/17/2021 09:33:03","**NVIDIA CORPORATION STOCK PRICE   2020-2021**
1.  These are the opening/closing of Nvidia Company from Sept 2020 - Sept 2021
2. ***There was an adjustment in their price ***  Do not forget to scale the data then 

***Task***
1. Apply Feature Engineering Techniques to pre-process the data
2. Predict the stock price October
3. Build RNN architecture ( Practice Purpose )","","NVIDIA - STOCK PRICE PREDICTION","NVIDIA","","0"
"6026","1574514","4764842","09/07/2021 02:45:13","## Task Details
Develop a binary classification model.","","Malaria Detection","","","0"
"6783","1702356","4789773","11/08/2021 10:13:14","I propose you check if these insiders are doing better than a benchmark (for example, SPY).","","Does insiders do better than market?","","","0"
"5669","1489973","4811132","08/11/2021 07:34:18","## Task Details
Build a predictive model to determine whether a given transaction will be fraud or not.","","Fraud or not","","","0"
"5544","1499376","4837769","08/04/2021 12:48:14","## Task Details
Rise Of COVID-19 is one of the most unexpected events in 21 th century. It has forced all of us to wear masks when in contact with outside world. Existing Facial Recognition Systems fail to identify people wearing masks. Re Training them with masked anchors in one solution , but it would perform poor if a person is not wearing a mask ! Hence, detecting masks has become an important task in recent times. 
## Expected Submission
Use the dataset and create notebooks which perform highly accurate face mask detection .

## Evaluation
Evaluation is just like any other object detection problem , MAP and frames are the evaluation metrices.","","Face Mask Detection","","","0"
"5864","1522995","4849300","08/24/2021 11:31:01","## Task Details
classification of venom and non-venom snake by using this three folder and try to train deep learning model with good accuracy.

## Expected Submission
You can submit Jupyter notebook

### Further help 
Motivation : We can improve our skills towards deep learning.
If you need  any additional motivation checkout this site.
inspiration.
https://savethesnakes.org/purpose/#:~:text=Together%20with%20our%20worldwide%20network,relationship%20between%20humans%20and%20snakes.","","Venom and Non-Venom Snake","classification of venom and non-venom snake","","0"
"5879","1553102","4875623","08/26/2021 05:57:17","Please contact for any help
!!!!","","Salary","Based on dataset predict the salary also Date o joining,Date of leaving ,Designation and Jobcity","","0"
"6235","1621130","4875623","10/02/2021 05:52:30","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?","","Predict data science salary or skills","","","0"
"6213","1619120","4875623","09/29/2021 11:26:59","Predict the future price of Iphone and help apple to fix price right.....","","Predict future Iphone Price","","","0"
"6269","1632636","4875623","10/06/2021 18:13:13","With available information from naukri.com you can search for job profile and its values .what is required to gathr higher job","","Job opening from Naukri.com","","","0"
"6298","1640018","4875623","10/10/2021 19:35:25","Hey it is a begineer level dataset to creat  a model to predict laptop specification.","","Predict the price of laptop based on specification","","","0"
"6445","1667579","4875623","10/24/2021 05:44:17","To create a model and predict the price of the laptop.
The dataset contains all about laptop brand ,processor,color etc.","","Price of the laptop","","","0"
"6447","1667619","4875623","10/24/2021 06:03:28","Develop a model to predict data analyst salary.","","Data Analyst","","","0"
"5789","1539143","4893527","08/18/2021 15:28:56","## Task Details
Build a regressor model which can predict the heart rate of an individual.

## Evaluation 
Evaluation Metric - MAE (Mean Absolute Error)","","Build a regressor model which can predict the heart rate of an individual","","","1"
"5676","1525403","4893527","08/11/2021 16:33:31","## Task Details
Construct a deep learning classifier such as LSTM or similar model to predict the category of a news article given its title and abstract.

Model A, the deep learning classifier only requires the news_text.csv dataset. The goal is to predict the ‚Äòcategory‚Äô label using the ‚Äòtitle‚Äô and ‚Äòabstract; columns.","","Deep Learning NLP Classifier to predict the category of a news article given its title and abstract","","","0"
"5677","1525403","4893527","08/11/2021 16:35:20","## Task Details
Model B, the recommendation system only requires user_news_clicks.csv but you can use the news_text.csv in addition if you‚Äôd like though it is not necessary for this exercise. The goal is to be able to recommend users news articles that they‚Äôre likely to click.","","Recommendation system to recommend posts that a user is more likely to click","","","0"
"5388","1496343","4905570","07/28/2021 11:04:23","## Task Details
You can implement Object detection techniques and detect gun in the images.","","Gun Detection using computer vision techniques","","","0"
"6252","1627569","4908082","10/04/2021 09:01:00","## Task Details
Use this dataset for image classification.
This is specifically preprocessed for YOLO detection.


## Evaluation
Mean Average Precision(MAP)/ F1/ Absolute Mean Error(AME)","","Yolov5-Image classification","","","1"
"5866","1548057","4912949","08/24/2021 16:41:05","## Task Details
Predict the Loading profile for Jan-2020 month.

## Expected Submission
Please take the 'test_Jan.csv' file and try to predict the 'nat_demand' and match the values in 'predict_Jan.csv' Lowest error will be our target

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
please search LSTM Time series forecasting. You will get all the help required.","","Predict the Loading Pattern for month of Jan-2020","","10/27/2021 23:59:00","0"
"5790","1536971","4656934","08/18/2021 16:22:53","Perform EDA.
1. **State Wise**
2. **Year Wise**
And come with your own ideas.","","Perform EDA.","","","0"
"6987","1761144","4996052","12/01/2021 18:30:18","## The objective of this competition is to build a model to recognise ten different everyday KSL signs present in the images, using machine learning or deep learning algorithms.","","Classification challenge","","","0"
"6022","1574501","5011653","09/06/2021 14:31:50","## Task Details
Visualize the data and find the most useful information out of it","","Exploratory Data Analysis","","","0"
"6033","1576574","5011653","09/07/2021 15:26:55","Analyze the various aspects of the data set and find out useful info","","Exploratory Data Analysis","","","0"
"6085","1588776","5011653","09/15/2021 05:45:16","Analyse the data and figure out important statistics, like most accepting countries to refugees, most hostile etc..","","Exploratory Data Analysis","","","0"
"6114","1588776","6489530","09/16/2021 23:40:48","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)

Data Mining class project","","Data Analysis","","12/10/2021 23:59:00","0"
"6143","1601748","5011653","09/20/2021 18:20:33","Compare ratings , users etc.. to determine the most relevant and popular extensions.","","Explanatory Data Analysis","","","0"
"5649","1522679","5024422","08/10/2021 06:00:00","Jahanna Chronicle, a technology company based in Lunakick, has reached out to you with a task that involves predicting the impact of a tweet and prepare a report on your analysis. You could think about ‚Äòimpact‚Äô as a value that could help Jahanna Chronicle decide if the tweet could go viral. 
Chronicle‚Äôs data team has worked hard and prepared a dataset for you. They compiled the data and decided to share it via HTTP. So here is the link, directly from the Chronicle‚Äôs data team! 
Dataset: Attached as part of email
There are 15 features and 1 dependent variable (also called as the output variable; Here it is named as ‚Äòimpact‚Äô). 
Post Content - The text in the tweet
Sentiment score - Ranges from -20 to +20 (0 - neutral)
Post Length - The length of the tweet
Hashtag Count - The number of hashtags used in the tweet
Content URL Count - The number of URLs mentioned in the tweet
Tweet Count - The total number of tweets posted by the author of the tweet
Followers Count - The number of followers of the author of the post
Listed Count - the number of lists the post author is a part of
Media Type - The media type of the post (Text, image, video)
Published Datetime - The published time of the tweet
Mentions Count - The number of user mentions in the tweet
Post Author Verified - 1 if author is a verified user
Likes - Likes received for the tweet
Shares - Retweets received for the tweet
Comments - Number of comments for the tweet
For some weird reason, Chronicle‚Äôs engineering team is adamant that you explore modeling using a decision tree (with some form of boosting and pruning), neural network and linear regression but give the model that fits best. 
To summarize here are the tasks required by Jahanna Chronicle: 
‚óè Give the model and code (by uploading it to a public repo) that can best predict the impact score 
‚óè Prepare a report of not more than 6 pages of your findings of data. It should include analysis like the training and testing error rates you obtained running the various learning algorithms on your problems, graphs that show performance on both training and test data as a function of training size, Why did you get the results you did? How fast were they in terms of wall clock time? How much performance was due to the problems you were given? How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can. 
‚óè All the code that was used to prepare a report (also by uploading it to a public repo)
Note from Jahanna Chronicle: The engineers at Chronicle must be able to recreate all experim","","Predicting-the-impact-of-tweets","","","0"
"5604","1519260","5033862","08/08/2021 11:33:06","Classify into correct category using any of the data provided.","","Classify into correct category given the resume data","","","0"
"5139","1469954","5046332","07/15/2021 19:49:47","## Task Details
Univariate Survival Analysis on turnover data.

## Expected Submission
Users should submit notebooks wherein they have analyzed the data thoroughly as well as predicting the hazard, cumulative hazard, risk, survival function, density, cumulative density.

## Evaluation
Having a high Concordance index, a high AIC score and an IBS &lt; 0.25.","","Univariate Survival Analysis","","","0"
"5140","1469954","5046332","07/15/2021 19:50:20","## Task Details
Multivariate Survival Analysis on turnover data.

## Expected Submission
Users should submit notebooks wherein they have analyzed the data thoroughly as well as predicting the hazard, cumulative hazard, risk, survival function, density, cumulative density.

## Evaluation
Having a high Concordance index, a high AIC score and an IBS &lt; 0.25.","","Multivariate Survival Analysis","","","0"
"5856","1548295","5048068","08/23/2021 18:21:36","If you are a beginner in data visualization, then this is the right dataset for you.","","Visualize the change throughout years","","10/23/2021 23:59:00","0"
"5799","1535670","5058161","08/19/2021 09:10:34","During 2021 lot of startups like Zomato, paytm and others are bringing their startups, so its a high time to analyze the returns of these IPOs through the historical data, and see if we can find some trend in these IPOS

### Further help
For web scraping : [BS4](https://www.kaggle.com/soumyadipghorai/web-scrapping-with-bs4-part-1)
EDA : [Olympic](https://www.kaggle.com/soumyadipghorai/olympic-2020-eda/comments)
[Covid](https://www.kaggle.com/soumyadipghorai/covid-19-vaccination-drive-india)","","EDA of IPOs in India","Exploratory Data Analysis of all IPOs in India","","0"
"6017","1573588","5059554","09/06/2021 06:01:57","## Task Details
Detect The Pipes using computer vision.","","Pipe_dataset","","","0"
"6048","1580348","5069700","09/09/2021 14:58:05","## Task Details
This challenge aims at the automatic nonlinear image registration of 2D microscopy images of histopathology tissue samples stained with different dyes. The task is difficult due to non-linear deformations affecting the tissue samples, the different appearance of each stain, repetitive texture, and the large size of the whole slide images.

### Further help
https://anhir.grand-challenge.org/","","Image Registration","Benchmark on Image Registration methods with Landmark validation","","0"
"5680","1525710","5103141","08/11/2021 20:26:30","Find out The best month for sales, and gather insights from the Dataset. the additional columns such as Month, City etc can be used for help.","","What was the best month for sales? How much was earned that month?","","","1"
"5681","1525710","5103141","08/11/2021 20:27:45","Perform EDA on the dataset to find out which city sold the most products. Additional insights and data visualisation is encouraged.","","What city sold the most product?","","","0"
"5682","1525710","5103141","08/11/2021 20:28:42","When are the best times to display ads so that there is a chance of a purchase?","","What time should we display advertisemens to maximize the likelihood of customer‚Äôs buying product?","","","0"
"5683","1525710","5103141","08/11/2021 20:29:15","What are the combinations of products that are sold together?","","What products are most often sold together?","","","0"
"5684","1525710","5103141","08/11/2021 20:29:44","Additional insights about the dataset.","","What product sold the most? Why do you think it sold the most?","","","0"
"5690","1525990","8023347","08/12/2021 05:47:43","reaveal the wonders","","Do a Exploratory Data Analysis","","","0"
"6702","1680097","5161394","10/28/2021 15:51:49","Task Details
Every task ha
s a story. Tell users what this task is all about and why you created it.
Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Visualize CAR data","Practise","10/30/2021 23:59:00","0"
"5215","1475653","5197665","07/19/2021 08:57:57","A couple of students doing this kind of thing for the first time managed to get to 
~&lt; 80% validation accuracy and ~1 validation loss. Let's set this as a baseline and see where you can take it!","","Pokemon classification","","","1"
"6150","1604160","5202634","09/22/2021 03:29:49","Feel free to use additional information for analysis.","","Data exploration","Analyze the trends in unemployment rates.","","0"
"5890","1555700","5232559","08/27/2021 12:04:42","The recommendation system has made finding the things easy that we need. Movie
recommendation systems aim at helping movie enthusiasts by suggesting what movie to watch
without having to go through the long process of choosing from a large set of movies which go up to
thousands and millions that is time-consuming and confusing.

Keywords: Movie recommendation, Rating, Genre, Recommender system, Content Based.","","Movie Recommender System Project","Content Based Recommender system with deployment","","0"
"5234","1478633","5256312","07/20/2021 06:12:47","With a large category of shoes, we can classify the type of shoes based on the dataset","","Classification of Different Types of Shoes","Classification","","3"
"5121","1467075","5256312","07/14/2021 10:45:53","Perform Vessel Tracking with the help of the 2D Ultrasound Sequences Of The Liver","","Vessel Tracking in Long Ultrasound Sequences","","","3"
"5122","1467075","5256312","07/14/2021 10:46:58","Perform Motion Monitoring of the upper abdomen utilizing the 5-10 minute sequences of the Liver of 7 healthy patients.","","Motion Monitoring in the Upper Abdomen","","","2"
"5177","1474532","5256312","07/18/2021 07:14:58","Visualize and Gather Insights from the Historical Data for Cardano Cryptocurrency","","Analyze ADA-USD Data","Visualize and Gather Insights","","1"
"5178","1474532","5256312","07/18/2021 07:15:50","Perform various Time Series methods on Historical Data of Cardano Cryptocurreny.","","Time Series Analysis of ADA-USD","Time Series Analysis","","1"
"5112","1466023","5256931","07/13/2021 21:38:27","Context
This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.

Content
The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.

Acknowledgements
Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.

Inspiration
Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not","","Pima Indians Diabetes Database","Predict the onset of diabetes based on diagnostic measures","","0"
"5371","1494629","5268791","07/27/2021 14:53:28","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Data Visualization of Olympics History

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Olympics_history_visualization","","","0"
"5805","1541302","5270083","08/19/2021 18:21:44","#### Task Details
Try to achieve more than R2-score = 85 on both training and testing.","","R2-score","","","2"
"6221","1546318","8483319","09/30/2021 22:16:43","I HAVE NO IDEA WHAT I'M DOING 


## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","TASK1213","","","52"
"6901","1546318","5270083","11/22/2021 09:51:05","## Task Details
Create an optimal number of clusters using different strategies like the elbow method or Silhouette score.

## Expected Submission
Submit your notebook if you think you have done everything properly. Also, define the type of customer which belongs to each cluster.

## Evaluation
Proper clustering of customers with proper explanation of customer present in each cluster group.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Create optimal number of cluster","","","7"
"6391","1652370","5272304","10/16/2021 14:19:03","## Task Details
Predict heart disease with the best accuracy

## Expected Submission
The solution should contain the accuracy using different ML models.

## Evaluation
By comparing the best accuracy, the evaluation will be done.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Heart Disease Prediction","","","1"
"6193","1611335","7212603","09/25/2021 20:40:19","## Task Details
This task is to find a list of shows which are now on hype. Use models and exhibit those shows.

## Expected Submission
You are asked to submit a list of shows which are hyped right now. The solution should contain details and description of shows according to their public views and rating.

## Evaluation
A good solution should predict the list of Top 10 shows which are arranged by netflix offical. The more accurate your list is with the list of netflix official, the better the solution.","","Most Engaging Shows","","","0"
"5305","1487502","5282241","07/24/2021 09:18:37","## Problem Overview
 
Deep learning has been proved to be an advanced technology for big data analysis with a large number of successful cases in image processing, speech recognition, object detection, and so on. It has also been introduced in food science and engineering, and agriculture technology. There are some research papers already published for the applications like food recognition, leaves recognition, fruits and vegetable recognition, identification of healthier food, etc.

We have provided you with more than 8000 images of more than 130 different types of fruits.

## Objective
 
You are required to build a machine learning or deep learning model that would recognize the name of a given fruit or vegetable.

## What will you learn?
 
Practical applications of Deep Learning Algorithms, optimizing neural networks, CNN, etc.","","Fruits Recognition","A Deep Learning Challenge","","1"
"6671","1666817","5396385","10/24/2021 23:25:21","## Task Details
Training an EfficientNetB0 using Tensorflow Keras, our trained model only achieved 13.42% accuracy on the training set and 11.32% on the validation set after 24 epochs of training. Yet, it produces features competitive to ImageNet on transfer tasks. 

Given the correlation between ImageNet validation set accuracy and transfer accuracy, it is of interest to see how far model optimization can improve transfer accuracy.

## Expected Submission & Evaluation
Notebooks training any model of choice on CytoImageNet. Must use the same 90-10 validation split. Any increase in validation set accuracy would be outstanding.

### Further help
If you need help getting started, feel free to explore the ***model\_pretraining.py*** script in the original github repository:
https://github.com/stan-hua/CytoImageNet/blob/master/scripts/model_pretraining.py","","Improve accuracy on validation set","","","0"
"6672","1666817","5396385","10/24/2021 23:40:41","## Task Details
Recent research from Google has pointed that sometimes more classes is not always better for transfer learning. In particular, they show that pretraining on a portion of ImageNet  (more similar to the target dataset) improves transfer accuracy. Read more [here](https://arxiv.org/pdf/1811.07056.pdf).

Given an arbitrary biological image dataset (e.g. mechanism of action classification), we would like to test if pretraining on labels from a related category (e.g. phenotype) can yield more distinctive features than features from pretraining on all labels.

## Expected Submission 
This requires the user to find a relevant biological image dataset, and choose a category label most relevant. After which, they will need to train models on labels from the relevant category, and evaluate the performance of the category-specific model and the all-category model.

**DISCLAIMER**: There is no reward for completing this task. However, accomplishment of the task will greatly improve our understanding of how to pretrain models on large-scale microscopy datasets.","","Pretrain on target-domain category labels","","","0"
"6674","1666817","5396385","10/25/2021 04:40:13","It would be of interest to explore ways to describe the relationships between classes, and to figure out which classes are most similar. Downstream application may consider filtering out redundant/overly similar classes.","","Exploratory Data Analysis","","","0"
"5262","1481244","5397373","07/21/2021 09:07:57","## Task Details
1) Predict the first dose and second dose separately when both will be completed in India.
2) Predict till this year December how many people will be vaccinated","","Predict After how should days or months vaccination will be complete in india.","","","0"
"6036","1577042","5401617","09/07/2021 22:10:41","The Aeronautics & Astronautics Abstracts dataset includes titles and abstracts of about 493 papers published by AIAA either in the journal of propulsion and power (JPP), or in the journal of thermophysics and heat transfers (JTHT) which were manually retrieved from https://arc.aiaa.org. 

The task is to build a classifier that is able to distinguish between abstracts and/or titles from each specific technical domain. The challenge lies in that both domains (propulsion, heat transfers) contains vocabulary that overlaps such as (combustion, exchange, thermal, fluid, etc‚Ä¶) which makes it harder to distinguish which journal it comes from.","","Classification","","","0"
"6912","1740668","5404173","11/23/2021 15:22:54","Predict future prices using ML and required packages.","","Predict future Prices","","","0"
"6913","1740668","5404173","11/23/2021 15:24:13","Find out what combination of Stocks would yield highest returns by assigning individual weights .","","Find out what combination of Stocks would yield highest returns","","","0"
"5613","1519890","5445078","08/08/2021 18:19:40","## Task Details
Your task is to visualize the data for your country.

## Expected Submission
No deadlines

## Evaluation
1.find how many athletes of your country wins medals throughout all the olympics.
2.find is your country host any of olympics or not.
3.find how many medals your country wins throughout all the olympics
4.find the athletes won maximum of medals throughout all the olympics.
5.more(up to you)","","Data Visualization","dataset contains three csv files of all the olympics","","0"
"5896","1556811","5449099","08/28/2021 05:01:13","A heart attack is a medical emergency. A heart attack usually occurs when a blood clot blocks blood flow to the heart. Without blood, tissue loses oxygen and dies.
Symptoms include tightness or pain in the chest, neck, back or arms, as well as fatigue, lightheadedness, abnormal heartbeat and anxiety. Women are more likely to have atypical symptoms than men.
Treatment ranges from lifestyle changes and cardiac rehabilitation to medication, stents and bypass surgery.","","Heart_Attack_Disease_Prediction","Prediction","","11"
"5272","1482221","5449099","07/21/2021 17:47:49","Fraud detection is an important aspect of banking and financial companies.its essential for both financial institution as well as their customers to be able to identify fraud quickly and accurately.objective is to build a predictive model to determine whether a given transactions will be fraudulent or not","","Fraud detection of banking and finacial company.","","","7"
"5352","1490581","5449099","07/26/2021 17:30:21","find out the accuracy of data","","Breast cancer","prediction using ada booter","","12"
"5843","1531846","5455145","08/22/2021 16:00:51","This task offers a choice of solving one of three problems. 
**1. Conduct an EDA.**
**2. Visualisation data.**
**3. Build a dashboard that displays metrics for you to choose from.**","","EDA, Data visualisation and create dashboard","","","2"
"5957","1565495","5472192","09/01/2021 09:30:08","Build A Model That Encaptures The Best Aspects Of Our Data","","Build A Robust Model","","09/30/2021 23:59:00","1"
"5958","1565495","5472192","09/01/2021 09:30:58","Make The Data Sing","","Perform EDA And Bring Out Deep Insights","","09/30/2021 23:59:00","1"
"5497","1507683","5472192","08/02/2021 08:18:19","Perform InDepth EDA. 
The Most Broad EDA Would Be Accepted.
ExtraPoints For Deep Analysis","","Perform The Most Indepth EDA","Go Deep","","1"
"5499","1507683","5472192","08/02/2021 08:19:33","Build The Most Comprehensive Model And Get The Best Out Of The Data.","","Perform Model Building","","","2"
"6224","1622735","5472192","10/01/2021 06:35:31","Perform The Best EDA In The World","","Perform EDA","","","2"
"6225","1622735","5472192","10/01/2021 06:36:22","Build A Robust Model For Prediction","","Build A Model","","","0"
"6726","1684892","5472192","10/31/2021 05:12:47","## Task Details
Build a robust model and help the company","","Build Robust Predictive Model","","","1"
"6727","1684892","5472192","10/31/2021 05:13:52","Bring Out Hidden Insights From The Data","","Perform EDA And Throw Light On The Dataset","","","0"
"6961","1684892","7212627","11/29/2021 10:25:24","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
I have created this task to submit the Project(""Tour & Travels Customer Churn Prediction"")

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

I have completed this project in R. I have
1. Uploaded the data.
2. Done EDA of the data.
3. Split the data into training and testing into 70:30.
4. then build the model , I have used Decision tree Model.
5. predict the model.
6. Check Accuracy of the model.


## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?
I have found my model to be 84% correct.
 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Tour & Travels Customer Churn Prediction","Tour & Travels Customer Churn Prediction","11/30/2021 23:59:00","0"
"5938","1564633","7949153","09/01/2021 01:28:34","## Task Details
Looking for biases, favourites in Oscars and what is the relationship between the winning movies and IMDb ratings. 

## Expected Submission
Submit a notebook to prove if there is a certain bias at Oscars

## Evaluation
Submission is better than the other?
Should have visualization","","EDA for the data","","","1"
"5979","1567469","5486204","09/02/2021 15:07:18","## Task Details
Using the above dataset find the the top 5 High value properties.

## Expected Submission
Submit your notebooks. You can try various approaches.","","Predict the High value properties in the dataset","","","0"
"5783","1538486","5491908","08/18/2021 09:15:16","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)


### Data Analysis

You can use SQL for DATA modeling or even Power BI for data analysis","","EDA and Data Analysis","Find and make projects from this data","","0"
"5431","1501344","5491908","07/30/2021 14:11:41","Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Exploratory Data Analysis","","","0"
"5334","1490577","5511173","07/25/2021 20:02:09","**This dataset has 9 columns. Please do the following:**
1.  From 1st columns (area) create two two columns: the one that has only area (ft) and another with number of bedrooms, check the corresponding numbers with add_info feature that has the number of bedrooms and bathrooms.
2. Get only those housing that are close to the beach (use Description column and address column.
3. We are looking for the apartment at which we can smoke, so get listings of those apartments that do not have smoking prohibition.
4. From coordinates feature extract geographical coordinates of the apartments. 
5. We have two cats to accommodate, please ensure that the landlord allows that.
6. Create a new feature which has the ratio of the area vs the apartments price. Extract the int (price) from the price column.
7. Get 10 apartments the lowest price/area ratio

**There are some questions to be answered in the EDA analysis:**
1. Did the area correlates with the apt's price? Expected the higher the higher
2. Can you plot coordinates of the apt's and show their prices?
3. What factors influence the price of accommodation's?","","Data Cleaning Challenge","","","0"
"5865","1537219","5513770","08/24/2021 14:55:25","## Task Details
Observe the PNG strips corresponding to a patient with glioblastoma, we can see that at least one of the four types ends up showing the tumor as either bright white against a dark brain or dark against a bright white brain. So we see stark image contrast. The question is, is this always true?

## Expected Submission
Use the dataset provided and a notebook to explore the data and give a summary statistic (count).

## Evaluation
A count of how many PNG strips which contain Glioblastoma which do NOT have the tumor showing as a contrasted shade.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Do tumours always show up as a different intensity from the brain?","Across each of the four types of MRI scans, at least one of them ends up revealing the tumor with stark contrast.","","0"
"6680","1672337","5525731","10/26/2021 12:37:49","## Task Details
To confirm hypothesis, that spread of corona virus will slow down in warm weather. 

## Expected Submission
Analyzation of proofs that adds to this hypothesis. 

## Evaluation
Will share it with community to predict further spread of corona virus. 

### Further help
For the weather data, please checkout https://www.kaggle.com/syedjaferk/indias-weather-data 
This dataset, is having a historical collection of the weather dataset for india.","","Hypothesis: During warmer temperature, rate of corona virus will be decreasing?","Can we confirm this hypothesis that spread of corona virus slow down in warm weather","","0"
"6681","1672337","5525731","10/26/2021 12:43:54","## Task Details
Visualizing the spread of the corona virus. 

## Expected Submission
1. Use folium to visualize data in map format. 
2. Use multiple plots to compare the data. 

## Evaluation
By sharing it to the community to better understand the data. 

### Further Help
For Folium Mapping  checkout this [notebook](https://www.kaggle.com/syedjaferk/covid-vaccination-india-plot-choropleth-folium)","","Data Visualization of the corona spread.","Visualizing the data to get a clear understanding of the spread of the virus.","","0"
"6682","1672337","5525731","10/26/2021 12:50:00","## Task Details
Use the RTPCR test data and analyse how the data is helpful in controlling the covid spread.","","Analyze how RTPCR Tests helps in controlling the spread of corona virus.","Correlation between RTPCR with the spread of coronavirus.","","0"
"6683","1672337","5525731","10/26/2021 12:53:50","## Task Details
Correlation of the lockdown data with the spread of corona virus for a particular district","","How lockdown was helpful in controlling the spread of virus.","Correlate lockdown data and see how it was helpful in controlling the spread of virus.","","0"
"6418","1660440","5525731","10/20/2021 13:14:04","Infosys was founded by seven engineers in Pune, Maharashtra, India with an initial capital of $250 in 1981. It was registered as Infosys Consultants Private Limited on 2 July 1981. In 1983, it relocated its office to Bangalore, Karnataka, India.

The company changed its name to Infosys Technologies Private Limited in April 1992 and to Infosys Technologies Limited when it became a public limited company in June 1992. It was later renamed to Infosys Limited in June 2011.

An initial public offering (IPO) was floated in February 1993 with an offer price of ‚Çπ95 (equivalent to ‚Çπ580 or US$7.80 in 2020) per share against a book value of ‚Çπ20 (equivalent to ‚Çπ120 or US$1.60 in 2020) per share. The IPO was undersubscribed but it was ""bailed out"" by US investment bank Morgan Stanley, which picked up a 13% equity stake at the offer price. Its shares were listed in June 1993 with trading opening at ‚Çπ145 (equivalent to ‚Çπ890 or US$12 in 2020) per share.

Infosys shares were listed on the Nasdaq stock exchange in 1999 as American depositary receipts. It became the first Indian company to be listed on Nasdaq. The share price surged to ‚Çπ8,100 (equivalent to ‚Çπ30,000 or US$390 in 2020) by 1999 making it the costliest share on the market at the time. At that time, Infosys was among the 20 biggest companies by market capitalization on the Nasdaq. The ADR listing was shifted from Nasdaq to NYSE Euronext to give European investors better access to the company's shares.

On 28 July 2010, the then British Prime Minister David Cameron visited Infosys HQ in Bangalore and addressed Infosys employees. Infosys, Bangalore, Its annual revenue reached US$100 million in 1999, US$1 billion in 2004 and US$10 billion in 2017.

In 2012, Infosys announced a new office in Milwaukee, Wisconsin, to serve Harley-Davidson, being the 18th international office in the United States. Infosys hired 1,200 United States employees in 2011, and expanded the workforce by an additional 2,000 employees in 2012. In April 2018 Infosys announced expanding in Indianapolis, Indiana. The development will include more than 120 acres and is expected to result in 3,000 new jobs‚Äî1,000 more than previously announced.

In July 2014, Infosys started a product subsidiary called EdgeVerve Systems, focusing on enterprise software products for business operations, customer service, procurement and commerce network domains. In August 2015, the Finacle Global Banking Solutions assets were officially transferred from Infosys and became part of the product company EdgeVerve Systems product portfolio.


Task : 

Visualize
Analyse
Predict","","Visualize | Analyse | Predict.","","","0"
"6266","1631356","5532900","10/06/2021 08:14:07","# Overview
Data scientist is the sexiest job in the world. How many times have you heard that? Analytics India Annual Salary Study which aims to understand a wide range of trends in data science says that the median analytics salary in India for the year 2017 is INR 12.7 Lakhs across all experience levels and skill set.

So given the job description and other key information can you predict the range of salary of the job posting? What kind of factors influence the salary of a data scientist? The study also says that in the world of analytics, Mumbai is the highest paymaster at almost 13.3 Lakhs per annum, followed by Bengaluru at 12.5 Lakhs. The industry of the data scientist can also influence the salary. The Telecom industry pays the highest median salaries to its analytics professionals at 18.6 Lakhs. What are you waiting for, solve the problem by predicting how much a data scientist or analytics professional will be paid by analysing the data given?

Bonus Tip: You can analyse the data and get key insights for your career as well. Data The dataset is based on salary and job postings in India across the internet. The train and the test data consists of attributes mentioned below.

The rows of train dataset has a rich amount of information regarding the job posting such as the name of the designation and key skills required for the job. The training data and test data comprise 19802 samples and 6601 samples each.

This is a dataset that has been collected over some time to gather relevant analytics jobs posting over the years. Features Name of the company (Encoded) Years of experience Job description Job designation Job Type Key skills Location Salary in Rupees Lakhs(To be predicted) Problem Statement Based on the given attributes and salary information, build a robust machine learning model that predicts the salary range of the salary p post.","","Predict The Data Scientists Salary In India","Predict The Data Scientists Job Salary In India Region","","8"
"6041","1577596","5532900","09/08/2021 07:12:16","## Task Details
Data is uncleaned using the basic skills to clean data and make new columns accordingly. EDA of the dataset is also good for many insights to see.

## Expected Submission
Use Dataset to clean and do basic EDA and find the following answers from the data

1. A company has the highest number of good rating ratios.
2. Price range having the highest number of good ratings.
3. Discount and rating, price relation. and so on...

![Amazon top rated smartphones 2021 image](https://bgr.com/wp-content/uploads/2020/09/amazon-sign-popular-gadgets.jpg?quality=70&strip=all&w=720&h=405&crop=1)","","Top Rated Smartphones & Accessories (Data Cleaning & EDA)","Amazon Top Rated Smartphones & Accessories 2021 (Data Cleaning & EDA)","","4"
"6091","1591616","5532900","09/15/2021 16:48:18","# Overview
Data visualization helps in presenting billions of data points into meaningful insights. It is a very important tool for all data scientists to learn and understand the data in more detail. Visualization also helps business leaders to take important decisions for company growth and to move in the right direction.

We often order food from food delivery apps like Zomato, Swiggy. Sometimes it often takes time to deliver the food. There are factors that impact the delivery time of the food.

In this, we are going to work on a data visualization challenge to analyse and visualize the food delivery time from the same location but different cuisines and ratings of the restaurants.

Data:
11095 rows x 9 columns

## Columns:
* Restaurants (unique ID of restaurants)
* Location
* Cuisines
* Average cost
* Minimum order
* Rating
* Votes
* Reviews
* Delivery time

## Skills:
* Power BI
* Tableau
* Visualization

# What to do with this dataset.
* Exploratory Data Analysis (EDA)","","Food Delivery Time Exploratory Data Analysis","Data Analysis on Food Delivery Time for Different Cuisines","","5"
"5897","1557299","5579261","08/28/2021 10:15:56","**Do Exploratory Data Analysis**","","Perform E.D.A","","","0"
"6014","1572619","5588522","09/05/2021 17:02:23","## Task Details
Predict car prices.

## Expected Submission
A good submission is one that has the best result on the test set.","","Predict car prices","","","0"
"6080","1588514","5601216","09/14/2021 08:56:54","Users have to classify between healthy voices and pathological voices.","","Classify between healthy and pathological voices","","","0"
"5883","1553740","5615506","08/26/2021 11:42:39","# Data Cleaning
-  Clean the raw data that contain  so many duplicates.
-  Set the Data in formate to perform EDA.

#EDA
- Get tinsight into Industries and stipeds  
- See which language is popular
- which technology is on demand.","","Data Cleaning and EDA","Clean raw collected data and perform EDA","","0"
"6983","1759892","5634573","12/01/2021 09:16:00","## Task Details
This task is designed specifically for students that are starting to learn about Machine Learning. 

## Expected Submission
The solution should contain images with accurate bounding box detecting the Moon.

## Evaluation
The evaluation matrix would be IoU (Intersection over Union)

### Further help
Please contact at team@spartificial for further help. Or visit spartificial.com for more projects and workshops related to AI and Space Explorations","","Moon Detection","Moon images for AI in Space Exploration Workshops by Spartificial","","0"
"5294","1485329","5663726","07/23/2021 16:31:56","## Task Details
Exist a spatiotemporal trend. Is interesting to see what is the evolution of satisfaction in customers in each district!

## Expected Submission
A notebook with its respective github repository showing the insights of dashboard. You can use R shiny, Dash Plotly, Visual Studio or maybe, a more complex tool.

## Evaluation
Here all submissions have 100/100. This type of works are always well received üòä

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/tavoosi/suicide-data-full-interactive-dashboard
- https://www.kaggle.com/khsamaha/reddit-vaccine-myths-eda-and-text-analysis","","Create an interactive dashboard","What restaurants should you visit?","","1"
"5363","1485329","5663726","07/27/2021 01:23:52","## Task Details
Exist spatial, temporal and text features. Use your skills and imagination to find interesting patterns in a EDA.
## Expected Submission
A notebook with its respective notebook showing diverse results. You can analyze the restaurants, the reviews or the districts.

## Evaluation
Here all submissions have 100/100. This type of works are always well received üòä

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/lazaro97/the-last-of-us-ii-a-sentiment-analysis
- https://www.kaggle.com/khsamaha/reddit-vaccine-myths-eda-and-text-analysis","","Do a Exploratory Data Analysis","Spatial, Temporal and Text Features!","","1"
"5232","1478271","5663726","07/20/2021 04:28:31","You can use R Shiny or Dash Plotly.","","Create a dashboard","","","0"
"6283","1636262","5744935","10/08/2021 12:48:08","## Task Details
Predicting the calculated value (y column) of the input data (x column). The prediction should be precise to three decimal places with test size = 0.4 or lesser. Apply mathematical models to fit the data (preferred). The objective of this is to obtain the mathematical model which fits the data precisely to three decimal places (Each row in the X column should produce a predicted Y precise to the corresponding value in Y column). Models like polynomial fitting, taylors expansion of functions, fourier series with suitable degree or any other combination of mathematical functions.","","Predict Y column precisely to three decimal places with X column as input","Use mathematical models to fit the data","","0"
"6214","1595322","5749915","09/29/2021 11:43:34","Given a statement, simply classify it into the given 11 categories. 

One good way to approach this is to vectorize the text, and input it into an embedding layer followed by a classifier. This gives more dimensionality into each word for better classification performance.","","Classify a statement according to its context","","","0"
"6273","1633508","5769741","10/07/2021 07:58:45","## Task Details
Design Cross-lingual speech emotion recognition deep learning model

## Expected Submission
Comparison of various audio set with and without Urdu Dataset

## Results
The data of multiple languages are used for training, results for emotion detection is increased even for URDU dataset, which is highly dissimilar from other databases. Also, accuracy boosted when a small fraction of testing data is included in the training of the model with single corpus. These findings would be very helpful for designing a robust emotion recognition systems even for the languages having limited or no dataset. [Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages](https://arxiv.org/pdf/1812.10411.pdf)","","Design Cross-lingual speech emotion recognition","","","0"
"5333","1490531","5769741","07/25/2021 19:35:15","## Task Details
Come up with new insight on engineers' data.

## Expected Submission
Notebook containing Visualization and explanation of what you discovered during analysis. 

## Evaluation
The best insight or unique insight will get extra points. 

### Further help
[Deepnote Engineering Data Analysis Pakistan](https://deepnote.com/project/Engineering-Data-Analysis-fn8aALtMQwaElv_AUuwssg/%2Fnotebook.ipynb)","","Engineering Data Analysis","Come up with new insight on engineers data.","11/18/2021 23:59:00","1"
"5988","1569312","5769741","09/03/2021 13:03:26","## Task Details
Calculate annual energy produced by recycling using all the datasets available.

## Expected Submission
The final version of the notebook should be without mistakes in both calculations and explanations.

## Evaluation
I have a solution to these tasks and I will compare the results.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/kingabzpro/singapore-recycling-and-waste-management?scriptVersionId=73890002
- https://dagshub.com/kingabzpro/Annual-Recycled-Energy-Saved-in-Singapore
- https://deepnote.com/project/Singapore-Recycled-Energy-Xh_wbptwTn2l-I1GQ-O1Vw/%2FAnnual-Recycled-Energy-Saved-in-Singapore%2Fnotebook.ipynb","","Exploratory Data Analysis","calculate annual energy produced by recycling","","4"
"5989","1569312","5769741","09/03/2021 13:08:38","## Task Details
Used machine learning model to predict future trend of recycling in Singapore.

## Expected Submission
show next 5 year progress of recycling

## Evaluation
Good documentation and completed model training cycle.","","Predict next 5-year trend of waste recycling","Used machine learning model to predict future trend of recycling in Singapore","","3"
"6829","1704388","5792998","11/12/2021 07:58:47","**Task Details
**
Many countries in the world are facing repeated corona waves.
In many countries, there is a public and scientific debate about the necessity and effectiveness of vaccines.
A robust model that can quantify and measure the effectiveness of Corona vaccines in preventing severe morbidity and mortality can help persuade skeptics and policy leaders and save many lives.
A good model is a model that knows how to take into account the different age groups in the effectiveness of the various vaccine doses and normalize the data. Population size since most of the population in Israel is vaccinated.
It is also recommended to use the following dataset https://www.kaggle.com/yvtsanlevy/israel-coronavirus-dataset which gives a more general picture of the Covi19 epidemic in Israel and contains data from March 2020.

**Expected Submission
**
Submit a notebook that implements the full lifecycle of data preparation, model creation, and Evaluation. Feel free to use this dataset plus any other data you have available. Since this is not a formal competition, you're not submitting a single submission file, but rather your whole approach to building a model.

**Evaluation**
This is not a formal competition, so we won't measure the results strictly against a given validation set using a strict metric. Rather, what we'd like to see is a well-defined process to build a model that can deliver decent results (evaluated by yourself).","","Vaccine efficacy","Proof of vaccine efficacy, with an emphasis on the third dose to prevent severe morbidity and mortality from corona","","1"
"5453","1504330","5796277","07/31/2021 18:39:11","Breast cancer is cancer that forms in the cells of the breasts.

After skin cancer, breast cancer is the most common cancer diagnosed in women in the United States. Breast cancer can occur in both men and women, but it's far more common in women.

Substantial support for breast cancer awareness and research funding has helped created advances in the diagnosis and treatment of breast cancer. Breast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.","","Breast_cancer_analysis","","","5"
"6308","1640669","5824337","10/11/2021 06:41:35","I hope this task will be easy?!?!?!","","Do EDA","","02/05/2022 23:59:00","0"
"5429","1501165","5870200","07/30/2021 13:18:42","Predict his future career.
„ÉªCan he be a Allstar someday?","","Predict his future career","","","0"
"5271","1480455","5879992","07/21/2021 16:44:11","## Task Details
Bob and Drew have seen thousands of spins, but on average which host witnessed the wheel traverse the most amount of spaces.

## Expected Submission
This submission should output which host saw bigger spins.

## Evaluation
You should specify how you calculated a ""big"" spin.

### Extra Credit
Which host failed to motivate their contestants enough to get the Big Wheel to revolve even once.","","Bob v. Drew","Spin differences between hosts","","0"
"6241","1625286","5879992","10/02/2021 20:50:58","## Task Details
There are 24 envelopes in the Bonus Round wheel, but which of them has been landed on the most?

## Expected Submission
This submission should output which envelope has been landed on the most.

## Evaluation
You should specify how you differentiated between the envelopes that have the same marking above them.

### Extra Credit
Are there any envelopes that historically have contained large prizes?","","Most Frequent Envelope","Which envelope has been landed on the most","","0"
"6215","1618348","5879992","09/29/2021 18:35:46","## Task Details
Every case gets an amount of money in it for every game, but in total which case has contained the most money overall?

## Expected Submission
This submission should output which case has held the most money in total.

## Evaluation
You should specify how you calculated the money total for the cases.

### Extra Credit
Which case has at the top prize in it the most number of times?","","Most Valuable Case","Case that has had the most money in it","","0"
"6244","1625542","5879992","10/03/2021 03:39:12","## Task Details
There are 10 possible balls that can be chosen (0-9), but which one has shown up most frequently?

## Expected Submission
The submission should output which ball has shown up the most.

## Evaluation
You should specify how you calculated the frequency counts.

### Extra Credit
Which combination of balls have shown up the most?","","Most Frequent Ball","Which ball shows up the most often","","0"
"6340","1647242","5890725","10/14/2021 09:14:16","The objective is to address a hypothetical business problem for a Flipkart Authorized Seller. According to the hypothesis the individual is looking to sell mobile phones on Flipkart. For this, the individual is looking for the best product, brand, specification and deals that can generate the most revenue with the least amount of investment and budget constraints.

Questions to be answered:

Whether he should sell product for a particular brand only or try to focus on model from different brands?
Using EDA and Data Visualization find out insights and relation between different features
Perform detailed analysis of each brand.
Assuming a budget for the problem come to a solution with maximum return.","","Detail Analysis","Obtain the optimum products to maximize the return","","1"
"6871","1709573","8927142","11/18/2021 14:39:53","Using R to explain the dataset.
Instructions:
1. Import data:
2. Data cleaning: NA (Not available), missing
3. Data visualization
(a) Transformation
(b) Descriptive statistics for each of the variables
(c) Graphs: hist, boxplot, pairs.
4. Hypothesis testing : t.test, z.test, Anova, Chi-squared test.
5. Fitting linear regression models

[Link for Dataset](https://drive.google.com/file/d/1rCqSl0QsS9oqLsxupjNF1SITne3BldVU/view?usp=drivesdk)","","Project of Probality and Statistic of Transportation engineering","","11/21/2021 23:59:00","1"
"5868","1548204","5900775","08/24/2021 23:32:13","## Task Details
Everyone knows the headache of a flight delay - but can we know if a flight WILL be delayed? An airline will tell you it's due to the weather, the airport, etc and not under their control. If this is the case, then flight delays should be reliably predicted using only measurable, external factors. If flight delays cannot be predicted from this information, is it possible that the error lies within the carrier's control?

## Expected Submission
Submit a notebook with a predictive model that predicts flight delays. Can flight delays be predicted from the features in this dataset, or is more data needed?

## Evaluation
A valuable solution will minimize false negatives and false positives. Focus on maximizing F1 over Accuracy.","","Predict Airline Departure Delays","Can airline delays be successfully predicted?","","4"
"5869","1548204","5900775","08/24/2021 23:35:39","## Task Details
15-20% of flights are delayed each year, and it is useful to explore why. Is it carrier related, airport related, weather related, or something else? 

## Expected Submission
Submit a notebook with an EDA and Visualizations exploring the features that contribute to a flight delay.

## Evaluation
Good data cleaning, smart feature engineering, and rich storytelling with descriptive visuals","","Explore/Visualize Reasons for Flight Delays","What can we learn from the data about flight delays?","","1"
"5756","1532133","7940796","08/16/2021 07:24:54","## Task Details

As we seen in the given Datasets, the severity of the Anaemia disease is not able to describe in words.
so, why don't we use our visualization techniques upon this dataset and clearly analyze the data.

## Expected Submission

Your Submission must clearly understandable to the non-technical user of this dataset.

## Evaluation

Nothing is better than the good visualisations on any dataset. So Make sure of them.

### Further help

If you need additional inspiration, check out the official website of WHO or you can also comment here for futher help.","","Exploratory Data Analysis","","","0"
"6915","1737896","5906164","11/23/2021 17:17:53","This dataset contains 100 normal Chest CT and 100 abnormal(COVID (+) patient) Chest CT
- Each person's CT is saved as nifti file, the tensor shape is (512,512,N)
- N is the # of slides: it varies

- Build a 3 dimensional CNN model that reads the whole slides of CT per person and tells whether particular CT represents COVID-positive case or not

- Ï†ïÏÉÅ, ÏΩîÎ°úÎÇò Í∞êÏóºÌôòÏûê 100Î™ÖÏî© Ï¥ù 200Î™ÖÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏùå
- Î™®Îì† ÌôòÏûêÏùò CTÎäî nii ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏñ¥ ÏûàÏúºÎ©∞, CT imageÎäî Í∞ÄÎ°ú ÏÑ∏Î°ú Î™®Îëê 512 ÌîΩÏÖÄ, Ìïú ÏÇ¨ÎûåÏùò Ïù¥ÎØ∏ÏßÄ Í∞ØÏàòÎäî ÏÇ¨ÎûåÏùò ÌùâÎ∂Ä Í∏∏Ïù¥Ïóê Îî∞Îùº Îã§Î¶Ñ
- Í∏∞Ï°¥ 2Ï∞®Ïõê CNNÎ™®Îç∏Í≥º Îã¨Î¶¨, Ïù¥ÎØ∏ÏßÄÏùò Í∞ÄÎ°ú,ÏÑ∏Î°ú Í∑∏Î¶¨Í≥† ÍπäÏù¥(anatomical positionÏúºÎ°ú Ïù¥ÏïºÍ∏∞ÌïòÏûêÎ©¥, superior-inferior)  Ï†ïÎ≥¥Î•º Î™®Îëê Í≥†Î†§ÌïòÏó¨ ÌïôÏäµÌïòÎäî 3Ï∞®Ïõê CNN Î™®Îç∏ÏùÑ ÎßåÎìúÎäî Í≤ÉÏù¥ Î™©Ï†Å","","COVID classification with 3D CNN model","","","0"
"5205","1473204","5916181","07/19/2021 04:59:09","# Task Details
You are given a dataset that contains medical information of children who have genetic disorders. 
Predict the following:
- Genetic disorder 
- Disorder subclass


# Expected Submission
- The index is the Patient Id column. 
- The targets are the Genetic Disorder and Disorder Subclass columns. 
- The submission file must be submitted in .csv format only.
- The size of this submission file must be 9465 x 3.
- **Submit your solution** [here](https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-genetic-testing/machine-learning/predict-the-genetic-disorders-9-76826a5e/)

# Evaluation
## Genetic Disorder
score1 = max(0, 100*metrics.f1_score(actual[""Genetic Disorder""], predicted[""Genetic
Disorder""], average=""macro""))
## Disorder Subclass
score2 = max(0, 100*metrics.f1_score(actual[""Disorder Subclass""], predicted[""Disorder
Subclass""], average=""macro""))
## Final score
score = (score1/2)+(score2/2)","","Instructions","","","0"
"6079","1588333","5926689","09/14/2021 07:13:40","## Task Details
Develop a price mechanism that explains the factors to how Udemy prices its courses

## Expected Submission
Notebooks containing analysis and model development, together with the evaluation of model. 

## Evaluation
Root Mean Squared Error (RMSE) of log(Observed Price) and log(Actual Price)

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Price Mechanism","","12/31/2021 23:59:00","0"
"5870","1550674","5934846","08/25/2021 02:38:08","Perform EDA describe about data and statistics of data in detail for each columns","","Data Visualization","perform EDA","","0"
"5815","1542755","5934846","08/20/2021 12:35:39","CREATE THE INDIVIDUAL DATA VISUALIZATION FOR EACH PLAYER","","EDA  DATA","INDIVIDUAL DATA","","0"
"5797","1539957","5934846","08/19/2021 03:23:31","The dataset contain various information create data visualization","","USING EDA  UNDERSTAND DATA","DATA VISUALIZATION","","0"
"5712","1528438","5945323","08/13/2021 12:05:24","make a machine learning model predict if the positive COVID 19 cases will be diseased or dicharged acoording to medical history","","classification","predicting","","0"
"5200","1475681","6043987","07/18/2021 18:23:13","# Task Details
Perform ‚ÄòExploratory Data Analysis‚Äô on dataset ‚ÄòSampleSuperstore‚Äô
‚óè As a business manager, try to find out the weak areas where you can
work to make more profit.
‚óè What are all business problems you can derive by exploring the data?","","Data Analysis","","","0"
"5188","1475020","6043987","07/18/2021 11:56:50","## Task Details
Variety of cars with different features such as model, production year, category, brand, fuel type, engine volume, mileage, cylinders, colour, airbags and many more, you have to predict the price of the car

## Dataset
Train.csv - 19237  x 18 
Attributes:
ID
Price: Price of the care(Target Column)
Levy
Manufacturer
Model
Prod. year
Category
Leather interior
Fuel type
Engine volume
Mileage
Cylinders
Gearbox type
Drive wheels
Doors
Wheel
Color
Airbags
Test.csv - 8245  x 17 

## Skills - 
Regression, overfitting, uderfitting","","Predicting the car prices","","","0"
"7030","1779246","6068802","12/08/2021 15:54:52","1. EDA: Gather insights from the data to understand what is driving the high customer churn rate.

2. Classification Model: Develop a Machine Learning model that can accurately predict the customers that are more likely to churn, prioritizing the ones with a higher recall.

3. Interpretation Techniques: Prescribe customized actions that could be taken by the company in order to retain each customer predicted by the model.","","EDA, Prediction and Prescription","","","0"
"6988","1761137","6096594","12/01/2021 18:30:26","Find insights from data","","Exploratory Data Analysis","","","0"
"6841","1720169","6096594","11/15/2021 06:59:02","Find insights from data","","Exploratory Data Analysis","","","0"
"6736","1687315","6096594","11/01/2021 09:24:35","Find insights from data","","Exploratory Data Analysis","","","2"
"6320","1642820","6096594","10/12/2021 07:16:54","Find insights from data using EDA","","Exploratory Data Analysis","","","0"
"6322","1642862","6096594","10/12/2021 09:14:02","Find insights from data using EDA","","Exploratory Data Analysis","","","0"
"6309","1640673","6096594","10/11/2021 06:52:12","Find insights from data","","Exploratory Data Analysis","","","0"
"6289","1637802","6096594","10/09/2021 10:52:35","Find insights from the data using EDA","","Exploratory Data Analysis","","","0"
"6176","1607646","6144275","09/23/2021 22:04:14","EDA on best typists, texts and months","","EDA on best typists, texts and months","EDA on best typists, texts and months","","1"
"6115","1594198","6151869","09/17/2021 03:22:58","Analysis and Visualize Unemployment before COVID and after COVID","","Try to visualize the impact of COVID on unemployment rate","","","0"
"6121","1596296","6151869","09/18/2021 01:30:10","Apply your analysis techniques and predict which team will win","","Predict which team is well deserved to win","","","1"
"5758","1532673","4656934","08/16/2021 08:45:08","Perform EDA and Time Series Analysis on Tesla Stocks.","","Tesla Stock Analysis.","","","1"
"5765","1534014","6151869","08/17/2021 01:59:11","Try to find some unknown insights about Indian cricketers by applying data exploration and data visualization.
You can also use ML algorithms to do the same task.
All the best.","","Find unknown insights about Indian Cricketers","","","1"
"6003","1534014","7526686","09/04/2021 11:19:36","I have seen a lot of datasets of international games. But it is not seen in domestic cricket. So if you can, try to create a dataset of domestic cricket in India. I have all data's but don't know to extract or scrap them. Thank you.","","Domestic","","","0"
"5125","1467287","6155856","07/14/2021 15:05:01","Remove outliers before creating the train and test datasets .","","Remove Outliers","","","0"
"5126","1467287","6155856","07/14/2021 15:06:17","Create train and test datasets each one containing equal amount of signal and background events .","","Train Test Split","","","0"
"6122","1572315","5532900","09/18/2021 10:01:08","**EDA on Countries participated, disciplines, events & coaches.**","","2020 Tokyo Paralympics Exploratory Data Analysis (EDA)","","","1"
"6073","1585527","6162421","09/12/2021 20:32:05","1. Perform EDA on Country-wise, Gender-wise trends of Paralympics over the years.
2. Which aspects of the data has improved or deteriorated.","","Exploratory Data Analysis","","","0"
"6081","1588730","6166660","09/14/2021 10:12:28","## Task Details
You need to identify the top 10 countries with the best recovery rate where the recovery rate is a 7 days decline of total active cases.

## Your Tasks
-- For each of these 10 countries provides the following information as of today.
-peak_date: the date when peak happened (maximum number of active cases)
-peak_active: number of active cases at the peak
-cases: number of total reported cases (as of today)
-active: number of active cases (as of today)
-recovered: number of patients recovered (as of today)
-deaths: number of patients died (as of today)
-gross_adds: number of reported cases from the day before","","Data Management & Variable Engineering","","","0"
"6210","1617278","6175354","09/28/2021 13:56:18","## Task Details
why some states have more companies than others?  Is there a correlation between the GDP and the Size of a company? Does the number of employees influence the Market_Cap?

why some states have more companies than others?  Is there a correlation between the GDP and the Size of a company? Does the number of employees influence the Market_Cap?

those are only some of the questions that could be asked, play with the data to find something that you wouldn't expect.

PS: I will upload a bigger database in the future","","Visualize and Find Correlation","","","1"
"6414","1660065","6205612","10/20/2021 12:21:15","## Task Details

Create a resultant CSV/Excel file with the following columns:
- Order ID
- AWB Number
- Total weight as per ABC (KG)
- Weight slab as per ABC (KG)
- Total weight as per Courier Company (KG)
- Weight slab charged by Courier Company (KG)
- Delivery Zone as per ABC
- Delivery Zone charged by Courier Company
- Expected Charge as per ABC(Rs.)
- Charges Billed by Courier Company (Rs.)
- Difference Between Expected Charges and Billed Charges (Rs.)

This task is created to utilize your EDA skills to Fullest.
## Expected Submission
 
Kaggle Notebooks which generate the resultant CSV/excel file either using Python /SQL/R

Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of provided data.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought-provoking, and fresh all at the same time.

Documentation - Are your code and notebook, and additional data sources well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","","Create Resultant CSV/Excel File","","","1"
"6430","1660065","6205612","10/21/2021 13:23:20","## Task Details
Create Resultant Summary Table as shown in the expected result summary sheet.(provided in dataset)

## Expected Submission
A notebook that accomplishes the task.
## Evaluation
Composition - Is there a clear narrative thread to the story that‚Äôs articulated and supported by data? The subject should be well defined, well researched, and well supported through the use of provided data.

Originality - Does the reader learn something new through this submission? Or is the reader challenged to think about something in a new way? A great entry will be informative, thought-provoking##  easy to follow and the process is reproducible.","","Create Order summary Table/CSV/Excel","","","1"
"5752","1532161","6245640","08/15/2021 20:11:06","## Task Details
An organization wants to predict who possible defaulters are for the consumer loans product. They have data about historic customer behavior based on what they have observed. Hence when they acquire new customers they want to predict who is riskier and who is not.

## What do you have to do?
You are required to use the training dataset to identify patterns that predict **‚Äúpotential‚Äù defaulters**.

## Expected Submission
Submissions should be made in the same format as the **Sample Notebook** provided. Train/Test split should be 80% for training & 20% for testing.

## Evaluation
Submissions will be evaluated on the basis of **`roc_auc_score`** on 20% of **train_dataset**.

##NOTE
The **`test_dataset`** was just a part of the Hackathon, the notebook you will be submitting should only train & test on **`train_data`** and predict a higher score.","","Classify Potential Loan Defaulters","Predict possible defaulters for the consumer loans product based on the customer historic behavior.","","11"
"5639","1505208","6245640","08/09/2021 16:58:23","## Context
Amazon catalog consists of billions of products that belong to thousands of browse nodes (each browse node represents a collection of items for sale). Browse nodes are used to help customers navigate through our website and classify products to product type groups. Hence, it is important to predict the node assignment at the time of listing of the product or when the browse node information is absent.

## Task Details
In this task, you will use the given dataset to **classify products into browse nodes**. You will have access to the product title, description, bullet points, etc. and labels for ~3MM products to train and test your submissions. 
**Note:** There is some noise in the data - real world data looks like this!!

## Expected Submission
For each `PRODUCT_ID` in the test data set, you are required to provide a browse node id prediction. The submission file should be a csv and contain a header followed by pairs of `PRODUCT_ID`, `BROWSE_NODE_ID`.

## Evaluation
This task uses **Accuracy** as the evaluation metric to measure submissions quality.

### Further help
In case you are using pandas to read the csv train and test datasets, use `escapechar = ""\\""` and `quoting = csv.QUOTE_NONE` options with read_csv to avoid errors during import.","","Product Browse Node Classification","A multiclass classification problem","","1"
"6797","1705707","6264560","11/09/2021 17:13:10","**Data Description:**
The data provided in the training set contains a set of images that act as a puzzle and can fit together in pairs. Please find below an example of images that would fit together.
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6264560%2Fe0a6d62f3a260182f2b94a773a33d01e%2F1.png?generation=1636477783644048&alt=media)
![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F6264560%2F972bfe412be2dbe44145663e83d9e2ac%2F2.png?generation=1636477812275100&alt=media)

**Note: The above 2 links provided contains to sample pictures which make a couple and fit together. **

The **solution.csv** file provided in the same directory contains two attributes: id, and pair. Both attributes represent file names in the training set. From the given comparison we can infer that image 1 is a pair of image 686 and they fit together like a puzzle. Same is the case for the entire set of attributes. Please note the whole set of tuples only have one to one mapping.

**Objective of the Problem:**
To find the pairs in the test data and write the same to a CSV file. The solution file would contain two attributes, one would be id and the second would be pair. Please write pairing file names in a CSV file and upload the same for getting a score. Please view the sample submission file as an example of how the solution is to be written.

**Evaluation Metric:**
The evaluation metric for this problem would be precision-based accuracy. The accuracy score would be normalized to 100. Accuracy is going to be calculated between the absolute correct submission and the submission made on the platform.","","Find Pairs that make a complete figure","","","0"
"6768","1698802","6279218","11/06/2021 14:39:15","Create EDA.","","Create EDA for this dataset","download the zip folder to see all files","","0"
"6997","1764856","6279218","12/03/2021 07:21:57","# Volcanic EDA for this dataset is must","","EDA is important part of life","Volcanic EDA for this dataset is must","","0"
"6981","1759565","6279218","12/01/2021 06:58:54","Analyze the changes in people's transportation behavior after the pandemic.","","Changes before ,in and after the pandemic","Analyze the changes in people's transportation behaviour after pandemic.","","0"
"6908","1739933","6335146","11/23/2021 10:36:20","This dataset can help you create a basic data visualization project for practice.

Your notebook may contain:- 
1. Top rated movies
2. Top grossing movies
3. Most voted movies on IMDB
4. Most liked on Facebook

The notebook can use basic python tools like Pandas, NumPy and Matplotlib to read and visualize these data onto graphical representation.","","Data Visualization for Beginners","","","3"
"6354","1641723","6356842","10/14/2021 12:44:40","## Task Details
Once I found the way to import tons of data about Curry and every other player, we can start using these data to build a machine learning or deep learning model (your choice).

## Expected Submission
You can do it with your own ideas of input and output, i dont want something specific, you can submit whatever you achieved, it can be just  a plot or lines of code.
I'll give a little suggestion: Build a model (ML or DL) to find Curry's stats that are more correlated to W/L.

## Evaluation
Model accuracy or error should be high enough to be confident with results. But again feel free to build your model and submit it.","","Curry model","Build a model on Curry's data","","0"
"5732","1530026","6359034","08/14/2021 12:14:53","## Task Details
Hi Kagglers. Your task is easy! You should train a model which cleans the ECG signals in this Dataset.

## Expected Submission
The submission consists of Notebooks as well as results in CSV format.

## Evaluation
I evaluate it with the correlation between submissions and cleaned signals

### Further help
You can reach me via: Ashkanshahbazi97@gmail.com","","Filtering ECG signals using a machine learning model","Can you provide a supervised method for cleaning ECG signal?","10/14/2021 23:59:00","0"
"6784","1702958","6369059","11/08/2021 14:46:20","Find insights from data using EDA  and Submit it.","","Exploratory Data Analysis","","","1"
"6792","1705292","6369059","11/09/2021 13:55:26","PREDICT FUTURE GOLD RATE","","PERFORM EDA","","","0"
"6796","1705472","6369059","11/09/2021 15:05:05","PREDICT FUTURE POPULATION OF INDIA","","PREDICTION USING EDA","","","0"
"6785","1702044","6374616","11/08/2021 15:32:03","Conduct an EDA to find the insights from the dataset.","","Exploratory Data Analysis","","","1"
"6999","1765424","6374616","12/03/2021 14:39:41","Conduct an EDA to find insights from the dataset.","","Exploratory Data Analysis","","","8"
"5446","1504044","6402617","07/31/2021 16:20:37","## Task Details
Every task has a story. Tell users what this task is all about and why you created","","Perform Exploratory Data Analysis","","","0"
"5502","1508604","6402661","08/02/2021 15:40:08","## Task Details
Our top priority in this health problem is to identify the stage of liver cirrhosis.

## Expected Submission
Solve the task primarily using Notebooks.
The solution should contain two columns:
1) ID
2) Stage

## Evaluation
Evaluation using AUC

### Acknowledgements
If you use this dataset in your research, please credit the author.","","Predict Cirrhosis","Classificastion task","","5"
"5307","1488071","6402661","07/24/2021 15:35:11","## Task Details
Implement a classifier using PyTorch and GPUs for QMNIST

## Expected Submission
Submit your Notebook.

## Evaluation
Obtain at least 0.90 accuracy.","","Implement a classifier using PyTorch and GPUs","Implement a classifier for handwritten numbers using PyTorch and GPUs","","1"
"6055","1582403","6402661","09/10/2021 18:46:46","## Task Details
Create a model to assess the likelihood of a possible heart disease event.
This can be used to help hospitals in assessing the severity of patients with cardiovascular diseases.","","Predict Heart Failure","Classificastion task","","101"
"7017","1770764","6402661","12/05/2021 20:23:11","## Task Details
The task is to create a strong model to estimate the excitation current of the synchronous machine.


## Expected Submission
Notebook","","Predict Excitation Current","Predict the exctitation current of the synchronous machine","","1"
"6314","1641609","6402661","10/11/2021 15:12:58","## Task Details
Implement a classifier using PyTorch and GPUs for QMNIST

## Expected Submission
Submit your Notebook.

## Evaluation
Obtain at least 0.90 accuracy.","","Implement a classifier using PyTorch and GPUs","Implement a classifier for LaTeX and math symbols using PyTorch and GPUs","","0"
"6843","1720050","6405452","11/15/2021 07:56:23","Create a recommendation system for action anime. The system can use different parameters like rating, genre, theme, and more to help with the recommendation.","","Anime Recommendation System","","","0"
"5635","1518898","6405452","08/09/2021 08:25:55","Create a recommendation system that is better than the one in the example notebook!","","Drama Recommendation System?","","","1"
"6419","1660759","6408287","10/20/2021 16:31:58","## Task Details
Select important features and build an Intrusion detection system using Machine Learning or Deep Learning

## Evaluation
Evaluation criteria are accuracy and confusion matrix score","","Feature Selection and Intrusion Detection","","","0"
"6940","1747794","6418202","11/26/2021 12:48:45","## Background
Medicine students may be faced with this type of dataset at some point. In a hospital database one can access the time between a patient received its diagnose until it went into surgery, and also all the complications that happened during this time.

## Expected Submission
Either a report detailing the strategy used to solve this problem, or a general conceptualization of how to solve it.

## Evaluation
One should expect the predictive power to be small given the data has been generated randomly, and that there is only one predictor. The real challenge here is in finding a method that can deal with multiclass prediction when the classes are not multually exclusive. 

### Further help
I suspect that this problem would be solvable with NN, however the dataset is much too small for that. Other solutions might involve recoding the classes taking into consideration their possible combinations, or even dropping the non-multually exclusive cases, with the right argumentation they could all be valid means of solving this issue.","","Predict complications based on time from diagnose to surgery","Many complications may happen simultaneously, however there is only one predictor. What method would suit best this type of problem?","","0"
"5867","1550243","6423545","08/24/2021 18:50:27","You can create ml or dl mdels to extract topics that would explain the reviews.","","Topic Modelling","","","4"
"5768","1534986","6426901","08/17/2021 07:35:39","## Task Details
This dataset is a preprocessed dataset of APTOS 2019 dataset. We applied CLAHE and cropping technique to enhance the images and resized into 512x512

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","multi-class classification","","","1"
"6902","1552048","6427605","11/22/2021 11:37:23","## Task Details
Mine for important Data from details of the article

## Expected Submission
Make submission via Jupyter notebook using Python

## Evaluation
Submission with the most value of information

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019?","","Mine Data from my Article","","","0"
"6238","1624500","6449262","10/02/2021 15:02:49","Perform a data analysis on 'Latest covid-19 dataset of India state-wise.","","Covid-19 dataset Explore","covid-19 dataset of India state-wise","","0"
"6410","1652634","7615293","10/20/2021 05:58:49","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
k","","New New","new","","0"
"6596","1667677","6450547","10/24/2021 08:27:30","**Task Details**
The Opioid Epidemic has entered a new phase with a resumed increase in overdoses coming from the job loss and social isolation resulting from the Coronavirus Pandemic. This is happening even as we have begun to turn the corner on the opioid over-prescription that started in the 1990s.

**Expected Submission**
Analyse the trends over time by State both for the overall overdose deaths and for the deaths per drug type. Submissions should focus heavily on data visualisation since there is a very strong regional component to the epidemic.

Notebooks should show the trends in different States over time as well has how different regions of the United States have fared over time.

**Evaluation**
This is not a formal competition, but rather an on going project. Analysis of this dataset will help reveal what features may be useful to add in to future datasets, which in turn will further deeper understanding of the causes of this critical public health crisis.","","Analyse the causes of the Opioid Epidemic","Can we understand what is causing the resumed upsurge in overdoses?","","0"
"6102","1581286","6457698","09/16/2021 11:02:35","submit all easy-to-use codes. Data visualization, data normalization and fill NULL values,  and more accuracy to fit model in machine learning.","","ESAY CODE SUBBMIT","easy to use code submit","","1"
"6104","1581286","6457698","09/16/2021 12:26:14","from sklearn.metrics import accuracy_score

#Import Library for Logistic Regression
from sklearn.linear_model import LogisticRegression

#Initialize the Logistic Regression Classifier
logisreg = LogisticRegression()

#Train the model using Training Dataset
logisreg.fit(X_train, y_train)

# Prediction using test data
y_pred = logisreg.predict(X_test)

# Calculate Model accuracy by comparing y_test and y_pred)
acc_logisreg = round( accuracy_score(y_test, y_pred) * 100, 2 )
print( 'Accuracy of Logistic Regression model : ', acc_logisreg )

****&gt; AttributeError: 'str' object has no attribute 'decode'****","","ERROR IN Logistic Regression","""AttributeError: 'str' object has no attribute 'decode"" submit this error code","","0"
"6009","1571223","6460953","09/04/2021 21:11:11","Create EDA on Participants, disciplines, and medal winners.","","EDA on Participants, disciplines, and medal winners.","","","0"
"6070","1585049","6534192","09/12/2021 17:42:08","Above is dataset of global 500 companies with country and other statistics.
You need to find out top 5/10 countries which have highest companies in this list

i.e,
Country X1 have 12 companies in list
Country X2 have 10 companies in list
Country X3 have 09 companies in list
....
....

You can create/submit notebook, or start discussion and answer.","","Which countries have highest companies count in forbes global 500 list ?","Which country (top 5 countries) have highest companies count in forbes global 500 list, you can use this dataset and find out answer","","2"
"6071","1585049","6534192","09/12/2021 17:47:07","Prepare EDA for top 5 companies statistics as per Sales, Profit, Assets and Market value

EDA for companies which are in top 5 for Sales
EDA for companies which are in top 5 for Profit
EDA for companies which are in top 5 for Assets
EDA for companies which are in top 5 for Market","","Prepare EDA for top 5 companies statistics","Prepare EDA for top 5 companies statistics as per Sales, Profit, Assets and Market value","","3"
"6211","1615173","6534192","09/29/2021 06:41:26","Just practice test,

Train and test image classification model with this dataset and upgrade your skills for machine learning classification","","Train and test image classification model with above data","Train and test image classification model with cat/dog image data","","1"
"7027","1776052","6534192","12/07/2021 12:59:06","Create EDA and Geography analysis
1. EDA
2. Geography map for city in which funded company resides","","Create EDA and Geography analysis","Create EDA and Geography analysis","","1"
"6263","1631137","6601487","10/06/2021 05:03:30","## Task Details
Implement a classifier from scratch and test the same like naive bayes.

## Expected Submission
Just for learning","","Implement classifier from scratch and test the same","","","0"
"7010","1765397","6625407","12/04/2021 16:09:57","## Task Details
The goal  of this task is to make Python script which takes a video as an input and returns all texts visible on the video 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","Extract text from video","Learn computer vision how to extract text image","","0"
"5825","1514542","6629286","08/21/2021 08:38:13","## Task Details
Split the data and predict the kills

## Expected Submission
a submission file containing the accuracy of prediction based on your (splits) test data","","Predict the kills","","","0"
"6878","1726877","6635043","11/19/2021 13:10:49","Weekly Task: a suitably normalized dataset is issued every week, containing data from over 400 cryptocurrencies. For each asset, the data relating to prices, volumes traded, quantitative elements, as well as alternative data (information on the blockchain and on the sentiment of the various providers) are aggregated. The last column instead contains the question to which the Data Scientists are asked to give an answer: the relative strength ranking of each asset, built on the forecast of the percentage change expected in the following week. 

Using ""Quickstart.ipynb"" Jupyter Notebook is possible to provide a .csv file with the predictions for all the assets contained inside the validation_dataset (a prediction value between 0 and 1). 

Next week the results obtained by market fluctuations will be compared with the provided predictions: the less the RMSE the best.

You can follow this link (https://rocketcapital.ai/dapp/) for instructions. 
The next challenge will start at 0 UTC on 22th November 2021 and will be closed on 29th November 2021. The submission window of next challenge will be closed at 0 UTC of 23rd of November 2021.

Follow us on 
Discord: https://discord.gg/Kxx5pNVw for further information ad doubts
Linkedin:
Twitter","","Rocket Competition Weekly Challenge 8","The weekly challenge inside Rocket Competition","11/22/2021 23:59:00","0"
"5998","1569305","6641125","09/04/2021 04:27:56","## Task Details","","Do Sentiment Analysis","Perform Sentiment Analysis","","0"
"5857","1548265","6641125","08/23/2021 18:29:27","## Task Details
Here You can Present EDA on Various STock Data Provided.You can use various Visualization Library and perform EDA  by Plotting various graph on Opening and Closing of Stock Price or also High price vs Close Price and You can also perform Time Series Analysis using various methods.

## Expected Submission
You can either submit csv or solve the task primarily using Notebooks. What should the solution contain? You can show various Visualizations and also using various techniques to perform time series analysis and future prediction of the Stock data","","Perform EDA and Time Series Analysis","Perform EDA and Time Series Analysis on the Top 10 SmartPhone Company which has past 5 years data","","1"
"5875","1549839","6641125","08/25/2021 15:38:37","## Task Details
Perform EDA on this dataset to show trend in rise and decrease in price of Bitcoin

## Expected Submission
You can provide Notebooks with various graph using visualization libraries and perform EDA report","","Perform EDA on Overall Bitcoin Price","Draw various graph on this dataset to show trend in rise and decrease in price of Bitcoin","","0"
"5876","1549839","6641125","08/25/2021 15:47:44","## Task Details
Use this data perform EDA and then You have to Predict Future Price of Bitcoin. You can use any ML model to perform Time series analysis.

## Expected Submission
You can submit Your Notebooks with EDA and ML Model that predict the future price of bitcoin with model evaluation metrices based on ML model used  

### Further help
For any help you can refer this notebook
https://www.kaggle.com/meetnagadia/bitcoin-price-prediction-using-lstm","","Bitcoin Price Prediction","You have to Predict Future Price of Bitcoin Based on this dataset","","0"
"6044","1579572","6641125","09/09/2021 08:54:15","## Task Details
Perform EDA on the Dataset to find trend in the dataset and then Perform Time Series Analysis

### Further help
You can use any Regression Techniques as well  any other  Algorithm to perform Time Series Analysis.
For example you can refer This notebook for Bitcoin Price Prediction you can refer to it and can perform on this dataset as well.

Bitcoin Price Prediction : https://www.kaggle.com/meetnagadia/bitcoin-price-prediction-using-lstm","","Perform Time Series Analysis","This task is for Time Series analysis on Apple Stock Price prediction","","2"
"6226","1623168","6648246","10/01/2021 12:37:30","## Task Details
Apply Simple Linear Regression , Multiple Linear Regression or any other regression model .","","Apply Different Regression models","","","2"
"5949","1565038","6648246","09/01/2021 05:04:01","## Task Details
You can use LSTM  or any other method to predict the Stock Prices.","","Predict Stock price","Use ML techniques to predict  Stock price movement.","","4"
"5973","1567435","6648246","09/02/2021 11:06:59","## Task Details

Use Data analytics Skills to analyze Hidden Patterns.","","Task-1 Explore Hidden Patterns in Dataset?","","","5"
"5421","1500459","6648246","07/30/2021 10:34:24","Compare one country which did not impose lockdown with another country which impose lockdown","","Task-1  Do Lockdown works.?","","","2"
"5422","1500459","6648246","07/30/2021 10:37:31","Compare a high median age country with Low median age country and describe conclusion","","Task-2  Does Median Age Effect Death Rate.?","","","1"
"5423","1500459","6648246","07/30/2021 10:40:52","Compare two countries with different  obesity rate with death per million column","","Task-3 How does Obesity Effect Death Rate.?","","","2"
"5424","1500459","6648246","07/30/2021 10:44:13","Compare two different countries having different diabetes rate and then conclude what you find from data","","Task-4 How Does Diabetes effect Death Rate.?","","","1"
"5425","1500459","6648246","07/30/2021 10:46:21","Compare two countries with different testing rate and conclude your observation from data.","","Task-5 Does More Testing Slow Deathrate.?","","","1"
"6382","1651768","6648246","10/16/2021 08:47:52","Practice your Machine Learning Algorithms on this dataset.","","Implement ML algorithms","","","1"
"5103","1464667","6668389","07/13/2021 08:17:21","## Task Details
Trying to find out if there was a significant impact of COVID-19 related measures, on Air Quality City-wise. If there was a drop can it be due to COVID-19 or simply due to weather parameters(Humidity, etc.) changed levels.

## Expected Submission
- Exploring the Data for insightful Trends and patterns.
- Testing Significant Changes in the Level of Parameters in 2020, for Any important City of Choice. 
- Finding out the possible reasons for such changes. Finding weather the changes are only due to changes in temperatures, humidity, dew, etc. 
- How strongly are the gases related among themselves and particulate matters. What can be the cause of such a relationship between the gases. 
- Some suggestions on what measures can curb the level of Pollutants. 
- In Many places in 2021 COVID-19 related measures are relaxed, How much did it impact the Level of Pollutants.
- Inference about the City generating the corresponding values of parameters and Levels of gases.
- Modelling the relationship between gases and weather parameters. Using that model to see how it performs in 2019, 2020 and 2021. Is the year 2020 different than the other two in terms of Air Quality? If so, in what sense ?
- You may try connecting the Air Quality Index to some other Data(Eg- COVID Spread, etc.)

## Evaluation
- Reasons for every patterns and trends noticed and formulated, will be given more value.
- Exhaustive search for all significant changes is appreciated.
- A better approach to modelling resulting to better predictions and overall explanation of relationship is given more points.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Impact of COVID-19 on Air Quality","Finding Out how much COVID-19 changed the Level of Air Pollutants","","1"
"5849","1545198","7945952","08/22/2021 22:35:32","Make The EDA for The Data","","Gold Prices EDA","","","1"
"6827","1710176","6693435","11/12/2021 01:42:17","## Task Details
Classify all the anomalies and normal videos 

## Expected Submission
AUC Score and approach","","Classification Task","Build Model to classify anomalies","","2"
"6764","1673289","8411860","11/05/2021 14:38:05","## Task Details
regenerate every aspect of data entry with reputed data base,Consider all forms of linear data into regressive constraints can also have done through df.
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","convert data info","","","0"
"5994","1569834","6737145","09/03/2021 18:42:12","Creating a machine learning model to predict oil, water, and gas rates based on the  other variables.","","A machine learning model to predict oil production (oil rate)","Using Volve field production dataset(released by Equinor)","","2"
"6010","1569834","6737145","09/05/2021 04:55:08","conducting exploratory data analysis","","volve production dataset EDA","","","1"
"5311","1486530","6831699","07/24/2021 17:22:19","use this dataset and submit the task
. Good luck","","What apps have made crazy impact on teens and have become tremendously popular","Use some great visualizations to prove your point","08/31/2021 23:59:00","1"
"5310","1486535","6831699","07/24/2021 17:05:26","play with the data and find out some unique insights about different factors that affect the life span of individuals","","find out trends in mortality rate of male, female, and infants","WHO ARE MOST LIKELY TO DIE?","09/30/2021 23:59:00","1"
"5309","1486538","6831699","07/24/2021 16:56:08","Expected Submission

proper visual analytics for determine top 5 wine producers in the world","","What countries are the highest wine producers?","Analyse the global wine production and find out top 5 wine producers of the world with proper visualization","10/28/2021 23:59:00","1"
"5318","1486542","6831699","07/25/2021 04:00:07","use this dataset and submit the tasks..
if you liked the dataset.. please **upvote** 
 good luck!","","Find out what's the toughest sport to excel?","Analyse the dataset and find out the top 5 toughest sports to excel and skills required to master them","08/28/2021 23:59:00","0"
"6328","1643764","6848996","10/12/2021 17:03:11","## Task Details
read the description and make a model for classification model for this dataset.","","is this picture vertical?","real-world use of this dataset","","1"
"6677","1671892","6867114","10/26/2021 05:14:36","Where should a drinks company run promotions?
üìñ Background

Your company owns a chain of stores across Russia that sell a variety of alcoholic drinks. The company recently ran a wine promotion in Saint Petersburg that was very successful. Due to the cost to the business, it isn‚Äôt possible to run the promotion in all regions. The marketing team would like to target 10 other regions that have similar buying habits to Saint Petersburg where they would expect the promotion to be similarly successful.
The data

The marketing team has sourced you with historical sales volumes per capita for several different drinks types.

    ""year"" - year (1998-2016)
    ""region"" - name of a federal subject of Russia. It could be oblast, republic, krai, autonomous okrug, federal city and a single autonomous oblast
    ""wine"" - sale of wine in litres by year per capita
    ""beer"" - sale of beer in litres by year per capita
    ""vodka"" - sale of vodka in litres by year per capita
    ""champagne"" - sale of champagne in litres by year per capita
    ""brandy"" - sale of brandy in litres by year per capita","","Where should a drinks company run alcoholic drink promotions","","","0"
"6679","1477116","6867114","10/26/2021 09:19:49","Here are the metadata and risk disclosure texts of 10000 projects launched on Kickstarter. The data has been pulled out randomly from the Harvard business database.

Here we are going to perform a sentiment analysis on the texts for risk disclosures. what we are going to do is to evaluate the text based on dictionary-based methods and check whether they have any effect on projects' success. In other words, do these texts affect investors in their decision to invest in a project or not.

In rhetorics, based on Aristotle, there are three appeals, Pathos, signals which appeal to emotions, Logos, which appeals to logic, and Ethos, which signals credibility.

I have uploaded 4 different word lists, positive and negative words which I brought from Diction word lists which are really popular among researchers. In addition,  I created two separate word lists for Ethos and Logos appeals which are gathered by assessing the texts of risks disclosures for 1700 projects manually.

In this project we gauge the effect of this appeal by counting how many words from those lists are used in the disclosures","","Textual Analysis of Crowdfunding risk disclosures","Bag of Words technique","","0"
"5306","1487705","6921900","07/24/2021 12:23:06","Just learn and shareüòä","","Do Data science and go beyond","","","0"
"5653","1523018","6925588","08/10/2021 09:42:01","### TASK 
Aim is to accurately identify mentions of disease, syndromes and symptoms (disease_syndrome) from the clinical notes. 
A baseline NLP tool has been employed to generate a list of annotations on these clinical notes. This tool has very high recall (identifying almost all disease_syndrome mentions), but cannot accurately determine which are true disease_syndrome mentions. 
Task is to do a binary classification on all given mentions to identify true disease_synrome anntoations. This is essentially a named entity recognition task on clinical notes.","","Name Entity Recognition and Classification","","08/31/2021 23:59:00","0"
"5723","1527635","6120101","08/13/2021 23:10:40","## Task Details
Create a classifier to determine which variant of pizza a customer wants, if you cannot create this classifier then try to predict the size of the pizza","","Create a classifier to determine which pizza variant you want","","","2"
"6013","1527635","7945952","09/05/2021 16:35:51","make a model that will predict the Price of the pizza","","Prediction for the pizza price","","","1"
"6798","1705826","6982946","11/09/2021 18:53:38","## Task Details
Do visualization on air quality data.","","Perform EDA on air quality data","","","0"
"6171","1530930","6990402","09/23/2021 13:16:48","01. Can you spot any major differences in the Airbnb market between cities?","","Complete EDA","","","0"
"5932","1520324","6990402","08/31/2021 03:47:59","Do a clean exploratory data analysis.","","EDA ON OLYMPIC HISTORY DATA","EXPLORATORY DATA ANALYSIS","","1"
"6072","1585230","1700889","09/12/2021 20:26:37","Simple task in order to think more and analyze data in suitable way","","EDA on Avengers","","09/30/2021 23:59:00","1"
"5406","1498206","7007159","07/29/2021 09:00:36","Project Brief
You work for Spark Funds, an asset management company. Spark Funds wants to make investments in a few companies. The CEO of Spark Funds wants to understand the global trends in investments so that she can take the investment decisions effectively.

 

Business and Data Understanding
Spark Funds has two minor constraints for investments:

It wants to invest between 5 to 15 million USD per round of investment

It wants to invest only in English-speaking countries because of the ease of communication with the companies it would invest in

For your analysis, consider a country to be English speaking only if English is one of the official languages in that country

You may use this list: Click here for a list of countries where English is an official language.

 

These conditions will give you sufficient information for your initial analysis. Before getting to specific questions, let‚Äôs understand the problem and the data first.

 

1. What is the strategy?

Spark Funds wants to invest where most other investors are investing. This pattern is often observed among early stage startup investors.

 

2. Where did we get the data from? 

We have taken real investment data from crunchbase.com, so the insights you get may be incredibly useful. For this assignment, we have divided the data into the following files:

 

You have to use three main data tables for the entire analysis (available for download on the next page):

 

3. What is Spark Funds‚Äô business objective?

The business objectives and goals of data analysis are pretty straightforward.

Business objective: The objective is to identify the best sectors, countries, and a suitable investment type for making investments. The overall strategy is to invest where others are investing, implying that the 'best' sectors and countries are the ones 'where most investors are investing'.
Goals of data analysis: Your goals are divided into three sub-goals:
Investment type analysis: Comparing the typical investment amounts in the venture, seed, angel, private equity etc. so that Spark Funds can choose the type that is best suited for their strategy.
Country analysis: Identifying the countries which have been the most heavily invested in the past. These will be Spark Funds‚Äô favourites as well.
Sector analysis: Understanding the distribution of investments across the eight main sectors. (Note that we are interested in the eight 'main sectors' provided in the mapping file. The two files ‚Äî companies and rounds2 ‚Äî have numerous sub-sector names; hence, you will need to map each sub-sector to its main sector.)
 

4. How do you approach the assignment? What are the deliverables?

The entire assignment is divided into checkpoints to help you navigate. For each checkpoint, you are advised to fill in the tables into the spreadsheet provided in the download segment. The tables are also mentioned under the 'Results Expected' section after each checkpoint. Since this is the first assignment, you have been provided with some additional guidance. Going forward you will be expected to structure and solve the problem by yourself, just like you would be solving problems in real life scenarios.

 

Important Note: All your code has to be submitted in one Jupyter notebook. For every checkpoint, keep writing code in one well-commented Jupyter notebook which you can submit at the end.","","Investment Assignment","","","0"
"5407","1498355","7007159","07/29/2021 09:12:47","Project Brief
You work for Spark Funds, an asset management company. Spark Funds wants to make investments in a few companies. The CEO of Spark Funds wants to understand the global trends in investments so that she can take the investment decisions effectively.

 

Business and Data Understanding
Spark Funds has two minor constraints for investments:

It wants to invest between 5 to 15 million USD per round of investment

It wants to invest only in English-speaking countries because of the ease of communication with the companies it would invest in

For your analysis, consider a country to be English speaking only if English is one of the official languages in that country

You may use this list: Click here for a list of countries where English is an official language.

 

These conditions will give you sufficient information for your initial analysis. Before getting to specific questions, let‚Äôs understand the problem and the data first.

 

1. What is the strategy?

Spark Funds wants to invest where most other investors are investing. This pattern is often observed among early stage startup investors.

 

2. Where did we get the data from? 

We have taken real investment data from crunchbase.com, so the insights you get may be incredibly useful. For this assignment, we have divided the data into the following files:

 

You have to use three main data tables for the entire analysis (available for download on the next page):

 

3. What is Spark Funds‚Äô business objective?

The business objectives and goals of data analysis are pretty straightforward.

Business objective: The objective is to identify the best sectors, countries, and a suitable investment type for making investments. The overall strategy is to invest where others are investing, implying that the 'best' sectors and countries are the ones 'where most investors are investing'.
Goals of data analysis: Your goals are divided into three sub-goals:
Investment type analysis: Comparing the typical investment amounts in the venture, seed, angel, private equity etc. so that Spark Funds can choose the type that is best suited for their strategy.
Country analysis: Identifying the countries which have been the most heavily invested in the past. These will be Spark Funds‚Äô favourites as well.
Sector analysis: Understanding the distribution of investments across the eight main sectors. (Note that we are interested in the eight 'main sectors' provided in the mapping file. The two files ‚Äî companies and rounds2 ‚Äî have numerous sub-sector names; hence, you will need to map each sub-sector to its main sector.)
 

4. How do you approach the assignment? What are the deliverables?

The entire assignment is divided into checkpoints to help you navigate. For each checkpoint, you are advised to fill in the tables into the spreadsheet provided in the download segment. The tables are also mentioned under the 'Results Expected' section after each checkpoint. Since this is the first assignment, you have been provided with some additional guidance. Going forward you will be expected to structure and solve the problem by yourself, just like you would be solving problems in real life scenarios.

 

Important Note: All your code has to be submitted in one Jupyter notebook. For every checkpoint, keep writing code in one well-commented Jupyter notebook which you can submit at the end.","","Investment Analysis  - spark funds","","","1"
"5909","1559045","7038105","08/29/2021 10:20:23","### Project Goal: 
I am mandated to determine how annual members and casual riders use Cyclistic bikes differently with the aim of converting casual riders into annual member.  

**Key stakeholders**:      
Lily Moreno: The director of marketing,    
Cyclistic executive team and  
Cyclistic marketing analytics team.    
                          
### Ask Phase
By the end of this project, i should be able to determine how annual members and casual riders use Cyclistic bikes differently, Why casual riders would want to buy Cyclistic annual memberships and how Cyclistic can use digital media to influence casual riders to become members.

#### Tools Used for this project.

Excel, SQL and R-Language

### Prepare Phase
The datasets being used for the project are 12 months of trip data owned by Motivate International Inc who have granted me a non-exclusive, royalty-free, limited, perpetual license to access, reproduce, analyze, copy, modify, distribute in my product or service and use the Data for any lawful purpose (‚ÄúLicense‚Äù).  

There are no issues of bias or credibility with these datasets. They are Reliable, Original, Comprehensive, Current and Cited.  

Upon initial inspection of each of the 12 csv datasets using ISBLANK function in excel, i noticed that some of the observations are missing. This i will address subsequently using RStudio.
                     
### Loading Datasets

Loading the needed 12 csv datasets, i will first load the tidyverse package which contains most of the dataframe manipulation packages needed 
for loading and cleaning the datasets for this project.","","Google Project","","","0"
"5282","1483741","7042824","07/22/2021 11:24:15","## Task Details
Try to learn as much as you can about the black hole in the center of the galaxy.","","ANALYSIS","Data Science","","1"
"5326","1489789","7042824","07/25/2021 12:03:29","## Task Details

Examine and analyze all centers. Identify the differences between them using mathematical ratios. Examine the images.","","ANALYSIS","Data Science Project","","0"
"5239","1479053","7042824","07/20/2021 09:10:05","Create your best analysis system with math!","","ANALYSIS","Data Science Project","","0"
"5154","1472322","7042824","07/17/2021 04:03:49","## Task Details
Create your best perpective for Proxima Centauri","","ANALYSIS","Data Science Process","","0"
"5157","1472393","7042824","07/17/2021 05:03:08","## Task Details
Analysis with plots and visualization methods.","","ANALYSIS","Data Science","","0"
"5143","1470844","7042824","07/16/2021 09:22:08","## Task Details
Create your best perpective about data analysis for M87 Galaxy","","ANALYSIS","Data Science","","1"
"6054","1581737","7042824","09/10/2021 14:19:10","Create your best perspective to impact","","ANALYSIS","Data Science","","0"
"6809","1705602","7093612","11/10/2021 09:35:57","This case study aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicants using EDA is the aim of this case study.

 

In other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default.  The company can utilise this knowledge for its portfolio and risk assessment.

To develop your understanding of the domain, you are advised to independently research a little about risk analytics - understanding the types of variables and their significance should be enough).","","Identifying patterns for loan defaulting","","","0"
"6885","1493447","7112628","11/20/2021 12:40:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
open source data...","","make full stack management visualization","","02/22/2022 23:59:00","1"
"5991","1569541","7151095","09/03/2021 15:21:31","Feel free to explore the dataset and come up with insights. That's the whole aim for this dataset","","Exploratory Analysis","","","1"
"5345","1491602","7237430","07/26/2021 09:05:55","The Movies Database  Datasets performs some Machine Learning and Deep Learning  Models and analyzes which movie is top rated and which movie is not.","","The Movies DataBase","The Movies Database  Datasets to performs some Models.","10/21/2021 23:59:00","2"
"5292","1486293","7237430","07/23/2021 14:56:32","Salary Dataset For Employee As A Work Experience .","","Salary Dataset For Employee As A Work Experience","Salary Dataset For Employee As A Work Experience","09/23/2021 23:59:00","3"
"6725","1684086","7237430","10/30/2021 16:54:02","# Task
You build an IMDB Recommendation system using any recommendation system techniques.","","IMDB top 250 english movies DataSet.","IMDB top 250 english movies DataSet.","","4"
"6873","1729072","7272885","11/18/2021 20:41:59","# 1. Introduccion
El correo electronico es una de las herramientas mas antiguas y utiles que nacieron con
internet y a pesar de que han surgido nuevo metodos de comunicacion, hoy en dƒ±a, el correo
electronico es ampliamente utilizado debido a la versatilidad en la transmision del mensaje, sin
embargo, tambien ha traƒ±do consigo una serie de problemas importantes, siendo dos de ellos,
el aumento de trafico de la red y el robo de identidad debido a correos electronicos de caracter
malicioso llamados correos spam.
## 1.1. Motivacion o justificacion
La identificacion de spam hasta la fecha sigue siendo un reto, esto es debido a que no es
facil identificar un correo de este tipo, pues existe una gran variedad y cada vez son mas, que
es muy difƒ±cil distinguir entre un correo organico y uno spam. Los modelos de aprendizaje automatico han mostrado un gran desempeÀúno en la deteccion de estos correos, y en este trabajo
abordaremos algunos aspectos clave antes de iniciar con el modelo, esto es, el conjunto de datos
y su preprocesamiento.
## 1.2. Planteamiento del problema
El conjunto de datos del cual se extraera un subset es el correspondiente un subset de la base
de datos ‚ÄúEnron Email Dataset‚Äù. Contiene datos de alrededor de 150 usuarios con un total de
alrededor de 500,000 mensajes de correo electronico. Originalmente el dataset se hizo publico
por la Comision Federal Regulatoria de Energƒ±a de Estados Unidos durante la investigacion
alrededor del colapso de la empresa Enron.
El subconjunto a utilizar consta de 5,854 correos, de los cuales 1,496 estan catalogados como
spam y el resto corresponden a correos electronicos legƒ±timos.
### 1.3. Objetivo de la investigacion
El objetivo de este trabajo es lograr clasificar de manera efectiva estos correos electronicos
reconociendo patrones en el texto para poder identificar si es spam o un correo legƒ±timo.
## 1.4. Descripcion del conjunto de datos
El conjunto de datos a utilizar es el correspondiente a la base de datos ‚ÄúEnron Email
Dataset‚Äù. Contiene datos de alrededor de 150 usuarios con un total de alrededor de 500,000
mensajes de correo electronico. Originalmente el dataset se hizo publico por la Comision Federal
Regulatoria de Energƒ±a de Estaos Unidos durante la investigacion alrededor del colapso de la
empresa Enron.

## Descargar documento completo

[Ver documento completo](https://drive.google.com/file/d/14te4-1IlYMbS1xrNoxW8f4DEsQugqdAZ/view?usp=sharing)","","Clasificador binario spam/no spam","Base de datos Enron para NLP","11/19/2021 23:59:00","0"
"6295","1639353","7292627","10/10/2021 09:35:15","Fire!Flowers!","","Flowers11-1","calssification","10/15/2021 23:59:00","0"
"5919","1560604","7331510","08/30/2021 06:12:53","Identification of trend","","Cyclist Bike Project - Trend Analysis","","","0"
"6135","1599740","7342868","09/19/2021 22:22:39","#Installed Packages: 
tidyverse
lubridate
janitor
readxl
writexl

##Download CSV files 12

## Combine the 12 .csv files 

## Calculate Trip Duration

## Create Summary Data Frame

## By Start Station

## Better Marketing","","Bike Ride Case Study","Database is about a bike share program that features approximately 4 million riders, 5800 bicycles and, 600 docking stations","","1"
"6242","1625288","7390295","10/02/2021 22:23:11","How casual riders and annual members use Cyclistic bikes differently? Design a new marketing strategy to convert casual riders into annual members.","","Maximizing the number of annual memberships.","How Does a Bike-Share Navigate Speedy Success?","","0"
"6286","1637466","7394924","10/09/2021 06:49:08","## Task Details
create a weather forecast model which predicts weather","","Create Weather Forecast Model","weather data, weather forecast, temperature data, humidity, wind","01/01/2024 00:00:00","1"
"5901","1558286","7406836","08/28/2021 22:53:13","## Task Details
How do annual members and casual riders use Cyclistic bikes
differently?

## Expected Submission
Solution should show average ride length between members and casual riders, frequency of use, and most popular usage days.

## Evaluation
Will be evaluated on supported argument data on how members use their bikes differently. 

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Cyclistic Rider Uses","","","0"
"6400","1655714","7407937","10/18/2021 11:23:49","Your Solution  Should contain Detailed EDA. Use Diff. Kind of Plots to show your Skills.","","Generate EDA Report","","","4"
"6786","1655714","2654224","11/08/2021 15:40:10","## Task Details
 1‚Ä§ Top Selling Products ‚Ä§
 2‚Ä§ Top selling brand 
 3‚Ä§  Comparison of prices of the same product and different brands ‚Ä§","","Top Brand","","12/12/2021 23:59:00","0"
"5802","1540612","7417245","08/19/2021 12:06:24","## Task Details
Experiment with Kaggle as a beginner can be a scary thing to do so this simple data set will allow for simple exercises in reading and plotting.
Here the task will be to plot distributions that could allow us to understand the evolution of the contractual relationship from 2015 to 2016.

## Expected Submission
A notebook should be submitted with the relevant distributions and comments with insights that might have been gathered from them.

## Evaluation
A good solution would be one with actionable insights.","","Beginner friendly task","","","0"
"5382","1495083","7435983","07/28/2021 08:02:46","Build a classification model to guess what's my favorite type of music trying a bunch of different songs","","Train the model","","","0"
"5383","1495083","7435983","07/28/2021 08:03:52","You have the details about how I collected the data , how about you do it yourself to build your own model ?","","It's your turn !","","","0"
"6401","1654398","7453978","10/18/2021 15:17:48","create a recommendation system and also analyze the data along the with the million dataset to create a better recommmendation system.","","Recommendation System","Music recommendation system  using features provided in the dataset","","0"
"6338","1646970","7454188","10/14/2021 06:20:53","MIDIAN FILTTER 
UNET segmentation 
feature Extraction
SVM Classification

must solve all 

Expected Submission
MIDIAN FILTTER ,UNET segmentation, feature Extraction ,SVM Classification 


Task completion work 
25000 INR","","BRAIN MRI UNET segmentation And SVM Classification","MIDIAN FILTTER ,UNET segmentation, feature Extraction ,SVM Classification","10/31/2021 00:00:00","0"
"5854","1534038","7458133","08/23/2021 08:46:48","In this dataset you need to forecast sales","","Forecast sales","","","0"
"5863","1549345","7462254","08/24/2021 09:37:59","The goal is to achieve high accuracy on a given dataset.
You can use any networks and any training tricks.
To prevent using a lot of GPU resources, you should use a network that can train by using the resources provided by Google Colab (Other resources are strictly not allowed). 
Threshold of accuracy is 0.85
Please set your team name as shown in your CV.","","Deep Learning Classification","","","0"
"6053","1581658","7478399","09/10/2021 10:44:48","Perform EDA, find out relationship between sales, categories, pet_types, ratings, etc. Let those plots rain !","","Explore the dataset - EDA, Pre-processing ideas and more.","","","0"
"6057","1582258","7494464","09/10/2021 20:29:44","## Task Details
Users today depend a lot on reviews before shopping online. So, performing EDA and analyzing the past is necessary to predict the future.

## Expected Submission
Complete EDA is expected

### Further help
Feel free to ask for any help","","Amazon Reviews Exploratory Data Analysis","In this task we will perform EDA on Amazon Reviews","","2"
"5753","1531068","7518522","08/15/2021 23:12:59","Bellabeat is a high-tech company that manufactures health-focused smart products for women.

Bellabeat is looking to find more opportunities for growth through data obtained from their line of products. This analysis seeks to find these opportunitues by answering the following questions:-

What are some trends in smart device usage? 
How could these trends apply to Bellabeat customers? 
How could these trends help influence Bellabeat marketing strategy?","","How can a wellness company play it smart?","","","1"
"5277","1482620","7523058","07/21/2021 23:41:51","## Task Details
Introduce a maximum number for pool's passengers.
Good values could be the maximum passengers count of the original dataset or a plausible value for a cab (e.g. 2, 4, ...).","","Maximum pool's passengers","","","0"
"6395","1654486","8278211","10/17/2021 23:48:57","## Task Details
it is a norm that after a tournament is completed, we find the team of the tournament.

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
A good solution would be a creative kernel that shows the performance of the chosen player within your team and the franchise they represent","","Team of the Tournament","","11/30/2021 00:00:00","1"
"6396","1654486","8278211","10/17/2021 23:53:01","A team can retain a maximum of 3 players for next year mega auction. 

Based on the performance of this years IPL, predict your choice of players for each team

## Task Details
With the mega auction coming up next year, teams needs your help to know which 3 players they can retain. The retention is based on the individual performance of the player in this edition of IPL

## Expected Submission
A notebook with code and relevant visualizations.

## Evaluation
A good solution would be a creative kernel that shows the performance of your chosen player","","Player Retention","","02/28/2022 00:00:00","0"
"5663","1523849","7533187","08/10/2021 18:58:51","Create a exploratory data analysis from the smart cities dataset, trying to find out insights that can help with future analisys.","","Smart Cities EDA","Exploratory Data Analysis","","3"
"6951","1713768","7545003","11/28/2021 15:45:24","Contained in each dataset are 3000 trials for each item to see how many of the given item is needed to make one bone meal. Using this data, try to figure out which items have similar rates of bone meal generation.","","Comparison of items","Which items have similar composting rates?","","1"
"6889","1733746","201765","11/20/2021 23:09:04","## Task Details
This is a junk task","","What are the oldest and newest coins?","trivial","01/01/2022 23:59:00","1"
"6904","1738387","7545003","11/22/2021 20:18:34","## Task Details
Task to demonstrate how I created the csv files. Feel free to demonstrate your own attempt.

## Expected Submission
Submit csv file containing collatz numbers. Any amount will be fine.

## Evaluation
Aim for code that is flexible and can be modified to generate a different amount of collatz numbers, or start at a different point.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Generate The Files","Create csv file containing collatz numbers.","","1"
"6956","1738387","7545003","11/29/2021 07:07:08","## Task Details
Which number takes the longest to get to one? Measure by how many iterations of the collatz conjecture it requires to reach the value one. Alternatively, rank numbers by the number of steps descending order, which would place the desired value at the top of the result.","","Steps to One","Which number takes the most steps to get to one?","","1"
"6972","1756126","7545003","11/30/2021 09:50:57","## Task Details
Find a way to visualize the congressional makeup over time","","Visualize congressional composition","","","0"
"6989","1761495","7545003","12/01/2021 23:05:48","Submission should be in the form of a table that lists each decade starting with the decade states began joining to the decade when the last state joined, as well as how many states joined in that decade","","How many states joined each decade?","","","1"
"6990","1761495","7545003","12/01/2021 23:07:17","Submission should calculate the difference between two subsequent states joining the union (i.e years, months, AND days), and then find the biggest gap between two states joining.","","What was the longest timespan between two states joining the union","","","1"
"6965","1751430","7545003","11/29/2021 17:04:42","Generate random names given the candidate/winner list!","","Random President/Presidential Candidate Name Generator!","","","1"
"6947","1751430","7545003","11/28/2021 05:44:16","Go through dataset ElectionResultsByCandidate and figure out which election had the most candidates. Certain elections had candidates vote for them through the electoral college, but received no votes. Assume such people with no votes in the popular vote column did not actually ""run"" in this task.","","Which election had the most candidates?","","","1"
"6948","1751430","7545003","11/28/2021 05:46:17","I will leave this task a little open-ended. You could go for ranking third parties by number of popular votes they received in a year, perhaps how many elections they ran in? Be sure to state how you wish to rank/compare third parties.

For specificity, by third parties, I mean parties that ran in the elections other than the main two. In modern times the two ""main"" parties would be the Democrats and Republicans, in the past it would've been the Democratic-Republicans, Federalists, Whigs as well.","","Third parties","Rank the third parties!","","1"
"6949","1751430","7545003","11/28/2021 06:02:17","Fairly simple task. Output should be a csv that ranks presidents by who won the most elections.","","Biggest winner","Rank presidents by who won the most elections","","2"
"6957","1754291","7545003","11/29/2021 07:11:12","## Task Details
Index minimum wage to some of the other factors, and see which one gives the best result. This is an open ended task, so play around with the numbers and see what you get","","Index minimum wage","","","1"
"7005","1766288","7545003","12/03/2021 18:39:55","## Task Details
The golden ratio is the ratio for a rectangle such that it could be made up of a square with a rectangle of the golden ratio on one side. The value can be found [here](http://www2.cs.arizona.edu/icon/oddsends/phi.htm)","","Approximate Golden Ratio!","Can you do it using this data?","","1"
"7008","1766288","7545003","12/04/2021 13:56:30","## Task Details
To calculate the millionth Fibonacci number would be doable, but pretty intensive. Using the provided dataset, estimate the number of digits the 1 millionth Fibonacci number would have. This doesn't have to be exact, and could be a confidence interval. For fun, you could also try this with the 10 millionth, 100 millionth, or even billionth Fibonacci numbers, extrapolating to estimate their digit counts.","","How many digits?","Use this dataset to estimate the number of digits for certain Fibonacci numbers","","1"
"7028","1777144","7545003","12/07/2021 21:50:21","## Task Details
Calculate for each year the defense budget as a percentage of GDP as well as per capita.","","How big is the military budget?","","","2"
"7018","1770526","7545003","12/06/2021 03:17:10","## Task Details
Using this dataset, analyze the term lengths of each governor. How does the distribution look? I would suggest removing any current or future governors (I included governor-elect of Virginia in this, despite them not taking office until next year). I would also recommend removing the governors listed who died before they took office, who are listed as having took and left office on the same day. 

Find which states have the longest and shortest mean term lengths. See how they compare.","","Term lengths","How long did each governor serve?","","1"
"6118","1593425","7548224","09/17/2021 15:32:29","# Do anything you want","","Visualize & Analyze East Sea Current!","","","0"
"6082","1586929","7551437","09/14/2021 15:00:06","## Task Details
Dataset contains the details about the software produced. App id is given along with User Interface type, Application Type, Department ,Sub department and test status result. This is a classification problem. All the columns are categorical variables. Task is for a given set of features  (User Interface type, Application Type, Department ,Sub department) find the classification of Test status (Target)

## Expected Submission
User should submit the classification program (Python) with the best value of Classification accuracy, Precision and recall and F1 score. Solution should contain Python code. Given Dataset and Notebook can be used


## Evaluation
Evaluation is done considering the best value of Classification accuracy, Precision and recall and F1 score

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Predict the quality of the software produced","","11/30/2021 23:59:00","0"
"6098","1589971","7586698","09/16/2021 05:13:56","## Task Details
Create a CSV file that is human-readable and looks more like this:
| gloss | ... | video_id |
| --- | --- | --- |
| ""book"" | ... | ""69241"" |
| ""book"" | ... | ""65225"" |
| ... | ... | ... |
| ""go"" | ... | ""24954"" |

## Expected Submission
A `.csv` file that have a minimum of `gloss` and `video_id` columns.

## Evaluation
- This could be `csv` file or other tabular file
- Correct data type","","Make a human-readable CSV file","Can we make this the json file more readable?","","0"
"6854","1723211","7593370","11/16/2021 09:08:53","Check whether the function has vulnerabilities, is a binary classification task.","","Defect Detection","Check whether the function has vulnerabilities, is a binary classification task.","","0"
"5506","1508998","7593520","08/02/2021 18:30:31","## Task Details
It would be interesting to predict how UPI increased after Demonitization on 2016 Nov 8th.

## Expected Submission
Trend prediction if we ban 500 rs notes now

## Evaluation
Best logic and explanation wins

### Further help
NPCI website","","UPI Vs Demonitization","","11/19/2021 23:59:00","1"
"5507","1508998","7593520","08/02/2021 18:33:23","## Task Details
UPI increase in volumes  basis how corona started increasing this after 2020 Feb.
Please use public data sets to try to get a trend between corona cases and increase in UPI volume and what would make it really interesting is It put lockdown windows in major states and restrictions into account.
## Expected Submission
How will UPI trend increase again in 3rd wave

## Evaluation
We will compare this with actual data from NPCI website post the deadline

### Further help
NPCI website","","UPI trend Vs corona trend","","11/30/2021 23:59:00","1"
"5782","1538332","7606874","08/18/2021 08:06:34","Create a model to predict based on descriptors whther the follwing is drug resistance or drug susceptible
Use double delta g for classification purpose","","Machine learning based prediction of r Mycobacterium tuberculosis","Artificial Intelligence and Machine learning based prediction of resistant and susceptible mutations in Mycobacterium tuberculosis","","0"
"5670","1524518","7660288","08/11/2021 08:17:06","## Task Details
Using the data of each state, use a visual representation of how the cases have increased and decreased.","","State-wise Analysis","","","1"
"5671","1524518","7660288","08/11/2021 08:18:28","## Task Details
Predicting the approximate commence of the 3rd COVID wave in INDIA","","predicting the 3rd wave of COVID","","","2"
"5884","1553167","6907725","08/26/2021 13:54:32","## Task Details
The data visualization you make should be comprehensible for those who can't read (or understand a language)

## Expected Submission
The solution should contain basic information required. Don't need absolute details or expansive output. However, it needs to be exhaustive without leaving out any data

## Evaluation
Good solution:
Completely intuitive
May not have to refer to the documentation to understand key features
[Use notebook]

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Make a data visualization for the illiterate","","","2"
"5812","1541110","7660288","08/20/2021 05:47:28","visually attractive EDA of the data","","EDA of the data","","","0"
"5324","1488273","7660288","07/25/2021 10:20:28","## Task Details
Cleaning the data and imputing the missing values

## Expected Submission
Data with no null values","","Data Cleaning","","","1"
"5340","1488273","7660288","07/26/2021 03:54:44","## Task Details
To find the best team for each formation

## Expected Submission
The teams and the formation are expected","","Dream Team Formation","Creating the best teams based on formation and stats","","0"
"6011","1572141","7660955","09/05/2021 09:47:15","## Task Details
Identify the Disease and Treatment entity by exploring the dataset

## Evaluation
Compute metrics F1score/ Sensitivity and Precision","","Build Model for Entity Recognition","","","0"
"6203","1613986","7663642","09/27/2021 05:19:55","Analyze the data","","Explore the data","","","0"
"6204","1613986","7663642","09/27/2021 05:29:19","Perform an Exploratory Data Analysis on this dataset-'Covid-19 Status in Bangladesh' month-wise.","","EDA on 'Covid-19 Status in Bangladesh'","","","0"
"6421","1613986","7663642","10/21/2021 02:36:32","# visualize total cases of covid19 with divisions and districts wise","","EDA with Divisions/Districts-wise.","","","0"
"6777","1688657","7707461","11/07/2021 23:57:35","## Task Details
In many restaurants, or food stores, there are many different recipes with different types of feedback from customers. In this data, there are a lot of food recipes with their respective name, category, ID, and nutritional content. The task of this upcoming analysis is to try and elaborate a model that's capable of determining whether a food recipe is going to be popular (HighScore = 1) or not popular (HighScore = 0). Good luck.

## Expected Submission
Expecting model with roc auc score higher than 75%, and precision higher or equal than 85%.","","Predict HighScore","Predicting whether a food recipe will be popular or not based on nutritional content and other factors.","","0"
"5709","1527428","7715494","08/12/2021 21:29:11","Exploratory Data Analysis with Titanic dataset","","EDA Titanic project","","","0"
"5554","1513706","7717502","08/05/2021 05:09:15","## Task Details
story telling with data.



## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?","","create visualization to better understand data","","","1"
"6094","1591754","7719894","09/15/2021 19:36:49","## Task Details
The task at hand is to perform data visualization on the given dataset and assess the growth of MCU over the years.

## Expected Submission
Everyone can offer their own insights , including the KPI's for the film industry","","Analyzing the Marvel Universe","","","0"
"6002","1570654","7730819","09/04/2021 10:37:08","## Task Details
The purpose of this study was to develop a deep learning algorithm that can differentiate benign and malignant bone lesions using MR.

#1. Imaging data models
Models for image classification should be developed by adapting the EfficientNet or any other model deep learning architecture.  final classification layer with a single node and sigmoid activation need to used for perform the binary classification task.

#2. Clinical data model
A logistic regression model using clinical variables was separately developed for the classification task. Inputs were patient age, sex, and lesion location. 21 locations (clavicle, cranium, proximal femur, distal femur, foot, proximal radius, distal radius, proximal ulna, distal,ulna, hand, hip, proximal humerus, distal humerus, proximal tibia, distal tibia, proximal fibula, distal fibula, mandible, rib/chest wall, scapula, or spine) were one-hot encoded such that the model
received 23 distinct quantified input variables.

#3.Ensemble model
The imaging and clinical feature models need to be combine by using a stacked ensemble approach in which a voting ensemble received malignancy probabilities from the imaging and clinical feature models as inputs and created outputs based upon a summation of the predicted probabilities. Each ensemble classification model consisted of the outputs of a model trained upon T1W imaging studies, a model trained upon T2W imaging studies, and a logistic regression model based upon clinical features.","","Bone_tumor_predicted","Deep Learning for Classification of Bone Lesions on Routine MRI","10/10/2021 23:59:00","0"
"5801","1540621","7736121","08/19/2021 11:30:57","## Task Details
Cyclistic, a bike-share company in Chicago, want to maximize its number of annual memberships. so, we have to provide insights on how and what market strategy help the company in maximizing memberships.

## Expected Submission
1.How do annual members and casual riders use Cyclistic bikes differently?
2.Why would casual riders buy Cyclistic annual memberships?
3.How can Cyclistic use digital media to influence casual riders to become members?","","Produce insights to improve annual membership of bikes","","","0"
"5221","1477511","7736237","07/19/2021 15:32:52","## Task Details
The director of marketing believes the company‚Äôs future success depends on maximizing the number of annual memberships. Therefore, my team wants to understand how casual riders and annual members use Cyclistic bikes differently. 

## Expected Submission
My team will design a new marketing strategy to convert casual riders into annual members from these insights. But first, Cyclistic executives must approve my recommendations, so they must be backed up with compelling data insights and professional data visualizations.

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Case Study Roadmap","Follow the steps of the data analysis process: ask, prepare, process, analyze, share, and act.","","0"
"5603","1519262","7762021","08/08/2021 11:01:17","## Task Details
The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.","","Sonar, Mines vs. Rocks","","","1"
"6254","1628136","7779584","10/04/2021 13:17:52","A housing rental company has hired you for a new project. They are interested in developing an application to help people estimate the money they could earn renting out their living space.

The company has provided you with a dataset that includes details about each property rented, as well as the price charged per night. They want to avoid estimating prices that are more than 25 dollars off of the actual price, as this may discourage people.","","Predict Price of Rentel house","","","0"
"6358","1649193","7785819","10/15/2021 05:53:29","Using a sentiment analysis","","Predict User Ratings","","","1"
"6330","1644310","7785819","10/12/2021 21:12:34","From this chart:
Which artists have the most albums?
What years are most albums from?
What are the most common descriptors?","","Perform an EDA","","","3"
"6373","1651057","7785819","10/15/2021 20:35:05","Using a sentiment analysis","","Predict User Ratings","","","1"
"6246","1625341","7785819","10/03/2021 05:41:03","Determine whether any change in the irradiance (i.e., the pulse columns) is due to randomness.

Is there a difference between materials?

Is there a difference between conditions?","","Are there changes in the irradiance when experimental parameters are varied?","If so, are they statistically significant?","","1"
"6042","1578287","7802543","09/08/2021 13:26:11","Commercial banks receive a lot of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this project, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.

1. Credit card applications
2. Inspecting the applications
3. Handling the missing values (part i)
4. Preprocessing the data (part i)
5. Splitting the dataset into train and test sets
7. Preprocessing the data (part ii)
8. Fitting models to the train set
9. Making predictions and evaluating performance
10. Grid searching and making the model perform better
11. Finding the best performing model using ensemble method","","build an automatic credit card approval predictor","","","0"
"5113","1466085","7806547","07/13/2021 22:56:57","## Task Details
Use your magic to make this datas story

## Expected Submission
Submissions that made a data analysis","","Time for analysis","Do some search and tell us what you see","","1"
"5587","1466085","7806547","08/06/2021 21:03:23","## Task Details
Explore the data using R Programming language!","","Analysis with R","","","1"
"5145","1463650","7806547","07/16/2021 11:00:37","## Task Details
Make a general exploring for the dataset

## Expected Submission
Good exploring and a Submission with explanation","","Exploring the data","Data analysis","","3"
"5098","1463650","7806547","07/12/2021 17:16:05","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
Only submissions with MAE less than 0.8 will be accepted .

## Evaluation
Calculate the score using Mean Absolute Error (MAE) .","","Prediction","Food Price Prediction","","4"
"6228","1620962","7847065","10/01/2021 15:43:52","## Expected Submission
- Build a model to identify key factors that affect the U.S home prices the most


### Further help
- https://www.kaggle.com/ankitsharma0467 just message me out if you have any doubts.","","Identify key factors that affect the U.S home prices the most","","","2"
"6874","1729138","7852923","11/19/2021 05:57:42","Perform exploratory data analysis on the dataset. Try to find out the highest number of words in different categories. The authors that write similar quotes and the correlation between quotes of different categories.","","Perform EDA","","","0"
"6327","1643721","7870225","10/12/2021 15:32:58","Finding what stats are most closely correlated with winning a match.","","Exploratory Analysis of Data","","","3"
"6355","1578385","7870225","10/14/2021 14:02:55","What makes for an effective and efficient offensive team?","","Exploratory Analysis","","","0"
"6883","1732820","7879785","11/20/2021 12:16:45","Train a CNN classifier for classification.","","Image classification","","","0"
"6938","1748001","7879785","11/26/2021 12:01:20","Your task is to train a model to classify images of the persons. You can build your own CNN network or use transfer learning. It's an easy challenge and beginner level one.

Important info:
I have purposefully given a small dataset. So to have enough data using data augmentation techniques such as horizontal flip, vertical flip, shear zoom, zoom in,
crop images etc.","","Image Classification","","","0"
"6389","1652339","7933107","10/16/2021 12:53:42","## Task Details
Using a world map to visualize the highest growing populations of the world, without the need to go through tedious tables

## Expected Submission
Submissions should include Notebooks with proper visualizations on a map, with various trends and insights from the dataset

## Evaluation
Proper documentation of steps
Concise and crisp visualizations 
Insightful trends

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Population Explosion - Data visulization","Visualizing which areas of the world have had the highest growth rate of population in the last 15 years","11/30/2021 23:59:00","0"
"6107","1580500","7934764","09/16/2021 15:30:22","## Task Details
When user clicks constituency , show a winner , and their vote percentage and more details in a interactive way.","","Interactive map","","","0"
"6019","1574065","7971521","09/06/2021 11:22:45","The purpose is to derive information form the datasets to better provide informed suggestion to individuals to improve their health and wellbeing on everyday life.","","How can we help individuals to make better lifestyle choices?","","","0"
"6040","1577408","7526686","09/08/2021 06:47:27","You have done a brilliant job. But the scorecard.CSV is a combined one. Can you do it seperately? IPL from 2008-2021 same with one's, two's......?","","Seperate for each format.","","","0"
"6219","1621821","8004208","09/30/2021 17:22:28","This analysis involved me finding out which features of the Bellabeat fitness app were being used the most and strategize a way to stress the importance of the lesser used features.","","An analysis project to help Bellabeat marketing.","Analysis in R language","","0"
"6967","1753715","8014035","11/30/2021 00:21:58","## If you see the helping notebook (https://www.kaggle.com/ebrahimhaquebhatti/predicting-pricing-of-houses-in-pakistan), you will find that it has a huge RMSE, can it be minimized?

Some guidelines that might assist you:
1) Remove ouliers¬∂
While removing outliers it is important to note that outliers for prices in 'rent' houses will have different bounds as compared to outliers of 'for sale' houses.
2) Feature Engineering
None of the features is highly correlated with the output in this approach. Make new features or maybe reconsider the dropped feature to improve the results.
3) Try Different Algorithms
To find which algorithm works best for this problem?
4) Hyper Parameter Tuning
To further improve the score without overfitting.
5) Stacking
Maybe the combination of best-performing algorithms is better than one.","","Reduce RMSE","Predict house prices with minimum RMSE","","0"
"5721","1529037","8072460","08/13/2021 18:32:13","## Task Details
This dataset contains the salary of employees based on their experience.


## Expected Submission
Predict the salary of a new employee based on his/her experience. You can use Simple Linear Regression.

## Evaluation
You can use any method, there's no certain ruleüòâ 

**Hope you'll enjoy it!**","","Predict the salary","","","0"
"5651","1522218","8076882","08/10/2021 06:55:09","Build a 3-channel (RGB) classification CNN with 200 output nodes. Train it for classification on B200C. Reserve 50 instances of each object for validation, and 100 instances each for testing. Submit your architecture and highest categorical accuracy.","","Accuracy Test","Let‚Äôs See What You‚Äôve Got","08/31/2021 23:59:00","4"
"5602","1519089","8084838","08/08/2021 08:49:48","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.dfsfsfsfsf

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?ggggggg

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?gggg

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
gg","","deep learning","","","0"
"6332","1644445","8090615","10/13/2021 00:31:31","## Task Details
Create a fresh notebook, and analyzing data sets with visualizations that help understand the data and underlying patterns.

## Expected Submission
A notebook with all your work.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and Data Visualization","","","0"
"6280","1634739","8090615","10/07/2021 21:37:26","## Task Details
Create a fresh notebook, and analyzing data sets with visualizations that help understand the data and underlying patterns.

## Expected Submission
A notebook with all your work.

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","EDA and Data Visualization","","","0"
"7031","1779849","8092729","12/08/2021 21:19:53","## Task Details
Does Riot have favorite champions? I'd think so. It is quite evident for people that play League of Legends that some champions simply sell more skins than others. Champions like Ezreal, Lux, Miss Fortune, and Alistar each have 14+ skins, which generates a lot of revenue for Riot. Do they balance their game around these - or other - popular champions, though? I think it is worth investigating.","","Compare champion usage across each year","","","0"
"5629","1520310","8093471","08/09/2021 02:33:50","We welcome trials on entity-level sentiment analysis on the validation set. The current label mapping is defined as 

```
label_mapping = {""Neutral"":0, ""Irrelevant"":0, ""Negative"":1, ""Positive"": 2}
```","","Sentiment Analysis","","","1"
"5738","1530722","8129646","08/15/2021 02:02:03","You can do EDA and then rate whether a person is likely to pay the loan or not.","","Classification","","","0"
"6001","1563217","7526686","09/04/2021 10:00:52","For India","","Indian Fc","Same for India.","","0"
"5814","1537314","8162557","08/20/2021 10:50:38","You can easily check the depression severity by dividing the score in the data set by 27 and comparing it to the table in this link: 

https://www.mdcalc.com/phq-9-patient-health-questionnaire-9#next-stepsto the","","Depression severity level","","","0"
"7020","1724232","9099588","12/06/2021 16:56:23","Ponts","","Ponts Pittsburgh","Ponts","","0"
"6223","1622752","8171244","10/01/2021 06:32:57","Calculate excess mortality and standardised mortality ratio for 2020 and 2021.","","Excess Mortality and SMR","","","1"
"6739","1690118","8196430","11/02/2021 14:04:54","The target is Genre and uses different ML Models to predict it. 
Accuracy will be the performance metric.","","predict music genre from features","Classify music into genres","","5"
"6740","1690118","8196430","11/02/2021 14:08:32","EDA and Feature selection","","EDA and Feature selection","","","6"
"6808","1706650","8230640","11/10/2021 05:12:36","Here , you can do EDA . and create different kind of graph.","","Creat EDA","","","1"
"7002","1707389","8230640","12/03/2021 17:21:15","Predict the airtel stock price","","Predict the price of the airtel stock","","","0"
"7022","1773415","8230640","12/06/2021 20:34:37","bulid a Movie Recommendation System","","Movie Recommendation System","","","0"
"7023","1773415","8230640","12/06/2021 20:36:59","Top Movie in the Netflix in EDA. 

like top movie in different  languages , released year, top rating movie","","Top Movie in the Netflix","Top Movie in the Netflix","","0"
"7003","1766154","8230640","12/03/2021 17:36:20","Here you can predict the price of the flat .","","Predict the Flat price","","","2"
"7004","1766154","8230640","12/03/2021 17:37:19","Here you can predict the what is the  EMI amount ??","","Predict the EMI amount","","","2"
"5910","1558592","654191","08/29/2021 12:22:10","Determine top countries for each sport","","Determine top countries for each sport","","","3"
"5984","1568982","8259449","09/03/2021 08:41:34","## Task Details
Exercise 2: Assigning Locations [20 Marks]
ValueBuild Constructions Inc. has to build n ‚àà N different types of factories, one at each of the n
locations. The cost of constructing (setup cost) the i
th facility at the j
th location provided in the
Table 2 below. ValueBuild Constructions Inc. wants to minimize the sum of the costs of assigning
all the facilities to the locations.


## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
 [R] Write a mathematical model to solve the assignment problem explained above. Define all
the variables and constraints clearly. Use appropriate notations and define appropriate sets
to be used in your optimization problem.
2. Construct a pyomo model for this problem for a general n. You can assume that the cost
matrix is given as data from a txt file and can be loaded as a numpy array.
3. Use the data in Table 2 to make a .txt file for your model. You can use the setupcosts.txt
file uploaded in Moodle to create the .txt file. Name the file as lab5 ex2.txt.
4. Copy the file to colab environment.
5. Use numpy.loadtxt to load the data from lab5 ex2.txt file into a numpy array.
6. Adapt the general pyomo model you created, to use the data loaded from the lab5 ex2.txt
file.
7. Use cbc solver to solve your optimization problem. In your code, remember to specify which
variables are integers.
8. [R] Solve the problem and report which facility must be opened at each location.
9. [R] Now change the integer variables in your model to continuous variables, and re-solve the
problem. Report the solution (only the non-zero values of the solution).
10. [R] Are the optimal costs for both problems same? Are the values of the variables still
integer-valued? If yes, explain why.
11. Will the solution to the continuous problem become fractional (non-integer) if the costs are
changed to non-integer values? Try changing the costs to different values and test whether
the solution to the LP becomes fractional for any of them.","","Bala sir lab 5","IITB","09/30/2021 23:59:00","0"
"7024","1733312","8266744","12/07/2021 02:34:44","## Task Details
Fill in the details for each dataset.","","Create descriptions for the datasets","","12/12/2021 23:59:00","0"
"6444","1658312","8267498","10/23/2021 22:44:45","TASK:
Using Wikidata, gather linked data (triple structure) for the following 8 movies.

HOW:
You can use the Wikidata API, Neo4j's Wikidata loader called Neosemantics, or you can do it by hand if you are not familiar with automatically loading Wikidata.

Neo4j created a handy query to help load the Wikidata RDF here on the git repo for this project: https://github.com/cj2001/spooky_movie_graph/blob/main/cypher_queries/create_rdf_data.cql

TARGET DATA:
Top 8 Movies Deemed ""Cursed"" by Hollywood:
Poltergeist (1982)
The Wizard of Oz (1939)
The Alamo (1960)
War Games (1983)
Le Mans (1971)
The Sixth Sense (1999)
The Shining (1980)
Amityville Horror (1976)","","(Optional) Task 1. Load ""Cursed"" Movie Wikidata to Neo4j","Using Wikidata API, Neosemantics, or manually","","0"
"6691","1658312","8267498","10/27/2021 13:00:40","TASK 3
Some data was not parsed correctly between Wikidata and CSV, go through the loaded from Task 2 and identify duplicate nodes that need to be merged.

HOW
Find the duplicate nodes for War Games and Poltreguist and add the nodes to the ""real"" or primary nodes for those movies.","","Task 3: Clean the Data","","","0"
"6692","1658312","8267498","10/27/2021 13:03:15","TASK 2
A manually curated list of ""cursed"" data points from media articles discussing the ""cursed"" nature of the movie was created for this task. Only data points that could be verified were used.
Load the CSV into the Wikidata cursed movie dataset already loaded from Task 1, or use the query listed below to load both the Wikidata and CSV graph.

HOW
You can follow along with Neo4j's walkthrough,or use the handy query Neo4j has created for this task.
Neo4j Walkthrough: https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/
Link to git repo for query:
IMPORTANT!!!! Make sure to use the queries in this file IN ORDER
https://github.com/cj2001/spooky_movie_graph/blob/main/cypher_queries/graph_import.cql","","Task 2. Load CSV to create ""Cursed Movie"" graph in Neo4j","Load CSV or use Neo4j query to load Wikidata+CSV","","0"
"6693","1658312","8267498","10/27/2021 13:07:25","TASK 4
Now that you have your base graph from Task 3, analyze the data to identify which node types and relationships would be best to answer the question, what characteristics do ""cursed"" and non-cursed movies have in common?

HOW
Find the relations and nodes that suit your needs and update the relations to form the same relationship between those nodes types.","","Task 4: Create a Monopartite graph","","","0"
"6694","1658312","8267498","10/27/2021 13:11:09","Task 5:
We now need to understand what nodes have the most relations and which clusters of nodes are connected to one another to answer the question, are there any major characteristics that are shared between ""cursed"" and non-cursed movies.

HOW and WHY:
In neo4j, run these two algorithms on the mono[artite graph created from Task 4:

Betweenness centrality algorithm to understand which nodes have the most relations, in other words, which characteristics have the most interconnections between these movies? 
Next, run a clustering coefficient to understand which node clusters are more likely to be connected to one another.","","Task 5: Run two graph algorithms","Betweenness centrality AND clustering coefficient","","0"
"6027","1575558","8294679","09/07/2021 06:08:54","## Task Details
Develop a classification model.","","Malaria Detection","","","0"
"6852","1722885","8294779","11/16/2021 06:18:21","## Task Details
1.1	Problem statement:
Data analysis team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights, your team will design a new marketing strategy to convert casual riders into annual members.
Design marketing strategies aimed at converting casual riders into annual members
why casual riders would buy a membership, and how digital media could affect their marketing tactics

## Expected findings
How do annual members and casual riders use Cyclistic bikes differently?","","Case Study: How Does a Bike-Share Navigate Speedy Success?","Part of google grow data analytics track","","0"
"6051","1576113","8263246","09/10/2021 06:46:05","A neural network could be used to classify the sound files.
The neural network will learn the differential characteristics to perform the classification.
Let's test a sound classification network by training it with the cough files.","","Neural network for sound classification.","","09/13/2021 23:59:00","2"
"6665","1668099","8328047","10/24/2021 11:08:42","## Task Details
This dataset has 5 classes of animals. You are required to develop model to predict the correct category of the animal class.","","Predict Animal Class","","","0"
"6077","1587862","8344248","09/13/2021 23:34:28","## Task Details
I am curious to see if there are any biases in the dataset between male/female participants, as well as if weight or age skew any of the data.","","Biases","Any biases within the dataset","09/17/2021 23:59:00","0"
"6329","1644108","8350476","10/12/2021 18:35:05","## Task Details
Every day, dozens of Redditors make anonymous confessions about some misdeed they have done, some controversial opinion they hold, or something else entirely. Most are supported and upvoted... some are not. What is the difference? Is it the poster's writing proficiency, the subject matter?

## Expected Submission
Make a notebook that tries to find the answer to the question - what makes a confession unsympathetic?","","What does Reddit not forgive?","Not all confessions are equally appreciated.","","3"
"6317","1642092","8350476","10/11/2021 19:50:43","## Task Details
Many people ask questions on Reddit. Many people have their questions die, getting neither any answers nor any upvotes. What's the difference between a question that is forgotten in its infancy, and the one that reaches the top of /r/all?

## Expected Submission
Give a notebook that contains a function, that, given a post's post time and its title, returns an expected score.

## Evaluation
A good solution will generalize well onto an unseen validation set. You can leave some questions out to test yourself, or use [SocialGrep](https://socialgrep.com) to generate another dataset to test yourself on.","","What makes a good Reddit question?","Predict a question's score by its title and post time.","","1"
"6826","1693342","8861823","11/12/2021 01:10:09","## None","","Five Years of AAPL on Reddit","2021 - BigData HomeWork","02/01/2022 23:59:00","0"
"6753","1695457","8350476","11/04/2021 21:32:14","## Task Details
As we've seen over the last few years, the funny dog meme is quite versatile. What if we want to split Doge into its different meanings? We could start with a simple one. 

## Expected Submission
Your classifier should take a comment, and classify that as a comment talking about a dog character that took the world by storm, or a comment talking about the cryptocurrency based on that character.

## Hints
Look into Logistic Regression and Naive Bayes - these might help you get a quick start. After that, the sky is your limit :)","","The Doge EDA challenge","Split discussion of DOGE the currency from Doge the dog","","1"
"6676","1671340","8350476","10/25/2021 20:32:56","## Task Details
This dataset has a million jokes. When you have this many, not all of them will be hitters. That's true for our dataset as well - not all jokes are good ones.

Thankfully, many Redditors have voted on those jokes' popularity. Can you predict how they would vote on a new joke?

## Expected Submission
Show us a feature, or a set of features, or a model that tells better jokes apart from worse ones. We all would like to impress our friends with something funny :)

## Evaluation
We won't score submissions in any official capacity. Give it your best, and it will be good enough.","","What makes a joke funny?","Tell good jokes from bad ones before they even hit Reddit","","1"
"6117","1595405","8374847","09/17/2021 14:05:03","##Aim 
Your task will be to predict the sales of next seven days (i.e. 1st Aug 2019 to 7th Aug 2019).Use sklearn in python.Do some feature engineering.
## Expected Submission
 Output file (should be a csv file with same format that I have shared, dates ranging 01/08/2019 to 07/08/2019).

## Evaluation
Your scores will be calculated based on the difference between predicted value and actual sales.","","Sales prediction for next 7 days","Use SKlearn","09/20/2021 00:00:00","0"
"6919","1602712","8392179","11/24/2021 06:31:18","stset","","testfortest","test","","0"
"6882","1732554","8392179","11/20/2021 09:58:02","","","Multi class Classification","try","","4"
"6872","1728455","8501190","11/18/2021 16:27:02","Create a prediction model helping predict student performance. This can be used to help educational institutions anticipate and improve student performance.","","Student performance prediction","","","5"
"6288","1637697","8400486","10/09/2021 09:39:56","It uses the previous year data to predict the current winner","","IPL Winner","","","0"
"6167","1603516","6466279","09/23/2021 07:20:17","The effect of vaccines on the disease is controversial in many countries. 

For this reason, does vaccination using the data have an effect on the number of cases or the death rate? 

If so, does it differ from country to country?","","Covid Data Correlations","","","1"
"6168","1603516","6466279","09/23/2021 07:22:55","To prepare a graph to find the breakdown of the average death toll due to disease by country.","","Covid Vaccination Group by Countries","","","4"
"6324","1625050","8423753","10/12/2021 12:36:20","During the interview, show your Python notebook and answer the following questions. You have 5 minutes max.

1)Show data (how many classes/features/examples? Why did you choose it?) and comment on visualization and pre-processing. 

2)Comments on features (how did you evaluate the features and find the best ones?). 

3)Show classification results (using different features).

4)What are your conclusions.","","Basics","","","0"
"6870","1728152","8427670","11/18/2021 08:04:37","## Task Details
Statistical analysis of YouTube over time.üëç 

## Expected Submission
A notebook showing the statistical analysis with different visual representation.üíØ 

## Evaluation
A clear visual representation!!‚úîÔ∏è","","Statistical analysis of YouTube over time.","","11/30/2021 23:59:00","0"
"6249","1626185","8458083","10/03/2021 16:40:29","**Clean**
-  Do we have anything?

**Merge**
sales.sku = stock_moves.sku

**Calculate**

The sales and inventory transactions can be correlated to produce the following
- Item based sales forecast
- Item based re-order signals
- warehouse and replenishment optimization to save warehouse space
- Detection of loss/thef (Ordered&lt;&gt;Moved Off Shelf)","","Prep, Clean and Merge","","","0"
"6248","1626569","8485536","10/03/2021 16:21:14","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","This is a test","","10/10/2021 23:59:00","0"
"6325","1640731","8487941","10/12/2021 12:50:21","## Task Details

**Linear Regression Model**
Produce the Regression Line. The best fit line should have the least square distance between the original data points and the predicted values.


## Submission
Mean Square Error and R2 Score.","","Linear Regression Model","Stock Price Prediction and Produce Mean Square Error and R2 Score","","1"
"6380","1643228","8487941","10/16/2021 06:54:38","## Task Details
Relation between Population and Covid-19 Cases.

## Expected Submission
The plot of Population and Covid-19 cases using Plotly.","","Relation between Population and Covid-19 Cases","","","1"
"6377","1651669","8487941","10/16/2021 06:40:14","## Task Details
Linear Regression Model
Produce the Regression Line. The best fit line should have the least square distance between the original data points and the predicted values.

## Submission
Mean Square Error and R2 Score.","","Linear Regression Model","Stock Price Prediction and Produce Mean Square Error and R2 Score","","1"
"6408","1659357","8487941","10/20/2021 02:46:29","## Task Details
Linear Regression Model
Produce the Regression Line. The best fit line should have the least square distance between the original data points and the predicted values.

## Submission
Mean Square Error and R2 Score.","","Linear Regression Model","Produce Mean Square Error and R2 Score","","1"
"6723","1683624","8487941","10/30/2021 12:16:44","Use Statistical Graphical Data Visualization methods.","","Exploratory Data Analysis","Exploratory Data Analysis on COVID-19 Statewise Vaccine Doses Dataset","","0"
"6678","1671668","8557619","10/26/2021 06:45:42","1. Total number of occupation through which person of the year is selected 
2. Total number of years for which data is given
3. Total number of peaple who has been selected for time cover
4. From which year person of year award started
5. Which year has most time covers
6. Which occupation has most time covers and its graphical representation","","EDA for analysing data using data science.","Some important information about dataset is to be explored using python","10/31/2021 23:59:00","0"
"6245","1625561","8497967","10/03/2021 04:03:29","Predicting whether employees will leave through data","","Employee Prediction","","12/03/2030 23:59:00","0"
"6247","1625875","8498917","10/03/2021 08:52:40","## Task Details
This heat map will show the importance of bradycardia episodes

## Expected Submission
notebook generating Heat Map using com.samsung.shealth.tracker.heart_rate.202109221041.csv file

## Evaluation
easy to run
at least as good as my excel ouput

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Heat Map for Minimum Heart Rate","Minimum Heart Rate x Hour x number of times","","0"
"6722","1673874","8551497","10/30/2021 12:00:21","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.
ÎπÖÎç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∏∞ÏÇ¨ Ïã§Í∏∞ Í≥µÎ∂ÄÌïòÍ∏∞ 
ÌååÏù¥Ïç¨ÏùÑ ÌÜµÌïú mtcars 
MinMaxScaler Ïã§Ïäµ

copy edit 
Î•º ÌÜµÌï¥ÏÑú Í∏∞Ï∂úÎ¨∏Ï†úÎ•º Ïã§ÏäµÌï† Ïàò ÏûàÏäµÎãàÎã§ 

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","[Python_1] mtcars  MinMaxScaler","mtcars","12/31/2022 23:59:00","1"
"6756","1673874","8551497","11/05/2021 04:06:13","[Python_3] Logistic Regression Sample data","","[Python_3] Logistic Regression Sample data","[Python_3] Logistic Regression Sample data","12/31/9999 23:59:00","1"
"6757","1673874","8551497","11/05/2021 04:16:01","[Python_4] RandomForest , SVC","","[Python_4] RandomForest , SVC","[Python_4] RandomForest , SVC","12/31/9999 23:59:00","1"
"6758","1673874","8551497","11/05/2021 04:37:22","[Python_5] Data Search & Data PreProcessing

import pandas as pd
data=pd.read_csv('Ex_CEOSalary.csv', encoding='utf-8')
data.info()


 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   salary    209 non-null    int64  
 1   sales     209 non-null    float64
 2   roe       209 non-null    float64
 3   industry  209 non-null    int64","","[Python_5] Data Search & Data PreProcessing","[Python_5] Data Search & Data PreProcessing","12/31/9999 23:59:00","1"
"6754","1673874","8551497","11/05/2021 01:40:42","## Task Details

Department Store Customer Regression Project 

use 3500 custommer data , 
and 
you can regress customer sex(male, female)


1. Read CSV file ( import pandas as pd )
a) pd.read_csv ( x_train  , x_test  ,y_train )
b) dataframe. iloc or loc data 


2. Data Preprocessing (from sklearn.preprocessing import LabelEncoder)
a) null point  fillna(0)
b) encording korean data  to numberic 

3. Logistic Regression ( from sklearn.linear_model import LogisticRegression) 
a) logistic regressing fit data ( train data )
b) check the score 
c) predict_proba 


4. export CSV ( dataframe.to_csv) 
a) to_csv  use path 



by Bigdata Exam KR (Korea Data Exam)

#big data exam study korea","","[Python_2] Department Store Customer Logistic Regression (Male Female )","[Python_2]  Department Store Customer Logistic Regression (Male Female )","12/31/9999 23:59:00","1"
"6806","1673874","8551497","11/10/2021 00:23:11","[Python_8] Random Search Hyperparameter

1) Data Read
pd.read_csv

2) Data Split
x y data set

3) Train , Test Data split
from sklearn.modelselection import traintest_split

4) Random Search 
from sklearn.model_selection import RandomizedSearchCV
Logistic Regression
Hyper prarmeter find
check Result","","[Python_8] Random Search Hyperparameter","[Python_8] Random Search Hyperparameter","12/31/9999 23:59:00","1"
"6790","1673874","8551497","11/09/2021 01:12:40","[Python_7] Grid Search

1) Data Read 
- pd.read_csv 

2) Data Split
- x y  data set 

3) Train , Test Data split 
- from sklearn.model_selection import train_test_split 

4) Grid Search
-from sklearn.model_selection import GridSearchCV
- Logistic Regression 
- Hyper prarmeter find 
- check Result","","[Python_7] Grid Search","[Python_7] Grid Search","12/31/9999 23:59:00","1"
"6773","1673874","8551497","11/07/2021 04:53:43","[Python Basic] Data Change Add ( Age type Add )

[Python Basic] Data Î≥ÄÍ≤ΩÌïòÍ∏∞(ÎÇòÏù¥Î•º 10ÎåÄ, 20ÎåÄÎ°ú)","","[Python Basic] Data Change Add ( Age type Add )","","12/31/9999 23:59:00","1"
"6781","1673874","8551497","11/08/2021 06:08:03","1. Data read_csv

2. Train Data , test Data Split 

3.  Min Max Sclaler 

4. Standard Scaler 

5. Model Score Check","","[Python_6] Regularization","","12/30/9999 23:59:00","1"
"6817","1673874","8551497","11/11/2021 04:29:59","[Python_8] KNeighborsClassifier

1) Data Read
pd.read_csv

2) Data Split
x y data set

3) Train , Test Data split
from sklearn.modelselection import traintest_split

4) KNeighborsClassifier  
from sklearn.neighbors  import KNeighborsClassifier  
Hyper prarmeter find
check Result

5) KNeighborsRegessor 
from sklearn.neighbors  import KNeighborsRegessor  
Hyper prarmeter find
check Result","","[Python_9] KNeighborsClassifier  Regressor","[Python_9] KNeighborsClassifier  Regressor","12/31/9999 23:59:00","1"
"6830","1673874","8551497","11/12/2021 08:44:06","[Python_10] BayesianRidge

1. get Data
 pd.read_csv

2. proprocessing

3. BayesianRigde
- basic model","","[Python_10] BayesianRidge","[Python_10] BayesianRidge","12/31/9999 23:59:00","1"
"6851","1673874","8551497","11/16/2021 01:26:05","[Python_11] BaggingClassifier

get Data
pd.read_csv

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import MinMaxScaler


BaggingClassifier

BaggingRegressor","","[Python_11] BaggingClassifier  BaggingRegressor","[Python_11] BaggingClassifier BaggingRegressor","12/31/9999 23:59:00","1"
"6869","1673874","8551497","11/18/2021 07:19:30","[Python_13] ensemble Stacking 

how to use 
[Python_13] ensemble StackingClassifier
[Python_13] ensemble StackingRegressor","","[Python_13] ensemble Stacking","[Python_13] ensemble Stacking","12/31/9999 23:59:00","1"
"6876","1673874","8551497","11/19/2021 08:29:53","[Python_14] Ridge","","[Python_14] Ridge","[Python_14] Ridge","12/31/9999 23:59:00","1"
"6862","1673874","8551497","11/17/2021 08:58:43","[Python_12] AdaBoostClassifier","","[Python_12] AdaBoostClassifier","[Python_12] AdaBoostClassifier","12/31/9999 23:59:00","1"
"6995","1673874","8551497","12/03/2021 05:40:41","[FINAL_CHECK] Python type 1


[FINAL_CHECK] Python type 1
IQRÍµ¨ÌïòÍ∏∞  quantile ÏÇ¨Ïö© 
ÎÇòÏù¥Î°ú Í∞í Ï†ïÎ†¨ ÌïòÍ∏∞ 
Ï°∞Í±¥ sex=femal  Survived = 1 ÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ ÎΩëÍ∏∞ 



[FINAL_CHECK] Python type 2
Î∞±ÌôîÏ†ê ÏÉòÌîåÎç∞Ïù¥ÌÑ∞ 
read_csv 

## EDA 
slicing 
fillna()

##Preprocessing 
LabelEncoding

#sklearn 
LogisticRegression 
predict_proba

#result concat 

#to_csv","","Final Check_Type1 _ Type2","Final Check_Type1 _ Type2","12/31/9999 23:59:00","1"
"6991","1673874","8551497","12/02/2021 08:11:45","Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò :https://www.kaggle.com/rsrishav/youtube-trending-video-dataset?select=KR_youtube_trending_data.csv

(Ï∞∏Í≥†, Îç∞Ïù¥ÌÑ∞ ÏàòÏ†ï)

Îç∞Ïù¥ÌÑ∞ ÏÑ§Î™Ö : Ïú†ÌäúÎ∏å Îç∞ÏùºÎ¶¨ Ïù∏Í∏∞ÎèôÏòÅÏÉÅ (ÌïúÍµ≠)

‚Äã

dataurl =https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv

‚Äã

Îç∞Ïù¥ÌÑ∞ ÏÉÅÏúÑ 5Í∞ú Ïª¨Îüº

baseline ÏΩîÎìú","","[python_type_1] Youtube Data precessing","[python_type_1] Youtube Data precessing","10/14/2031 23:59:00","1"
"6975","1673874","8551497","12/01/2021 00:06:47","[python_type_1] Ïã§Í∏∞ 1Ïú†Ìòï ÏµúÏ¢ÖÎ≥µÏäµ2

Ï∂úÏ≤ò : https://m.cafe.naver.com/ca-fe/web/cafes/19039057/articles/21060","","[python_type_1] Exam Type 1 Review","[python_type_1] Exam Type 1 Review","01/01/9999 23:59:00","1"
"6928","1673874","8551497","11/25/2021 13:31:10","[Python_19]chipotle Data Analysis
ÏãùÎãπÎç∞Ïù¥ÌÑ∞ Ïó∞Í¥ÄÎ∂ÑÏÑù
ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ VS Ïó∞Í¥ÄÍ¥ÄÍ≥Ñ
-ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ : Îç∞Ïù¥ÌÑ∞AÍ∞Ä Î≥ÄÌôîÌï† Îïå Îç∞Ïù¥ÌÑ∞BÍ∞Ä Î≥ÄÌïòÎäî ÏòÅÌñ•Ïùò Ï†ïÎèÑ
-Ïó∞Í¥ÄÍ¥ÄÍ≥Ñ : Îç∞Ïù¥ÌÑ∞AÍ∞Ä ÏûàÏùÑ Îïå Îç∞Ïù¥ÌÑ∞BÍ∞Ä Í∞ôÏù¥ ÏûàÎäî Ï†ïÎèÑ



ÏÜêÎãò : ""Ïñ¥Îñ§ Î©îÎâ¥Î•º Ï£ºÎ°ú Î®πÎÇòÏöî?""

ÏßÅÏõê : ""ÌÜ†ÎßàÌÜ†ÌååÏä§ÌÉÄÏôÄ ÎßàÎ•¥Í≤åÎ¶¨Îî∞ ÌîºÏûêÎ•º Ìï®Íªò Ï∂îÏ≤úÎìúÎ†§Ïöî""



Ï∞∏Ï°∞ÏÇ¨Ïù¥Ìä∏ : https://tjansry354.tistory.com/10?category=927195","","[Python_19]chipotle Data Analysis","[Python_19]chipotle Data Analysis","12/31/9999 23:59:00","1"
"6931","1673874","8551497","11/26/2021 00:38:27","[Python] Basic How to use help() dir()

import pandas as pd 

print(dir(pd))

Í∏∞Î≥∏Ìï®Ïàò 
## pefume memo 
#prepropcessing
#feature_selection
#feature_extraction 
#metrics
#model_selection 


Î™®Îç∏ Ïô∏Ïö∞Í∏∞ 
ÏïôÎ¶¨Í∞Ä ÎÑ§ÎÑ§ÏπòÌÇ®ÏùÑ Î®πÏúºÎ©¥ÏÑú Ïä§ÌÉÄÌÅ¨ÎûòÌîÑÌä∏Î•º ÌïúÎã§

ÏïôÎ¶¨
ensemble linear_model

ÎÑ§ÎÑ§ÏπòÌÇ®
navie_bayes neighbors

Ïä§ÌÉÄÌÅ¨
svm tree cluster","","[Python] Basic How to use help() dir()","[Python] Basic How to use help() dir()","12/31/9999 23:59:00","1"
"6933","1673874","8551497","11/26/2021 01:04:59","Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò :https://www.data.go.kr/data/15051872/fileData.do(Ï∞∏Í≥†, Îç∞Ïù¥ÌÑ∞ ÏàòÏ†ï)

Îç∞Ïù¥ÌÑ∞ ÏÑ§Î™Ö : ÏÑúÏö∏ÌäπÎ≥ÑÏãú_Í≥µÍ≥µÏûêÏ†ÑÍ±∞ ÏãúÍ∞ÑÎåÄÎ≥Ñ Ïù¥Ïö©Ï†ïÎ≥¥

‚Äã

data url = https://raw.githubusercontent.com/Datamanim/datarepo/main/bicycle/seoul_bi.csv

‚Äã

Îç∞Ïù¥ÌÑ∞ ÏÉÅÏúÑ 5Í∞ú Ïª¨Îüº

baseline ÏΩîÎìú


[Ï∂úÏ≤ò] 13Ï£ºÏ∞® ÏòàÏÉÅÎ¨∏Ï†ú (Ïã§Í∏∞1Ïú†Ìòï 10Î¨∏Ï†ú) (Ïù¥Í∏∞Ï†Å Ïä§ÌÑ∞Îîî Ïπ¥Ìéò / Ïª¥Ìôú,Ï†ïÎ≥¥Ï≤òÎ¶¨,ÎπÖÎ∂ÑÍ∏∞) | ÏûëÏÑ±Ïûê ÎπÖÎ∂ÑÍ∏∞ Ïã§Í∏∞Ï°∞Íµê","","[Python_20] Seoul Bicycle Use Data","[Python_20] Seoul Bicycle Use Data","12/31/9999 23:59:00","1"
"6920","1673874","8551497","11/24/2021 08:33:54","# 3Ïõî1Ïùº ÌôïÏßÑÏûêÍ∞Ä 100Î™Ö  Ïù¥ÏÉÅÏù∏ Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌïòÏó¨ Íµ≠Í∞ÄÎ™ÖÏπ≠, ÎÇ†Ïßú, ÌôïÏßÑÏûêÏàò ÌëúÏãú ÌïòÎùº

# condition count 
1. date   2020-03-01
2. ConfirmedCase count &gt; 100","","[Python_18] COVID_DATA_confirmedCase count by date","[Python_18] COVID_DATA_confirmedCase count by date","12/31/9999 23:59:00","1"
"6894","1673874","8551497","11/21/2021 23:58:58","## 25ÎÖÑ Ïù¥ÏÉÅ Îêú ÏßëÏùò room Í∞ØÏàò 5Í∞úÏù¥ÏÉÅ 
cond1=( data['housing_age']&gt;25) & (data['rooms']&gt;5)

##Ï°∞Í±¥ÎßåÏ°± house_value_ÌèâÍ∑†
 data[cond1]['house_value'].mean()","","[Python_16] Data Condition mean","[Python_16] Data Condition mean","12/30/9999 23:59:00","1"
"6905","1673874","8551497","11/23/2021 04:01:04","[Python_17] Covid Data Preprocessing","","[Python_17] Covid Data Preprocessing","[Python_17] Covid Data Preprocessing","12/31/9999 23:59:00","1"
"6959","1673874","8551497","11/29/2021 08:42:40","[python_type_1] Ïã§Í∏∞ 1Ïú†Ìòï ÏµúÏ¢ÖÎ≥µÏäµ

Ï∂úÏ≤ò: https://m.cafe.naver.com/ca-fe/web/cafes/yjbooks/articles/21979?useCafeId=false&or=memo.naver.com&buid=21e18a18-2eed-4e6f-8950-6bbe21eeca4c&art=ZXh0ZXJuYWwtc2VydmljZS1uYXZlci1ldGMtZm9yLWNvbW1lbnQ.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJjYWZlVHlwZSI6IkNBRkVfSUQiLCJhcnRpY2xlSWQiOjIxOTc5LCJpc3N1ZWRBdCI6MTYzODE3NTAzNjA3NywiY2FmZUlkIjoxOTAzOTA1N30.uqt9lZLdGPR4Lz2Ll6ATWB2xZFvCu99mNIX3zJW9Pq8","","[Python_type_1] Final Check type_1","[Python_type_1] Final Check type_1","12/31/9999 23:59:00","1"
"6760","1696172","8551497","11/05/2021 06:52:52","Section 1 ) GET API DATA


You must change the private Certification key. 

https://ecos.bok.or.kr/jsp/openapi/OpenApiController.jsp?t=main","","Section 1 ) GET API DATA","Section 1 ) GET API DATA","12/31/9999 23:59:00","0"
"6301","1640188","8554435","10/10/2021 23:23:29","## Task Details
It is necessary to predict which laptop (Notebook_SerialNumber) will have a service request based on the service history of laptops in the past period. It is also desirable to predict the type of service request (ServiceType).

## Expected Submission
Colab Notebooks

## Evaluation
The decision should predict the event in the next month. The probability of prediction is not less than 80%. An event means a service (ServiceType) of a laptop (Notebook_SerialNumber).

The term ""next month"" means the next month after the period during which the model was trained:
1. If the model is trained on data from December 2019 to January 2021, then the prediction should be for February 2021.
2. If the model is trained on data from December 2019 to February 2021, then the prediction should be for March 2021.
3. If the model is trained on data from December 2019 to March 2021, then the prediction should be for April 2021.
4. And so on.","","Predicting laptop service requests in the next month","","10/13/2021 23:59:00","1"
"6310","1640739","8564727","10/11/2021 07:09:06","## Task Details
Every task has a story. Tell users what this task is all about and why you created it.

## Expected Submission
What should users submit? Should they solve the task primarily using Notebooks or Datasets? What should the solution contain?

## Evaluation
What makes a good solution? How do you evaluate which submission is better than another?

### Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)
feqafewfaef","","ffffhuie","gg","11/11/2021 23:59:00","0"
"7034","1780959","8595330","12/09/2021 08:52:22","Task: Outlier Detection and Removal Using Python","","Outliner Detection and Removal","","12/31/2021 23:59:00","0"
"6921","1742362","8603991","11/24/2021 09:24:17","This task is created to determine the top countries where Nigerian imported goods comes from in 2019, it will help us to understand the amount of dependency the country has on the world's major economic powerhouses like the US and China.
A user can submit a dataset containing the answer to the task or a visualization of the final results. The result should contain the names of the countries, the total volume and value of goods in USD and the major types of goods imported from such country.","","Top import trading partners of Nigeria in 2019","","","0"
"6819","1709188","8657926","11/11/2021 09:39:46","You are free to use any ML model. Try to perform different model and see which model give the better accuracy","","Make a prediction of video game sales","","","0"
"6431","1662806","8662650","10/21/2021 14:50:17","Introduction
will be working with a specified dataset of images.  You will then explore the data and try the statistical learning approaches that we have covered in this course to tackle the task associated with the dataset.  The statistical approaches should cover both conventional machine learning (i.e. not deep learning), from the first half of the unit, and deep learning from the second half.  Which methods you choose are (mostly) entirely up to you; a goal of the project is for you to explore the approaches you've been taught, or perhaps beyond those, in order to build a high-performing system.

We're specifying an image dataset as this is one of the types of data to which deep learning has been most extensively applied, as in the MNIST dataset used as a running example in the Geron textbook.

The image dataset will have an associated classification task.  The goal of your statistical learning approaches is to build models that will perform well on that classification task.  To evaluate how well you do, we'll be using both a public test set and a private test set.  You'll have access to the public test set for the whole duration of the project, so you can see how well your system is performing.  The private test set, which will only be released at the end of semester for a limited period, is to see how well your system generalises to an unseen dataset.

Project Task
For this, you'll be working with a dataset of human faces.  Your task is to predict the age group of the person whose face is in the image.  
We expect that, by default, you'll be developing your solutions under Google Colab.  Images are relatively large data items, so we've produced a cut-down version of the dataset that should not cause any problems with Colab's memory limits.  It's cut down both in terms of image size, and number of items in the dataset.  This reduced dataset is available in Kaggle, although you can use whatever data you like to train your model.  We also make available a devset that's separate from the training set.

For testing, there'll be a public test set and a private test set.  The private test set will be released for a limited time in the last week of class.

Special Features
While you are mostly free to explore different conventional machine learning and deep learning methods, there are two things that you are expected to try during your exploration:

HOG features.  This will apply only to conventional ML.  It is possible to just use the raw data as input to a conventional ML, as the textbook does in Ch 3.  However, image processing has a long history of extracting useful features that help with the relatively high dimensionality of the input, such as HOG features.  You can read about these in another OReilly book, Hands-On Image Processing with Python by Sandipan Dey (chapter ""Extracting Image Features and Descriptors""), which you can access via the OReilly website in the same way as the textbook.  You do not have to use these features as input with all your attempts using conventional ML, but should use them for at least one.
Multiple instances of images of the same person.  In this dataset, you will see that the label of each image contains an ID of the person in that image, and that in general you may have multiple images of a particular person.  In at least one of your models, you should take this into account.  (You can take it into account in, for example, a post-processing step, aggregating the predictions for all of the images with the same ID.)


As noted in the project Description, you'll be working with a dataset of human faces.  The images have dimensionality 100x100, and are (mostly) colour.

The data is split into train_images.npy, val_images.npy and test_images.npy, with a test_private_images.npy to come later.  Labels are also available for the training data and validation data, in train_labels.npy and val_labels.npy respectively.  (npy files are a standard format for files using numpy.)  

The labels have the form x-y, where

x is an identifier specific to an individual; and
y is the part of the label that corresponds to age.
As an example, the label 12726-37 corresponds to a person who is 37 years old, with ID 12726.  (This happens to be the second person in the devset, the actor Gabrielle Anwar.)

The ages fall into 3 groups: ages 6-20, 35-40, and 55-98.  There can be multiple images of the same person: there are several images of Gabrielle Anwar, ID 12726.  The dataset is a noisy one, scraped from the web, so not all the images that should be of the same person, with the same ID, are in fact of that person.  (The ninth image in the devset, which also has ID 12726, is of another actor from the series Burn Notice that Gabrielle Anwar was a lead actor in.)  In the provided dataset, it is guaranteed that all of the images with a particular ID will be in the same age group","","Minstum","","10/29/2021 23:59:00","0"
"6439","1666443","8668806","10/23/2021 12:38:48","## Task Details
It is also tough to figure the best cars from the user's need. So, you are responsible to list the best cars for the user from the given data.

## Expected Submission
You must ask the user some details which can help you give the best car to the user. You should also use the data. You should not ask the user more than 4 questions.

## Evaluation
If the given rules are not followed then they would not be accepted!","","State a Car for the User","","","1"
"6729","1665081","7912008","10/31/2021 14:11:16","practice","","resnet","samad","","0"
"6675","1670458","8695881","10/25/2021 13:09:14","## Dataset
The dataset comes from a service from which anyone present on the French territory benefits without social, cultural or administrative distinction (with or without papers). Nationalities have only been inferred from individuals' last names.

## Task Details
The text below is based on an article from the French Observatory for Immigration and Demography entitled: [The ¬´ Great Replacement ¬ª: Fantasy or Reality?](https://observatoire-immigration.fr/grand-remplacement/) 
The notion of ¬´&nbsp;great replacement ¬ª in France now haunts editorials, social networks and major audiovisual media platforms, but places of power and simple family discussions.
The importance of migratory flows, coupled with the birth rate of immigrants or of immigrant origin, resulted in 11% of the population residing in France being immigrant in 2017 and 25% being of immigrant origin - counting children of the second generation from immigration - according to figures from the French Office for Immigration and Integration (OFII) published in October 2018. This represents a quarter of the French population. And these are all stocks - that is, what is and not what will be in the future, as a result of migratory flows and future births.
However, it is necessary to take into account the fertility differential between women descending from indigenous peoples (less than 1.8 children per woman on average in 2017), women descending from immigrants (2.02 children per woman on average) and immigrant women (2.73 children per woman on average). This fertility varies greatly according to the origin of the women: 3.6 children per woman on average for Algerian immigrants, 3.5 children per woman for Tunisian immigrants, 3.4 children per woman for Moroccan immigrants and 3.1 children per woman for Turkish immigrants, which is higher than the fertility of their country of origin (respectively 3; 2.4; 2.2; 2.1).
Over the same twenty-year period, between 1998 and 2018: 
    ‚Ä¢ The number of births to children with both French parents fell by 13.7%. 
    ‚Ä¢ The number of births of children with at least one foreign parent increased by 63.6% 
    ‚Ä¢ The number of births to children with both foreign parents increased by 43%. 
In 2018, almost a third of children born (31.4%) had at least one parent born abroad.
While a part of the French political class remains in denial about this phenomenon and its consequences, officials in other countries source of immigration, have openly claimed this contemporary mode of conquest since the 70s: 1974, former Algerian President Houari Boumedienne said in a U.N. speech: ‚ÄúOne day, millions of men will leave the Southern Hemisphere to go to the Northern Hemisphere. And they will not go there as friends. The wombs of our women will give us victory.‚Äù A precisely anti-France hatred is even cultivated by certain African states for which France happens to be the perfect scapegoat for the failure of their successive policies. For Algeria, this hatred even goes so far as to be included in its national anthem (cf. [[Wikipedia] National anthem of Algeria](https://en.wikipedia.org/wiki/Kassaman)).

## Expected Submission
Using the data provided, support a diagnosis on the current state and **future** of the French civilization. And if the replacement of the French population and its customs a fantasy or reality?

### Further help
- 2020 France population pyramide : https://www.insee.fr/fr/statistiques/2381472 
- For cartographic representations, the projection RGF93 / Lambert93, reference EPSG:2154  or IGNF:LAMB93 should be favored.","","Future of French civilization","The ¬´ Great Replacement ¬ª: Fantasy or Reality?","","1"
"6737","1689167","8726614","11/02/2021 04:42:31","The dataset contains the information about the people's posts on Twitter and with a likes data .so with the help of NLP we may predict the number of likes of our post also .......lets do this...","","Predict the number of likes based on the post","","","0"
"6950","1752874","8773098","11/28/2021 15:20:30","&gt; ‚ÄúFix the cause, not the symptom.‚Äù ‚Äì Steve Maguire&gt; &gt;

Here is the list of top 30 MNC dataset which contains detailed information about the companies 
Task list
1) The Basic Dataset for practice to use for plotting and data visualization
2)You need to visualize the dataset correctly and choose carefully important columns for data visualization that leads these companies to come around the top 30. 
3) Although it is a practice dataset, there is no deadline and no time limit for submission.
4) Clean up the untreated data to obtain a perfect data set.


Hope you like this!!!!
Happy Codingüëê","","Top 30 MNC Dataset","","","0"
"6746","1691857","8779528","11/03/2021 08:58:19","## Task Details
As a runescape player i see alot of acounts that i think are bots, most people don't like bots and we want to improve the game by detecting them and notifying the responsible team to ban them. This dataset has the hiscores of known banned bots, we would like to know if there is similarity in hiscores of these bots.

## Expected Submission
We would like to have some Notebooks with different approaches to group together the data of these bots, based on game knowledge we have accounts grouped but we would like you to apply your datascience knowledge to see if you can discover new groups, and we will try to label them.

## Evaluation
We would like to see well defined groups that on manual inspection make sense. e.g. a group of accounts that only does wintertod, a skilling boss.","","Data exploration","","","1"
"6748","1691863","8779528","11/03/2021 09:05:17","## Task Details
As a runescape player I see a lot of accounts that I think are bots, most people don't like bots and we want to improve the game by detecting them and notifying the responsible team to ban them. This dataset contains the top 5 locations of known banned accounts, we would like to know if there is any similarity in this data.

## Expected Submission
We would like to have some Notebooks with different approaches to group together the data of these bots, based on game knowledge we have accounts grouped but we would like you to apply your datascience knowledge to see if you can discover new groups, and we will try to label them.

## Evaluation
We would like to see well defined groups that on manual inspection make sense. e.g. a group of accounts that only does mining, we would expect to see in a mining location.","","Data exploration","","","0"
"6888","1733824","8781959","11/20/2021 22:59:08","EDA.","","Global Dataset for Visualisation","","","0"
"6927","1740501","8844224","11/25/2021 12:33:59","&gt; Find out the Individual Select Investors and their total number of share in start-ups
&gt; Year to Year Basis Investors Investment in particular Industry and their share in country","","Task#1","","","1"
"6982","1757154","8854154","12/01/2021 09:14:04","213123","","????????","","","1"
"6866","1726834","8868931","11/17/2021 17:23:11","I am researching beers and have had an expert panel taste and rate the beers in different styles.

I have sent those beers for chemical analysis using GCMS SPME. I have a qualitive profile of the samples with record or presence or absence of certain chemicals.

Can we use non machine learning techniques to predict a samples sensory performance based on its analytical profile?

I have given the majority of the data I have but have kept some as a special secret test to see if we can predict the sensory outcome awarded to the samples.

Other interesting questions 

Are the judges consistent?
Are some beers easier to model that others?

I am open as to what to submit so long as its workable and exportable.

A good solution will be able to predict the outcome +-1 point the more accurate the ability to predict the better the solution.","","Beer Sensory Data Vs Analytical Data - Deterministic Modelling, Bayesian Methods","Can we determine a high scoring beer based on its analytical profile and the compounds present or absent?","","0"
"6863","1724853","8887807","11/17/2021 12:47:00","Follow the process below to develop a model that can be used by real estate companies and
real estate agents to predict the price of a house.

Business Understanding
-Conduct a literature review to understand the factors that determine the price of houses
globally and locally.
-Based on the dataset provided, formulate a business question to be answered through the
analysis.

Data Understanding
-The data in the dataset provided was collected through webs scrapping. Conduct further
reading to understand the process of web scrapping, how it is conducted (methods and tools)
and any ethical challenges related to it.

Data Preparation
-Conduct a detailed exploratory analysis on the dataset.
-Prepare the dataset for modeling
-Identify the technique relevant for answering the business question stated above.
-Ensure that the dataset meets all the assumptions of the technique identified.
-Conduct preliminary feature selection by identifying the set of features that are likely to
provide a model with good performance.

Modeling
-Split the dataset into two; training set and validation set. With justifications, decide on the
ratio of the training set to the validation set.
-Generate the required model

Evaluation
-Interpret the model in terms of its goodness of fit in predicting the price of houses.
-Assume that the model is not good enough and then conduct further feature engineering or use
any other model tuning strategies at your disposal to generate additional two instances of the
model.
-Settle on the best model instance and then re-interpret.

Implementation
-Think of how the model can be implemented and used by real estate firms and agents.
-Identify possible challenges of applying the model.
-Recommendations on how the model can be improved in future

Further help
If you need additional inspiration, check out these existing high-quality tasks:
- https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508 (Predict the Spreading of Coronavirus)
- https://www.kaggle.com/paultimothymooney/open-elections-data-usa/tasks?taskId=12 (Create CSV files from PDF files)
- https://www.kaggle.com/leonardopena/top50spotify2019/tasks?taskId=198 (Top 50 Spotify Songs 2019)","","Melbourne Housing Market Data Analysis","","","1"
"6893","1735749","8954615","11/21/2021 19:17:19","Do the EDA for store dataset","","EDA for Store data","","","0"
"7035","1781046","9128743","12/09/2021 09:52:11","## Detalhamento da tarefa ( T-SGP-SELDE-0001 )
O gestor da unidade administrativa SELDE, ligada a Coordenadoria de Desenvolvimento e Sa√∫de (CODES), na Secretaria de Gest√£o de Pessoas(SGP), deseja saber quais s√£o as RUBRICAS de or√ßamento sob sua responsabilidade. Al√©m disso, ele precisa saber:
  
  a) Quanto do or√ßamento previsto (ValorLoaUA) para cada rubrica foi informado no PACONT - Plano de Contrata√ß√µes do TRE-PB (ValorPACONTPreliminar) e a diferen√ßa entre esses dois valores iniciais;

  b) ANTES das contrata√ß√µes ou renova√ß√µes de contrato efetivamente realizadas, AP√ìS a pesquisa de pre√ßos(quando houver nova licita√ß√£o) o ""ValorPreliminarmenteReservado""; 
     Obs: Caso n√£o haja nova licita√ß√£o, caso haja uma renova√ß√£o de contrato o valor dessa renova√ß√£o negociado com a contratante;
   
c) O valor efetivamente utilizado do or√ßamento em ValorPACONTDefinitivo;

d) O valor efetivamente utilizado do or√ßamento  (ValorLoaUA - ValorPACONTDefinitivo)

## O que esperamos que seja submetido ?
Os colaboradores devem submeter um DataSet devidamente organizado com as informa√ß√µes do detalhamento da tarefa ( T-SGP-SELDE-0001 )

## Avalia√ß√£o
A avalia√ß√£o consistir√° na verifica√ß√£o dos requisitos especificados no detalhamento da tarefa ( T-SGP-SELDE-0001 )

### Ajuda adicional
N√£o se aplica.","","Descri√ß√£o do Or√ßamento da SGP/SELDE","Ajude a unidade administrativa a ter informa√ß√µes b√°sicas sobre o or√ßamento dela.","12/31/2021 23:59:00","0"
